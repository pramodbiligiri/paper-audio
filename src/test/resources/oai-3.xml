<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2021-06-04T13:01:29Z</responseDate>
<request verb="ListRecords" resumptionToken="5392151|2001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00774</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00774</id><submitter>David Alvarez-Melis</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:13:18 GMT</date><size>6202kb</size><source_type>D</source_type></version><title>Optimizing Functionals on the Space of Probabilities with Input Convex
  Neural Networks</title><authors>David Alvarez-Melis, Yair Schiff, Youssef Mroueh</authors><categories>stat.ML cs.LG cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient flows are a powerful tool for optimizing functionals in general
metric spaces, including the space of probabilities endowed with the
Wasserstein metric. A typical approach to solving this optimization problem
relies on its connection to the dynamic formulation of optimal transport and
the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation
involves optimization over convex functions, which is challenging, especially
in high dimensions. In this work, we propose an approach that relies on the
recently introduced input-convex neural networks (ICNN) to parameterize the
space of convex functions in order to approximate the JKO scheme, as well as in
designing functionals over measures that enjoy convergence guarantees. We
derive a computationally efficient implementation of this JKO-ICNN framework
and use various experiments to demonstrate its feasibility and validity in
approximating solutions of low-dimensional partial differential equations with
known solutions. We also explore the use of our JKO-ICNN approach in high
dimensions with an experiment in controlled generation for molecular discovery.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00776</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00776</id><submitter>Margaret Chapman Dr.</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:15:35 GMT</date><size>2601kb</size><source_type>D</source_type></version><title>Risk-sensitive safety analysis via state-space augmentation</title><authors>Margaret P. Chapman, Michael Fauss, H. Vincent Poor, Kevin M. Smith</authors><categories>eess.SY cs.SY</categories><comments>under review for IEEE Transactions on Automatic Control, submitted
  June 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk-sensitive safety analysis is a safety analysis method for stochastic
systems on Borel spaces that uses a risk functional from finance called
Conditional Value-at-Risk (CVaR). CVaR provides a particularly expressive way
to quantify the safety of a control system, as it represents the average cost
in a fraction of worst cases. In prior work, the notion of a risk-sensitive
safe set was defined in terms of a non-standard optimal control problem, in
which a maximum cost is assessed via CVaR. Here, we provide a method to compute
risk-sensitive safe sets exactly in principle by utilizing a state-space
augmentation technique. In addition, we prove the existence of an optimal
pre-commitment policy under a measurable selection condition. The proposed
framework assumes continuous system dynamics and cost functions, but is
otherwise flexible. In particular, it can accommodate probabilistic control
policies, fairly general disturbance distributions, and control-dependent,
non-monotonic, and non-convex stage costs. We demonstrate how risk-sensitive
safety analysis is useful for a stormwater infrastructure application. Our
numerical examples are inspired by current challenges that cities face in
managing precipitation uncertainty.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00780</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00780</id><submitter>Ran Zmigrod</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:23:41 GMT</date><size>230kb</size><source_type>D</source_type></version><title>On Finding the $K$-best Non-projective Dependency Trees</title><authors>Ran Zmigrod, Tim Vieira, Ryan Cotterell</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The connection between the maximum spanning tree in a directed graph and the
best dependency tree of a sentence has been exploited by the NLP community.
However, for many dependency parsing schemes, an important detail of this
approach is that the spanning tree must have exactly one edge emanating from
the root. While work has been done to efficiently solve this problem for
finding the one-best dependency tree, no research has attempted to extend this
solution to finding the $K$-best dependency trees. This is arguably a more
important extension as a larger proportion of decoded trees will not be subject
to the root constraint of dependency trees. Indeed, we show that the rate of
root constraint violations increases by an average of $13$ times when decoding
with $K\!=\!50$ as opposed to $K\!=\!1$. In this paper, we provide a
simplification of the $K$-best spanning tree algorithm of Camerini et al.
(1980). Our simplification allows us to obtain a constant time speed-up over
the original algorithm. Furthermore, we present a novel extension of the
algorithm for decoding the $K$-best dependency trees of a graph which are
subject to a root constraint.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00782</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00782</id><submitter>Haitham Abdelhafez</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:29:19 GMT</date><size>561kb</size></version><title>Resource allocation for D2D-Based AMI Communications Underlaying LTE
  Cellular Networks</title><authors>H. H. Esmat, Mahmoud M. Elmesalawy, I. I. Ibrahim</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart meters are utilized to transmit the consumption information to the
metering data management system for observing and management in smart grid
advanced metering infrastructure systems. In the meantime, for efficient
utilization for spectrum, Device-to-Device (D2D) communications underlaying LTE
networks are a promising wireless communication technology for advanced
metering infrastructure which supporting a technique for reusing the same radio
resources (RRs) of LTE networks. Therefore, we examine the utilization of D2D
communication technology for advanced metering infrastructure communications
underlaying LTE networks. A novel approach is suggested for provisioning the
mandatory communication between serving data concentrator and its set of smart
meters using this technology. The suggested approach is dependent on two main
stages. The group of permissible cellular user equipment reuse candidates for
every smart meter is calculated with taking the quality of service demands for
cellular user devices and smart meters into consideration in the first stage.
The optimal RR allocation for every smart meter is determined based on
maximizing the access rate of smart meters which can be accepted and operated
in D2D reuse mode in the second stage. Simulation results prove the efficacy of
the suggested approach for efficient advanced metering infrastructure
communication underlaying LTE systems with accepting remarkable number of SMs
and accomplishing outstanding throughput gain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00783</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00783</id><submitter>Dario Fuoli</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:34:52 GMT</date><size>26637kb</size><source_type>D</source_type></version><title>Fourier Space Losses for Efficient Perceptual Image Super-Resolution</title><authors>Dario Fuoli, Luc Van Gool, and Radu Timofte</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many super-resolution (SR) models are optimized for high performance only and
therefore lack efficiency due to large model complexity. As large models are
often not practical in real-world applications, we investigate and propose
novel loss functions, to enable SR with high perceptual quality from much more
efficient models. The representative power for a given low-complexity generator
network can only be fully leveraged by strong guidance towards the optimal set
of parameters. We show that it is possible to improve the performance of a
recently introduced efficient generator architecture solely with the
application of our proposed loss functions. In particular, we use a Fourier
space supervision loss for improved restoration of missing high-frequency (HF)
content from the ground truth image and design a discriminator architecture
working directly in the Fourier domain to better match the target HF
distribution. We show that our losses' direct emphasis on the frequencies in
Fourier-space significantly boosts the perceptual image quality, while at the
same time retaining high restoration quality in comparison to previously
proposed loss functions for this task. The performance is further improved by
utilizing a combination of spatial and frequency domain losses, as both
representations provide complementary information during training. On top of
that, the trained generator achieves comparable results with and is 2.4x and
48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow
respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00786</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00786</id><submitter>Peter Hase</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:36:48 GMT</date><size>364kb</size><source_type>D</source_type></version><title>Search Methods for Sufficient, Socially-Aligned Feature Importance
  Explanations with In-Distribution Counterfactuals</title><authors>Peter Hase, Harry Xie, Mohit Bansal</authors><categories>cs.LG cs.AI cs.CL</categories><comments>26 pages, 4 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature importance (FI) estimates are a popular form of explanation, and they
are commonly created and evaluated by computing the change in model confidence
caused by removing certain input features at test time. For example, in the
standard Sufficiency metric, only the top-k most important tokens are kept. In
this paper, we study several under-explored dimensions of FI-based
explanations, providing conceptual and empirical improvements for this form of
explanation. First, we advance a new argument for why it can be problematic to
remove features from an input when creating or evaluating explanations: the
fact that these counterfactual inputs are out-of-distribution (OOD) to models
implies that the resulting explanations are socially misaligned. The crux of
the problem is that the model prior and random weight initialization influence
the explanations (and explanation metrics) in unintended ways. To resolve this
issue, we propose a simple alteration to the model training process, which
results in more socially aligned explanations and metrics. Second, we compare
among five approaches for removing features from model inputs. We find that
some methods produce more OOD counterfactuals than others, and we make
recommendations for selecting a feature-replacement function. Finally, we
introduce four search-based methods for identifying FI explanations and compare
them to strong baselines, including LIME, Integrated Gradients, and random
search. On experiments with six diverse text classification datasets, we find
that the only method that consistently outperforms random search is a Parallel
Local Search that we introduce. Improvements over the second-best method are as
large as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All
supporting code is publicly available at
https://github.com/peterbhase/ExplanationSearch.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00787</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00787</id><submitter>Piyush Sharma</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:39:25 GMT</date><size>786kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 01:56:22 GMT</date><size>786kb</size></version><title>Image-Audio Encoding to Improve C2 Decision-Making in Multi-Domain
  Environment</title><authors>Piyush K. Sharma and Adrienne Raglin</authors><categories>cs.LG cs.IT math.IT stat.AP</categories><comments>Published in: The 25th International Command and Control Research and
  Technology Symposium (ICCRTS - 2020)</comments><journal-ref>https://internationalc2institute.org/2020-experimentation-analysis-assessment-and-metrics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The military is investigating methods to improve communication and agility in
its multi-domain operations (MDO). Nascent popularity of Internet of Things
(IoT) has gained traction in public and government domains. Its usage in MDO
may revolutionize future battlefields and may enable strategic advantage. While
this technology offers leverage to military capabilities, it comes with
challenges where one is the uncertainty and associated risk. A key question is
how can these uncertainties be addressed. Recently published studies proposed
information camouflage to transform information from one data domain to
another. As this is comparatively a new approach, we investigate challenges of
such transformations and how these associated uncertainties can be detected and
addressed, specifically unknown-unknowns to improve decision-making.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00790</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00790</id><submitter>SueYeon Chung</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:49:14 GMT</date><size>18830kb</size><source_type>D</source_type></version><title>Statistical Mechanics of Neural Processing of Object Manifolds</title><authors>SueYeon Chung</authors><categories>q-bio.NC cond-mat.dis-nn cs.AI</categories><comments>PhD thesis, Harvard University, Cambridge, Massachusetts, USA. 2017.
  Some chapters report joint work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invariant object recognition is one of the most fundamental cognitive tasks
performed by the brain. In the neural state space, different objects with
stimulus variabilities are represented as different manifolds. In this
geometrical perspective, object recognition becomes the problem of linearly
separating different object manifolds. In feedforward visual hierarchy, it has
been suggested that the object manifold representations are reformatted across
the layers, to become more linearly separable. Thus, a complete theory of
perception requires characterizing the ability of linear readout networks to
classify object manifolds from variable neural responses.
  A theory of the perceptron of isolated points was pioneered by E. Gardner who
formulated it as a statistical mechanics problem and analyzed it using replica
theory. In this thesis, we generalize Gardner's analysis and establish a theory
of linear classification of manifolds synthesizing statistical and geometric
properties of high dimensional signals. [..] Next, we generalize our theory
further to linear classification of general perceptual manifolds, such as point
clouds. We identify that the capacity of a manifold is determined that
effective radius, R_M, and effective dimension, D_M. Finally, we show
extensions relevant for applications to real data, incorporating correlated
manifolds, heterogenous manifold geometries, sparse labels and nonlinear
classifications. Then, we demonstrate how object-based manifolds transform in
standard deep networks.
  This thesis lays the groundwork for a computational theory of neuronal
processing of objects, providing quantitative measures for linear separability
of object manifolds. We hope this theory will provide new insights into the
computational principles underlying processing of sensory representations in
biological and artificial neural networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00791</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00791</id><submitter>Xinyu Hua</submitter><version version="v1"><date>Tue, 1 Jun 2021 20:56:10 GMT</date><size>607kb</size><source_type>D</source_type></version><title>DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text
  Generation</title><authors>Xinyu Hua, Ashwin Sreevatsa, and Lu Wang</authors><categories>cs.CL</categories><comments>Accepted at ACL 2021. Project page:
  https://xinyuhua.github.io/Resources/acl21/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the task of long-form opinion text generation, which faces at least
two distinct challenges. First, existing neural generation models fall short of
coherence, thus requiring efficient content planning. Second, diverse types of
information are needed to guide the generator to cover both subjective and
objective content. To this end, we propose DYPLOC, a generation framework that
conducts dynamic planning of content while generating the output based on a
novel design of mixed language models. To enrich the generation with diverse
content, we further propose to use large pre-trained models to predict relevant
concepts and to generate claims. We experiment with two challenging tasks on
newly collected datasets: (1) argument generation with Reddit ChangeMyView, and
(2) writing articles using New York Times' Opinion section. Automatic
evaluation shows that our model significantly outperforms competitive
comparisons. Human judges further confirm that our generations are more
coherent with richer content.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00792</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00792</id><submitter>Ramon Winterhalder</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:01:39 GMT</date><size>1478kb</size><source_type>D</source_type></version><title>Latent Space Refinement for Deep Generative Models</title><authors>Ramon Winterhalder, Marco Bellagente, Benjamin Nachman</authors><categories>stat.ML cs.LG hep-ex hep-ph physics.data-an</categories><comments>14 pages, 5 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep generative models are becoming widely used across science and industry
for a variety of purposes. A common challenge is achieving a precise implicit
or explicit representation of the data probability density. Recent proposals
have suggested using classifier weights to refine the learned density of deep
generative models. We extend this idea to all types of generative models and
show how latent space refinement via iterated generative modeling can
circumvent topological obstructions and improve precision. This methodology
also applies to cases were the target model is non-differentiable and has many
internal latent dimensions which must be marginalized over before refinement.
We demonstrate our Latent Space Refinement (LaSeR) protocol on a variety of
examples, focusing on the combinations of Normalizing Flows and Generative
Adversarial Networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00793</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00793</id><submitter>Zhengbao Jiang</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:01:43 GMT</date><size>503kb</size><source_type>D</source_type></version><title>CoRI: Collective Relation Integration with Data Augmentation for Open
  Information Extraction</title><authors>Zhengbao Jiang, Jialong Han, Bunyamin Sisman, Xin Luna Dong</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating extracted knowledge from the Web to knowledge graphs (KGs) can
facilitate tasks like question answering. We study relation integration that
aims to align free-text relations in subject-relation-object extractions to
relations in a target KG. To address the challenge that free-text relations are
ambiguous, previous methods exploit neighbor entities and relations for
additional context. However, the predictions are made independently, which can
be mutually inconsistent. We propose a two-stage Collective Relation
Integration (CoRI) model, where the first stage independently makes candidate
predictions, and the second stage employs a collective model that accesses all
candidate predictions to make globally coherent predictions. We further improve
the collective model with augmented data from the portion of the target KG that
is otherwise unused. Experiment results on two datasets show that CoRI can
significantly outperform the baselines, improving AUC from .677 to .748 and
from .716 to .780, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00794</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00794</id><submitter>Nikita Nangia</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:05:52 GMT</date><size>5631kb</size><source_type>D</source_type></version><title>What Ingredients Make for an Effective Crowdsourcing Protocol for
  Difficult NLU Data Collection Tasks?</title><authors>Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara
  Vania, Samuel R. Bowman</authors><categories>cs.CL cs.AI cs.HC</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing is widely used to create data for common natural language
understanding tasks. Despite the importance of these datasets for measuring and
refining model understanding of language, there has been little focus on the
crowdsourcing methods used for collecting the datasets. In this paper, we
compare the efficacy of interventions that have been proposed in prior work as
ways of improving data quality. We use multiple-choice question answering as a
testbed and run a randomized trial by assigning crowdworkers to write questions
under one of four different data collection protocols. We find that asking
workers to write explanations for their examples is an ineffective stand-alone
strategy for boosting NLU example difficulty. However, we find that training
crowdworkers, and then using an iterative process of collecting data, sending
feedback, and qualifying workers based on expert judgments is an effective
means of collecting challenging data. But using crowdsourced, instead of expert
judgments, to qualify workers and send feedback does not prove to be effective.
We observe that the data from the iterative protocol with expert assessments is
more challenging by several measures. Notably, the human--model gap on the
unanimous agreement portion of this data is, on average, twice as large as the
gap for the baseline protocol data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00795</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00795</id><submitter>Wing Chau Ng</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:08:13 GMT</date><size>680kb</size></version><title>On Classification of MIMO Equalizers</title><authors>Wing Chau Ng and Chuandong Li</authors><categories>cs.IT eess.SP math.IT</categories><comments>This work was submitted to ECOC 2021. This theoretical paper also
  explains the principle of the experimental demonstration in Joint Transmitter
  and Receiver IQ Differential Phase Calibration using a single 4x8 MIMO
  Equalizer, Proc. Advanced Photonics Congress 2021 (SPPCom), SpTh1D.4</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this theoretical work, the DSP-perceived channel in optical coherent
communications is first simplified, based on which we categorize linear MIMO
equalizers into four classes according to their reference locations. The entire
channel inverse can be represented by a complex conjugate-dependent system,
coinciding with the widely linear equalization theory. Suboptimally removing FO
dynamics, relatively static channel inverses parameterized with common device
and channel parameters are presented for monitoring or calibration purposes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00796</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00796</id><submitter>Jeffrey Ovall</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:08:39 GMT</date><size>76kb</size><source_type>D</source_type></version><title>Quadrature for Implicitly-defined Finite Element Functions on
  Curvilinear Polygons</title><authors>Jeffrey Ovall and Samuel Reynolds</authors><categories>math.NA cs.NA</categories><msc-class>65D30, 65D32, 65N30, 65R20, 31A30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $H^1$-conforming Galerkin methods on polygonal meshes such as VEM, BEM-FEM
and Trefftz-FEM employ local finite element functions that are implicitly
defined as solutions of Poisson problems having polynomial source and boundary
data. Recently, such methods have been extended to allow for mesh cells that
are curvilinear polygons. Such extensions present new challenges for
determining suitable quadratures. We describe an approach for integrating
products of these implicitly defined functions, as well as products of their
gradients, that reduces integrals on cells to integrals along their boundaries.
Numerical experiments illustrate the practical performance of the proposed
methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00797</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00797</id><submitter>Maxime Vono</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:08:54 GMT</date><size>2727kb</size><source_type>D</source_type></version><title>QLSD: Quantised Langevin stochastic dynamics for Bayesian federated
  learning</title><authors>Maxime Vono, Vincent Plassier, Alain Durmus, Aymeric Dieuleveut, Eric
  Moulines</authors><categories>cs.LG cs.AI stat.CO stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated learning aims at conducting inference when data are decentralised
and locally stored on several clients, under two main constraints: data
ownership and communication overhead. In this paper, we address these issues
under the Bayesian paradigm. To this end, we propose a novel Markov chain Monte
Carlo algorithm coined \texttt{QLSD} built upon quantised versions of
stochastic gradient Langevin dynamics. To improve performance in a big data
regime, we introduce variance-reduced alternatives of our methodology referred
to as \texttt{QLSD}$^\star$ and \texttt{QLSD}$^{++}$. We provide both
non-asymptotic and asymptotic convergence guarantees for the proposed
algorithms and illustrate their benefits on several federated learning
benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00799</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00799</id><submitter>Laura Elena Cue La Rosa</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:10:10 GMT</date><size>19327kb</size><source_type>D</source_type></version><title>Multi-task fully convolutional network for tree species mapping in dense
  forests using small training hyperspectral data</title><authors>Laura Elena Cu\'e La Rosa, Camile Sothe, Raul Queiroz Feitosa,
  Cl\'audia Maria de Almeida, Marcos Benedito Schimalski, Dario Augusto Borges
  Oliveira</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a multi-task fully convolutional architecture for tree
species mapping in dense forests from sparse and scarce polygon-level
annotations using hyperspectral UAV-borne data. Our model implements a partial
loss function that enables dense tree semantic labeling outcomes from non-dense
training samples, and a distance regression complementary task that enforces
tree crown boundary constraints and substantially improves the model
performance. Our multi-task architecture uses a shared backbone network that
learns common representations for both tasks and two task-specific decoders,
one for the semantic segmentation output and one for the distance map
regression. We report that introducing the complementary task boosts the
semantic segmentation performance compared to the single-task counterpart in up
to 10% reaching an overall F1 score of 87.5% and an overall accuracy of 85.9%,
achieving state-of-art performance for tree species classification in tropical
forests.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00801</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00801</id><submitter>Ver\'onica  Becher</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:12:27 GMT</date><size>24kb</size></version><title>Insertion in constructed normal numbers</title><authors>Ver\'onica Becher</authors><categories>math.NT cs.DM math.CO</categories><msc-class>11K16, 05C45, 68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defined by Borel, a real number is normal to an integer base $b$, greater
than or equal to $2$, if in its base-$b$ expansion every block of digits occurs
with the same limiting frequency as every other block of the same length. We
consider the problem of insertion in constructed base-$b$ normal expansions to
obtain normality to base $(b+1)$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00805</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00805</id><submitter>Yulin Zhang</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:14:47 GMT</date><size>104kb</size><source_type>D</source_type></version><title>Lattices of sensors reconsidered when less information is preferred</title><authors>Yulin Zhang and Dylan A. Shell</authors><categories>cs.RO</categories><comments>3 pages, 2 figures, accepted by 2021 ICRA Workshop on Compositional
  Robotics: Mathematics and Tools</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To treat sensing limitations (with uncertainty in both conflation of
information and noise) we model sensors as covers. This leads to a semilattice
organization of abstract sensors that is appropriate even when additional
information is problematic (e.g., for tasks involving privacy considerations).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00806</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00806</id><submitter>Octavio Alberto Agust\'in-Aquino</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:17:53 GMT</date><size>634kb</size><source_type>D</source_type></version><title>Exploring Exotic Counterpoint Compositions</title><authors>Octavio A. Agust\'in-Aquino and Jeffery Liu and Guerino Mazzola</authors><categories>cs.SD eess.AS</categories><comments>18 pages, 7 figures</comments><msc-class>00A65, 13P99</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper, first musical compositions are presented, which are created
using the mathematical counterpoint theory of Guerino Mazzola and his
collaborators. These compositions also use the RUBATO(R) software's components
for counterpoint constructions. The present work aims at opening new &quot;exotic&quot;
directions of contrapuntal composition in non-Fuxian worlds. The authors would
like to receive first impressions about these compositions, which are available
as scores and audio files.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00808</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00808</id><submitter>Sorawit Saengkyongam</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:20:48 GMT</date><size>109kb</size><source_type>D</source_type></version><title>Invariant Policy Learning: A Causal Perspective</title><authors>Sorawit Saengkyongam, Nikolaj Thams, Jonas Peters and Niklas Pfister</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decade, contextual bandit and reinforcement learning algorithms
have been successfully used in various interactive learning systems such as
online advertising, recommender systems, and dynamic pricing. However, they
have yet to be widely adopted in high-stakes application domains, such as
healthcare. One reason may be that existing approaches assume that the
underlying mechanisms are static in the sense that they do not change over time
or over different environments. In many real world systems, however, the
mechanisms are subject to shifts across environments which may invalidate the
static environment assumption. In this paper, we tackle the problem of
environmental shifts under the framework of offline contextual bandits. We view
the environmental shift problem through the lens of causality and propose
multi-environment contextual bandits that allow for changes in the underlying
mechanisms. We adopt the concept of invariance from the causality literature
and introduce the notion of policy invariance. We argue that policy invariance
is only relevant if unobserved confounders are present and show that, in that
case, an optimal invariant policy is guaranteed, under certain assumptions, to
generalize across environments. Our results do not only provide a solution to
the environmental shift problem but also establish concrete connections among
causality, invariance and contextual bandits.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00810</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00810</id><submitter>Alessio Russo</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:22:41 GMT</date><size>266kb</size></version><title>Some Ethical Issues in the Review Process of Machine Learning
  Conferences</title><authors>Alessio Russo</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent successes in the Machine Learning community have led to a steep
increase in the number of papers submitted to conferences. This increase made
more prominent some of the issues that affect the current review process used
by these conferences. The review process has several issues that may undermine
the nature of scientific research, which is of being fully objective,
apolitical, unbiased and free of misconduct (such as plagiarism, cheating,
improper influence, and other improprieties). In this work, we study the
problem of reviewers' recruitment, infringements of the double-blind process,
fraudulent behaviors, biases in numerical ratings, and the appendix phenomenon
(i.e., the fact that it is becoming more common to publish results in the
appendix section of a paper). For each of these problems, we provide a short
description and possible solutions. The goal of this work is to raise awareness
in the Machine Learning community regarding these issues.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00815</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00815</id><submitter>Sunnie S. Y. Kim</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:36:26 GMT</date><size>852kb</size><source_type>D</source_type></version><title>Cleaning and Structuring the Label Space of the iMet Collection 2020</title><authors>Vivien Nguyen and Sunnie S. Y. Kim</authors><categories>cs.CV</categories><comments>A shorter version of this work was accepted to the CVPR 2021 FGVC
  Workshop</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The iMet 2020 dataset is a valuable resource in the space of fine-grained art
attribution recognition, but we believe it has yet to reach its true potential.
We document the unique properties of the dataset and observe that many of the
attribute labels are noisy, more than is implied by the dataset description.
Oftentimes, there are also semantic relationships between the labels (e.g.,
identical, mutual exclusion, subsumption, overlap with uncertainty) which we
believe are underutilized. We propose an approach to cleaning and structuring
the iMet 2020 labels, and discuss the implications and value of doing so.
Further, we demonstrate the benefits of our proposed approach through several
experiments. Our code and cleaned labels are available at
https://github.com/sunniesuhyoung/iMet2020cleaned.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00817</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00817</id><submitter>Michael Baumgartner</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:55:03 GMT</date><size>2984kb</size><source_type>D</source_type></version><title>nnDetection: A Self-configuring Method for Medical Object Detection</title><authors>Michael Baumgartner, Paul F. Jaeger, Fabian Isensee, Klaus H.
  Maier-Hein</authors><categories>cs.CV</categories><comments>*Michael Baumgartner and Paul F. J\&quot;ager contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous localisation and categorization of objects in medical images,
also referred to as medical object detection, is of high clinical relevance
because diagnostic decisions often depend on rating of objects rather than e.g.
pixels. For this task, the cumbersome and iterative process of method
configuration constitutes a major research bottleneck. Recently, nnU-Net has
tackled this challenge for the task of image segmentation with great success.
Following nnU-Net's agenda, in this work we systematize and automate the
configuration process for medical object detection. The resulting
self-configuring method, nnDetection, adapts itself without any manual
intervention to arbitrary medical detection problems while achieving results en
par with or superior to the state-of-the-art. We demonstrate the effectiveness
of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10
further medical object detection tasks on public data sets for comprehensive
method evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection .
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00822</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00822</id><submitter>David G\'omez-Guti\'errez</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:05:42 GMT</date><size>743kb</size><source_type>D</source_type></version><title>An arbitrary-order predefined-time exact differentiator for signals with
  exponential growth bound</title><authors>David G\'omez-Guti\'errez, Rodrigo Aldana-L\'opez, Richard Seeber,
  Marco Tulio Angulo, Leonid Fridman</authors><categories>math.OC cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructing differentiation algorithms with a fixed-time convergence and a
predefined Upper Bound on their Settling Time (\textit{UBST}), i.e.,
predefined-time differentiators, is attracting attention for solving estimation
and control problems under time constraints. However, existing methods are
limited to signals having an $n$-th Lipschitz derivative. Here, we introduce a
general methodology to design $n$-th order predefined-time differentiators for
a broader class of signals: for signals, whose $(n+1)$-th derivative is bounded
by a function with bounded logarithmic derivative, i.e., whose $(n+1)$-th
derivative grows at most exponentially. Our approach is based on a class of
time-varying gains known as Time-Base Generators (\textit{TBG}). The only
assumption to construct the differentiator is that the class of signals to be
differentiated $n$-times have a $(n+1)$-th derivative bounded by a known
function with a known bound for its $(n+1)$-th logarithmic derivative. We show
how our methodology achieves an \textit{UBST} equal to the predefined time,
better transient responses with smaller error peaks than autonomous
predefined-time differentiators, and a \textit{TBG} gain that is bounded at the
settling time instant.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00827</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00827</id><submitter>Eric Bunch</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:14:22 GMT</date><size>3794kb</size><source_type>D</source_type></version><title>Weighting vectors for machine learning: numerical harmonic analysis
  applied to boundary detection</title><authors>Eric Bunch, Jeffery Kline, Daniel Dickinson, Suhaas Bhat, Glenn Fung</authors><categories>cs.LG math.AT stat.ML</categories><comments>16 pages. arXiv admin note: text overlap with arXiv:2006.14063</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric space magnitude, an active field of research in algebraic topology, is
a scalar quantity that summarizes the effective number of distinct points that
live in a general metric space. The {\em weighting vector} is a closely-related
concept that captures, in a nontrivial way, much of the underlying geometry of
the original metric space. Recent work has demonstrated that when the metric
space is Euclidean, the weighting vector serves as an effective tool for
boundary detection. We recast this result and show the weighting vector may be
viewed as a solution to a kernelized SVM. As one consequence, we apply this new
insight to the task of outlier detection, and we demonstrate performance that
is competitive or exceeds performance of state-of-the-art techniques on
benchmark data sets. Under mild assumptions, we show the weighting vector,
which has computational cost of matrix inversion, can be efficiently
approximated in linear time. We show how nearest neighbor methods can
approximate solutions to the minimization problems defined by SVMs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00828</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00828</id><submitter>Emre Can Kaya</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:16:06 GMT</date><size>461kb</size><source_type>D</source_type></version><title>Refining the bounding volumes for lossless compression of voxelized
  point clouds geometry</title><authors>Emre Can Kaya, Sebastian Schwarz, Ioan Tabus</authors><categories>cs.CV cs.MM</categories><comments>ICIP \c{opyright} 2021 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses, in any
  current or future media, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work in other works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel lossless compression method for point cloud
geometry, building on a recent lossy compression method that aimed at
reconstructing only the bounding volume of a point cloud. The proposed scheme
starts by partially reconstructing the geometry from the two depthmaps
associated to a single projection direction. The partial reconstruction
obtained from the depthmaps is completed to a full reconstruction of the point
cloud by sweeping section by section along one direction and encoding the
points which were not contained in the two depthmaps. The main ingredient is a
list-based encoding of the inner points (situated inside the feasible regions)
by a novel arithmetic three dimensional context coding procedure that
efficiently utilizes rotational invariances present in the input data.
State-of-the-art bits-per-voxel results are obtained on benchmark datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00829</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00829</id><submitter>Alexander Fabbri</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:17:13 GMT</date><size>5355kb</size><source_type>D</source_type></version><title>ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive
  Summarization with Argument Mining</title><authors>Alexander R. Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li,
  Yashar Mehdad, Dragomir Radev</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While online conversations can cover a vast amount of information in many
different formats, abstractive text summarization has primarily focused on
modeling solely news articles. This research gap is due, in part, to the lack
of standardized datasets for summarizing online discussions. To address this
gap, we design annotation protocols motivated by an
issues--viewpoints--assertions framework to crowdsource four new datasets on
diverse online conversation forms of news comments, discussion forums,
community question answering forums, and email threads. We benchmark
state-of-the-art models on our datasets and analyze characteristics associated
with the data. To create a comprehensive benchmark, we also evaluate these
models on widely-used conversation summarization datasets to establish strong
baselines in this domain. Furthermore, we incorporate argument mining through
graph construction to directly model the issues, viewpoints, and assertions
present in a conversation and filter noisy input, showing comparable or
improved results according to automatic and human evaluations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00831</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00831</id><submitter>Thirupathaiah Vasantam</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:19:33 GMT</date><size>1912kb</size></version><title>Stability Analysis of a Quantum Network with Max-Weight Scheduling</title><authors>Thirupathaiah Vasantam, Don Towsley</authors><categories>quant-ph cs.PF</categories><comments>21 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We study a quantum network that distributes entangled quantum states to
multiple sets of users that are connected to the network. Each user is
connected to a switch of the network via a link. All the links of the network
generate bipartite Bell-state entangled states in each time-slot with certain
probabilities, and each end node stores one qubit of the entanglement generated
by the link. To create shared entanglements for a set of users, measurement
operations are performed on qubits of link-level entanglements on a set of
related links, and these operations are probabilistic in nature and are
successful with certain probabilities. Requests arrive to the system seeking
shared entanglements for different sets of users. Each request is for the
creation of shared entanglements for a fixed set of users using link-level
entanglements on a fixed set of links. Requests are processed according to
First-Come-First-Served service discipline and unserved requests are stored in
buffers. Once a request is selected for service, measurement operations are
performed on qubits of link-level entanglements on related links to create a
shared entanglement. For given set of request arrival rates and link-level
entanglement generation rates, we obtain necessary conditions for the stability
of queues of requests. In each time-slot, the scheduler has to schedule
entanglement swapping operations for different sets of users to stabilize the
network. Next, we propose a Max-Weight scheduling policy and show that this
policy stabilizes the network for all feasible arrival rates. We also provide
numerical results to support our analysis. The analysis of a single quantum
switch that creates multipartite entanglements for different sets of users is a
special case of our work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00834</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00834</id><submitter>Nelly Elsayed</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:27:41 GMT</date><size>1021kb</size><source_type>D</source_type></version><title>Green IoT System Architecture for Applied Autonomous Network
  Cybersecurity Monitoring</title><authors>Zaghloul Saad Zaghloul, Nelly Elsayed, Chengcheng Li, Magdy Bayoumi</authors><categories>cs.NI cs.CR</categories><comments>Cybersecurity, IoT, NSM, packet capture, sensor, green systems, oil
  and gas, Network Security Monitoring. Accepted in IEEE WF-IoT 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network security morning (NSM) is essential for any cybersecurity system,
where the average cost of a cyberattack is $1.1 million. No matter how much a
system is secure, it will eventually fail without proper and continuous
monitoring. No wonder that the cybersecurity market is expected to grow up to
$170.4 billion in 2022. However, the majority of legacy industries do not
invest in NSM implementation until it is too late due to the initial and
operation cost and static unutilized resources. Thus, this paper proposes a
novel dynamic Internet of things (IoT) architecture for an industrial NSM that
features a low installation and operation cost, low power consumption,
intelligent organization behavior, and environmentally friendly operation. As a
case study, the system is implemented in a midrange oil a gas manufacture
facility in the southern states with more than 300 machines and servers over
three remote locations and a production plant that features a challenging
atmosphere condition. The proposed system successfully shows a significant
saving (&gt;65%) in power consumption, acquires one-tenth the installation cost,
develops an intelligent operation expert system tools as well as saves the
environment from more than 500 mg of CO2 pollution per hour, promoting green
IoT systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00837</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00837</id><submitter>Mohammad Mahdi Dehshibi Dr.</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:31:17 GMT</date><size>3792kb</size><source_type>D</source_type></version><title>On stimulating fungi $Pleurotus~ostreatus$ with Cortisol</title><authors>Mohammad Mahdi Dehshibi, Alessandro Chiolerio, Anna Nikolaidou,
  Richard Mayne, Antoni Gandia, Mona Ashtari, Andrew Adamatzky</authors><categories>cs.ET</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fungi cells are capable of sensing extracellular cues through reception,
transduction and response systems which allow them to communicate with their
host and adapt to their environment. They display effective regulatory protein
expressions which enhance and regulate their response and adaptation to a
variety of triggers such as stress, hormones, light, chemicals and host
factors. In our recent studies, we have shown that $Pleurotus$ oyster fungi
generate electrical potential impulses in the form of spike events as a result
of their exposure to environmental, mechanical and chemical triggers,
demonstrating that it is possible to discern the nature of stimuli from the
fungi electrical responses. Harnessing the power of fungi sensing and
intelligent capabilities, we explored the communication protocols of fungi as
reporters of human chemical secretions such as hormones, addressing the
question if fungi can sense human signals. We exposed $Pleurotus$ oyster fungi
to cortisol, directly applied to a surface of a hemp shavings substrate
colonised by fungi, and recorded the electrical activity of fungi. The response
of fungi to cortisol was also supplementary studied through the application of
X-ray to identify changes in the fungi tissue, where receiving cortisol by the
substrate can inhibit the flow of calcium and, in turn, reduce its
physiological changes. This study could pave the way for future research on
adaptive fungal wearables capable for detecting physiological states of humans
and biosensors made of living fungi.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00839</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00839</id><submitter>Agni Orfanoudaki</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:32:02 GMT</date><size>6040kb</size><source_type>D</source_type></version><title>Pricing Algorithmic Insurance</title><authors>Dimitris Bertsimas, Agni Orfanoudaki</authors><categories>cs.LG q-fin.RM stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As machine learning algorithms start to get integrated into the
decision-making process of companies and organizations, insurance products will
be developed to protect their owners from risk. We introduce the concept of
algorithmic insurance and present a quantitative framework to enable the
pricing of the derived insurance contracts. We propose an optimization
formulation to estimate the risk exposure and price for a binary classification
model. Our approach outlines how properties of the model, such as accuracy,
interpretability and generalizability, can influence the insurance contract
evaluation. To showcase a practical implementation of the proposed framework,
we present a case study of medical malpractice in the context of breast cancer
detection. Our analysis focuses on measuring the effect of the model parameters
on the expected financial loss and identifying the aspects of algorithmic
performance that predominantly affect the price of the contract.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00840</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00840</id><submitter>Clara Vania</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:33:53 GMT</date><size>14128kb</size><source_type>D</source_type></version><title>Comparing Test Sets with Item Response Theory</title><authors>Clara Vania, Phu Mon Htut, William Huang, Dhara Mungra, Richard
  Yuanzhe Pang, Jason Phang, Haokun Liu, Kyunghyun Cho, Samuel R. Bowman</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent years have seen numerous NLP datasets introduced to evaluate the
performance of fine-tuned models on natural language understanding tasks.
Recent results from large pretrained models, though, show that many of these
datasets are largely saturated and unlikely to be able to detect further
progress. What kind of datasets are still effective at discriminating among
strong models, and what kind of datasets should we expect to be able to detect
future improvements? To measure this uniformly across datasets, we draw on Item
Response Theory and evaluate 29 datasets using predictions from 18 pretrained
Transformer models on individual test examples. We find that Quoref, HellaSwag,
and MC-TACO are best suited for distinguishing among state-of-the-art models,
while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong
models. We also observe span selection task format, which is used for QA
datasets like QAMR or SQuAD2.0, is effective in differentiating between strong
and weak models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00841</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00841</id><submitter>Mashbat  Suzuki</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:39:42 GMT</date><size>968kb</size></version><title>Two Birds With One Stone: Fairness and Welfare via Transfers</title><authors>Vishnu V. Narayan, Mashbat Suzuki, Adrian Vetta</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the question of dividing a collection of indivisible goods amongst a
set of agents. The main objective of research in the area is to achieve one of
two goals: fairness or efficiency. On the fairness side, envy-freeness is the
central fairness criterion in economics, but envy-free allocations typically do
not exist when the goods are indivisible. A recent line of research shows that
envy-freeness can be achieved if a small quantity of a homogeneous divisible
good (money) is introduced into the system, or equivalently, if transfer
payments are allowed between the agents. A natural question to explore, then,
is whether transfer payments can be used to provide high welfare in addition to
envy-freeness, and if so, how much money is needed to be transferred.
  We show that for general monotone valuations, there always exists an
allocation with transfers that is envy-free and whose Nash social welfare (NSW)
is at least an $e^{-1/e}$-fraction of the optimal Nash social welfare.
Additionally, when the agents have additive valuations, an envy-free allocation
with negligible transfers and whose NSW is within a constant factor of optimal
can be found in polynomial time. Consequently, we demonstrate that the
seemingly incompatible objectives of fairness and high welfare can be achieved
simultaneously via transfer payments, even for general valuations, when the
welfare objective is NSW. On the other hand, we show that a similar result is
impossible for utilitarian social welfare: any envy-freeable allocation that
achieves a constant fraction of the optimal welfare requires non-negligible
transfers. To complement this result we present algorithms that compute an
envy-free allocation with a given target welfare and with bounded transfers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00842</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00842</id><submitter>M. Ali Vosoughi</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:42:51 GMT</date><size>891kb</size><source_type>D</source_type></version><title>Leveraging Pre-Images to Discover Nonlinear Relationships in
  Multivariate Environments</title><authors>M. Ali Vosoughi and Axel Wismuller</authors><categories>cs.LG cs.AI</categories><comments>5 pages, 4 Figures, Conference paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal discovery, beyond the inference of a network as a collection of
connected dots, offers a crucial functionality in scientific discovery using
artificial intelligence. The questions that arise in multiple domains, such as
physics, physiology, the strategic decision in uncertain environments with
multiple agents, climatology, among many others, have roots in causality and
reasoning. It became apparent that many real-world temporal observations are
nonlinearly related to each other. While the number of observations can be as
high as millions of points, the number of temporal samples can be minimal due
to ethical or practical reasons, leading to the curse-of-dimensionality in
large-scale systems. This paper proposes a novel method using kernel principal
component analysis and pre-images to obtain nonlinear dependencies of
multivariate time-series data. We show that our method outperforms
state-of-the-art causal discovery methods when the observations are restricted
by time and are nonlinearly related. Extensive simulations on both real-world
and synthetic datasets with various topologies are provided to evaluate our
proposed methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00845</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00845</id><submitter>Babatunji Omoniwa</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:49:42 GMT</date><size>276kb</size><source_type>D</source_type></version><title>Energy-aware placement optimization of UAV base stations via
  decentralized multi-agent Q-learning</title><authors>Babatunji Omoniwa, Boris Galkin, Ivana Dusparic</authors><categories>cs.MA cs.LG cs.NI</categories><comments>Submitted to IEEE Globecom SAC 2021, Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unmanned aerial vehicles serving as aerial base stations (UAV-BSs) can be
deployed to provide wireless connectivity to ground devices in events of
increased network demand, points-of-failure in existing infrastructure, or
disasters. However, it is challenging to conserve the energy of UAVs during
prolonged coverage tasks, considering their limited on-board battery capacity.
Reinforcement learning-based (RL) approaches have been previously used to
improve energy utilization of multiple UAVs, however, a central cloud
controller is assumed to have complete knowledge of the end-devices' locations,
i.e., the controller periodically scans and sends updates for UAV
decision-making. This assumption is impractical in dynamic network environments
with mobile ground devices. To address this problem, we propose a decentralized
Q-learning approach, where each UAV-BS is equipped with an autonomous agent
that maximizes the connectivity to ground devices while improving its energy
utilization. Experimental results show that the proposed design significantly
outperforms the centralized approaches in jointly maximizing the number of
connected ground devices and the energy utilization of the UAV-BSs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00846</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00846</id><submitter>Babatunji Omoniwa</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:57:51 GMT</date><size>708kb</size><source_type>D</source_type></version><title>An Optimal Relay Scheme for Outage Minimization in Fog-based
  Internet-of-Things (IoT) Networks</title><authors>Babatunji Omoniwa, Riaz Hussain, Muhammad Adil, Atif Shakeel, Ahmed
  Kamal Tahir, Qadeer Ul Hasan, and Shahzad A. Malik</authors><categories>cs.NI</categories><comments>Accepted and Published in IEEE Internet of Things Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fog devices are beginning to play a key role in relaying data and services
within the Internet-of-Things (IoT) ecosystem. These relays may be static or
mobile, with the latter offering a new degree of freedom for performance
improvement via careful relay mobility design. Besides that, power conservation
has been a prevalent issue in IoT networks with devices being
power-constrained, requiring optimal power-control mechanisms. In this paper,
we consider a multi-tier fog-based IoT architecture where a mobile/static fog
node acts as an amplify and forward relay that transmits received information
from a sensor node to a higher hierarchically-placed static fog device, which
offers some localized services. The outage probability of the presented
scenario was efficiently minimized by jointly optimizing the mobility pattern
and the transmit power of the fog relay. A closed-form analytical expression
for the outage probability was derived. Furthermore, due to the intractability
and non-convexity of the formulated problem, we applied an iterative algorithm
based on the steepest descent method to arrive at a desirable objective.
Simulations reveal that the outage probability was improved by 62.7% in the
optimized-location fixed-power (OLFP) scheme, 79.3% in the optimized-power
fixed-location (OPFL) scheme, and 94.2% in the optimized-location
optimized-power (OLOP) scheme, as against the fixed-location and fixed-power
(FLFP) scheme (i.e., without optimization). Lastly, we present an optimal relay
selection strategy that chooses an appropriate relay node from randomly
distributed relaying candidates.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00847</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00847</id><submitter>Ron J Weiss</submitter><version version="v1"><date>Tue, 1 Jun 2021 22:58:47 GMT</date><size>69kb</size><source_type>D</source_type></version><title>Sparse, Efficient, and Semantic Mixture Invariant Training: Taming
  In-the-Wild Unsupervised Sound Separation</title><authors>Scott Wisdom, Aren Jansen, Ron J. Weiss, Hakan Erdogan, John R.
  Hershey</authors><categories>eess.AS cs.SD</categories><comments>5 pages, 1 figure. submitted to WASPAA 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised neural network training has led to significant progress on
single-channel sound separation. This approach relies on ground truth isolated
sources, which precludes scaling to widely available mixture data and limits
progress on open-domain tasks. The recent mixture invariant training (MixIT)
method enables training on in-the wild data; however, it suffers from two
outstanding problems. First, it produces models which tend to over-separate,
producing more output sources than are present in the input. Second, the
exponential computational complexity of the MixIT loss limits the number of
feasible output sources. These problems interact: increasing the number of
output sources exacerbates over-separation. In this paper we address both
issues. To combat over-separation we introduce new losses: sparsity losses that
favor fewer output sources and a covariance loss that discourages correlated
outputs. We also experiment with a semantic classification loss by predicting
weak class labels for each mixture. To extend MixIT to larger numbers of
sources, we introduce an efficient approximation using a fast least-squares
solution, projected onto the MixIT constraint set. Our experiments show that
the proposed losses curtail over-separation and improve overall performance.
The best performance is achieved using larger numbers of output sources,
enabled by our efficient MixIT loss, combined with sparsity losses to prevent
over-separation. On the FUSS test set, we achieve over 13 dB in multi-source
SI-SNR improvement, while boosting single-source reconstruction SI-SNR by over
17 dB.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00851</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00851</id><submitter>Louis Castricato</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:24:51 GMT</date><size>157kb</size><source_type>D</source_type></version><title>Parameter-Efficient Neural Question Answering Models via Graph-Enriched
  Document Representations</title><authors>Louis Castricato, Stephen Fitz, Won Young Shin</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the computational footprint of modern NLP systems grows, it becomes
increasingly important to arrive at more efficient models. We show that by
employing graph convolutional document representation, we can arrive at a
question answering system that performs comparably to, and in some cases
exceeds the SOTA solutions, while using less than 5\% of their resources in
terms of trainable parameters. As it currently stands, a major issue in
applying GCNs to NLP is document representation. In this paper, we show that a
GCN enriched document representation greatly improves the results seen in
HotPotQA, even when using a trivial topology. Our model (gQA), performs
admirably when compared to the current SOTA, and requires little to no
preprocessing. In Shao et al. 2020, the authors suggest that graph networks are
not necessary for good performance in multi-hop QA. In this paper, we suggest
that large language models are not necessary for good performance by showing a
na\&quot;{i}ve implementation of a GCN performs comparably to SoTA models based on
pretrained language models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00853</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00853</id><submitter>Ashkan Kazemi</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:28:05 GMT</date><size>6186kb</size><source_type>D</source_type></version><title>Claim Matching Beyond English to Scale Global Fact-Checking</title><authors>Ashkan Kazemi, Kiran Garimella, Devin Gaffney and Scott A. Hale</authors><categories>cs.CL</categories><comments>to appear in ACL 2021 as a long paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manual fact-checking does not scale well to serve the needs of the internet.
This issue is further compounded in non-English contexts. In this paper, we
discuss claim matching as a possible solution to scale fact-checking. We define
claim matching as the task of identifying pairs of textual messages containing
claims that can be served with one fact-check. We construct a novel dataset of
WhatsApp tipline and public group messages alongside fact-checked claims that
are first annotated for containing &quot;claim-like statements&quot; and then matched
with potentially similar items and annotated for claim matching. Our dataset
contains content in high-resource (English, Hindi) and lower-resource (Bengali,
Malayalam, Tamil) languages. We train our own embedding model using knowledge
distillation and a high-quality &quot;teacher&quot; model in order to address the
imbalance in embedding quality between the low- and high-resource languages in
our dataset. We provide evaluations on the performance of our solution and
compare with baselines and existing state-of-the-art multilingual embedding
models, namely LASER and LaBSE. We demonstrate that our performance exceeds
LASER and LaBSE in all settings. We release our annotated datasets, codebooks,
and trained embedding model to allow for further research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00854</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00854</id><submitter>Yongsheng Cao</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:31:05 GMT</date><size>2810kb</size></version><title>Smart Online Charging Algorithm for Electric Vehicles via Customized
  Actor-Critic Learning</title><authors>Yongsheng Cao, Hao Wang, Demin Li, Guanglin Zhang</authors><categories>eess.SY cs.SY</categories><comments>11 pages, 11 figures</comments><journal-ref>Published by IEEE Internet of Things Journal, 2021</journal-ref><doi>10.1109/JIOT.2021.3084923</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advances in the Internet of Things technology, electric vehicles
(EVs) have become easier to schedule in daily life, which is reshaping the
electric load curve. It is important to design efficient charging algorithms to
mitigate the negative impact of EV charging on the power grid. This paper
investigates an EV charging scheduling problem to reduce the charging cost
while shaving the peak charging load, under unknown future information about
EVs, such as arrival time, departure time, and charging demand. First, we
formulate an EV charging problem to minimize the electricity bill of the EV
fleet and study the EV charging problem in an online setting without knowing
future information. We develop an actor-critic learning-based smart charging
algorithm (SCA) to schedule the EV charging against the uncertainties in EV
charging behaviors. The SCA learns an optimal EV charging strategy with
continuous charging actions instead of discrete approximation of charging. We
further develop a more computationally efficient customized actor-critic
learning charging algorithm (CALC) by reducing the state dimension and thus
improving the computational efficiency. Finally, simulation results show that
our proposed SCA can reduce EVs' expected cost by 24.03%, 21.49%, 13.80%,
compared with the Eagerly Charging Algorithm, Online Charging Algorithm,
RL-based Adaptive Energy Management Algorithm, respectively. CALC is more
computationally efficient, and its performance is close to that of SCA with
only a gap of 5.56% in the cost.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00856</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00856</id><submitter>Nathan Howard</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:39:08 GMT</date><size>304kb</size><source_type>D</source_type></version><title>A Neural Acoustic Echo Canceller Optimized Using An Automatic Speech
  Recognizer And Large Scale Synthetic Data</title><authors>Nathan Howard, Alex Park, Turaj Zakizadeh Shabestary, Alexander
  Gruenstein, Rohit Prabhavalkar</authors><categories>eess.AS cs.SD</categories><comments>To appear in ICASSP 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider the problem of recognizing speech utterances spoken to a device
which is generating a known sound waveform; for example, recognizing queries
issued to a digital assistant which is generating responses to previous user
inputs. Previous work has proposed building acoustic echo cancellation (AEC)
models for this task that optimize speech enhancement metrics using both neural
network as well as signal processing approaches.
  Since our goal is to recognize the input speech, we consider enhancements
which improve word error rates (WERs) when the predicted speech signal is
passed to an automatic speech recognition (ASR) model. First, we augment the
loss function with a term that produces outputs useful to a pre-trained ASR
model and show that this augmented loss function improves WER metrics. Second,
we demonstrate that augmenting our training dataset of real world examples with
a large synthetic dataset improves performance. Crucially, applying SpecAugment
style masks to the reference channel during training aids the model in adapting
from synthetic to real domains. In experimental evaluations, we find the
proposed approaches improve performance, on average, by 57% over a signal
processing baseline and 45% over the neural AEC model without the proposed
changes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00857</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00857</id><submitter>Jie Yang</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:43:39 GMT</date><size>1506kb</size></version><title>Fine-grained Finger Gesture Recognition Using WiFi Signals</title><authors>Sheng Tan, Jie Yang</authors><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gesture recognition has become increasingly important in human-computer
interaction and can support different applications such as smart home, VR, and
gaming. Traditional approaches usually rely on dedicated sensors that are worn
by the user or cameras that require line of sight. In this paper, we present
fine-grained finger gesture recognition by using commodity WiFi without
requiring user to wear any sensors. Our system takes advantages of the
fine-grained Channel State Information available from commodity WiFi devices
and the prevalence of WiFi network infrastructures. It senses and identifies
subtle movements of finger gestures by examining the unique patterns exhibited
in the detailed CSI. We devise environmental noise removal mechanism to
mitigate the effect of signal dynamic due to the environment changes. Moreover,
we propose to capture the intrinsic gesture behavior to deal with individual
diversity and gesture inconsistency. Lastly, we utilize multiple WiFi links and
larger bandwidth at 5GHz to achieve finger gesture recognition under multi-user
scenario. Our experimental evaluation in different environments demonstrates
that our system can achieve over 90% recognition accuracy and is robust to both
environment changes and individual diversity. Results also show that our system
can provide accurate gesture recognition under different scenarios.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00858</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00858</id><submitter>Jiri Navratil</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:46:44 GMT</date><size>7194kb</size><source_type>D</source_type></version><title>Uncertainty Characteristics Curves: A Systematic Assessment of
  Prediction Intervals</title><authors>Jiri Navratil, Benjamin Elder, Matthew Arnold, Soumya Ghosh, Prasanna
  Sattigeri</authors><categories>cs.LG cs.AI stat.ML</categories><comments>10 pages main paper, 9 pages appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate quantification of model uncertainty has long been recognized as a
fundamental requirement for trusted AI. In regression tasks, uncertainty is
typically quantified using prediction intervals calibrated to a specific
operating point, making evaluation and comparison across different studies
difficult. Our work leverages: (1) the concept of operating characteristics
curves and (2) the notion of a gain over a simple reference, to derive a novel
operating point agnostic assessment methodology for prediction intervals. The
paper describes the corresponding algorithm, provides a theoretical analysis,
and demonstrates its utility in multiple scenarios. We argue that the proposed
method addresses the current need for comprehensive assessment of prediction
intervals and thus represents a valuable addition to the uncertainty
quantification toolbox.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00859</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00859</id><submitter>Jie Yang</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:53:49 GMT</date><size>1535kb</size></version><title>A Continuous Liveness Detection for Voice Authentication on Smart
  Devices</title><authors>Linghan Zhang, Jie Yang</authors><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice biometrics is drawing increasing attention as it is a promising
alternative to legacy passwords for user authentication. Recently, a growing
body of work shows that voice biometrics is vulnerable to spoofing through
replay attacks, where an adversary tries to spoof voice authentication systems
by using a pre-recorded voice sample collected from a genuine user. To this
end, we propose VoiceGesture, a liveness detection solution for voice
authentication on smart devices such as smartphones and smart speakers.
VoiceGesture detects a live user by leveraging both the unique articulatory
gesture of the user when speaking a passphrase and the audio hardware advances
on these smart devices. Specifically, our system re-uses a pair of built-in
speaker and microphone on a smart device as a Doppler radar, which transmits a
high-frequency acoustic sound from the speaker and listens to the reflections
at the microphone when a user speaks a passphrase. Then we extract Doppler
shifts resulted from the user's articulatory gestures for liveness detection.
VoiceGesture is practical as it requires neither cumbersome operations nor
additional hardware but a speaker and a microphone commonly available on smart
devices that support voice input. Our experimental evaluation with 21
participants and different smart devices shows that VoiceGesture achieves over
99% and around 98% detection accuracy for text-dependent and text-independent
liveness detection, respectively. Results also show that VoiceGesture is robust
to different device placements, low audio sampling frequency, and supports
medium range liveness detection on smart speakers in various use scenarios like
smart homes and smart vehicles.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00860</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00860</id><submitter>Jie Yang</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:58:57 GMT</date><size>1314kb</size></version><title>Object Sensing for Fruit Ripeness Detection Using WiFi Signals</title><authors>Sheng Tan, Jie Yang</authors><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents FruitSense, a novel fruit ripeness sensing system that
leverages wireless signals to enable non-destructive and low-cost detection of
fruit ripeness. Such a system can reuse existing WiFi devices in homes without
the need for additional sensors. It uses WiFi signals to sense the
physiological changes associated with fruit ripening for detecting the ripeness
of fruit. FruitSense leverages the larger bandwidth at 5GHz (i.e., over 600MHz)
to extract the multipath-independent signal components to characterize the
physiological compounds of the fruit. It then measures the similarity between
the extracted features and the ones in ripeness profiles for identifying the
ripeness level. We evaluate FruitSense in different multipath environments with
two types of fruits (i.e, kiwi and avocado) under four levels of ripeness.
Experimental results show that FruitSense can detect the ripeness levels of
fruits with an accuracy of over 90%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00861</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00861</id><submitter>Anthony G\'omez-Fonseca</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:59:27 GMT</date><size>358kb</size></version><title>Necessary and Sufficient Girth Conditions for LDPC Tanner Graphs with
  Denser Protographs</title><authors>Anthony G\'omez-Fonseca, Roxana Smarandache, David G. M. Mitchell</authors><categories>cs.IT math.IT</categories><comments>Submitted to the International Symposium on Topics in Coding 2021.
  arXiv admin note: text overlap with arXiv:2105.03462</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives necessary and sufficient conditions for the Tanner graph of
a quasi-cyclic (QC) low-density parity-check (LDPC) code based on the all-one
protograph to have girth 6, 8, 10, and 12, respectively, in the case of
parity-check matrices with column weight 4. These results are a natural
extension of the girth results of the already-studied cases of column weight 2
and 3, and it is based on the connection between the girth of a Tanner graph
given by a parity-check matrix and the properties of powers of the product
between the matrix and its transpose. The girth conditions can be easily
incorporated into fast algorithms that construct codes of desired girth between
6 and 12; our own algorithms are presented for each girth, together with
constructions obtained from them and corresponding computer simulations. More
importantly, this paper emphasizes how the girth conditions of the Tanner graph
corresponding to a parity-check matrix composed of circulants relate to the
matrix obtained by adding (over the integers) the circulant columns of the
parity-check matrix. In particular, we show that imposing girth conditions on a
parity-check matrix is equivalent to imposing conditions on a square circulant
submatrix of size 4 obtained from it.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00865</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00865</id><submitter>Jie Yang</submitter><version version="v1"><date>Wed, 2 Jun 2021 00:25:19 GMT</date><size>1753kb</size></version><title>Multi-User Activity Recognition and Tracking Using Commodity WiFi</title><authors>Sheng Tan, Jie Yang</authors><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents MultiTrack, a commodity WiFi-based human sensing system
that can track multiple users and recognize the activities of multiple users
performing them simultaneously. Such a system can enable easy and large-scale
deployment for multi-user tracking and sensing without the need for additional
sensors through the use of existing WiFi devices (e.g., desktops, laptops, and
smart appliances). The basic idea is to identify and extract the signal
reflection corresponding to each individual user with the help of multiple WiFi
links and all the available WiFi channels at 5GHz. Given the extracted signal
reflection of each user, MultiTrack examines the path of the reflected signals
at multiple links to simultaneously track multiple users. It further
reconstructs the signal profile of each user as if only a single user has
performed activity in the environment to facilitate multi-user activity
recognition. We evaluate MultiTrack in different multipath environments with up
to 4 users for multi-user tracking and up to 3 users for activity recognition.
Experimental results show that our system can achieve decimeter localization
accuracy and over 92% activity recognition accuracy under multi-user scenarios.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00872</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00872</id><submitter>Divyansh Kaushik</submitter><version version="v1"><date>Wed, 2 Jun 2021 00:48:33 GMT</date><size>310kb</size><source_type>D</source_type></version><title>On the Efficacy of Adversarial Data Collection for Question Answering:
  Results from a Large-Scale Randomized Study</title><authors>Divyansh Kaushik, Douwe Kiela, Zachary C. Lipton, Wen-tau Yih</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted at ACL-IJCNLP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In adversarial data collection (ADC), a human workforce interacts with a
model in real time, attempting to produce examples that elicit incorrect
predictions. Researchers hope that models trained on these more challenging
datasets will rely less on superficial patterns, and thus be less brittle.
However, despite ADC's intuitive appeal, it remains unclear when training on
adversarial datasets produces more robust models. In this paper, we conduct a
large-scale controlled study focused on question answering, assigning workers
at random to compose questions either (i) adversarially (with a model in the
loop); or (ii) in the standard fashion (without a model). Across a variety of
models and datasets, we find that models trained on adversarial data usually
perform better on other adversarial datasets but worse on a diverse collection
of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of
adversarial (vs standard) data, identifying key differences and offering
guidance for future research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00873</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00873</id><submitter>Zhenyu Zhong</submitter><version version="v1"><date>Wed, 2 Jun 2021 00:49:59 GMT</date><size>18727kb</size><source_type>D</source_type></version><title>Coverage-based Scene Fuzzing for Virtual Autonomous Driving Testing</title><authors>Zhisheng Hu, Shengjian Guo, Zhenyu Zhong, Kang Li</authors><categories>cs.AI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation-based virtual testing has become an essential step to ensure the
safety of autonomous driving systems. Testers need to handcraft the virtual
driving scenes and configure various environmental settings like surrounding
traffic, weather conditions, etc. Due to the huge amount of configuration
possibilities, the human efforts are subject to the inefficiency in detecting
flaws in industry-class autonomous driving system. This paper proposes a
coverage-driven fuzzing technique to automatically generate diverse
configuration parameters to form new driving scenes. Experimental results show
that our fuzzing method can significantly reduce the cost in deriving new risky
scenes from the initial setup designed by testers. We expect automated fuzzing
will become a common practice in virtual testing for autonomous driving
systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00874</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00874</id><submitter>Munazza Zaib</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:06:34 GMT</date><size>2118kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 01:02:38 GMT</date><size>2117kb</size><source_type>D</source_type></version><title>Conversational Question Answering: A Survey</title><authors>Munazza Zaib and Wei Emma Zhang and Quan Z. Sheng and Adnan Mahmood
  and Yang Zhang</authors><categories>cs.CL cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Question answering (QA) systems provide a way of querying the information
available in various formats including, but not limited to, unstructured and
structured data in natural languages. It constitutes a considerable part of
conversational artificial intelligence (AI) which has led to the introduction
of a special research topic on Conversational Question Answering (CQA), wherein
a system is required to understand the given context and then engages in
multi-turn QA to satisfy the user's information needs. Whilst the focus of most
of the existing research work is subjected to single-turn QA, the field of
multi-turn QA has recently grasped attention and prominence owing to the
availability of large-scale, multi-turn QA datasets and the development of
pre-trained language models. With a good amount of models and research papers
adding to the literature every year recently, there is a dire need of arranging
and presenting the related work in a unified manner to streamline future
research. This survey, therefore, is an effort to present a comprehensive
review of the state-of-the-art research trends of CQA primarily based on
reviewed papers from 2016-2021. Our findings show that there has been a trend
shift from single-turn to multi-turn QA which empowers the field of
Conversational AI from different perspectives. This survey is intended to
provide an epitome for the research community with the hope of laying a strong
foundation for the field of CQA.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00875</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00875</id><submitter>Oliver Korten</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:14:23 GMT</date><size>101kb</size><source_type>D</source_type></version><title>The Hardest Explicit Construction</title><authors>Oliver Korten</authors><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of explicit construction problems, where the
goal is to produce a particular object of size $n$ possessing some pseudorandom
property in time polynomial in $n$. We give overwhelming evidence that ${\bf
APEPP}$, defined originally by Kleinberg et al., is the natural complexity
class associated with explicit constructions for objects whose existence
follows from the probabilistic method, by proving that a host of well-studied
explicit construction problems lie in this class. We then observe that a result
of Je\v{r}\'{a}bek on provability in Bounded Arithmetic, when reinterpreted as
a reduction between search problems, shows that constructing a truth table of
high circuit complexity is complete for ${\bf APEPP}$ under ${\bf P}^{\bf NP}$
reductions. This demonstrates that constructing a hard truth table is a
universal explicit construction problem in a concrete sense. This result in
fact gives a precise algorithmic characterization of proving $2^{\Omega(n)}$
circuit lower bounds for ${\bf E}^{\bf NP}$: the complete problem for ${\bf
APEPP}$ has a ${\bf P}^{\bf NP}$ algorithm if and only if such a lower bound
holds. Together with our proof that pseudorandom generators can be constructed
in ${\bf APEPP}$, this also yields a self-contained and significantly
simplified proof of the celebrated result of Impagliazzo and Wigderson that
worst-case-hard truth tables can be used to derandomize algorithms (although
the conclusion is weaker as our derandomization requires an ${\bf NP}$ oracle).
As another corollary of this completeness result, we show that ${\bf E}^{\bf
NP}$ contains a language of circuit complexity $2^{\Omega(n)}$ if and only if
it contains a language of circuit complexity $\frac{2^n}{3n}$. Finally, for
several of the problems shown to lie in ${\bf APEPP}$, we demonstrate direct
polynomial time reductions to the explicit construction of hard truth tables.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00877</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00877</id><submitter>S\'ilvia Casacuberta</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:29:11 GMT</date><size>1382kb</size><source_type>D</source_type></version><title>Evaluating Word Embeddings with Categorical Modularity</title><authors>S\'ilvia Casacuberta, Karina Halevy, Dami\'an E. Blasi</authors><categories>cs.CL</categories><comments>Accepted to Findings of ACL 2021 (Long Paper)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce categorical modularity, a novel low-resource intrinsic metric to
evaluate word embedding quality. Categorical modularity is a graph modularity
metric based on the $k$-nearest neighbor graph constructed with embedding
vectors of words from a fixed set of semantic categories, in which the goal is
to measure the proportion of words that have nearest neighbors within the same
categories. We use a core set of 500 words belonging to 59 neurobiologically
motivated semantic categories in 29 languages and analyze three word embedding
models per language (FastText, MUSE, and subs2vec). We find moderate to strong
positive correlations between categorical modularity and performance on the
monolingual tasks of sentiment analysis and word similarity calculation and on
the cross-lingual task of bilingual lexicon induction both to and from English.
Overall, we suggest that categorical modularity provides non-trivial predictive
information about downstream task performance, with breakdowns of correlations
by model suggesting some meta-predictive properties about semantic information
loss as well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00880</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00880</id><submitter>Pourya Shamsolmoali</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:33:49 GMT</date><size>7708kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 01:16:48 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Rotation Equivariant Feature Image Pyramid Network for Object Detection
  in Optical Remote Sensing Imagery</title><authors>Pourya Shamsolmoali, Masoumeh Zareapoor, Jocelyn Chanussot, Huiyu
  Zhou, and Jie Yang</authors><categories>cs.CV</categories><comments>We submitted the old version of the manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, there has been substantial progress in object
detection on remote sensing images (RSIs) where objects are generally
distributed with large-scale variations and have different types of
orientations. Nevertheless, most of the current convolution neural network
approaches lack the ability to deal with the challenges such as size and
rotation variations. To address these problems, we propose the rotation
equivariant feature image pyramid network (REFIPN), an image pyramid network
based on rotation equivariance convolution. The proposed pyramid network
extracts features in a wide range of scales and orientations by using novel
convolution filters. These features are used to generate vector fields and
determine the weight and angle of the highest-scoring orientation for all
spatial locations on an image. Finally, the extracted features go through the
prediction layers of the detector. The detection performance of the proposed
model is validated on two commonly used aerial benchmarks and the results show
our propose model can achieve state-of-the-art performance with satisfactory
efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00881</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00881</id><submitter>Denis Kleyko</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:33:56 GMT</date><size>1662kb</size><source_type>D</source_type></version><title>Hyperdimensional Computing for Efficient Distributed Classification with
  Randomized Neural Networks</title><authors>Antonello Rosato, Massimo Panella, Denis Kleyko</authors><categories>cs.LG cs.DC</categories><comments>1 table, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the supervised learning domain, considering the recent prevalence of
algorithms with high computational cost, the attention is steering towards
simpler, lighter, and less computationally extensive training and inference
approaches. In particular, randomized algorithms are currently having a
resurgence, given their generalized elementary approach. By using randomized
neural networks, we study distributed classification, which can be employed in
situations were data cannot be stored at a central location nor shared. We
propose a more efficient solution for distributed classification by making use
of a lossy compression approach applied when sharing the local classifiers with
other agents. This approach originates from the framework of hyperdimensional
computing, and is adapted herein. The results of experiments on a collection of
datasets demonstrate that the proposed approach has usually higher accuracy
than local classifiers and getting close to the benchmark - the centralized
classifier. This work can be considered as the first step towards analyzing the
variegated horizon of distributed randomized neural networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00882</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00882</id><submitter>Ikuya Yamada</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:34:42 GMT</date><size>98kb</size><source_type>D</source_type></version><title>Efficient Passage Retrieval with Hashing for Open-domain Question
  Answering</title><authors>Ikuya Yamada, Akari Asai, Hannaneh Hajishirzi</authors><categories>cs.CL cs.IR</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the-art open-domain question answering systems use a neural
retrieval model to encode passages into continuous vectors and extract them
from a knowledge source. However, such retrieval models often require large
memory to run because of the massive size of their passage index. In this
paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural
retrieval model that integrates a learning-to-hash technique into the
state-of-the-art Dense Passage Retriever (DPR) to represent the passage index
using compact binary codes rather than continuous vectors. BPR is trained with
a multi-task objective over two tasks: efficient candidate generation based on
binary codes and accurate reranking based on continuous vectors. Compared with
DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss
of accuracy on two standard open-domain question answering benchmarks: Natural
Questions and TriviaQA. Our code and trained models are available at
https://github.com/studio-ousia/bpr.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00883</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00883</id><submitter>Andrew Adamatzky</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:35:13 GMT</date><size>9471kb</size><source_type>D</source_type></version><title>Towards proteinoid computers</title><authors>Andrew Adamatzky</authors><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proteinoids -- thermal proteins -- are produced by heating amino acids to
their melting point and initiation of polymerisation to produce polymeric
chains. Proteinoids swell in aqueous solution into hollow microspheres. The
proteinoid microspheres produce endogenous burst of electrical potential spikes
and change patterns of their electrical activity in response to illumination.
The microspheres can interconnect by pores and tubes and form networks with a
programmable growth. We speculate on how ensembles of the proteinoid
microspheres can be developed into unconventional computing devices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00884</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00884</id><submitter>Brian Kidd</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:36:53 GMT</date><size>9686kb</size><source_type>D</source_type></version><title>Deep Personalized Glucose Level Forecasting Using Attention-based
  Recurrent Neural Networks</title><authors>Mohammadreza Armandpour, Brian Kidd, Yu Du, Jianhua Z. Huang</authors><categories>cs.LG cs.NE</categories><comments>8 pages, submitted to IJCNN 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we study the problem of blood glucose forecasting and provide
a deep personalized solution. Predicting blood glucose level in people with
diabetes has significant value because health complications of abnormal glucose
level are serious, sometimes even leading to death. Therefore, having a model
that can accurately and quickly warn patients of potential problems is
essential. To develop a better deep model for blood glucose forecasting, we
analyze the data and detect important patterns. These observations helped us to
propose a method that has several key advantages over existing methods: 1- it
learns a personalized model for each patient as well as a global model; 2- it
uses an attention mechanism and extracted time features to better learn
long-term dependencies in the data; 3- it introduces a new, robust training
procedure for time series data. We empirically show the efficacy of our model
on a real dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00885</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00885</id><submitter>Fengzhuo Zhang</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:37:52 GMT</date><size>2611kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 07:23:19 GMT</date><size>2365kb</size></version><title>Robustifying Algorithms of Learning Latent Trees with Vector Variables</title><authors>Fengzhuo Zhang, Vincent Y. F. Tan</authors><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning the structures of Gaussian latent tree models with
vector observations when a subset of them are arbitrarily corrupted. First, we
present the sample complexities of Recursive Grouping (RG) and Chow-Liu
Recursive Grouping (CLRG) without the assumption that the effective depth is
bounded in the number of observed nodes, significantly generalizing the results
in Choi et al. (2011). We show that Chow-Liu initialization in CLRG greatly
reduces the sample complexity of RG from being exponential in the diameter of
the tree to only logarithmic in the diameter for the hidden Markov model (HMM).
Second, we robustify RG, CLRG, Neighbor Joining (NJ) and Spectral NJ (SNJ) by
using the truncated inner product. These robustified algorithms can tolerate a
number of corruptions up to the square root of the number of clean samples.
Finally, we derive the first known instance-dependent impossibility result for
structure learning of latent trees. The optimalities of the robust version of
CLRG and NJ are verified by comparing their sample complexities and the
impossibility result.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00886</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00886</id><submitter>Keisuke Kawano</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:48:41 GMT</date><size>587kb</size><source_type>D</source_type></version><title>Partial Wasserstein Covering</title><authors>Keisuke Kawano, Satoshi Koide, Keisuke Otaki</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider a general task called partial Wasserstein covering with the goal
of emulating a large dataset (e.g., application dataset) using a small dataset
(e.g., development dataset) in terms of the empirical distribution by selecting
a small subset from a candidate dataset and adding it to the small dataset. We
model this task as a discrete optimization problem with partial Wasserstein
divergence as an objective function. Although this problem is NP-hard, we prove
that it has the submodular property, allowing us to use a greedy algorithm with
a 0.63 approximation. However, the greedy algorithm is still inefficient
because it requires linear programming for each objective function evaluation.
To overcome this difficulty, we propose quasi-greedy algorithms for
acceleration, which consist of a series of techniques such as sensitivity
analysis based on strong duality and the so-called $C$-transform in the optimal
transport field. Experimentally, we demonstrate that we can efficiently make
two datasets similar in terms of partial Wasserstein divergence, including
driving scene datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00887</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00887</id><submitter>Zanbo Wang</submitter><version version="v1"><date>Wed, 2 Jun 2021 01:52:07 GMT</date><size>6289kb</size><source_type>D</source_type></version><title>Exploiting Global Contextual Information for Document-level Named Entity
  Recognition</title><authors>Zanbo Wang, Wei Wei, Xianling Mao, Shanshan Feng, Pan Zhou, Zhiyong He
  and Sheng Jiang</authors><categories>cs.CL cs.AI</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing named entity recognition (NER) approaches are based on sequence
labeling models, which focus on capturing the local context dependencies.
However, the way of taking one sentence as input prevents the modeling of
non-sequential global context, which is useful especially when local context
information is limited or ambiguous. To this end, we propose a model called
Global Context enhanced Document-level NER (GCDoc) to leverage global
contextual information from two levels, i.e., both word and sentence. At
word-level, a document graph is constructed to model a wider range of
dependencies between words, then obtain an enriched contextual representation
for each word via graph neural networks (GNN). To avoid the interference of
noise information, we further propose two strategies. First we apply the
epistemic uncertainty theory to find out tokens whose representations are less
reliable, thereby helping prune the document graph. Then a selective auxiliary
classifier is proposed to effectively learn the weight of edges in document
graph and reduce the importance of noisy neighbour nodes. At sentence-level,
for appropriately modeling wider context beyond single sentence, we employ a
cross-sentence module which encodes adjacent sentences and fuses it with the
current sentence representation via attention and gating mechanisms. Extensive
experiments on two benchmark NER datasets (CoNLL 2003 and Ontonotes 5.0 English
dataset) demonstrate the effectiveness of our proposed model. Our model reaches
F1 score of 92.22 (93.40 with BERT) on CoNLL 2003 dataset and 88.32 (90.49 with
BERT) on Ontonotes 5.0 dataset, achieving new state-of-the-art performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00891</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00891</id><submitter>Zhiwen Tang</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:10:07 GMT</date><size>955kb</size><source_type>D</source_type></version><title>High-Quality Diversification for Task-Oriented Dialogue Systems</title><authors>Zhiwen Tang, Hrishikesh Kulkarni, Grace Hui Yang</authors><categories>cs.CL cs.HC</categories><comments>Accepted by ACL-IJCNLP 2021 (Findings of ACL)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many task-oriented dialogue systems use deep reinforcement learning (DRL) to
learn policies that respond to the user appropriately and complete the tasks
successfully. Training DRL agents with diverse dialogue trajectories prepare
them well for rare user requests and unseen situations. One effective
diversification method is to let the agent interact with a diverse set of
learned user models. However, trajectories created by these artificial user
models may contain generation errors, which can quickly propagate into the
agent's policy. It is thus important to control the quality of the
diversification and resist the noise. In this paper, we propose a novel
dialogue diversification method for task-oriented dialogue systems trained in
simulators. Our method, Intermittent Short Extension Ensemble (I-SEE),
constrains the intensity to interact with an ensemble of diverse user models
and effectively controls the quality of the diversification. Evaluations on the
Multiwoz dataset show that I-SEE successfully boosts the performance of several
state-of-the-art DRL dialogue agents.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00893</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00893</id><submitter>Kaden Griffith</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:12:45 GMT</date><size>42kb</size></version><title>Solving Arithmetic Word Problems with Transformers and Preprocessing of
  Problem Text</title><authors>Kaden Griffith and Jugal Kalita</authors><categories>cs.CL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1912.00871</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines the use of Transformer networks trained to translate math
word problems to equivalent arithmetic expressions in infix, prefix, and
postfix notations. We compare results produced by many neural configurations
and find that most configurations outperform previously reported approaches on
three of four datasets with significant increases in accuracy of over 20
percentage points. The best neural approaches boost accuracy by 30% when
compared to the previous state-of-the-art on some datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00895</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00895</id><submitter>Tongjia Zheng</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:19:36 GMT</date><size>2661kb</size></version><title>Field Estimation using Robotic Swarms through Bayesian Regression and
  Mean-Field Feedback</title><authors>Tongjia Zheng, Hai Lin</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen an increased interest in using mean-field density
based modelling and control strategy for deploying robotic swarms. In this
paper, we study how to dynamically deploy the robots subject to their physical
constraints to efficiently measure and reconstruct certain unknown spatial
field (e.g. the air pollution index over a city). Specifically, the evolution
of the robots' density is modelled by mean-field partial differential equations
(PDEs) which are uniquely determined by the robots' individual dynamics.
Bayesian regression models are used to obtain predictions and return a variance
function that represents the confidence of the prediction. We formulate a PDE
constrained optimization problem based on this variance function to dynamically
generate a reference density signal which guides the robots to uncertain areas
to collect new data, and design mean-field feedback-based control laws such
that the robots' density converges to this reference signal. We also show that
the proposed feedback law is robust to density estimation errors in the sense
of input-to-state stability. Simulations are included to verify the
effectiveness of the algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00896</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00896</id><submitter>Jiachun Pan</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:21:18 GMT</date><size>40kb</size><source_type>D</source_type></version><title>Asymptotics of Sequential Composite Hypothesis Testing under
  Probabilistic Constraints</title><authors>Jiachun Pan, Yonglong Li, Vincent Y. F. Tan</authors><categories>cs.IT math.IT math.ST stat.TH</categories><comments>The paper was presented in part at the 2021 International Symposium
  on Information Theory (ISIT). It was submitted to Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the sequential composite binary hypothesis testing problem in
which one of the hypotheses is governed by a single distribution while the
other is governed by a family of distributions whose parameters belong to a
known set $\Gamma$. We would like to design a test to decide which hypothesis
is in effect. Under the constraints that the probabilities that the length of
the test, a stopping time, exceeds $n$ are bounded by a certain threshold
$\epsilon$, we obtain certain fundamental limits on the asymptotic behavior of
the sequential test as $n$ tends to infinity. Assuming that $\Gamma$ is a
convex and compact set, we obtain the set of all first-order error exponents
for the problem. We also prove a strong converse. Additionally, we obtain the
set of second-order error exponents under the assumption that $\mathcal{X}$ is
a finite alphabet. In the proof of second-order asymptotics, a main technical
contribution is the derivation of a central limit-type result for a maximum of
an uncountable set of log-likelihood ratios under suitable conditions. This
result may be of independent interest. We also show that some important
statistical models satisfy the conditions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00897</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00897</id><submitter>Wanqi Xue</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:22:52 GMT</date><size>2753kb</size><source_type>D</source_type></version><title>Solving Large-Scale Extensive-Form Network Security Games via Neural
  Fictitious Self-Play</title><authors>Wanqi Xue, Youzhi Zhang, Shuxin Li, Xinrun Wang, Bo An, Chai Kiat Yeo</authors><categories>cs.AI cs.GT cs.LG cs.MA</categories><comments>Published as a conference paper in IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Securing networked infrastructures is important in the real world. The
problem of deploying security resources to protect against an attacker in
networked domains can be modeled as Network Security Games (NSGs).
Unfortunately, existing approaches, including the deep learning-based
approaches, are inefficient to solve large-scale extensive-form NSGs. In this
paper, we propose a novel learning paradigm, NSG-NFSP, to solve large-scale
extensive-form NSGs based on Neural Fictitious Self-Play (NFSP). Our main
contributions include: i) reforming the best response (BR) policy network in
NFSP to be a mapping from action-state pair to action-value, to make the
calculation of BR possible in NSGs; ii) converting the average policy network
of an NFSP agent into a metric-based classifier, helping the agent to assign
distributions only on legal actions rather than all actions; iii) enabling NFSP
with high-level actions, which can benefit training efficiency and stability in
NSGs; and iv) leveraging information contained in graphs of NSGs by learning
efficient graph node embeddings. Our algorithm significantly outperforms
state-of-the-art algorithms in both scalability and solution quality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00898</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00898</id><submitter>Shiyu Jin</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:24:49 GMT</date><size>1472kb</size><source_type>D</source_type></version><title>Trajectory Optimization for Manipulation of Deformable Objects: Assembly
  of Belt Drive Units</title><authors>Shiyu Jin, Diego Romeres, Arvind Ragunathan, Devesh K. Jha and
  Masayoshi Tomizuka</authors><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a novel trajectory optimization formulation to solve the
robotic assembly of the belt drive unit. Robotic manipulations involving
contacts and deformable objects are challenging in both dynamic modeling and
trajectory planning. For modeling, variations in the belt tension and contact
forces between the belt and the pulley could dramatically change the system
dynamics. For trajectory planning, it is computationally expensive to plan
trajectories for such hybrid dynamical systems as it usually requires planning
for discrete modes separately. In this work, we formulate the belt drive unit
assembly task as a trajectory optimization problem with complementarity
constraints to avoid explicitly imposing contact mode sequences. The problem is
solved as a mathematical program with complementarity constraints (MPCC) to
obtain feasible and efficient assembly trajectories. We validate the proposed
method both in simulations with a physics engine and in real-world experiments
with a robotic manipulator.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00899</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00899</id><submitter>Tongjia Zheng</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:35:13 GMT</date><size>1762kb</size><source_type>D</source_type></version><title>Feedback Interconnected Mean-Field Estimation and Control</title><authors>Tongjia Zheng, Qing Han, Hai Lin</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm robotic systems have foreseeable applications in the near future.
Recently, there has been an increasing amount of literature that employs
mean-field partial differential equations (PDEs) to model the time-evolution of
the probability density of swarm robotic systems and uses mean-field feedback
to design stable control laws that act on individuals such that their density
converges to a target profile. However, it remains largely unexplored
considering problems of how to estimate the mean-field density, how the density
estimation algorithms affect the control performance, and whether the
estimation performance in turn depends on the control algorithms. In this work,
we focus on studying the interplay of these algorithms. Specially, we propose
new mean-field control laws which use the real-time density and its gradient as
feedback, and prove that they are globally input-to-state stable (ISS) to
estimation errors. Then, we design filtering algorithms to obtain estimates of
the density and its gradient, and prove that these estimates are convergent
assuming the control laws are known. Finally, we show that the feedback
interconnection of these estimation and control algorithms is still globally
ISS, which is attributed to the bilinearity of the mean-field PDE system. An
agent-based simulation is included to verify the stability of these algorithms
and their feedback interconnection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00901</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00901</id><submitter>Hiroshi Kajino</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:40:17 GMT</date><size>499kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 07:15:32 GMT</date><size>499kb</size><source_type>D</source_type></version><title>A Differentiable Point Process with Its Application to Spiking Neural
  Networks</title><authors>Hiroshi Kajino</authors><categories>cs.NE cs.LG stat.ML</categories><comments>Accepted to ICML 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is concerned about a learning algorithm for a probabilistic model
of spiking neural networks (SNNs). Jimenez Rezende &amp; Gerstner (2014) proposed a
stochastic variational inference algorithm to train SNNs with hidden neurons.
The algorithm updates the variational distribution using the score function
gradient estimator, whose high variance often impedes the whole learning
algorithm. This paper presents an alternative gradient estimator for SNNs based
on the path-wise gradient estimator. The main technical difficulty is a lack of
a general method to differentiate a realization of an arbitrary point process,
which is necessary to derive the path-wise gradient estimator. We develop a
differentiable point process, which is the technical highlight of this paper,
and apply it to derive the path-wise gradient estimator for SNNs. We
investigate the effectiveness of our gradient estimator through numerical
simulation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00903</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00903</id><submitter>Liang Ding</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:41:40 GMT</date><size>159kb</size><source_type>D</source_type></version><title>Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in
  Non-Autoregressive Translation</title><authors>Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao and
  Zhaopeng Tu</authors><categories>cs.CL cs.AI</categories><comments>ACL 2021</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Knowledge distillation (KD) is commonly used to construct synthetic data for
training non-autoregressive translation (NAT) models. However, there exists a
discrepancy on low-frequency words between the distilled and the original data,
leading to more errors on predicting low-frequency words. To alleviate the
problem, we directly expose the raw data into NAT by leveraging pretraining. By
analyzing directed alignments, we found that KD makes low-frequency source
words aligned with targets more deterministically but fails to align sufficient
low-frequency words from target to source. Accordingly, we propose reverse KD
to rejuvenate more alignments for low-frequency target words. To make the most
of authentic and synthetic data, we combine these complementary approaches as a
new training strategy for further boosting NAT performance. We conduct
experiments on five translation benchmarks over two advanced architectures.
Results demonstrate that the proposed approach can significantly and
universally improve translation quality by reducing translation errors on
low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU
points on the WMT14 English-German and WMT16 Romanian-English datasets,
respectively. Our code, data, and trained models are available at
\url{https://github.com/longyuewangdcu/RLFW-NAT}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00905</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00905</id><submitter>Rakhmatulin Ildar</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:55:03 GMT</date><size>566kb</size></version><title>Low-cost Stereovision system (disparity map) for few dollars</title><authors>R. Ildar and E. Pomazov</authors><categories>cs.AI</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The paper presents an analysis of the latest developments in the field of
stereo vision in the low-cost segment, both for prototypes and for industrial
designs. We described the theory of stereo vision and presented information
about cameras and data transfer protocols and their compatibility with various
devices. The theory in the field of image processing for stereo vision
processes is considered and the calibration process is described in detail.
Ultimately, we presented the developed stereo vision system and provided the
main points that need to be considered when developing such systems. The final,
we presented software for adjusting stereo vision parameters in real-time in
the python language in the Windows operating system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00906</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00906</id><submitter>Daniel McKenzie</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:55:46 GMT</date><size>699kb</size><source_type>D</source_type></version><title>Learn to Predict Equilibria via Fixed Point Networks</title><authors>Howard Heaton, Daniel McKenzie, Qiuwei Li, Samy Wu Fung, Stanley
  Osher, Wotao Yin</authors><categories>cs.LG cs.GT math.OC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Systems of interacting agents can often be modeled as contextual games, where
the context encodes additional information, beyond the control of any agent
(e.g. weather for traffic and fiscal policy for market economies). In such
systems, the most likely outcome is given by a Nash equilibrium. In many
practical settings, only game equilibria are observed, while the optimal
parameters for a game model are unknown. This work introduces Nash Fixed Point
Networks (N-FPNs), a class of implicit-depth neural networks that output Nash
equilibria of contextual games. The N-FPN architecture fuses data-driven
modeling with provided constraints. Given equilibrium observations of a
contextual game, N-FPN parameters are learnt to predict equilibria outcomes
given only the context. We present an end-to-end training scheme for N-FPNs
that is simple and memory efficient to implement with existing
autodifferentiation tools. N-FPNs also exploit a novel constraint decoupling
scheme to avoid costly projections. Provided numerical examples show the
efficacy of N-FPNs on atomic and non-atomic games (e.g. traffic routing).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00908</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00908</id><submitter>Zhuchen Shao</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:57:54 GMT</date><size>8767kb</size><source_type>D</source_type></version><title>TransMIL: Transformer based Correlated Multiple Instance Learning for
  Whole Slide Image Classication</title><authors>Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang
  Ji, Yongbing Zhang</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multiple instance learning (MIL) is a powerful tool to solve the weakly
supervised classification in whole slide image (WSI) based pathology diagnosis.
However, the current MIL methods are usually based on independent and identical
distribution hypothesis, thus neglect the correlation among different
instances. To address this problem, we proposed a new framework, called
correlated MIL, and provided a proof for convergence. Based on this framework,
we devised a Transformer based MIL (TransMIL), which explored both
morphological and spatial information. The proposed TransMIL can effectively
deal with unbalanced/balanced and binary/multiple classification with great
visualization and interpretability. We conducted various experiments for three
different computational pathology problems and achieved better performance and
faster convergence compared with state-of-the-art methods. The test AUC for the
binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And
the AUC over the cancer subtypes classification can be up to 96.03% and 98.82%
over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00909</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00909</id><submitter>Nate Veldt</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:58:35 GMT</date><size>1350kb</size><source_type>D</source_type></version><title>The Generalized Mean Densest Subgraph Problem</title><authors>Nate Veldt and Austin R. Benson and Jon Kleinberg</authors><categories>cs.DS cs.DM cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding dense subgraphs of a large graph is a standard problem in graph
mining that has been studied extensively both for its theoretical richness and
its many practical applications. In this paper we introduce a new family of
dense subgraph objectives, parameterized by a single parameter $p$, based on
computing generalized means of degree sequences of a subgraph. Our objective
captures both the standard densest subgraph problem and the maximum $k$-core as
special cases, and provides a way to interpolate between and extrapolate beyond
these two objectives when searching for other notions of dense subgraphs. In
terms of algorithmic contributions, we first show that our objective can be
minimized in polynomial time for all $p \geq 1$ using repeated submodular
minimization. A major contribution of our work is analyzing the performance of
different types of peeling algorithms for dense subgraphs both in theory and
practice. We prove that the standard peeling algorithm can perform arbitrarily
poorly on our generalized objective, but we then design a more sophisticated
peeling method which for $p \geq 1$ has an approximation guarantee that is
always at least $1/2$ and converges to 1 as $p \rightarrow \infty$. In
practice, we show that this algorithm obtains extremely good approximations to
the optimal solution, scales to large graphs, and highlights a range of
different meaningful notions of density on graphs coming from numerous domains.
Furthermore, it is typically able to approximate the densest subgraph problem
better than the standard peeling algorithm, by better accounting for how the
removal of one node affects other nodes in its neighborhood.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00910</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00910</id><submitter>Erkan Kayacan</submitter><version version="v1"><date>Wed, 2 Jun 2021 02:59:48 GMT</date><size>2326kb</size><source_type>D</source_type></version><title>Concurrent Learning Based Tracking Control of Nonlinear Systems using
  Gaussian Process</title><authors>Vedant Bhandari and Erkan Kayacan</authors><categories>eess.SY cs.LG cs.SY</categories><comments>6 pages, 3 figures, conference paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper demonstrates the applicability of the combination of concurrent
learning as a tool for parameter estimation and non-parametric Gaussian Process
for online disturbance learning. A control law is developed by using both
techniques sequentially in the context of feedback linearization. The
concurrent learning algorithm estimates the system parameters of structured
uncertainty without requiring persistent excitation, which are used in the
design of the feedback linearization law. Then, a non-parametric Gaussian
Process learns unstructured uncertainty. The closed-loop system stability for
the nth-order system is proven using the Lyapunov stability theorem. The
simulation results show that the tracking error is minimized (i) when true
values of model parameters have not been provided, (ii) in the presence of
disturbances introduced once the parameters have converged to their true values
and (iii) when system parameters have not converged to their true values in the
presence of disturbances.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00912</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00912</id><submitter>Wentong Li</submitter><version version="v1"><date>Wed, 2 Jun 2021 03:10:51 GMT</date><size>6902kb</size><source_type>D</source_type></version><title>Translational Symmetry-Aware Facade Parsing for 3D Building
  Reconstruction</title><authors>Hantang Liu, Wentong Li, Jianke Zhu</authors><categories>cs.CV</categories><comments>12 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Effectively parsing the facade is essential to 3D building reconstruction,
which is an important computer vision problem with a large amount of
applications in high precision map for navigation, computer aided design, and
city generation for digital entertainments. To this end, the key is how to
obtain the shape grammars from 2D images accurately and efficiently. Although
enjoying the merits of promising results on the semantic parsing, deep learning
methods cannot directly make use of the architectural rules, which play an
important role for man-made structures. In this paper, we present a novel
translational symmetry-based approach to improving the deep neural networks.
Our method employs deep learning models as the base parser, and a module taking
advantage of translational symmetry is used to refine the initial parsing
results. In contrast to conventional semantic segmentation or bounding box
prediction, we propose a novel scheme to fuse segmentation with anchor-free
detection in a single stage network, which enables the efficient training and
better convergence. After parsing the facades into shape grammars, we employ an
off-the-shelf rendering engine like Blender to reconstruct the realistic
high-quality 3D models using procedural modeling. We conduct experiments on
three public datasets, where our proposed approach outperforms the
state-of-the-art methods. In addition, we have illustrated the 3D building
models built from 2D facade images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00917</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00917</id><submitter>Weihua Deng Professor</submitter><version version="v1"><date>Wed, 2 Jun 2021 03:26:03 GMT</date><size>64kb</size></version><title>An inverse random source problem for the time-space fractional diffusion
  equation driven by fractional Brownian motion</title><authors>Daxin Nie and Weihua Deng</authors><categories>math.PR cs.NA math.NA</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the inverse random source problem for the time-space fractional
diffusion equation driven by fractional Brownian motion with Hurst index
$H\in(0,1)$. With the aid of a novel estimate, by using the operator approach
we propose regularity analyses for the direct problem. Then we provide a
reconstruction scheme for the source terms $f$ and $g$ up to the sign. Next,
combining the properties of Mittag-Leffler function, the complete uniqueness
and instability analyses are provided. It's worth mentioning that all the
analyses are unified for $H\in(0,1)$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00918</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00918</id><submitter>Jari Korhonen</submitter><version version="v1"><date>Wed, 2 Jun 2021 03:31:44 GMT</date><size>171kb</size></version><title>Consumer Image Quality Prediction using Recurrent Neural Networks for
  Spatial Pooling</title><authors>Jari Korhonen, Yicheng Su, Junyong You</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Promising results for subjective image quality prediction have been achieved
during the past few years by using convolutional neural networks (CNN).
However, the use of CNNs for high resolution image quality assessment remains a
challenge, since typical CNN architectures have been designed for small
resolution input images. In this study, we propose an image quality model that
attempts to mimic the attention mechanism of human visual system (HVS) by using
a recurrent neural network (RNN) for spatial pooling of the features extracted
from different spatial areas (patches) by a deep CNN-based feature extractor.
The experimental study, conducted by using images with different resolutions
from two recently published image quality datasets, indicates that the quality
prediction accuracy of the proposed method is competitive against benchmark
models representing the state-of-the-art, and the proposed method also performs
consistently on different resolution versions of the same dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00919</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00919</id><submitter>Minh-Son To</submitter><version version="v1"><date>Wed, 2 Jun 2021 03:34:10 GMT</date><size>2662kb</size></version><title>Self-supervised Lesion Change Detection and Localisation in Longitudinal
  Multiple Sclerosis Brain Imaging</title><authors>Minh-Son To, Ian G Sarno, Chee Chong, Mark Jenkinson and Gustavo
  Carneiro</authors><categories>eess.IV cs.CV</categories><comments>Provisional accepted for MICCAI 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Longitudinal imaging forms an essential component in the management and
follow-up of many medical conditions. The presence of lesion changes on serial
imaging can have significant impact on clinical decision making, highlighting
the important role for automated change detection. Lesion changes can represent
anomalies in serial imaging, which implies a limited availability of
annotations and a wide variety of possible changes that need to be considered.
Hence, we introduce a new unsupervised anomaly detection and localisation
method trained exclusively with serial images that do not contain any lesion
changes. Our training automatically synthesises lesion changes in serial
images, introducing detection and localisation pseudo-labels that are used to
self-supervise the training of our model. Given the rarity of these lesion
changes in the synthesised images, we train the model with the imbalance robust
focal Tversky loss. When compared to supervised models trained on different
datasets, our method shows competitive performance in the detection and
localisation of new demyelinating lesions on longitudinal magnetic resonance
imaging in multiple sclerosis patients. Code for the models will be made
available on GitHub.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00920</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00920</id><submitter>Rishabh Joshi</submitter><version version="v1"><date>Wed, 2 Jun 2021 03:34:36 GMT</date><size>5389kb</size><source_type>D</source_type></version><title>DialoGraph: Incorporating Interpretable Strategy-Graph Networks into
  Negotiation Dialogues</title><authors>Rishabh Joshi, Vidhisha Balachandran, Shikhar Vashishth, Alan Black,
  Yulia Tsvetkov</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted at ICLR 2021; https://openreview.net/forum?id=kDnal_bbb-E</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To successfully negotiate a deal, it is not enough to communicate fluently:
pragmatic planning of persuasive negotiation strategies is essential. While
modern dialogue agents excel at generating fluent sentences, they still lack
pragmatic grounding and cannot reason strategically. We present DialoGraph, a
negotiation system that incorporates pragmatic strategies in a negotiation
dialogue using graph neural networks. DialoGraph explicitly incorporates
dependencies between sequences of strategies to enable improved and
interpretable prediction of next optimal strategies, given the dialogue
context. Our graph-based method outperforms prior state-of-the-art negotiation
models both in the accuracy of strategy/dialogue act prediction and in the
quality of downstream dialogue response generation. We qualitatively show
further benefits of learned strategy-graphs in providing explicit associations
between effective negotiation strategies over the course of the dialogue,
leading to interpretable and strategic dialogues.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00922</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00922</id><submitter>Sina Ghiassian</submitter><version version="v1"><date>Wed, 2 Jun 2021 03:45:43 GMT</date><size>3388kb</size><source_type>D</source_type></version><title>An Empirical Comparison of Off-policy Prediction Learning Algorithms on
  the Collision Task</title><authors>Sina Ghiassian, Richard S. Sutton</authors><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Off-policy prediction -- learning the value function for one policy from data
generated while following another policy -- is one of the most challenging
subproblems in reinforcement learning. This paper presents empirical results
with eleven prominent off-policy learning algorithms that use linear function
approximation: five Gradient-TD methods, two Emphatic-TD methods, Off-policy
TD($\lambda$), Vtrace, and versions of Tree Backup and ABQ modified to apply to
a prediction setting. Our experiments used the Collision task, a small
idealized off-policy problem analogous to that of an autonomous car trying to
predict whether it will collide with an obstacle. We assessed the performance
of the algorithms according to their learning rate, asymptotic error level, and
sensitivity to step-size and bootstrapping parameters. By these measures, the
eleven algorithms can be partially ordered on the Collision task. In the top
tier, the two Emphatic-TD algorithms learned the fastest, reached the lowest
errors, and were robust to parameter settings. In the middle tier, the five
Gradient-TD algorithms and Off-policy TD($\lambda$) were more sensitive to the
bootstrapping parameter. The bottom tier comprised Vtrace, Tree Backup, and
ABQ; these algorithms were no faster and had higher asymptotic error than the
others. Our results are definitive for this task, though of course experiments
with more tasks are needed before an overall assessment of the algorithms'
merits can be made.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00925</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00925</id><submitter>Yunqi Wang</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:01:22 GMT</date><size>1311kb</size><source_type>D</source_type></version><title>Contrastive ACE: Domain Generalization Through Alignment of Causal
  Mechanisms</title><authors>Yunqi Wang, Furui Liu, Zhitang Chen, Qing Lian, Shoubo Hu, Jianye Hao,
  Yik-Chung Wu</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain generalization aims to learn knowledge invariant across different
distributions while semantically meaningful for downstream tasks from multiple
source domains, to improve the model's generalization ability on unseen target
domains. The fundamental objective is to understand the underlying &quot;invariance&quot;
behind these observational distributions and such invariance has been shown to
have a close connection to causality. While many existing approaches make use
of the property that causal features are invariant across domains, we consider
the causal invariance of the average causal effect of the features to the
labels. This invariance regularizes our training approach in which
interventions are performed on features to enforce stability of the causal
prediction by the classifier across domains. Our work thus sheds some light on
the domain generalization problem by introducing invariance of the mechanisms
into the learning process. Experiments on several benchmark datasets
demonstrate the performance of the proposed method against SOTAs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00926</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00926</id><submitter>Paulo Eduardo Zanni Junior</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:03:35 GMT</date><size>212kb</size><source_type>D</source_type></version><title>A systematic mapping on quantum software development in the context of
  software engineering</title><authors>Paulo Eduardo Zanni Junior and Valter Vieira de Camargo</authors><categories>cs.SE</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Quantum Computing is a new paradigm that enables several advances which are
impossible using classical technology. With the rise of quantum computers, the
software is also invited to change so that it can better fit this new
computation way. However, although a lot of research is being conducted in the
quantum computing field, it is still scarce studies about the differences of
the software and software engineering in this new context. Therefore, this
article presents a systematic mapping study to present a wide review on the
particularities and characteristics of software that are developed for quantum
computers. A total of 24 papers were selected using digital libraries with the
objective of answering three research questions elaborated in the conduct of
this research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00927</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00927</id><submitter>Yuanda Xu</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:12:54 GMT</date><size>1290kb</size></version><title>An Extendible, Graph-Neural-Network-Based Approach for Accurate Force
  Field Development of Large Flexible Organic Molecules</title><authors>Xufei Wang, Yuanda Xu, Han Zheng, Kuang Yu</authors><categories>physics.chem-ph cond-mat.soft cs.CE cs.LG physics.comp-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An accurate force field is the key to the success of all molecular mechanics
simulations on organic polymers and biomolecules. Accuracy beyond density
functional theory is often needed to describe the intermolecular interactions,
while most correlated wavefunction (CW) methods are prohibitively expensive for
large molecules. Therefore, it posts a great challenge to develop an extendible
ab initio force field for large flexible organic molecules at CW level of
accuracy. In this work, we face this challenge by combining the physics-driven
nonbonding potential with a data-driven subgraph neural network bonding model
(named sGNN). Tests on polyethylene glycol polymer chains show that our
strategy is highly accurate and robust for molecules of different sizes.
Therefore, we can develop the force field from small molecular fragments (with
sizes easily accessible to CW methods) and safely transfer it to large
polymers, thus opening a new path to the next-generation organic force fields.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00931</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00931</id><submitter>Victor Chen</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:31:45 GMT</date><size>1984kb</size><source_type>D</source_type></version><title>Understanding the Design Space of Mouth Microgestures</title><authors>Victor Chen, Xuhai Xu, Richard Li, Yuanchun Shi, Shwetak Patel, Yuntao
  Wang</authors><categories>cs.HC</categories><comments>14 page, 5 figures; Accepted to DIS 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As wearable devices move toward the face (i.e. smart earbuds, glasses), there
is an increasing need to facilitate intuitive interactions with these devices.
Current sensing techniques can already detect many mouth-based gestures;
however, users' preferences of these gestures are not fully understood. In this
paper, we investigate the design space and usability of mouth-based
microgestures. We first conducted brainstorming sessions (N=16) and compiled an
extensive set of 86 user-defined gestures. Then, with an online survey (N=50),
we assessed the physical and mental demand of our gesture set and identified a
subset of 14 gestures that can be performed easily and naturally. Finally, we
conducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to
various daily smartphone operations under a sitting and walking context. From
these studies, we develop a taxonomy for mouth gestures, finalize a practical
gesture set for common applications, and provide design guidelines for future
mouth-based gesture interactions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00932</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00932</id><submitter>Khushi Shah</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:42:01 GMT</date><size>1123kb</size></version><title>Proposed DBMS for OTT platforms in line with new age requirements</title><authors>Khushi Shah, Aryan Shah, Charmi Shah, Devansh Shah, Mustafa
  Africawala, Rushabh Shah, Nishant Doshi</authors><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Database management has become an enormous tool for on-demand content
distribution services, proffering required information and providing custom
services to the user. Also plays a major role for the platforms to manage their
data in such a way that data redundancy is minimized. This paper emphasizes
improving the user experience for the platform by efficiently managing data.
Keeping in mind all the new age requirements, especially after COVID-19 the
sudden surge in subscription has led the stakeholders to try new things to lead
the OTT market. Collection of shows being the root of the tree here, this paper
improvises the currently existing branches via various tables and suggests some
new features on how the data collected can be utilized for introducing new and
much-required query results for the consumer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00933</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00933</id><submitter>Yilun Zhu</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:42:51 GMT</date><size>53kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 13:39:50 GMT</date><size>53kb</size><source_type>D</source_type></version><title>OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12
  More Genres</title><authors>Yilun Zhu, Sameer Pradhan, Amir Zeldes</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  SOTA coreference resolution produces increasingly impressive scores on the
OntoNotes benchmark. However lack of comparable data following the same scheme
for more genres makes it difficult to evaluate generalizability to open domain
data. This paper provides a dataset and comprehensive evaluation showing that
the latest neural LM based end-to-end systems degrade very substantially out of
domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly
available, converted from GUM, an English corpus covering 12 genres, using
deterministic rules, which we evaluate. Thanks to the rich syntactic and
discourse annotations in GUM, we are able to create the largest human-annotated
coreference corpus following the OntoNotes guidelines, and the first to be
evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation
across 12 genres shows nearly 15-20% degradation for both deterministic and
deep learning systems, indicating a lack of generalizability or covert
overfitting in existing coreference resolution models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00934</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00934</id><submitter>Nada Almarwani</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:43:54 GMT</date><size>255kb</size><source_type>D</source_type></version><title>Discrete Cosine Transform as Universal Sentence Encoder</title><authors>Nada Almarwani and Mona Diab</authors><categories>cs.CL</categories><comments>to be published in ACL-IJCNLP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern sentence encoders are used to generate dense vector representations
that capture the underlying linguistic characteristics for a sequence of words,
including phrases, sentences, or paragraphs. These kinds of representations are
ideal for training a classifier for an end task such as sentiment analysis,
question answering and text classification. Different models have been proposed
to efficiently generate general purpose sentence representations to be used in
pretraining protocols. While averaging is the most commonly used efficient
sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an
alternative that captures the underlying syntactic characteristics of a given
text without compromising practical efficiency compared to averaging. However,
as with most other sentence encoders, the DCT sentence encoder was only
evaluated in English. To this end, we utilize DCT encoder to generate universal
sentence representation for different languages such as German, French, Spanish
and Russian. The experimental results clearly show the superior effectiveness
of DCT encoding in which consistent performance improvements are achieved over
strong baselines on multiple standardized datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00936</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00936</id><submitter>Salar Asayesh</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:49:33 GMT</date><size>4062kb</size><source_type>D</source_type></version><title>Least-Restrictive Multi-Agent Collision Avoidance via Deep Meta
  Reinforcement Learning and Optimal Control</title><authors>Salar Asayesh, Mo Chen, Mehran Mehrandezh, Kamal Gupta</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent collision-free trajectory planning and control subject to
different goal requirements and system dynamics has been extensively studied,
and is gaining recent attention in the realm of machine and reinforcement
learning. However, in particular when using a large number of agents,
constructing a least-restrictive collision avoidance policy is of utmost
importance for both classical and learning-based methods. In this paper, we
propose a Least-Restrictive Collision Avoidance Module (LR-CAM) that evaluates
the safety of multi-agent systems and takes over control only when needed to
prevent collisions. The LR-CAM is a single policy that can be wrapped around
policies of all agents in a multi-agent system. It allows each agent to pursue
any objective as long as it is safe to do so. The benefit of the proposed
least-restrictive policy is to only interrupt and overrule the default
controller in case of an upcoming inevitable danger. We use a Long Short-Term
Memory (LSTM) based Variational Auto-Encoder (VAE) to enable the LR-CAM to
account for a varying number of agents in the environment. Moreover, we propose
an off-policy meta-reinforcement learning framework with a novel reward
function based on a Hamilton-Jacobi value function to train the LR-CAM. The
proposed method is fully meta-trained through a ROS based simulation and tested
on real multi-agent system. Our results show that LR-CAM outperforms the
classical least-restrictive baseline by 30 percent. In addition, we show that
even if a subset of agents in a multi-agent system use LR-CAM, the success rate
of all agents will increase significantly.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00937</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00937</id><submitter>Oren Ish Shalom</submitter><version version="v1"><date>Wed, 2 Jun 2021 04:51:35 GMT</date><size>51kb</size></version><title>Putting the Squeeze on Array Programs: Loop Verification via Inductive
  Rank Reduction</title><authors>Oren Ish Shalom, Shachar Itzhaky, Noam Rinetzky, Sharon Shoham</authors><categories>cs.PL cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Automatic verification of array manipulating programs is a challenging
problem because it often amounts to the inference of in ductive quantified loop
invariants which, in some cases, may not even be firstorder expressible. In
this paper, we suggest a novel verification tech nique that is based on
induction on userdefined rank of program states as an alternative to
loopinvariants. Our technique, dubbed inductive rank reduction, works in two
steps. Firstly, we simplify the verification problem and prove that the program
is correct when the input state con tains an input array of length B or less,
using the length of the array as the rank of the state. Secondly, we employ a
squeezing function g which converts a program state sigma with an array of
length &gt; B to a state g(sigma) containing an array of length minus 1 or less.
We prove that when g satisfies certain natural conditions then if the program
violates its specification on sigma then it does so also on g(sigma). The
correctness of the program on inputs with arrays of arbitrary lengths follows
by induction. We make our technique automatic for array programs whose length
of execution is proportional to the length of the input arrays by (i) perform
ing the first step using symbolic execution, (ii) verifying the conditions
required of g using Z3, and (iii) providing a heuristic procedure for syn
thesizing g. We implemented our technique and applied it successfully to
several interesting arraymanipulating programs, including a bidirec tional
summation program whose loop invariant cannot be expressed in firstorder logic
while its specification is quantifier free.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00941</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00941</id><submitter>Wenxiang Jiao</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:01:36 GMT</date><size>428kb</size><source_type>D</source_type></version><title>Self-Training Sampling with Monolingual Data Uncertainty for Neural
  Machine Translation</title><authors>Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi, Michael R. Lyu,
  Irwin King</authors><categories>cs.CL cs.IT math.IT</categories><comments>ACL 2021 main conference, long paper, 11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-training has proven effective for improving NMT performance by
augmenting model training with synthetic parallel data. The common practice is
to construct synthetic data based on a randomly sampled subset of large-scale
monolingual data, which we empirically show is sub-optimal. In this work, we
propose to improve the sampling procedure by selecting the most informative
monolingual sentences to complement the parallel data. To this end, we compute
the uncertainty of monolingual sentences using the bilingual dictionary
extracted from the parallel data. Intuitively, monolingual sentences with lower
uncertainty generally correspond to easy-to-translate patterns which may not
provide additional gains. Accordingly, we design an uncertainty-based sampling
strategy to efficiently exploit the monolingual data for self-training, in
which monolingual sentences with higher uncertainty would be sampled with
higher probability. Experimental results on large-scale WMT
English$\Rightarrow$German and English$\Rightarrow$Chinese datasets demonstrate
the effectiveness of the proposed approach. Extensive analyses suggest that
emphasizing the learning on uncertain monolingual sentences by our approach
does improve the translation quality of high-uncertainty sentences and also
benefits the prediction of low-frequency words at the target side.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00942</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00942</id><submitter>Kourosh Hakhamaneshi</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:03:38 GMT</date><size>4987kb</size><source_type>D</source_type></version><title>JUMBO: Scalable Multi-task Bayesian Optimization using Offline Data</title><authors>Kourosh Hakhamaneshi, Pieter Abbeel, Vladimir Stojanovic, Aditya
  Grover</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The goal of Multi-task Bayesian Optimization (MBO) is to minimize the number
of queries required to accurately optimize a target black-box function, given
access to offline evaluations of other auxiliary functions. When offline
datasets are large, the scalability of prior approaches comes at the expense of
expressivity and inference quality. We propose JUMBO, an MBO algorithm that
sidesteps these limitations by querying additional data based on a combination
of acquisition signals derived from training two Gaussian Processes (GP): a
cold-GP operating directly in the input domain and a warm-GP that operates in
the feature space of a deep neural network pretrained using the offline data.
Such a decomposition can dynamically control the reliability of information
derived from the online and offline data and the use of pretrained neural
networks permits scalability to large offline datasets. Theoretically, we
derive regret bounds for JUMBO and show that it achieves no-regret under
conditions analogous to GP-UCB (Srinivas et. al. 2010). Empirically, we
demonstrate significant performance improvements over existing approaches on
two real-world optimization problems: hyper-parameter optimization and
automated circuit design.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00943</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00943</id><submitter>Xinyi Zhang</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:09:09 GMT</date><size>2497kb</size><source_type>D</source_type></version><title>A Topological Solution of Entanglement for Complex-shaped Parts in
  Robotic Bin-picking</title><authors>Xinyi Zhang, Keisuke Koyama, Yukiyasu Domae, Weiwei Wan and Kensuke
  Harada</authors><categories>cs.RO</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of picking up only one object at a time
avoiding any entanglement in bin-picking. To cope with a difficult case where
the complex-shaped objects are heavily entangled together, we propose a
topology-based method that can generate non-tangle grasp positions on a single
depth image. The core technique is entanglement map, which is a feature map to
measure the entanglement possibilities obtained from the input image. We use
the entanglement map to select probable regions containing graspable objects.
The optimum grasping pose is detected from the selected regions considering the
collision between robot hand and objects. Experimental results show that our
analytic method provides a more comprehensive and intuitive observation of
entanglement and exceeds previous learning-based work in success rates.
Especially, our topology-based method does not rely on any object models or
time-consuming training process, so that it can be easily adapted to more
complex bin-picking scenes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00944</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00944</id><submitter>Timm Faulwasser</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:09:18 GMT</date><size>257kb</size><source_type>D</source_type></version><title>Teaching MPC: Which Way to the Promised Land?</title><authors>Timm Faulwasser, Sergio Lucia, Moritz Schulze Darup, Martin
  M\&quot;onnigmann</authors><categories>math.OC cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the earliest conceptualizations by Lee and Markus, and Propoi in the
1960s, Model Predictive Control (MPC) has become a major success story of
systems and control with respect to industrial impact and with respect to
continued and wide-spread research interest. The field has evolved from
conceptually simple linear-quadratic (convex) settings in discrete and
continuous time to nonlinear and distributed settings including hybrid,
stochastic, and infinite-dimensional systems. Put differently, essentially the
entire spectrum of dynamic systems can be considered in the MPC framework with
respect to both -- system theoretic analysis and tailored numerics. Moreover,
recent developments in machine learning also leverage MPC concepts and
learning-based and data-driven MPC have become highly active research areas.
  However, this evident and continued success renders it increasingly complex
to live up to industrial expectations while enabling graduate students for
state-of-the-art research in teaching MPC. Hence, this position paper attempts
to trigger a discussion on teaching MPC. To lay the basis for a fruitful
debate, we subsequently investigate the prospect of covering MPC in
undergraduate courses; we comment on teaching textbooks; and we discuss the
increasing complexity of research-oriented graduate teaching of~MPC.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00948</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00948</id><submitter>Keyang Xu</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:21:25 GMT</date><size>378kb</size><source_type>D</source_type></version><title>Unsupervised Out-of-Domain Detection via Pre-trained Transformers</title><authors>Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng and Caiming Xiong</authors><categories>cs.CL cs.LG</categories><comments>Accepted by ACL 2021. Code is available at
  https://github.com/rivercold/BERT-unsupervised-OOD</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployed real-world machine learning applications are often subject to
uncontrolled and even potentially malicious inputs. Those out-of-domain inputs
can lead to unpredictable outputs and sometimes catastrophic safety issues.
Prior studies on out-of-domain detection require in-domain task labels and are
limited to supervised classification scenarios. Our work tackles the problem of
detecting out-of-domain samples with only unsupervised in-domain data. We
utilize the latent representations of pre-trained transformers and propose a
simple yet effective method to transform features across all layers to
construct out-of-domain detectors efficiently. Two domain-specific fine-tuning
approaches are further proposed to boost detection accuracy. Our empirical
evaluations of related methods on two datasets validate that our method greatly
improves out-of-domain detection ability in a more general scenario.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00949</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00949</id><submitter>Hiroshi Sato</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:31:45 GMT</date><size>66kb</size><source_type>D</source_type></version><title>Should We Always Separate?: Switching Between Enhanced and Observed
  Signals for Overlapping Speech Recognition</title><authors>Hiroshi Sato, Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita,
  Takafumi Moriya, Naoyuki Kamo</authors><categories>eess.AS cs.SD</categories><comments>5 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Although recent advances in deep learning technology improved automatic
speech recognition (ASR), it remains difficult to recognize speech when it
overlaps other people's voices. Speech separation or extraction is often used
as a front-end to ASR to handle such overlapping speech. However, deep neural
network-based speech enhancement can generate `processing artifacts' as a side
effect of the enhancement, which degrades ASR performance. For example, it is
well known that single-channel noise reduction for non-speech noise
(non-overlapping speech) often does not improve ASR. Likewise, the processing
artifacts may also be detrimental to ASR in some conditions when processing
overlapping speech with a separation/extraction method, although it is usually
believed that separation/extraction improves ASR. In order to answer the
question `Do we always have to separate/extract speech from mixtures?', we
analyze ASR performance on observed and enhanced speech at various noise and
interference conditions, and show that speech enhancement degrades ASR under
some conditions even for overlapping speech. Based on these findings, we
propose a simple switching algorithm between observed and enhanced speech based
on the estimated signal-to-interference ratio and signal-to-noise ratio. We
demonstrated experimentally that such a simple switching mechanism can improve
recognition performance when processing artifacts are detrimental to ASR.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00950</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00950</id><submitter>Canasai Kruengkrai</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:40:12 GMT</date><size>478kb</size><source_type>D</source_type></version><title>A Multi-Level Attention Model for Evidence-Based Fact Checking</title><authors>Canasai Kruengkrai, Junichi Yamagishi, Xin Wang</authors><categories>cs.CL</categories><comments>Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidence-based fact checking aims to verify the truthfulness of a claim
against evidence extracted from textual sources. Learning a representation that
effectively captures relations between a claim and evidence can be challenging.
Recent state-of-the-art approaches have developed increasingly sophisticated
models based on graph structures. We present a simple model that can be trained
on sequence structures. Our model enables inter-sentence attentions at
different levels and can benefit from joint training. Results on a large-scale
dataset for Fact Extraction and VERification (FEVER) show that our model
outperforms the graph-based approaches and yields 1.09% and 1.42% improvements
in label accuracy and FEVER score, respectively, over the best published model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00951</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00951</id><submitter>Minh Hoang Trinh</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:40:16 GMT</date><size>977kb</size><source_type>D</source_type></version><title>Finite-time bearing-only maneuver of acyclic leader-follower formations</title><authors>Minh Hoang Trinh and Hyo-Sung Ahn</authors><categories>math.OC cs.SY eess.SY</categories><comments>6 pages, 2 figures, preprint, accepted to L-CSS</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This letter proposes two finite-time bearing-based control laws for acyclic
leader-follower formations. The leaders in formation move with a bounded
continuous reference velocity and each follower controls its position with
regard to three agents in the formation. The first control law uses only
bearing vectors, and finite-time convergence is achieved by properly selecting
two state-dependent control gains. The second control law requires both bearing
vectors and communications between agents. Each agent simultaneously localizes
and follows a virtual target. Finite-time convergence of the desired formation
under both control laws is proved by mathematical induction and supported by
numerical simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00952</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00952</id><submitter>Tuan-Anh Nguyen Dang</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:42:51 GMT</date><size>7105kb</size><source_type>D</source_type></version><title>End-to-End Information Extraction by Character-Level Embedding and
  Multi-Stage Attentional U-Net</title><authors>Tuan-Anh Nguyen Dang and Dat-Thanh Nguyen</authors><categories>cs.CV cs.LG eess.IV</categories><comments>Accepted to BMVC 2019</comments><journal-ref>30th British Machine Vision Conference (BMVC) 2019</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Information extraction from document images has received a lot of attention
recently, due to the need for digitizing a large volume of unstructured
documents such as invoices, receipts, bank transfers, etc. In this paper, we
propose a novel deep learning architecture for end-to-end information
extraction on the 2D character-grid embedding of the document, namely the
\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and
spatial relations between 2D elements, our model leverages a specialized
multi-stage encoder-decoders design, in conjunction with efficient uses of the
self-attention mechanism and the box convolution. Experimental results on
different datasets show that our model outperforms the baseline U-Net
architecture by a large margin while using 40\% fewer parameters. Moreover, it
also significantly improved the baseline in erroneous OCR and limited training
data scenario, thus becomes practical for real-world applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00953</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00953</id><submitter>Zhongjian Wang</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:45:15 GMT</date><size>1580kb</size></version><title>Convergence analysis of a Lagrangian numerical scheme in computing
  effective diffusivity of 3D time-dependent flows</title><authors>Zhongjian Wang, Jack Xin, Zhiwen Zhang</authors><categories>math.NA cs.NA</categories><comments>arXiv admin note: text overlap with arXiv:1808.06309</comments><msc-class>35B27, 37A30, 60H35, 65M12, 65M75</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the convergence analysis for a robust stochastic
structure-preserving Lagrangian numerical scheme in computing effective
diffusivity of time-dependent chaotic flows, which are modeled by stochastic
differential equations (SDEs). Our numerical scheme is based on a splitting
method to solve the corresponding SDEs in which the deterministic subproblem is
discretized using structure-preserving schemes while the random subproblem is
discretized using the Euler-Maruyama scheme. We obtain a sharp and
uniform-in-time convergence analysis for the proposed numerical scheme that
allows us to accurately compute long-time solutions of the SDEs. As such, we
can compute the effective diffusivity for time-dependent flows. Finally, we
present numerical results to demonstrate the accuracy and efficiency of the
proposed method in computing effective diffusivity for the time-dependent
Arnold-Beltrami-Childress (ABC) flow and Kolmogorov flow in three-dimensional
space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00954</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00954</id><submitter>Zhe Liu</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:45:42 GMT</date><size>689kb</size><source_type>D</source_type></version><title>When and Why does a Model Fail? A Human-in-the-loop Error Detection
  Framework for Sentiment Analysis</title><authors>Zhe Liu, Yufan Guo, Jalal Mahmud</authors><categories>cs.CL cs.HC</categories><comments>NAACL2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Although deep neural networks have been widely employed and proven effective
in sentiment analysis tasks, it remains challenging for model developers to
assess their models for erroneous predictions that might exist prior to
deployment. Once deployed, emergent errors can be hard to identify in
prediction run-time and impossible to trace back to their sources. To address
such gaps, in this paper we propose an error detection framework for sentiment
analysis based on explainable features. We perform global-level feature
validation with human-in-the-loop assessment, followed by an integration of
global and local-level feature contribution analysis. Experimental results show
that, given limited human-in-the-loop intervention, our method is able to
identify erroneous model predictions on unseen data with high precision.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00955</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00955</id><submitter>Luca Soldaini</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:45:49 GMT</date><size>31kb</size></version><title>Answer Generation for Retrieval-based Question Answering Systems</title><authors>Chao-Chun Hsu, Eric Lind, Luca Soldaini, Alessandro Moschitti</authors><categories>cs.CL</categories><comments>Short paper, Accepted at Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in transformer-based models have greatly improved the
ability of Question Answering (QA) systems to provide correct answers; in
particular, answer sentence selection (AS2) models, core components of
retrieval-based systems, have achieved impressive results. While generally
effective, these models fail to provide a satisfying answer when all retrieved
candidates are of poor quality, even if they contain correct information. In
AS2, models are trained to select the best answer sentence among a set of
candidates retrieved for a given question. In this work, we propose to generate
answers from a set of AS2 top candidates. Rather than selecting the best
candidate, we train a sequence to sequence transformer model to generate an
answer from a candidate set. Our tests on three English AS2 datasets show
improvement up to 32 absolute points in accuracy over the state of the art.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00957</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00957</id><submitter>Junwei Bao Doctor</submitter><version version="v1"><date>Wed, 2 Jun 2021 05:46:01 GMT</date><size>860kb</size><source_type>D</source_type></version><title>RevCore: Review-augmented Conversational Recommendation</title><authors>Yu Lu, Junwei Bao, Yan Song, Zichen Ma, Shuguang Cui, Youzheng Wu, and
  Xiaodong He</authors><categories>cs.CL</categories><comments>Accepted by ACL-Findings 2021. 13 pages, 3 figures, and 10 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Existing conversational recommendation (CR) systems usually suffer from
insufficient item information when conducted on short dialogue history and
unfamiliar items. Incorporating external information (e.g., reviews) is a
potential solution to alleviate this problem. Given that reviews often provide
a rich and detailed user experience on different interests, they are potential
ideal resources for providing high-quality recommendations within an
informative conversation. In this paper, we design a novel end-to-end
framework, namely, Review-augmented Conversational Recommender (RevCore), where
reviews are seamlessly incorporated to enrich item information and assist in
generating both coherent and informative responses. In detail, we extract
sentiment-consistent reviews, perform review-enriched and entity-based
recommendations for item suggestions, as well as use a review-attentive
encoder-decoder for response generation. Experimental results demonstrate the
superiority of our approach in yielding better performance on both
recommendation and conversation responding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00958</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00958</id><submitter>Diogo Almeida</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:03:18 GMT</date><size>545kb</size><source_type>D</source_type></version><title>A Generalizable Approach to Learning Optimizers</title><authors>Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba</authors><categories>cs.LG cs.AI cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A core issue with learning to optimize neural networks has been the lack of
generalization to real world problems. To address this, we describe a system
designed from a generalization-first perspective, learning to update optimizer
hyperparameters instead of model parameters directly using novel features,
actions, and a reward function. This system outperforms Adam at all neural
network tasks including on modalities not seen during training. We achieve 2x
speedups on ImageNet, and a 2.5x speedup on a language modeling task using over
5 orders of magnitude more compute than the training tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00961</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00961</id><submitter>Hojin Lee</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:06:17 GMT</date><size>208kb</size><source_type>D</source_type></version><title>Distributed Control-Estimation Synthesis for Stochastic Multi-Agent
  Systems via Virtual Interaction between Non-neighboring Agents</title><authors>Hojin Lee, Cheolhyeon Kwon</authors><categories>eess.SY cs.SY</categories><comments>13 pages, 5 figures, to be published in IEEE/Control Systems Letters
  (L-CSS) 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the optimal distributed control problem for a linear
stochastic multi-agent system (MAS). Due to the distributed nature of MAS
network, the information available to an individual agent is limited to its
vicinity. From the entire MAS aspect, this imposes the structural constraint on
the control law, making the optimal control law computationally intractable.
This paper attempts to relax such a structural constraint by expanding the
neighboring information for each agent to the entire MAS, enabled by the
distributed estimation algorithm embedded in each agent. By exploiting the
estimated information, each agent is not limited to interact with its
neighborhood but further establishing the `virtual interactions' with the
non-neighboring agents. Then the optimal distributed MAS control problem is
cast as a synthesized control-estimation problem. An iterative optimization
procedure is developed to find the control-estimation law, minimizing the
global objective cost of MAS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00962</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00962</id><submitter>Michael Ruderman</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:07:20 GMT</date><size>1400kb</size></version><title>Convergent dynamics of optimal nonlinear damping control</title><authors>Michael Ruderman</authors><categories>eess.SY cs.SY math.OC</categories><comments>4 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Following Demidovich's concept and definition of convergent systems, we
analyze the optimal nonlinear damping control, recently proposed [1] for the
second-order systems. Targeting the problem of output regulation,
correspondingly tracking of $\mathcal{C}^1$-trajectories, it is shown that all
solutions of the control system are globally uniformly asymptotically stable.
The existence of the unique limit solution in the origin of the control error
and its time derivative coordinates are shown in the sense of Demidovich's
convergent dynamics. Explanative numerical examples are also provided along
with analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00965</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00965</id><submitter>Marc Zeller</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:15:58 GMT</date><size>2631kb</size></version><title>ALFRED: a methodology to enable component fault trees for layered
  architectures</title><authors>Kai Hoefig, Marc Zeller, Reiner Heilmann</authors><categories>cs.SE</categories><journal-ref>2015 41st Euromicro Conference on Software Engineering and
  Advanced Applications</journal-ref><doi>10.1109/SEAA.2015.26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying drawbacks or insufficiencies in terms of safety is important also
in early development stages of safety critical systems. In industry,
development artefacts such as components or units, are often reused from
existing artefacts to save time and costs. When development artefacts are
reused, their existing safety analysis models are an important input for an
early safety assessment for the new system, since they already provide a valid
model. Component fault trees support such reuse strategies by a compositional
horizontal approach. But current development strategies do not only divide
systems horizontally, e.g., By encapsulating different functionality into
separate components and hierarchies of components, but also vertically, e.g.
Into software and hardware architecture layers. Current safety analysis
methodologies, such as component fault trees, do not support such vertical
layers. Therefore, we present here a methodology that is able to divide safety
analysis models into different layers of a systems architecture. We use so
called Architecture Layer Failure Dependencies to enable component fault trees
on different layers of an architecture. These dependencies are then used to
generate safety evidence for the entire system and over all different
architecture layers. A case study applies the approach to hardware and software
layers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00966</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00966</id><submitter>Oded Padon</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:26:29 GMT</date><size>273kb</size><source_type>D</source_type></version><title>Temporal Prophecy for Proving Temporal Properties of Infinite-State
  Systems</title><authors>Oded Padon, Jochen Hoenicke, Kenneth L. McMillan, Andreas Podelski,
  Mooly Sagiv, Sharon Shoham</authors><categories>cs.LO</categories><comments>11 pages, presented at FMCAD 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various verification techniques for temporal properties transform temporal
verification to safety verification. For infinite-state systems, these
transformations are inherently imprecise. That is, for some instances, the
temporal property holds, but the resulting safety property does not. This paper
introduces a mechanism for tackling this imprecision. This mechanism, which we
call temporal prophecy, is inspired by prophecy variables. Temporal prophecy
refines an infinite-state system using first-order linear temporal logic
formulas, via a suitable tableau construction. For a specific
liveness-to-safety transformation based on first-order logic, we show that
using temporal prophecy strictly increases the precision. Furthermore, temporal
prophecy leads to robustness of the proof method, which is manifested by a cut
elimination theorem. We integrate our approach into the Ivy deductive
verification system, and show that it can handle challenging temporal
verification examples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00967</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00967</id><submitter>Truong Son Hy</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:28:47 GMT</date><size>13323kb</size><source_type>D</source_type></version><title>Multiresolution Graph Variational Autoencoder</title><authors>Truong Son Hy and Risi Kondor</authors><categories>cs.LG cs.SI physics.chem-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose Multiresolution Graph Networks (MGN) and
Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate
graphs in a multiresolution and equivariant manner. At each resolution level,
MGN employs higher order message passing to encode the graph while learning to
partition it into mutually exclusive clusters and coarsening into a lower
resolution. MGVAE constructs a hierarchical generative model based on MGN to
variationally autoencode the hierarchy of coarsened graphs. Our proposed
framework is end-to-end permutation equivariant with respect to node ordering.
Our methods have been successful with several generative tasks including link
prediction on citation graphs, unsupervised molecular representation learning
to predict molecular properties, molecular generation, general graph generation
and graph-based image generation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00969</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00969</id><submitter>Shikhar Singh</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:31:55 GMT</date><size>2699kb</size><source_type>D</source_type></version><title>COM2SENSE: A Commonsense Reasoning Benchmark with Complementary
  Sentences</title><authors>Shikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormolabashi, Te-Lin Wu,
  Xuezhe Ma, Nanyun Peng</authors><categories>cs.CL cs.AI</categories><comments>In Proceedings of Findings of the Association for Computational
  Linguistics: ACL 2021 (ACL-Findings). Contains 16 pages, 14 figures and 11
  tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commonsense reasoning is intuitive for humans but has been a long-term
challenge for artificial intelligence (AI). Recent advancements in pretrained
language models have shown promising results on several commonsense benchmark
datasets. However, the reliability and comprehensiveness of these benchmarks
towards assessing model's commonsense reasoning ability remains unclear. To
this end, we introduce a new commonsense reasoning benchmark dataset comprising
natural language true/false statements, with each sample paired with its
complementary counterpart, resulting in 4k sentence pairs. We propose a
pairwise accuracy metric to reliably measure an agent's ability to perform
commonsense reasoning over a given situation. The dataset is crowdsourced and
enhanced with an adversarial model-in-the-loop setup to incentivize challenging
samples. To facilitate a systematic analysis of commonsense capabilities, we
design our dataset along the dimensions of knowledge domains, reasoning
scenarios and numeracy. Experimental results demonstrate that our strongest
baseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and
~51% pairwise accuracy, well below human performance (~95% for both metrics).
The dataset is available at https://github.com/PlusLabNLP/Com2Sense.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00974</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00974</id><submitter>Marc Zeller</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:41:20 GMT</date><size>850kb</size></version><title>Meta model application for consistency management of models for avionic
  systems design</title><authors>Jef Stegen, Stefan Dutre, Joe Zhensheng Guo, Marc Zeller, Stefan
  Rothbauer</authors><categories>cs.SE</categories><journal-ref>2019 ACM/IEEE 22nd International Conference on Model Driven
  Engineering Languages and Systems Companion (MODELS-C)</journal-ref><doi>10.1109/MODELS-C.2019.00125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the application of a meta model and single underlying
model on an applied avionics system design use case. System models, safety
assurance cases and safety requirements are maintained in a central repository.
This enables to link these data which are originally developed in unrelated
tools. By having such a central repository, traceability can be established,
and consistency can be ensured, which leads to less errors and a shorter
development time. A meta model was constructed which matches the central
repository to enable bidirectional synchronization with an external authoring
tool.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00976</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00976</id><submitter>Xin Liu</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:49:19 GMT</date><size>945kb</size><source_type>D</source_type></version><title>Exploring Discourse Structures for Argument Impact Classification</title><authors>Xin Liu, Jiefu Ou, Yangqiu Song, Xin Jiang</authors><categories>cs.CL</categories><comments>Accepted by ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discourse relations among arguments reveal logical structures of a debate
conversation. However, no prior work has explicitly studied how the sequence of
discourse relations influence a claim's impact. This paper empirically shows
that the discourse relations between two arguments along the context path are
essential factors for identifying the persuasive power of an argument. We
further propose DisCOC to inject and fuse the sentence-level structural
discourse information with contextualized features derived from large-scale
language models. Experimental results and extensive analysis show that the
attention and gate mechanisms that explicitly model contexts and texts can
indeed help the argument impact classification task defined by Durmus et al.
(2019), and discourse structures among the context path of the claim to be
classified can further boost the performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00978</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00978</id><submitter>Tuan-Anh Nguyen Dang</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:50:04 GMT</date><size>1480kb</size><source_type>D</source_type></version><title>A Span Extraction Approach for Information Extraction on Visually-Rich
  Documents</title><authors>Tuan-Anh D. Nguyen, Hieu M. Vu, Nguyen Hong Son, Minh-Tien Nguyen</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Information extraction (IE) from visually-rich documents (VRDs) has achieved
SOTA performance recently thanks to the adaptation of Transformer-based
language models, which demonstrates great potential of pre-training methods. In
this paper, we present a new approach to improve the capability of language
model pre-training on VRDs. Firstly, we introduce a new IE model that is
query-based and employs the span extraction formulation instead of the commonly
used sequence labelling approach. Secondly, to further extend the span
extraction formulation, we propose a new training task which focuses on
modelling the relationships between semantic entities within a document. This
task enables the spans to be extracted recursively and can be used as both a
pre-training objective as well as an IE downstream task. Evaluation on various
datasets of popular business documents (invoices, receipts) shows that our
proposed method can improve the performance of existing models significantly,
while providing a mechanism to accumulate model knowledge from multiple
downstream IE tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00980</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00980</id><submitter>Tuan-Anh Nguyen Dang</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:51:35 GMT</date><size>8219kb</size><source_type>D</source_type></version><title>End-to-End Hierarchical Relation Extraction for Generic Form
  Understanding</title><authors>Tuan-Anh Nguyen Dang, Duc-Thanh Hoang, Quang-Bach Tran, Chih-Wei Pan,
  Thanh-Dat Nguyen</authors><categories>cs.AI</categories><comments>Accepted to ICPR 2020</comments><journal-ref>2020 25th International Conference on Pattern Recognition (ICPR)</journal-ref><doi>10.1109/ICPR48806.2021.9412778</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Form understanding is a challenging problem which aims to recognize semantic
entities from the input document and their hierarchical relations. Previous
approaches face significant difficulty dealing with the complexity of the task,
thus treat these objectives separately. To this end, we present a novel deep
neural network to jointly perform both entity detection and link prediction in
an end-to-end fashion. Our model extends the Multi-stage Attentional U-Net
architecture with the Part-Intensity Fields and Part-Association Fields for
link prediction, enriching the spatial information flow with the additional
supervision from entity linking. We demonstrate the effectiveness of the model
on the Form Understanding in Noisy Scanned Documents (FUNSD) dataset, where our
method substantially outperforms the original model and state-of-the-art
baselines in both Entity Labeling and Entity Linking task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00982</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00982</id><submitter>Yongxing Wang</submitter><version version="v1"><date>Wed, 2 Jun 2021 06:56:08 GMT</date><size>15130kb</size><source_type>D</source_type></version><title>A monolithic one-velocity-field optimal control formulation for
  fluid-structure interaction problems with large solid deformation</title><authors>Yongxing Wang</authors><categories>cs.CE math.OC</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this article, we formulate a monolithic optimal control method for general
time-dependent Fluid-Structure Interaction (FSI) systems with large solid
deformation. We consider a displacement-tracking type of objective with a
constraint of the solid velocity, tackle the time-dependent control problems by
a piecewise-in-time control, cope with large solid displacement using a
one-velocity fictitious domain method, and solve the fully-coupled FSI and the
corresponding adjoint equations in a monolithic manner. We implement the
proposed method in the open-source software package FreeFEM++ and assess it by
three numerical experiments, in the aspects of stability of the numerical
scheme for different regularisation parameters, and efficiency of reducing the
objective function with control of the solid velocity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00984</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00984</id><submitter>Guoxian Yu</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:03:54 GMT</date><size>553kb</size><source_type>D</source_type></version><title>Few-Shot Partial-Label Learning</title><authors>Yunfeng Zhao, Guoxian Yu, Lei Liu, Zhongmin Yan, Lizhen Cui and
  Carlotta Domeniconi</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted by International Joint Conference on Artificial Intelligence
  (IJCAI2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial-label learning (PLL) generally focuses on inducing a noise-tolerant
multi-class classifier by training on overly-annotated samples, each of which
is annotated with a set of labels, but only one is the valid label. A basic
promise of existing PLL solutions is that there are sufficient partial-label
(PL) samples for training. However, it is more common than not to have just few
PL samples at hand when dealing with new tasks. Furthermore, existing few-shot
learning algorithms assume precise labels of the support set; as such,
irrelevant labels may seriously mislead the meta-learner and thus lead to a
compromised performance. How to enable PLL under a few-shot learning setting is
an important problem, but not yet well studied. In this paper, we introduce an
approach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distance
metric learning by an embedding network and rectifying prototypes on the tasks
previously encountered. Next, it calculates the prototype of each class of a
new task in the embedding network. An unseen example can then be classified via
its distance to each prototype. Experimental results on widely-used few-shot
datasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve a
superior performance than the state-of-the-art methods across different
settings, and it needs fewer samples for quickly adapting to new tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00985</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00985</id><submitter>Qinyan Dai</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:05:17 GMT</date><size>12732kb</size><source_type>D</source_type></version><title>Feedback Network for Mutually Boosted Stereo Image Super-Resolution and
  Disparity Estimation</title><authors>Qinyan Dai, Juncheng Li, Qiaosi Yi, Faming Fang and Guixu Zhang</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Under stereo settings, the problem of image super-resolution (SR) and
disparity estimation are interrelated that the result of each problem could
help to solve the other. The effective exploitation of correspondence between
different views facilitates the SR performance, while the high-resolution (HR)
features with richer details benefit the correspondence estimation. According
to this motivation, we propose a Stereo Super-Resolution and Disparity
Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the
stereo image super-resolution and disparity estimation in a unified framework
and interact them with each other to further improve their performance.
Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for
left and right views. Besides the cross-view information exploitation in the
low-resolution (LR) space, HR representations produced by the SR process are
utilized to perform HR disparity estimation with higher accuracy, through which
the HR features can be aggregated to generate a finer SR result. Afterward, the
proposed HR Disparity Information Feedback (HRDIF) mechanism delivers
information carried by HR disparity back to previous layers to further refine
the SR image reconstruction. Extensive experiments demonstrate the
effectiveness and advancement of SSRDE-FNet.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00988</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00988</id><submitter>Bogdan Trasnea</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:10:54 GMT</date><size>14683kb</size><source_type>D</source_type></version><title>OctoPath: An OcTree Based Self-Supervised Learning Approach to Local
  Trajectory Planning for Mobile Robots</title><authors>Bogdan Trasnea, Cosmin Ginerica, Mihai Zaha, Gigel Macesanu, Claudiu
  Pozna, Sorin Grigorescu</authors><categories>cs.RO cs.AI</categories><journal-ref>Sensors 2021, 21(11), 3606</journal-ref><doi>10.3390/s21113606</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Autonomous mobile robots are usually faced with challenging situations when
driving in complex environments. Namely, they have to recognize the static and
dynamic obstacles, plan the driving path and execute their motion. For
addressing the issue of perception and path planning, in this paper, we
introduce OctoPath , which is an encoder-decoder deep neural network, trained
in a self-supervised manner to predict the local optimal trajectory for the
ego-vehicle. Using the discretization provided by a 3D octree environment
model, our approach reformulates trajectory prediction as a classification
problem with a configurable resolution. During training, OctoPath minimizes the
error between the predicted and the manually driven trajectories in a given
training dataset. This allows us to avoid the pitfall of regression-based
trajectory estimation, in which there is an infinite state space for the output
trajectory points. Environment sensing is performed using a 40-channel
mechanical LiDAR sensor, fused with an inertial measurement unit and wheels
odometry for state estimation. The experiments are performed both in simulation
and real-life, using our own developed GridSim simulator and RovisLab's
Autonomous Mobile Test Unit platform. We evaluate the predictions of OctoPath
in different driving scenarios, both indoor and outdoor, while benchmarking our
system against a baseline hybrid A-Star algorithm and a regression-based
supervised learning method, as well as against a CNN learning-based optimal
path planning method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00990</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00990</id><submitter>Shih-Hung Tsai</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:15:06 GMT</date><size>144kb</size><source_type>D</source_type></version><title>Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving</title><authors>Shih-hung Tsai, Chao-Chun Liang, Hsin-Min Wang, Keh-Yih Su</authors><categories>cs.AI</categories><comments>ACL2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the recent advancements in deep learning, neural solvers have gained
promising results in solving math word problems. However, these SOTA solvers
only generate binary expression trees that contain basic arithmetic operators
and do not explicitly use the math formulas. As a result, the expression trees
they produce are lengthy and uninterpretable because they need to use multiple
operators and constants to represent one single formula. In this paper, we
propose sequence-to-general tree (S2G) that learns to generate interpretable
and executable operation trees where the nodes can be formulas with an
arbitrary number of arguments. With nodes now allowed to be formulas, S2G can
learn to incorporate mathematical domain knowledge into problem-solving, making
the results more interpretable. Experiments show that S2G can achieve a better
performance against strong baselines on problems that require domain knowledge.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00992</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00992</id><submitter>Bac Nguyen</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:19:58 GMT</date><size>1041kb</size><source_type>D</source_type></version><title>NVC-Net: End-to-End Adversarial Voice Conversion</title><authors>Bac Nguyen and Fabien Cardinaux</authors><categories>cs.SD cs.AI eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice conversion has gained increasing popularity in many applications of
speech synthesis. The idea is to change the voice identity from one speaker
into another while keeping the linguistic content unchanged. Many voice
conversion approaches rely on the use of a vocoder to reconstruct the speech
from acoustic features, and as a consequence, the speech quality heavily
depends on such a vocoder. In this paper, we propose NVC-Net, an end-to-end
adversarial network, which performs voice conversion directly on the raw audio
waveform of arbitrary length. By disentangling the speaker identity from the
speech content, NVC-Net is able to perform non-parallel traditional
many-to-many voice conversion as well as zero-shot voice conversion from a
short utterance of an unseen target speaker. Importantly, NVC-Net is
non-autoregressive and fully convolutional, achieving fast inference. Our model
is capable of producing samples at a rate of more than 3600 kHz on an NVIDIA
V100 GPU, being orders of magnitude faster than state-of-the-art methods under
the same hardware configurations. Objective and subjective evaluations on
non-parallel many-to-many voice conversion tasks show that NVC-Net obtains
competitive results with significantly fewer parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00993</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00993</id><submitter>Jiawei Huang</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:26:29 GMT</date><size>757kb</size></version><title>On the Convergence Rate of Off-Policy Policy Optimization Methods with
  Density-Ratio Correction</title><authors>Jiawei Huang, Nan Jiang</authors><categories>cs.LG cs.AI</categories><comments>47 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the convergence properties of off-policy policy
improvement algorithms with state-action density ratio correction under
function approximation setting, where the objective function is formulated as a
max-max-min optimization problem. We characterize the bias of the learning
objective and present two strategies with finite-time convergence guarantees.
In our first strategy, we present algorithm P-SREDA with convergence rate
$O(\epsilon^{-3})$, whose dependency on $\epsilon$ is optimal. In our second
strategy, we propose a new off-policy actor-critic style algorithm named
O-SPIM. We prove that O-SPIM converges to a stationary point with total
complexity $O(\epsilon^{-4})$, which matches the convergence rate of some
recent actor-critic algorithms in the on-policy setting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00995</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00995</id><submitter>Mounssif Krouka</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:36:27 GMT</date><size>6499kb</size><source_type>D</source_type></version><title>Energy-Efficient Model Compression and Splitting for Collaborative
  Inference Over Time-Varying Channels</title><authors>Mounssif Krouka, Anis Elgabli, Chaouki Ben Issaid and Mehdi Bennis</authors><categories>cs.LG cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Today's intelligent applications can achieve high performance accuracy using
machine learning (ML) techniques, such as deep neural networks (DNNs).
Traditionally, in a remote DNN inference problem, an edge device transmits raw
data to a remote node that performs the inference task. However, this may incur
high transmission energy costs and puts data privacy at risk. In this paper, we
propose a technique to reduce the total energy bill at the edge device by
utilizing model compression and time-varying model split between the edge and
remote nodes. The time-varying representation accounts for time-varying
channels and can significantly reduce the total energy at the edge device while
maintaining high accuracy (low loss). We implement our approach in an image
classification task using the MNIST dataset, and the system environment is
simulated as a trajectory navigation scenario to emulate different channel
conditions. Numerical simulations show that our proposed solution results in
minimal energy consumption and $CO_2$ emission compared to the considered
baselines while exhibiting robust performance across different channel
conditions and bandwidth regime choices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00997</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00997</id><submitter>Changhee Han Dr.</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:46:02 GMT</date><size>426kb</size></version><title>Tips and Tricks to Improve CNN-based Chest X-ray Diagnosis: A Survey</title><authors>Changhee Han, Takayuki Okamoto, Koichi Takeuchi, Dimitris Katsios,
  Andrey Grushnikov, Masaaki Kobayashi, Antoine Choppin, Yutaka Kurashina, Yuki
  Shimahara</authors><categories>eess.IV cs.CV</categories><comments>7 pages, 2 figures, to be published in Japanese Journal of Medical
  Imaging and Information Sciences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) intrinsically requires large-scale data
whereas Chest X-Ray (CXR) images tend to be data/annotation-scarce, leading to
over-fitting. Therefore, based on our development experience and related work,
this paper thoroughly introduces tricks to improve generalization in the CXR
diagnosis: how to (i) leverage additional data, (ii) augment/distillate data,
(iii) regularize training, and (iv) conduct efficient segmentation. As a
development example based on such optimization techniques, we also feature
LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved
radiologists/non-radiologists' nodule detection sensitivity by 0.100/0.131,
respectively, while maintaining specificity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.00999</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.00999</id><submitter>Mounssif Krouka</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:49:41 GMT</date><size>15802kb</size><source_type>D</source_type></version><title>Communication-Efficient Split Learning Based on Analog Communication and
  Over the Air Aggregation</title><authors>Mounssif Krouka, Anis Elgabli, Chaouki ben Issaid, and Mehdi Bennis</authors><categories>cs.LG cs.DC cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Split-learning (SL) has recently gained popularity due to its inherent
privacy-preserving capabilities and ability to enable collaborative inference
for devices with limited computational power. Standard SL algorithms assume an
ideal underlying digital communication system and ignore the problem of scarce
communication bandwidth. However, for a large number of agents, limited
bandwidth resources, and time-varying communication channels, the communication
bandwidth can become the bottleneck. To address this challenge, in this work,
we propose a novel SL framework to solve the remote inference problem that
introduces an additional layer at the agent side and constrains the choices of
the weights and the biases to ensure over the air aggregation. Hence, the
proposed approach maintains constant communication cost with respect to the
number of agents enabling remote inference under limited bandwidth. Numerical
results show that our proposed algorithm significantly outperforms the digital
implementation in terms of communication-efficiency, especially as the number
of agents grows large.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01000</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01000</id><submitter>Simon Praetorius</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:52:27 GMT</date><size>492kb</size><source_type>D</source_type></version><title>Tangential Errors of Tensor Surface Finite Elements</title><authors>Hanne Hardering and Simon Praetorius</authors><categories>math.NA cs.NA</categories><comments>42 pages</comments><msc-class>65N30, 65N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discretize a tangential tensor field equation using a surface-finite
element approach with a penalization term to ensure almost tangentiality. It is
natural to measure the quality of such a discretization intrinsically, i.e., to
examine the tangential convergence behavior in contrast to the normal behavior.
We show optimal order convergence with respect to the tangential quantities in
particular for an isogeometric penalization term that is based only on the
geometric information of the discrete surface.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01001</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01001</id><submitter>Nicolas Vecoven</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:53:54 GMT</date><size>510kb</size><source_type>D</source_type></version><title>Warming-up recurrent neural networks to maximize reachable
  multi-stability greatly improves learning</title><authors>Nicolas Vecoven and Damien Ernst and Guillaume Drion</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Training recurrent neural networks is known to be difficult when time
dependencies become long. Consequently, training standard gated cells such as
gated recurrent units and long-short term memory on benchmarks where long-term
memory is required remains an arduous task. In this work, we propose a general
way to initialize any recurrent network connectivity through a process called
&quot;warm-up&quot; to improve its capability to learn arbitrarily long time
dependencies. This initialization process is designed to maximize network
reachable multi-stability, i.e. the number of attractors within the network
that can be reached through relevant input trajectories. Warming-up is
performed before training, using stochastic gradient descent on a specifically
designed loss. We show that warming-up greatly improves recurrent neural
network performance on long-term memory benchmarks for multiple recurrent cell
types, but can sometimes impede precision. We therefore introduce a parallel
recurrent network structure with partial warm-up that is shown to greatly
improve learning on long time-series while maintaining high levels of
precision. This approach provides a general framework for improving learning
abilities of any recurrent cell type when long-term memory is required.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01006</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01006</id><submitter>Liang Qiu</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:07:42 GMT</date><size>5883kb</size><source_type>D</source_type></version><title>SocAoG: Incremental Graph Parsing for Social Relation Inference in
  Dialogues</title><authors>Liang Qiu, Yuan Liang, Yizhou Zhao, Pan Lu, Baolin Peng, Zhou Yu, Ying
  Nian Wu, Song-Chun Zhu</authors><categories>cs.CL cs.AI</categories><comments>Long paper accepted by ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring social relations from dialogues is vital for building emotionally
intelligent robots to interpret human language better and act accordingly. We
model the social network as an And-or Graph, named SocAoG, for the consistency
of relations among a group and leveraging attributes as inference cues.
Moreover, we formulate a sequential structure prediction task, and propose an
$\alpha$-$\beta$-$\gamma$ strategy to incrementally parse SocAoG for the
dynamic inference upon any incoming utterance: (i) an $\alpha$ process
predicting attributes and relations conditioned on the semantics of dialogues,
(ii) a $\beta$ process updating the social relations based on related
attributes, and (iii) a $\gamma$ process updating individual's attributes based
on interpersonal social relations. Empirical results on DialogRE and MovieGraph
show that our model infers social relations more accurately than the
state-of-the-art methods. Moreover, the ablation study shows the three
processes complement each other, and the case study demonstrates the dynamic
relational inference.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01008</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01008</id><submitter>Bin Yang</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:10:49 GMT</date><size>32kb</size></version><title>Convergence and Optimal Complexity of the Adaptive Planewave Method for
  Eigenvalue Computations</title><authors>Xiaoying Dai, Yan Pan, Bin Yang, Aihui Zhou</authors><categories>math.NA cs.NA</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study an adaptive planewave method for multiple eigenvalues
of second-order elliptic partial equations. Inspired by the technique for the
adaptive finite element analysis, we prove that the adaptive planewave method
has the linear convergence rate and optimal complexity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01009</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01009</id><submitter>Jindong Wang</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:10:50 GMT</date><size>1161kb</size><source_type>D</source_type></version><title>FedHealth 2: Weighted Federated Transfer Learning via Batch
  Normalization for Personalized Healthcare</title><authors>Yiqiang Chen, Wang Lu, Jindong Wang, Xin Qin</authors><categories>cs.LG cs.AI</categories><comments>Technical report</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The success of machine learning applications often needs a large quantity of
data. Recently, federated learning (FL) is attracting increasing attention due
to the demand for data privacy and security, especially in the medical field.
However, the performance of existing FL approaches often deteriorates when
there exist domain shifts among clients, and few previous works focus on
personalization in healthcare. In this article, we propose FedHealth 2, an
extension of FedHealth \cite{chen2020fedhealth} to tackle domain shifts and get
personalized models for local clients. FedHealth 2 obtains the client
similarities via a pretrained model, and then it averages all weighted models
with preserving local batch normalization. Wearable activity recognition and
COVID-19 auxiliary diagnosis experiments have evaluated that FedHealth 2 can
achieve better accuracy (10%+ improvement for activity recognition) and
personalized healthcare without compromising privacy and security.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01011</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01011</id><submitter>Robin Scheibler</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:18:14 GMT</date><size>43kb</size><source_type>D</source_type></version><title>Refinement of Direction of Arrival Estimators by
  Majorization-Minimization Optimization on the Array Manifold</title><authors>Robin Scheibler, Masahito Togami</authors><categories>eess.SP cs.SD eess.AS math.OC</categories><comments>5 pages, 2 figures, 2 tables. Presented at IEEE ICASSP 2021</comments><journal-ref>Proc. IEEE ICASSP, pp. 436-440, June, 2021</journal-ref><doi>10.1109/ICASSP39728.2021.9414798</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generalized formulation of direction of arrival estimation that
includes many existing methods such as steered response power, subspace,
coherent and incoherent, as well as speech sparsity-based methods. Unlike most
conventional methods that rely exclusively on grid search, we introduce a
continuous optimization algorithm to refine DOA estimates beyond the resolution
of the initial grid. The algorithm is derived from the
majorization-minimization (MM) technique. We derive two surrogate functions,
one quadratic and one linear. Both lead to efficient iterative algorithms that
do not require hyperparameters, such as step size, and ensure that the DOA
estimates never leave the array manifold, without the need for a projection
step. In numerical experiments, we show that the accuracy after a few
iterations of the MM algorithm nearly removes dependency on the resolution of
the initial grid used. We find that the quadratic surrogate function leads to
very fast convergence, but the simplicity of the linear algorithm is very
attractive, and the performance gap small.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01013</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01013</id><submitter>Martino Trevisan Dr</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:25:19 GMT</date><size>7679kb</size><source_type>D</source_type></version><title>Debate on Online Social Networks at the Time of COVID-19: An Italian
  Case Study</title><authors>Martino Trevisan, Luca Vassio, Danilo Giordano</authors><categories>cs.SI cs.CY</categories><journal-ref>TREVISAN, Martino; VASSIO, Luca; GIORDANO, Danilo. Debate on
  online social networks at the time of COVID-19: An Italian case study. Online
  Social Networks and Media, 2021, 23: 100136</journal-ref><doi>10.1016/j.osnem.2021.100136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The COVID-19 pandemic is not only having a heavy impact on healthcare but
also changing people's habits and the society we live in. Countries such as
Italy have enforced a total lockdown lasting several months, with most of the
population forced to remain at home. During this time, online social networks,
more than ever, have represented an alternative solution for social life,
allowing users to interact and debate with each other. Hence, it is of
paramount importance to understand the changing use of social networks brought
about by the pandemic. In this paper, we analyze how the interaction patterns
around popular influencers in Italy changed during the first six months of
2020, within Instagram and Facebook social networks. We collected a large
dataset for this group of public figures, including more than 54 million
comments on over 140 thousand posts for these months. We analyze and compare
engagement on the posts of these influencers and provide quantitative figures
for aggregated user activity. We further show the changes in the patterns of
usage before and during the lockdown, which demonstrated a growth of activity
and sizable daily and weekly variations. We also analyze the user sentiment
through the psycholinguistic properties of comments, and the results testified
the rapid boom and disappearance of topics related to the pandemic. To support
further analyses, we release the anonymized dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01016</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01016</id><submitter>Jun Moon</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:30:14 GMT</date><size>3362kb</size><source_type>D</source_type></version><title>Deep Reinforcement Learning-based UAV Navigation and Control: A Soft
  Actor-Critic with Hindsight Experience Replay Approach</title><authors>Myoung Hoon Lee, Jun Moon</authors><categories>eess.SY cs.LG cs.RO cs.SY</categories><comments>12 page, 9 figures</comments><msc-class>60J20, 68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose SACHER (soft actor-critic (SAC) with hindsight
experience replay (HER)), which constitutes a class of deep reinforcement
learning (DRL) algorithms. SAC is known as an off-policy model-free DRL
algorithm based on the maximum entropy framework, which outperforms earlier DRL
algorithms in terms of exploration, robustness and learning performance.
However, in SAC, maximizing the entropy-augmented objective may degrade the
optimality of the learning outcomes. HER is known as a sample-efficient replay
method that enhances the performance of off-policy DRL algorithms by allowing
them to learn from both failures and successes. We apply HER to SAC and propose
SACHER to improve the learning performance of SAC. More precisely, SACHER
achieves the desired optimal outcomes faster and more accurately than SAC,
since HER improves the sample efficiency of SAC. We apply SACHER to the
navigation and control problem of unmanned aerial vehicles (UAVs), where SACHER
generates the optimal navigation path of the UAV under various obstacles in
operation. Specifically, we show the effectiveness of SACHER in terms of the
tracking error and cumulative reward in UAV operation by comparing them with
those of state-of-the-art DRL algorithms, SAC and DDPG. Note that SACHER in UAV
navigation and control problems can be applied to arbitrary models of UAVs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01019</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01019</id><submitter>Avi Cohen</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:35:37 GMT</date><size>28kb</size></version><title>Simple Economies are Almost Optimal</title><authors>Amir Ban, Avi Cohen, Shahar Dobzinski, Itai Ashlagi</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a seller that intends to auction some item. The seller can invest
money and effort in advertising in different market segments in order to
recruit $n$ bidders to the auction. Alternatively, the seller can have a much
cheaper and focused marketing operation and recruit the same number of bidders
from a single market segment. Which marketing operation should the seller
choose?
  More formally, let $D=\{\mathcal D_1,\ldots, \mathcal D_n\}$ be a set of
distributions. Our main result shows that there is always $\mathcal D_i\in D$
such that the revenue that can be extracted from $n$ bidders, where the value
of each is independently drawn from $\mathcal D_i$, is at least $\frac 1 2
\cdot (1-\frac 1 e)$ of the revenue that can be obtained by any possible mix of
bidders, where the value of each bidder is drawn from some (possibly different)
distribution that belongs to $D$.
  We next consider situations in which the auctioneer cannot use the optimal
auction and is required to use a second price auction. We show that there is
always $\mathcal D_i\in D$ such that if the value of all bidders is
independently drawn from $\mathcal D_i$ then running a second price auction
guarantees a constant fraction of the revenue that can be obtained by a
second-price auction by any possible mix of bidders. Finally, we show that for
any $\varepsilon&gt;0$ there exists a function $f$ that depends only on
$\varepsilon$ (in particular, the function does not depend on $n$ or on the set
$D$), such that recruiting $n$ bidders which have at most $f(\varepsilon)$
different distributions, all from $D$, guarantees $(1-\varepsilon)$-fraction of
the revenue that can be obtained by a second-price auction by any possible mix
of bidders.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01021</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01021</id><submitter>Chao Zhang</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:41:04 GMT</date><size>1636kb</size><source_type>D</source_type></version><title>Decision-making Oriented Clustering: Application to Pricing and Power
  Consumption Scheduling</title><authors>Chao Zhang, Samson Lasaulce, Martin Hennebel, Lucas Saludjian, Patrick
  Panciatici, and H. Vincent Poor</authors><categories>cs.LG</categories><comments>Published in Applied Energy</comments><journal-ref>Applied Energy, 297:117106, 2021</journal-ref><doi>10.1016/j.apenergy.2021.117106.</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data clustering is an instrumental tool in the area of energy resource
management. One problem with conventional clustering is that it does not take
the final use of the clustered data into account, which may lead to a very
suboptimal use of energy or computational resources. When clustered data are
used by a decision-making entity, it turns out that significant gains can be
obtained by tailoring the clustering scheme to the final task performed by the
decision-making entity. The key to having good final performance is to
automatically extract the important attributes of the data space that are
inherently relevant to the subsequent decision-making entity, and partition the
data space based on these attributes instead of partitioning the data space
based on predefined conventional metrics. For this purpose, we formulate the
framework of decision-making oriented clustering and propose an algorithm
providing a decision-based partition of the data space and good representative
decisions. By applying this novel framework and algorithm to a typical problem
of real-time pricing and that of power consumption scheduling, we obtain
several insightful analytical results such as the expression of the best
representative price profiles for real-time pricing and a very significant
reduction in terms of required clusters to perform power consumption scheduling
as shown by our simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01023</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01023</id><submitter>Chuhan Wu</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:42:33 GMT</date><size>569kb</size><source_type>D</source_type></version><title>One Teacher is Enough? Pre-trained Language Model Distillation from
  Multiple Teachers</title><authors>Chuhan Wu, Fangzhao Wu, Yongfeng Huang</authors><categories>cs.CL</categories><comments>Findings of ACL-IJCNLP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Pre-trained language models (PLMs) achieve great success in NLP. However,
their huge model sizes hinder their applications in many practical systems.
Knowledge distillation is a popular technique to compress PLMs, which learns a
small student model from a large teacher PLM. However, the knowledge learned
from a single teacher may be limited and even biased, resulting in low-quality
student model. In this paper, we propose a multi-teacher knowledge distillation
framework named MT-BERT for pre-trained language model compression, which can
train high-quality student model from multiple teacher PLMs. In MT-BERT we
design a multi-teacher co-finetuning method to jointly finetune multiple
teacher PLMs in downstream tasks with shared pooling and prediction layers to
align their output space for better collaborative teaching. In addition, we
propose a multi-teacher hidden loss and a multi-teacher distillation loss to
transfer the useful knowledge in both hidden states and soft labels from
multiple teacher PLMs to the student model. Experiments on three benchmark
datasets validate the effectiveness of MT-BERT in compressing PLMs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01024</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01024</id><submitter>Yuxuan Lai</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:43:12 GMT</date><size>320kb</size><source_type>D</source_type></version><title>Why Machine Reading Comprehension Models Learn Shortcuts?</title><authors>Yuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang, and Dongyan Zhao</authors><categories>cs.CL</categories><comments>13 pages, 8 figures, ACL 2021 (findings)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies report that many machine reading comprehension (MRC) models
can perform closely to or even better than humans on benchmark datasets.
However, existing works indicate that many MRC models may learn shortcuts to
outwit these benchmarks, but the performance is unsatisfactory in real-world
applications. In this work, we attempt to explore, instead of the expected
comprehension skills, why these models learn the shortcuts. Based on the
observation that a large portion of questions in current datasets have shortcut
solutions, we argue that larger proportion of shortcut questions in training
data make models rely on shortcut tricks excessively. To investigate this
hypothesis, we carefully design two synthetic datasets with annotations that
indicate whether a question can be answered using shortcut solutions. We
further propose two new methods to quantitatively analyze the learning
difficulty regarding shortcut and challenging questions, and revealing the
inherent learning mechanism behind the different performance between the two
kinds of questions. A thorough empirical analysis shows that MRC models tend to
learn shortcut questions earlier than challenging questions, and the high
proportions of shortcut questions in training sets hinder models from exploring
the sophisticated reasoning skills in the later stage of training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01028</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01028</id><submitter>Fangying Chen</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:49:24 GMT</date><size>4985kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 04:30:36 GMT</date><size>5057kb</size><source_type>D</source_type></version><title>A Hypergraph Convolutional Neural Network for Molecular Properties
  Prediction using Functional Group</title><authors>Fangying Chen, Junyoung Park, Jinkyoo Park</authors><categories>cs.CE</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Molecular Hypergraph Convolutional Network (MolHGCN) that
predicts the molecular properties of a molecule using the atom and functional
group information as inputs. Molecules can contain many types of functional
groups, which will affect the properties the molecules. For example, the
toxicity of a molecule is associated with toxicophores, such as nitroaromatic
groups and thiourea. Conventional graph-based methods that consider the
pair-wise interactions between nodes are inefficient in expressing the complex
relationship between multiple nodes in a graph flexibly, and applying
multi-hops may result in oversmoothing and overfitting problems. Hence, we
propose MolHGCN to capture the substructural difference between molecules using
the atom and functional group information. MolHGCN constructs a hypergraph
representation of a molecule using functional group information from the input
SMILES strings, extracts hidden representation using a two-stage message
passing process (atom and functional group message passing), and predicts the
properties of the molecules using the extracted hidden representation. We
evaluate the performance of our model using Tox21, ClinTox, SIDER, BBBP, BACE,
ESOL, FreeSolv and Lipophilicity datasets. We show that our model is able to
outperform other baseline methods for most of the datasets. We particularly
show that incorporating functional group information along with atom
information results in better separability in the latent space, thus increasing
the prediction accuracy of the molecule property prediction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01030</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01030</id><submitter>Kalev Alpernas</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:52:49 GMT</date><size>570kb</size><source_type>D</source_type></version><title>Some Complexity Results for Stateful Network Verification</title><authors>Kalev Alpernas (Tel Aviv University), Aurojit Panda (NYU), Alexander
  Rabinovich (Tel Aviv University), Mooly Sagiv (Tel Aviv University), Scott
  Shenker (UC Berkeley), Sharon Shoham (Tel Aviv University), Yaron Velner
  (Hebrew University of Jerusalem)</authors><categories>cs.LO cs.PL</categories><comments>This is a pre-print of an article published in Formal Methods in
  System Design. The final authenticated version is available online at:
  https://doi.org/10.1007/s10703-018-00330-9</comments><journal-ref>Formal Methods in System Design 54 (2019) 191-231</journal-ref><doi>10.1007/s10703-018-00330-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern networks, forwarding of packets often depends on the history of
previously transmitted traffic. Such networks contain stateful middleboxes,
whose forwarding behaviour depends on a mutable internal state. Firewalls and
load balancers are typical examples of stateful middleboxes.
  This work addresses the complexity of verifying safety properties, such as
isolation, in networks with finite-state middleboxes. Unfortunately, we show
that even in the absence of forwarding loops, reasoning about such networks is
undecidable due to interactions between middleboxes connected by unbounded
ordered channels. We therefore abstract away channel ordering. This abstraction
is sound for safety, and makes the problem decidable. Specifically, safety
checking becomes EXPSPACE-complete in the number of hosts and middleboxes in
the network. To tackle the high complexity, we identify two useful subclasses
of finite-state middleboxes which admit better complexities. The simplest class
includes, e.g., firewalls and permits polynomial-time verification. The second
class includes, e.g., cache servers and learning switches, and makes the safety
problem coNP-complete.
  Finally, we implement a tool for verifying the correctness of stateful
networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01031</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01031</id><submitter>Yushan Li</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:54:59 GMT</date><size>1608kb</size><source_type>D</source_type></version><title>On Topology Inference for Networked Dynamical Systems: Principles and
  Performances</title><authors>Yushan Li, Jianping He, Cailian Chen and Xinping Guan</authors><categories>eess.SP cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topology inference for networked dynamical systems (NDSs) plays a crucial
role in many areas. Knowledge of the system topology can aid in detecting
anomalies, spotting trends, predicting future behavior and so on. Different
from the majority of pioneering works, this paper investigates the principles
and performances of topology inference from the perspective of node causality
and correlation. Specifically, we advocate a comprehensive analysis framework
to unveil the mutual relationship, convergence and accuracy of the proposed
methods and other benchmark methods, i.e., the Granger and ordinary least
square (OLS) estimators. Our method allows for unknown observation noises, both
asymptotic and marginal stabilities for NDSs, while encompasses a
correlation-based modification design to alleviate performance degradation in
small observation scale. To explicitly demonstrate the inference performance of
the estimators, we leverage the concentration measure in Gaussian space, and
derive the non-asymptotic rates of the inference errors for linear
time-invariant (LTI) cases. Considering when the observations are not
sufficient to support the estimators, we provide an excitation-based method to
infer the one-hop and multi-hop neighbors with probability guarantees.
Furthermore, we point out the theoretical results can be extended to switching
topologies and nonlinear dynamics cases. Extensive simulations highlight the
outperformance of the proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01033</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01033</id><submitter>Kunwoo Park</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:02:14 GMT</date><size>1237kb</size><source_type>D</source_type></version><title>Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment
  Extraction in News Text</title><authors>Kunwoo Park, Zhufeng Pan, and Jungseock Joo</authors><categories>cs.CL cs.CY cs.IR</categories><comments>To appear at ACL-IJCNLP 2021 (Long paper, Findings)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Understanding who blames or supports whom in news text is a critical research
question in computational social science. Traditional methods and datasets for
sentiment analysis are, however, not suitable for the domain of political text
as they do not consider the direction of sentiments expressed between entities.
In this paper, we propose a novel NLP task of identifying directed sentiment
relationship between political entities from a given news document, which we
call directed sentiment extraction. From a million-scale news corpus, we
construct a dataset of news sentences where sentiment relations of political
entities are manually annotated. We present a simple but effective approach for
utilizing a pretrained transformer, which infers the target class by predicting
multiple question-answering tasks and combining the outcomes. We demonstrate
the utility of our proposed method for social science research questions by
analyzing positive and negative opinions between political entities in two
major events: 2016 U.S. presidential election and COVID-19. The newly proposed
problem, data, and method will facilitate future studies on interdisciplinary
NLP methods and applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01034</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01034</id><submitter>Seyed Mohammadhossein Tabatabaee</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:06:14 GMT</date><size>36303kb</size><source_type>D</source_type></version><title>Deficit Round-Robin: A Second Network Calculus Analysis</title><authors>Seyed Mohammadhossein Tabatabaee, Jean-Yves Le Boudec</authors><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deficit Round-Robin (DRR) is a widespread scheduling algorithm that provides
fair queueing with variable-length packets. Bounds on worst-case delays for DRR
were found by Boyer et al., who used a rigorous network calculus approach and
characterized the service obtained by one flow of interest by means of a convex
strict service curve. These bounds do not make any assumptions on the
interfering traffic hence are pessimistic when the interfering traffic is
constrained by some arrival curves. For such cases, two improvements were
proposed. The former, by Soni et al., uses a correction term derived from a
semi-rigorous heuristic; unfortunately, these bounds are incorrect, as we show
by exhibiting a counter-example. The latter, by Bouillard, rigorously derive
convex strict service curves for DRR that account for the arrival curve
constraints of the interfering traffic. In this paper, we improve on these
results in two ways. First, we derive a non-convex strict service curve for DRR
that improves on Boyer et al. when there is no arrival constraint on the
interfering traffic. Second, we provide an iterative method to improve any
strict service curve (including Bouillard's) when there are arrival constraints
for the interfering traffic. As of today, our results provide the best-known
worst-case delay bounds for DRR. They are obtained by using the method of the
pseudo-inverse.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01035</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01035</id><submitter>Daochang Liu</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:06:43 GMT</date><size>12566kb</size><source_type>D</source_type></version><title>Towards Unified Surgical Skill Assessment</title><authors>Daochang Liu, Qiyue Li, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei
  Shan, Ziyu Li</authors><categories>cs.CV</categories><comments>CVPR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surgical skills have a great influence on surgical safety and patients'
well-being. Traditional assessment of surgical skills involves strenuous manual
efforts, which lacks efficiency and repeatability. Therefore, we attempt to
automatically predict how well the surgery is performed using the surgical
video. In this paper, a unified multi-path framework for automatic surgical
skill assessment is proposed, which takes care of multiple composing aspects of
surgical skills, including surgical tool usage, intraoperative event pattern,
and other skill proxies. The dependency relationships among these different
aspects are specially modeled by a path dependency module in the framework. We
conduct extensive experiments on the JIGSAWS dataset of simulated surgical
tasks, and a new clinical dataset of real laparoscopic surgeries. The proposed
framework achieves promising results on both datasets, with the
state-of-the-art on the simulated dataset advanced from 0.71 Spearman's
correlation to 0.80. It is also shown that combining multiple skill aspects
yields better performance than relying on a single aspect.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01036</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01036</id><submitter>Shaked Matar</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:10:08 GMT</date><size>861kb</size><source_type>D</source_type></version><title>Ultra-Sparse Near-Additive Emulators</title><authors>Michael Elkin, Shaked Matar</authors><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Near-additive (aka $(1+\epsilon,\beta)$-) emulators and spanners are a
fundamental graph-algorithmic construct, with numerous applications for
computing approximate shortest paths and related problems in distributed,
streaming and dynamic settings.
  Known constructions of near-additive emulators enable one to trade between
their sparsity (i.e., number of edges) and the additive stretch $\beta$.
Specifically, for any pair of parameters $\epsilon &gt;0$, $ \kappa=1,2,\dots$,
one can have a $(1+\epsilon,\beta)$-emulator with $O(n^{1+1/\kappa})$ edges,
with $\beta = \left(\frac{\log \kappa}{\epsilon}\right)^{\log \kappa}$. At
their sparsest, these emulators employ $c\cdot n$ edges, for some constant
$c\geq 2$.
  We tighten this bound, and show that in fact precisely $n^{1+1/\kappa}$ edges
suffice.
  In particular, our emulators can be \emph{ultra-sparse}, i.e., we can have an
emulator with $n+o(n)$ edges and $\beta = \left(\frac{\log {\log n}}{\epsilon
}\right)^{{\log {\log n}}(1+o(1))}$.
  We also devise a distributed deterministic algorithm in the CONGEST model
that builds these emulators in low polynomial time (i.e., in $O(n^\rho)$ time,
for an arbitrarily small constant parameter $\rho &gt;0$).
  Finally, we also improve the state-of-the-art distributed deterministic
\congest-model construction of
  $(1+\epsilon,\beta)$-spanners devised in the PODC'19 paper
  [ElkinM19]. Specifically, the spanners of [ElkinM19] have $O(\beta\cdot
n^{1+1/\kappa})$ edges, i.e., at their sparsest they employ
  $ O\left(\frac{\log {\log n}}{\epsilon }\right)^{{\log {\log n}}}\cdot n$
edges. In this paper, we devise an efficient distributed deterministic
CONGEST-model algorithm that builds such spanners with $O(n^{1+1/\kappa})$
edges for $\kappa = O\left(\frac{\log n}{\log ^{(3)}n}\right)$. At their
sparsest, these spanners employ only $O(n\cdot {\log {\log n}})$ edges.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01040</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01040</id><submitter>Chuhan Wu</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:30:29 GMT</date><size>551kb</size><source_type>D</source_type></version><title>Hi-Transformer: Hierarchical Interactive Transformer for Efficient and
  Effective Long Document Modeling</title><authors>Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang</authors><categories>cs.CL</categories><comments>ACL-IJCNLP 2021</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Transformer is important for text modeling. However, it has difficulty in
handling long documents due to the quadratic complexity with input text length.
In order to handle this problem, we propose a hierarchical interactive
Transformer (Hi-Transformer) for efficient and effective long document
modeling. Hi-Transformer models documents in a hierarchical way, i.e., first
learns sentence representations and then learns document representations. It
can effectively reduce the complexity and meanwhile capture global document
context in the modeling of each sentence. More specifically, we first use a
sentence Transformer to learn the representations of each sentence. Then we use
a document Transformer to model the global document context from these sentence
representations. Next, we use another sentence Transformer to enhance sentence
modeling using the global document context. Finally, we use hierarchical
pooling method to obtain document embedding. Extensive experiments on three
benchmark datasets validate the efficiency and effectiveness of Hi-Transformer
in long document modeling.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01043</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01043</id><submitter>Rohan Giriraj</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:33:05 GMT</date><size>279kb</size><source_type>D</source_type></version><title>Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties
  of Non-Gaussian Distributions</title><authors>Rohan Giriraj, Sinnu Susan Thomas</authors><categories>cs.LG cs.AI</categories><comments>12 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, causal modelling has been used widely to improve
generalization and to provide interpretability in machine learning models. To
determine cause-effect relationships in the absence of a randomized trial, we
can model causal systems with counterfactuals and interventions given enough
domain knowledge. However, there are several cases where domain knowledge is
almost absent and the only recourse is using a statistical method to estimate
causal relationships. While there have been several works done in estimating
causal relationships in unstructured data, we are yet to find a well-defined
framework for estimating causal relationships in Knowledge Graphs (KG). It is
commonly used to provide a semantic framework for data with complex
inter-domain relationships. In this work, we define a hybrid approach that
allows us to discover cause-effect relationships in KG. The proposed approach
is based around the finding of the instantaneous causal structure of a
non-experimental matrix using a non-Gaussian model, i.e; finding the causal
ordering of the variables in a non-Gaussian setting. The non-experimental
matrix is a low-dimensional tensor projection obtained by decomposing the
adjacency tensor of a KG. We use two different pre-existing algorithms, one for
the causal discovery and the other for decomposing the KG and combining them to
get the causal structure in a KG.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01044</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01044</id><submitter>Jennifer C. White</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:34:32 GMT</date><size>10660kb</size><source_type>D</source_type></version><title>Examining the Inductive Bias of Neural Language Models with Artificial
  Languages</title><authors>Jennifer C. White and Ryan Cotterell</authors><categories>cs.CL</categories><comments>Accepted at ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since language models are used to model a wide variety of languages, it is
natural to ask whether the neural architectures used for the task have
inductive biases towards modeling particular types of languages. Investigation
of these biases has proved complicated due to the many variables that appear in
the experimental setup. Languages vary in many typological dimensions, and it
is difficult to single out one or two to investigate without the others acting
as confounders. We propose a novel method for investigating the inductive
biases of language models using artificial languages. These languages are
constructed to allow us to create parallel corpora across languages that differ
only in the typological feature being investigated, such as word order. We then
use them to train and test language models. This constitutes a fully controlled
causal framework, and demonstrates how grammar engineering can serve as a
useful tool for analyzing neural models. Using this method, we find that
commonly used neural architectures exhibit different inductive biases: LSTMs
display little preference with respect to word ordering, while transformers
display a clear preference for some orderings over others. Further, we find
that neither the inductive bias of the LSTM nor that of the transformer appears
to reflect any tendencies that we see in attested natural languages.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01045</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01045</id><submitter>Marco Gaido</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:37:37 GMT</date><size>6179kb</size><source_type>D</source_type></version><title>Cascade versus Direct Speech Translation: Do the Differences Still Make
  a Difference?</title><authors>Luisa Bentivogli, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto
  Martinelli, Matteo Negri, Marco Turchi</authors><categories>cs.CL</categories><comments>Accepted at ACL2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Five years after the first published proofs of concept, direct approaches to
speech translation (ST) are now competing with traditional cascade solutions.
In light of this steady progress, can we claim that the performance gap between
the two is closed? Starting from this question, we present a systematic
comparison between state-of-the-art systems representative of the two
paradigms. Focusing on three language directions
(English-German/Italian/Spanish), we conduct automatic and manual evaluations,
exploiting high-quality professional post-edits and annotations. Our
multi-faceted analysis on one of the few publicly available ST benchmarks
attests for the first time that: i) the gap between the two paradigms is now
closed, and ii) the subtle differences observed in their behavior are not
sufficient for humans neither to distinguish them nor to prefer one over the
other.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01048</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01048</id><submitter>Conor F. Hayes</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:42:42 GMT</date><size>306kb</size><source_type>D</source_type></version><title>Expected Scalarised Returns Dominance: A New Solution Concept for
  Multi-Objective Decision Making</title><authors>Conor F. Hayes, Timothy Verstraeten, Diederik M. Roijers, Enda Howley,
  Patrick Mannion</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In many real-world scenarios, the utility of a user is derived from the
single execution of a policy. In this case, to apply multi-objective
reinforcement learning, the expected utility of the returns must be optimised.
Various scenarios exist where a user's preferences over objectives (also known
as the utility function) are unknown or difficult to specify. In such
scenarios, a set of optimal policies must be learned. However, settings where
the expected utility must be maximised have been largely overlooked by the
multi-objective reinforcement learning community and, as a consequence, a set
of optimal solutions has yet to be defined. In this paper we address this
challenge by proposing first-order stochastic dominance as a criterion to build
solution sets to maximise expected utility. We also propose a new dominance
criterion, known as expected scalarised returns (ESR) dominance, that extends
first-order stochastic dominance to allow a set of optimal policies to be
learned in practice. We then define a new solution concept called the ESR set,
which is a set of policies that are ESR dominant. Finally, we define a new
multi-objective distributional tabular reinforcement learning (MOT-DRL)
algorithm to learn the ESR set in a multi-objective multi-armed bandit setting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01051</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01051</id><submitter>Rahul Aralikatte</submitter><version version="v1"><date>Wed, 2 Jun 2021 09:53:06 GMT</date><size>224kb</size><source_type>D</source_type></version><title>Minimax and Neyman-Pearson Meta-Learning for Outlier Languages</title><authors>Edoardo Maria Ponti, Rahul Aralikatte, Disha Shrivastava, Siva Reddy,
  Anders S{\o}gaard</authors><categories>cs.CL</categories><comments>Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-agnostic meta-learning (MAML) has been recently put forth as a strategy
to learn resource-poor languages in a sample-efficient fashion. Nevertheless,
the properties of these languages are often not well represented by those
available during training. Hence, we argue that the i.i.d. assumption ingrained
in MAML makes it ill-suited for cross-lingual NLP. In fact, under a
decision-theoretic framework, MAML can be interpreted as minimising the
expected risk across training languages (with a uniform prior), which is known
as Bayes criterion. To increase its robustness to outlier languages, we create
two variants of MAML based on alternative criteria: Minimax MAML reduces the
maximum risk across languages, while Neyman-Pearson MAML constrains the risk in
each language to a maximum threshold. Both criteria constitute fully
differentiable two-player games. In light of this, we propose a new adaptive
optimiser solving for a local approximation to their Nash equilibrium. We
evaluate both model variants on two popular NLP tasks, part-of-speech tagging
and question answering. We report gains for their average and minimum
performance across low-resource languages in zero- and few-shot settings,
compared to joint multi-source transfer and vanilla MAML.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01053</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01053</id><submitter>Anastasiya Gorodilova</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:00:45 GMT</date><size>1378kb</size><source_type>D</source_type></version><title>The Seventh International Olympiad in Cryptography NSUCRYPTO: problems
  and solutions</title><authors>A. Gorodilova, N. Tokareva, S. Agievich, C. Carlet, V. Idrisova, K.
  Kalgin, D. Kolegov, A. Kutsenko, N. Mouha, M. Pudovkina, A. Udovenko</authors><categories>cs.CR cs.DM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The International Olympiad in Cryptography NSUCRYPTO is the unique Olympiad
containing scientific mathematical problems for professionals, school and
university students from any country. Its aim is to involve young researchers
in solving curious and tough scientific problems of modern cryptography. In
2020, it was held for the seventh time. Prizes and diplomas were awarded to 84
participants in the first round and 49 teams in the second round from 32
countries. In this paper, problems and their solutions of NSUCRYPTO'2020 are
presented. We consider problems related to attacks on ciphers and hash
functions, protocols, permutations, primality tests, etc. We discuss several
open problems on JPEG encoding, Miller -- Rabin primality test, special bases
in the vector space, AES-GCM. The problem of a modified Miller -- Rabin
primality test was solved during the Olympiad. The problem for finding special
bases was partially solved.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01055</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01055</id><submitter>Ciprian Orhei</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:06:50 GMT</date><size>3840kb</size><source_type>D</source_type></version><title>A Novel Edge Detection Operator for Identifying Buildings in Augmented
  Reality Applications</title><authors>Ciprian Orhei and Silviu Vert and Radu Vasiu</authors><categories>cs.CV</categories><doi>10.1007/978-3-030-59506-7_18</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Augmented Reality is an environment-enhancing technology, widely applied in
many domains, such as tourism and culture. One of the major challenges in this
field is precise detection and extraction of building information through
Computer Vision techniques. Edge detection is one of the building blocks
operations for many feature extraction solutions in Computer Vision. AR systems
use edge detection for building extraction or for extraction of facade details
from buildings. In this paper, we propose a novel filter operator for edge
detection that aims to extract building contours or facade features better. The
proposed filter gives more weight for finding vertical and horizontal edges
that is an important feature for our aim.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01056</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01056</id><submitter>Johannes Gerster</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:07:58 GMT</date><size>689kb</size><source_type>D</source_type></version><title>Comparison of Random Sampling and Heuristic Optimization-Based Methods
  for Determining the Flexibility Potential at Vertical System Interconnections</title><authors>Gerster Johannes, Marcel Sarstedt, Eric MSP Veith, Sebastian Lehnhoff,
  Lutz Hofmann</authors><categories>eess.SY cs.SY</categories><comments>9pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:2102.03430</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to prevent conflicting or counteracting use of flexibility options,
the coordination between distribution system operator and transmission system
operator has to be strengthened. For this purpose, methods for the standardized
description and identification of the aggregated flexibility potential of
distribution grids are developed. Approaches for identifying the feasible
operation region (FOR) of distribution grids can be categorized into two main
classes: Random sampling/stochastic approaches and optimization-based
approaches. While the latter have the advantage of working in real-world
scenarios where no full grid models exist, when relying on naive sampling
strategies, they suffer from poor coverage of the edges of the FOR due to
convoluted distributions. In this paper, we tackle the problem from two
different angles. First, we present a random sampling approach which mitigates
the convolution problem by drawing sample values from a multivariate Dirichlet
distribution. Second, we come up with a hybrid approach which solves the
underlying optimal power flow problems of the optimization-based approach by
means of a stochastic evolutionary optimization algorithm codenamed REvol. By
means of synthetic feeders, we compare the two proposed FOR identification
methods with regard to how well the FOR is covered and number of power flow
calculations required.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01060</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01060</id><submitter>Yova Kementchedjhieva</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:26:07 GMT</date><size>1716kb</size><source_type>D</source_type></version><title>John praised Mary because he? Implicit Causality Bias and Its
  Interaction with Explicit Cues in LMs</title><authors>Yova Kementchedjhieva, Mark Anderson and Anders S{\o}gaard</authors><categories>cs.CL</categories><comments>To appear at Findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Some interpersonal verbs can implicitly attribute causality to either their
subject or their object and are therefore said to carry an implicit causality
(IC) bias. Through this bias, causal links can be inferred from a narrative,
aiding language comprehension. We investigate whether pre-trained language
models (PLMs) encode IC bias and use it at inference time. We find that to be
the case, albeit to different degrees, for three distinct PLM architectures.
However, causes do not always need to be implicit -- when a cause is explicitly
stated in a subordinate clause, an incongruent IC bias associated with the verb
in the main clause leads to a delay in human processing. We hypothesize that
the temporary challenge humans face in integrating the two contradicting
signals, one from the lexical semantics of the verb, one from the
sentence-level semantics, would be reflected in higher error rates for models
on tasks dependent on causal links. The results of our study lend support to
this hypothesis, suggesting that PLMs tend to prioritize lexical patterns over
higher-order signals.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01061</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01061</id><submitter>Chen Liang</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:26:13 GMT</date><size>1788kb</size><source_type>D</source_type></version><title>Rethinking Cross-modal Interaction from a Top-down Perspective for
  Referring Video Object Segmentation</title><authors>Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang, Zongxin Yang, Yunchao
  Wei and Yi Yang</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Referring video object segmentation (RVOS) aims to segment video objects with
the guidance of natural language reference. Previous methods typically tackle
RVOS through directly grounding linguistic reference over the image lattice.
Such bottom-up strategy fails to explore object-level cues, easily leading to
inferior results. In this work, we instead put forward a two-stage, top-down
RVOS solution. First, an exhaustive set of object tracklets is constructed by
propagating object masks detected from several sampled frames to the entire
video. Second, a Transformer-based tracklet-language grounding module is
proposed, which models instance-level visual relations and cross-modal
interactions simultaneously and efficiently. Our model ranks first place on
CVPR2021 Referring Youtube-VOS challenge.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01062</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01062</id><submitter>Jeffrey Shallit</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:33:48 GMT</date><size>1649kb</size><source_type>D</source_type></version><title>Hilbert's spacefilling curve described by automatic, regular, and
  synchronized sequences</title><authors>Jeffrey Shallit</authors><categories>cs.FL cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Hilbert's spacefilling curve in several different ways: as an
automatic sequence of directions,as a regular and synchronized sequence of
coordinates of lattice points encountered, and as an automatic bitmap image.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01063</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01063</id><submitter>Nils Weidmann</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:34:20 GMT</date><size>1260kb</size><source_type>D</source_type></version><title>Tolerance in Model-Driven Engineering: A Systematic Literature Review
  with Model-Driven Tool Support</title><authors>Nils Weidmann, Suganya Kannan, Anthony Anjorin</authors><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Managing models in a consistent manner is an important task in the field of
Model-Driven Engineering (MDE). Although restoring and maintaining consistency
is desired in general, recent work has pointed out that always strictly
enforcing consistency at any point of time is often not feasible in real-world
scenarios, and sometimes even contrary to what a user expects from a
trustworthy MDE tool. The challenge of tolerating inconsistencies has been
discussed from different viewpoints within and outside the modelling community,
but there exists no structured overview of existing and current work in this
regard. In this paper, we provide such an overview to help join forces tackling
the unresolved problems of tolerating inconsistencies in MDE. We follow the
standard process of a Systematic Literature Review (SLR) to point out what
tolerance means, how it relates to uncertainty, which examples for tolerant
software systems have already been discussed, and which benefits and drawbacks
tolerating inconsistencies entails. Furthermore, we propose a tool-chain that
helps conducting SLRs in computer science and also eases the reproduction of
results. Relevant meta-data of the collected sources is uniformly described in
a textual modelling language and exported to the graph database Neo4j to query
aggregated information.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01064</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01064</id><submitter>Shahbaz Syed</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:35:59 GMT</date><size>214kb</size><source_type>D</source_type></version><title>Generating Informative Conclusions for Argumentative Texts</title><authors>Shahbaz Syed, Khalid Al-Khatib, Milad Alshomary, Henning Wachsmuth,
  and Martin Potthast</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The purpose of an argumentative text is to support a certain conclusion. Yet,
they are often omitted, expecting readers to infer them rather. While
appropriate when reading an individual text, this rhetorical device limits
accessibility when browsing many texts (e.g., on a search engine or on social
media). In these scenarios, an explicit conclusion makes for a good candidate
summary of an argumentative text. This is especially true if the conclusion is
informative, emphasizing specific concepts from the text. With this paper we
introduce the task of generating informative conclusions: First,
Webis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of
argumentative texts and their conclusions. Second, two paradigms for conclusion
generation are investigated; one extractive, the other abstractive in nature.
The latter exploits argumentative knowledge that augment the data via control
codes and finetuning the BART model on several subsets of the corpus. Third,
insights are provided into the suitability of our corpus for the task, the
differences between the two generation paradigms, the trade-off between
informativeness and conciseness, and the impact of encoding argumentative
knowledge. The corpus, code, and the trained models are publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01065</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01065</id><submitter>Yujian Gan</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:36:23 GMT</date><size>328kb</size><source_type>D</source_type></version><title>Towards Robustness of Text-to-SQL Models against Synonym Substitution</title><authors>Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R.
  Woodward, Jinxia Xie, Pengsheng Huang</authors><categories>cs.CL</categories><comments>To appear in ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been significant progress in studying neural networks to
translate text descriptions into SQL queries. Despite achieving good
performance on some public benchmarks, existing text-to-SQL models typically
rely on the lexical matching between words in natural language (NL) questions
and tokens in table schemas, which may render the models vulnerable to attacks
that break the schema linking mechanism. In this work, we investigate the
robustness of text-to-SQL models to synonym substitution. In particular, we
introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for
text-to-SQL translation. NL questions in Spider-Syn are modified from Spider,
by replacing their schema-related words with manually selected synonyms that
reflect real-world question paraphrases. We observe that the accuracy
dramatically drops by eliminating such explicit correspondence between NL
questions and table schemas, even if the synonyms are not adversarially
selected to conduct worst-case adversarial attacks. Finally, we present two
categories of approaches to improve the model robustness. The first category of
approaches utilizes additional synonym annotations for table schemas by
modifying the model input, while the second category is based on adversarial
training. We demonstrate that both categories of approaches significantly
outperform their counterparts without the defense, and the first category of
approaches are more effective.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01070</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01070</id><submitter>Viet Anh Nguyen</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:51:39 GMT</date><size>4652kb</size></version><title>Testing Group Fairness via Optimal Transport Projections</title><authors>Nian Si and Karthyek Murthy and Jose Blanchet and Viet Anh Nguyen</authors><categories>stat.ML cs.CY cs.LG math.ST stat.TH</categories><journal-ref>International Conference on Machine Learning 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a statistical testing framework to detect if a given machine
learning classifier fails to satisfy a wide range of group fairness notions.
The proposed test is a flexible, interpretable, and statistically rigorous tool
for auditing whether exhibited biases are intrinsic to the algorithm or due to
the randomness in the data. The statistical challenges, which may arise from
multiple impact criteria that define group fairness and which are discontinuous
on model parameters, are conveniently tackled by projecting the empirical
measure onto the set of group-fair probability models using optimal transport.
This statistic is efficiently computed using linear programming and its
asymptotic distribution is explicitly obtained. The proposed framework can also
be used to test for testing composite fairness hypotheses and fairness with
multiple sensitive attributes. The optimal transport testing formulation
improves interpretability by characterizing the minimal covariate perturbations
that eliminate the bias observed in the audit.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01071</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01071</id><submitter>Lixing Zhu</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:57:44 GMT</date><size>7241kb</size><source_type>D</source_type></version><title>Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion
  Detection</title><authors>Lixing Zhu and Gabriele Pergola and Lin Gui and Deyu Zhou and Yulan He</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotion detection in dialogues is challenging as it often requires the
identification of thematic topics underlying a conversation, the relevant
commonsense knowledge, and the intricate transition patterns between the
affective states. In this paper, we propose a Topic-Driven Knowledge-Aware
Transformer to handle the challenges above. We firstly design a topic-augmented
language model (LM) with an additional layer specialized for topic detection.
The topic-augmented LM is then combined with commonsense statements derived
from a knowledge base based on the dialogue contextual information. Finally, a
transformer-based encoder-decoder architecture fuses the topical and
commonsense information, and performs the emotion label sequence prediction.
The model has been experimented on four datasets in dialogue emotion detection,
demonstrating its superiority empirically over the existing state-of-the-art
approaches. Quantitative and qualitative results show that the model can
discover topics which help in distinguishing emotion categories.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01072</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01072</id><submitter>James Thorne</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:00:17 GMT</date><size>2343kb</size><source_type>D</source_type></version><title>Evidence-based Factual Error Correction</title><authors>James Thorne, Andreas Vlachos</authors><categories>cs.CL cs.AI cs.LG</categories><comments>To appear at ACL2021. arXiv admin note: text overlap with
  arXiv:2012.15788</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the task of factual error correction: performing edits
to a claim so that the generated rewrite is better supported by evidence. This
extends the well-studied task of fact verification by providing a mechanism to
correct written texts that are refuted or only partially supported by evidence.
We demonstrate that it is feasible to train factual error correction systems
from existing fact checking datasets which only contain labeled claims
accompanied by evidence, but not the correction. We achieve this by employing a
two-stage distant supervision approach that incorporates evidence into masked
claims when generating corrections. Our approach, based on the T5 transformer
and using retrieved evidence, achieved better results than existing work which
used a pointer copy network and gold evidence, producing accurate factual error
corrections for 5x more instances in human evaluation and a .125 increase in
SARI score. The evaluation is conducted on a dataset of 65,000 instances based
on a recent fact verification shared task and we release it to enable further
work on the task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01074</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01074</id><submitter>James Thorne</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:09:40 GMT</date><size>1193kb</size><source_type>D</source_type></version><title>Database Reasoning Over Text</title><authors>James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,
  Sebastian Riedel, Alon Halevy</authors><categories>cs.CL cs.AI cs.DB</categories><comments>To appear at ACL2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural models have shown impressive performance gains in answering queries
from natural language text. However, existing works are unable to support
database queries, such as &quot;List/Count all female athletes who were born in 20th
century&quot;, which require reasoning over sets of relevant facts with operations
such as join, filtering and aggregation. We show that while state-of-the-art
transformer models perform very well for small databases, they exhibit
limitations in processing noisy data, numerical operations, and queries that
aggregate facts. We propose a modular architecture to answer these
database-style queries over multiple spans from text and aggregating these at
scale. We evaluate the architecture using WikiNLDB, a novel dataset for
exploring such queries. Our architecture scales to databases containing
thousands of facts whereas contemporary models are limited by how many facts
can be encoded. In direct comparison on small databases, our approach increases
overall answer accuracy from 85% to 90%. On larger databases, our approach
retains its accuracy whereas transformer baselines could not encode the
context.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01077</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01077</id><submitter>Hitomi Yanaka</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:24:41 GMT</date><size>5550kb</size><source_type>D</source_type></version><title>SyGNS: A Systematic Generalization Testbed Based on Natural Language
  Semantics</title><authors>Hitomi Yanaka, Koji Mineshima, Kentaro Inui</authors><categories>cs.CL</categories><comments>Findings (long paper) of ACL-IJCNLP2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, deep neural networks (DNNs) have achieved great success in
semantically challenging NLP tasks, yet it remains unclear whether DNN models
can capture compositional meanings, those aspects of meaning that have been
long studied in formal semantics. To investigate this issue, we propose a
Systematic Generalization testbed based on Natural language Semantics (SyGNS),
whose challenge is to map natural language sentences to multiple forms of
scoped meaning representations, designed to account for various semantic
phenomena. Using SyGNS, we test whether neural networks can systematically
parse sentences involving novel combinations of logical expressions such as
quantifiers and negation. Experiments show that Transformer and GRU models can
generalize to unseen combinations of quantifiers, negations, and modifiers that
are similar to given training instances in form, but not to the others. We also
find that the generalization performance to unseen combinations is better when
the form of meaning representations is simpler. The data and code for SyGNS are
publicly available at https://github.com/verypluming/SyGNS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01078</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01078</id><submitter>Yingtao Luo</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:24:49 GMT</date><size>5005kb</size><source_type>D</source_type></version><title>KO-PDE: Kernel Optimized Discovery of Partial Differential Equations
  with Varying Coefficients</title><authors>Yingtao Luo, Qiang Liu, Yuntian Chen, Wenbo Hu, Jun Zhu</authors><categories>cs.LG cs.NA math.NA</categories><comments>Preprint. Under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Partial differential equations (PDEs) fitting scientific data can represent
physical laws with explainable mechanisms for various mathematically-oriented
subjects. Most natural dynamics are expressed by PDEs with varying coefficients
(PDEs-VC), which highlights the importance of PDE discovery. Previous
algorithms can discover some simple instances of PDEs-VC but fail in the
discovery of PDEs with coefficients of higher complexity, as a result of
coefficient estimation inaccuracy. In this paper, we propose KO-PDE, a kernel
optimized regression method that incorporates the kernel density estimation of
adjacent coefficients to reduce the coefficient estimation error. KO-PDE can
discover PDEs-VC on which previous baselines fail and is more robust against
inevitable noise in data. In experiments, the PDEs-VC of seven challenging
spatiotemporal scientific datasets in fluid dynamics are all discovered by
KO-PDE, while the three baselines render false results in most cases. With
state-of-the-art performance, KO-PDE sheds light on the automatic description
of natural phenomenons using discovered PDEs in the real world.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01079</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01079</id><submitter>Chenyang Xu</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:26:34 GMT</date><size>4065kb</size><source_type>D</source_type></version><title>Using Predicted Weights for Ad Delivery</title><authors>Thomas Lavastida, Benjamin Moseley, R. Ravi and Chenyang Xu</authors><categories>cs.DS</categories><comments>15 pages, 10 figures. To appear in ACDA 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of a proportional weights algorithm for online
capacitated bipartite matching modeling the delivery of impression ads. The
algorithm uses predictions on the advertiser nodes to match arriving impression
nodes fractionally in proportion to the weights of its neighbors. This paper
gives a thorough empirical study of the performance of the algorithm on a
data-set of ad impressions from Yahoo! and shows its superior performance
compared to natural baselines such as a greedy water-filling algorithm and the
ranking algorithm. The proportional weights algorithm has recently received
interest in the theoretical literature where it was shown to have strong
guarantees beyond the worst-case model of algorithms augmented with
predictions. We extend these results to the case where the advertisers'
capacities are no longer stationary over time. Additionally, we show the
algorithm has near optimal performance in the random-order arrival model when
the number of impressions and the optimal matching are sufficiently large.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01080</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01080</id><submitter>Marcel Sarstedt</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:28:43 GMT</date><size>1715kb</size><source_type>D</source_type></version><title>Comparison of Convexificated SQCQP and PSO for the Optimal Transmission
  System Operation based on Incremental In-Phase and Quadrature Voltage
  Controlled Transformers</title><authors>Marcel Sarstedt, Thomas Leveringhaus, Leonard Klu{\ss}, Lutz Hofmann</authors><categories>eess.SY cs.SY</categories><comments>8 pager, 7 figures, conference, under review</comments><doi>10.13140/RG.2.2.10051.32806</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The optimal operation of electrical energy systems by solving a security
constrained optimal power flow (SCOPF) problem is still a challenging research
aspect. Especially, for conventional optimization methods like sequential
quadratic constrained quadratic programming (SQCQP) the formulation of the
incremental control variables like in-phase and quadrature voltage controlled
transformers in a solver suitable way is complex. Compared to this, the
implementation of these control variables within heuristic approaches like the
particle swarm optimization (PSO) is simple but problem specific adaptations of
the classic PSO algorithm are necessary to avoid an unfortunate swarm behavior
and local convergence in bad results. The objective of this paper is to
introduce a SQCQP and a modified PSO approach in detail to solve the SCOPF
problem adequately under consideration of flexible incremental in-phase and
quadrature transformers tap sets and to compare and benchmark the results of
both approaches for an adapted IEEE 118-bus system. The casestudy shows that
both approaches lead to suitable results of the SCOPF with individual
advantages of the SQCQP concerning the quality and the reproducibility of the
results while the PSO lead to faster solutions when the complexity of the
investigation scenario increases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01083</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01083</id><submitter>Kai Li</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:35:55 GMT</date><size>1133kb</size></version><title>The data paper as a socio-linguistic epistemic object: A content
  analysis on the rhetorical moves used in data paper abstracts</title><authors>Kai Li, Chenyue Jiao</authors><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The data paper is an emerging academic genre that focuses on the description
of research data objects. However, there is a lack of empirical knowledge about
this rising genre in quantitative science studies, particularly from the
perspective of its linguistic features. To fill this gap, this research aims to
offer a first quantitative examination of which rhetorical moves-rhetorical
units performing a coherent narrative function-are used in data paper
abstracts, as well as how these moves are used. To this end, we developed a new
classification scheme for rhetorical moves in data paper abstracts by expanding
a well-received system that focuses on English-language research article
abstracts. We used this expanded scheme to classify and analyze rhetorical
moves used in two flagship data journals, Scientific Data and Data in Brief. We
found that data papers exhibit a combination of IMRaD- and data-oriented moves
and that the usage differences between the journals can be largely explained by
journal policies concerning abstract and paper structure. This research offers
a novel examination of how the data paper, a novel data-oriented knowledge
representation, is composed, which greatly contributes to a deeper
understanding of data and data publication in the scholarly communication
system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01084</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01084</id><submitter>Ayed Alrashdi</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:38:56 GMT</date><size>1147kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 01:25:34 GMT</date><size>1204kb</size><source_type>D</source_type></version><title>Asymptotic Characterisation of Regularised Zero-Forcing Receiver for
  Imperfect and Correlated Massive MIMO Systems with Optimal Power Allocation</title><authors>Ayed M. Alrashdi</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we present asymptotic high dimensional analysis of the
regularised zero-forcing (RZF) receiver in terms of its mean squared error
(MSE) and bit error rate (BER) when used for the recovery of binary phase shift
keying (BPSK) modulated signals in a massive multiple-input multiple-output
(MIMO) communication system. We assume that the channel matrix is spatially
correlated and not perfectly known. We use the linear minimum mean squared
error (LMMSE) method to estimate the channel matrix. The asymptotic
approximations of the MSE and BER enable us to solve various practical
optimisation problems. Under MSE/BER minimisation, we derive 1) the optimal
regularisation factor for RZF; 2) the optimal power allocation scheme.
Numerical simulations show a close match to the derived asymptotic results even
for a few dozens of the problem dimensions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01085</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01085</id><submitter>Divyam Madaan</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:39:25 GMT</date><size>3202kb</size><source_type>D</source_type></version><title>Online Coreset Selection for Rehearsal-based Continual Learning</title><authors>Jaehong Yoon, Divyam Madaan, Eunho Yang, Sung Ju Hwang</authors><categories>cs.LG cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a target
dataset while selecting high-affinity samples to past tasks, which directly
inhibits catastrophic forgetting. We validate the effectiveness of our coreset
selection mechanism over various standard, imbalanced, and noisy datasets
against strong continual learning baselines, demonstrating that it improves
task adaptation and prevents catastrophic forgetting in a sample-efficient
manner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01086</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01086</id><submitter>Junyoung Park</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:40:22 GMT</date><size>2344kb</size><source_type>D</source_type></version><title>Learning to schedule job-shop problems: Representation and policy
  learning using graph neural network and reinforcement learning</title><authors>Junyoung Park, Jaehyeong Chun, Sang Hun Kim, Youngkook Kim, Jinkyoo
  Park</authors><categories>cs.AI cs.MA</categories><comments>16 pages, 8 figures</comments><journal-ref>International Journal of Production Research International Journal
  of Production Research, Volume 59, 2021 - Issue 11, Pages 3360-3377</journal-ref><doi>10.1080/00207543.2020.1870013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework to learn to schedule a job-shop problem (JSSP) using a
graph neural network (GNN) and reinforcement learning (RL). We formulate the
scheduling process of JSSP as a sequential decision-making problem with graph
representation of the state to consider the structure of JSSP. In solving the
formulated problem, the proposed framework employs a GNN to learn that node
features that embed the spatial structure of the JSSP represented as a graph
(representation learning) and derive the optimum scheduling policy that maps
the embedded node features to the best scheduling action (policy learning). We
employ Proximal Policy Optimization (PPO) based RL strategy to train these two
modules in an end-to-end fashion. We empirically demonstrate that the GNN
scheduler, due to its superb generalization capability, outperforms practically
favored dispatching rules and RL-based schedulers on various benchmark JSSP. We
also confirmed that the proposed framework learns a transferable scheduling
policy that can be employed to schedule a completely new JSSP (in terms of size
and parameters) without further training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01087</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01087</id><submitter>Isabelle Augenstein</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:42:56 GMT</date><size>6691kb</size><source_type>D</source_type></version><title>Is Sparse Attention more Interpretable?</title><authors>Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell</authors><categories>cs.CL</categories><journal-ref>Proceedings of ACL-IJCNLP 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse attention has been claimed to increase model interpretability under
the assumption that it highlights influential inputs. Yet the attention
distribution is typically over representations internal to the model rather
than the inputs themselves, suggesting this assumption may not have merit. We
build on the recent work exploring the interpretability of attention; we design
a set of experiments to help us understand how sparsity affects our ability to
use attention as an explainability tool. On three text classification tasks, we
verify that only a weak relationship between inputs and co-indexed intermediate
representations exists -- under sparse attention and otherwise. Further, we do
not find any plausible mappings from sparse attention distributions to a sparse
set of influential inputs through other avenues. Rather, we observe in this
setting that inducing sparsity may make it less plausible that attention can be
used as a tool for understanding model behavior.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01088</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01088</id><submitter>Haisheng Su</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:43:49 GMT</date><size>6867kb</size><source_type>D</source_type></version><title>TSI: Temporal Saliency Integration for Video Action Recognition</title><authors>Haisheng Su, Jinyuan Feng, Dongliang Wang, Weihao Gan, Wei Wu, Yu Qiao</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient spatiotemporal modeling is an important yet challenging problem for
video action recognition. Existing state-of-the-art methods exploit motion
clues to assist in short-term temporal modeling through temporal difference
over consecutive frames. However, background noises will be inevitably
introduced due to the camera movement. Besides, movements of different actions
can vary greatly. In this paper, we propose a Temporal Saliency Integration
(TSI) block, which mainly contains a Salient Motion Excitation (SME) module and
a Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to
highlight the motion-sensitive area through local-global motion modeling, where
the background suppression and pyramidal feature difference are conducted
successively between neighboring frames to capture motion dynamics with less
background noises. CTI is designed to perform multi-scale temporal modeling
through a group of separate 1D convolutions respectively. Meanwhile, temporal
interactions across different scales are integrated with attention mechanism.
Through these two modules, long short-term temporal relationships can be
encoded efficiently by introducing limited additional parameters. Extensive
experiments are conducted on several popular benchmarks (i.e.,
Something-Something v1 &amp; v2, Kinetics-400, UCF-101, and HMDB-51), which
demonstrate the effectiveness and superiority of our proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01090</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01090</id><submitter>Jan Westerdiep</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:47:56 GMT</date><size>711kb</size><source_type>D</source_type></version><title>Minimal residual space-time discretizations of parabolic equations:
  Asymmetric spatial operators</title><authors>Rob Stevenson, Jan Westerdiep</authors><categories>math.NA cs.NA</categories><msc-class>35K20, 41A25, 65M12, 65M15, 65M60, 35B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a minimal residual discretization of a simultaneous space-time
variational formulation of parabolic evolution equations. Under the usual `LBB'
stability condition on pairs of trial- and test spaces we show quasi-optimality
of the numerical approximations without assuming symmetry of the spatial part
of the differential operator. Under a stronger LBB condition we show error
estimates in an energy-norm which are independent of this spatial differential
operator.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01091</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01091</id><submitter>Janna De Boer</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:50:49 GMT</date><size>1060kb</size></version><title>belabBERT: a Dutch RoBERTa-based language model applied to psychiatric
  classification</title><authors>Joppe Wouts, Janna de Boer, Alban Voppel, Sanne Brederoo, Sander van
  Splunter and Iris Sommer</authors><categories>cs.CL</categories><comments>arXiv admin note: substantial text overlap with arXiv:2008.01543</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Natural language processing (NLP) is becoming an important means for
automatic recognition of human traits and states, such as intoxication,
presence of psychiatric disorders, presence of airway disorders and states of
stress. Such applications have the potential to be an important pillar for
online help lines, and may gradually be introduced into eHealth modules.
However, NLP is language specific and for languages such as Dutch, NLP models
are scarce. As a result, recent Dutch NLP models have a low capture of long
range semantic dependencies over sentences. To overcome this, here we present
belabBERT, a new Dutch language model extending the RoBERTa architecture.
belabBERT is trained on a large Dutch corpus (+32 GB) of web crawled texts. We
applied belabBERT to the classification of psychiatric illnesses. First, we
evaluated the strength of text-based classification using belabBERT, and
compared the results to the existing RobBERT model. Then, we compared the
performance of belabBERT to audio classification for psychiatric disorders.
Finally, a brief exploration was performed, extending the framework to a hybrid
text- and audio-based classification. Our results show that belabBERT
outperformed the current best text classification network for Dutch, RobBERT.
belabBERT also outperformed classification based on audio alone.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01092</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01092</id><submitter>Henry WJ Reeve</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:52:31 GMT</date><size>81kb</size></version><title>Statistical optimality conditions for compressive ensembles</title><authors>Henry W. J. Reeve, Ata Kaban</authors><categories>cs.LG math.ST stat.TH</categories><msc-class>62-08</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a framework for the theoretical analysis of ensembles of
low-complexity empirical risk minimisers trained on independent random
compressions of high-dimensional data. First we introduce a general
distribution-dependent upper-bound on the excess risk, framed in terms of a
natural notion of compressibility. This bound is independent of the dimension
of the original data representation, and explains the in-built regularisation
effect of the compressive approach. We then instantiate this general bound to
classification and regression tasks, considering Johnson-Lindenstrauss mappings
as the compression scheme. For each of these tasks, our strategy is to develop
a tight upper bound on the compressibility function, and by doing so we
discover distributional conditions of geometric nature under which the
compressive algorithm attains minimax-optimal rates up to at most
poly-logarithmic factors. In the case of compressive classification, this is
achieved with a mild geometric margin condition along with a flexible moment
condition that is significantly more general than the assumption of bounded
domain. In the case of regression with strongly convex smooth loss functions we
find that compressive regression is capable of exploiting spectral decay with
near-optimal guarantees. In addition, a key ingredient for our central upper
bound is a high probability uniform upper bound on the integrated deviation of
dependent empirical processes, which may be of independent interest.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01093</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01093</id><submitter>Ruisheng Cao</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:53:35 GMT</date><size>5875kb</size><source_type>D</source_type></version><title>LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and
  Non-Local Relations</title><authors>Ruisheng Cao, Lu Chen, Zhi Chen, Su Zhu and Kai Yu</authors><categories>cs.CL</categories><comments>15 pages, 8 figures, accepted to ACL 2021 main conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims to tackle the challenging heterogeneous graph encoding problem
in the text-to-SQL task. Previous methods are typically node-centric and merely
utilize different weight matrices to parameterize edge types, which 1) ignore
the rich semantics embedded in the topological structure of edges, and 2) fail
to distinguish local and non-local relations for each node. To this end, we
propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying
relational features without constructing meta-paths. By virtue of the line
graph, messages propagate more efficiently through not only connections between
nodes, but also the topology of directed edges. Furthermore, both local and
non-local relations are integrated distinctively during the graph iteration. We
also design an auxiliary task called graph pruning to improve the
discriminative capability of the encoder. Our framework achieves
state-of-the-art results (62.8% with Glove, 72.0% with Electra) on the
cross-domain text-to-SQL benchmark Spider at the time of writing.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01096</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01096</id><submitter>Zhu Zhang</submitter><version version="v1"><date>Wed, 2 Jun 2021 11:58:30 GMT</date><size>691kb</size><source_type>D</source_type></version><title>Learning to Rehearse in Long Sequence Memorization</title><authors>Zhu Zhang, Chang Zhou, Jianxin Ma, Zhijie Lin, Jingren Zhou, Hongxia
  Yang and Zhou Zhao</authors><categories>cs.LG</categories><comments>Accepted by ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing reasoning tasks often have an important assumption that the input
contents can be always accessed while reasoning, requiring unlimited storage
resources and suffering from severe time delay on long sequences. To achieve
efficient reasoning on long sequences with limited storage resources, memory
augmented neural networks introduce a human-like write-read memory to compress
and memorize the long input sequence in one pass, trying to answer subsequent
queries only based on the memory. But they have two serious drawbacks: 1) they
continually update the memory from current information and inevitably forget
the early contents; 2) they do not distinguish what information is important
and treat all contents equally. In this paper, we propose the Rehearsal Memory
(RM) to enhance long-sequence memorization by self-supervised rehearsal with a
history sampler. To alleviate the gradual forgetting of early information, we
design self-supervised rehearsal training with recollection and familiarity
tasks. Further, we design a history sampler to select informative fragments for
rehearsal training, making the memory focus on the crucial information. We
evaluate the performance of our rehearsal memory by the synthetic bAbI task and
several downstream tasks, including text/video question answering and
recommendation on long sequences.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01097</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01097</id><submitter>Sidharth Pancholi</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:01:47 GMT</date><size>4303kb</size><source_type>D</source_type></version><title>T-BERT -- Model for Sentiment Analysis of Micro-blogs Integrating Topic
  Model and BERT</title><authors>Sarojadevi Palani, Prabhu Rajagopal, Sidharth Pancholi</authors><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sentiment analysis (SA) has become an extensive research area in recent years
impacting diverse fields including ecommerce, consumer business, and politics,
driven by increasing adoption and usage of social media platforms. It is
challenging to extract topics and sentiments from unsupervised short texts
emerging in such contexts, as they may contain figurative words, strident data,
and co-existence of many possible meanings for a single word or phrase, all
contributing to obtaining incorrect topics. Most prior research is based on a
specific theme/rhetoric/focused-content on a clean dataset. In the work
reported here, the effectiveness of BERT(Bidirectional Encoder Representations
from Transformers) in sentiment classification tasks from a raw live dataset
taken from a popular microblogging platform is demonstrated. A novel T-BERT
framework is proposed to show the enhanced performance obtainable by combining
latent topics with contextual BERT embeddings. Numerical experiments were
conducted on an ensemble with about 42000 datasets using NimbleBox.ai platform
with a hardware configuration consisting of Nvidia Tesla K80(CUDA), 4 core CPU,
15GB RAM running on an isolated Google Cloud Platform instance. The empirical
results show that the model improves in performance while adding topics to BERT
and an accuracy rate of 90.81% on sentiment classification using BERT with the
proposed approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01098</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01098</id><submitter>Bastian Rieck</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:04:29 GMT</date><size>595kb</size><source_type>D</source_type></version><title>Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and
  Practical Solutions</title><authors>Leslie O'Bray, Max Horn, Bastian Rieck, Karsten Borgwardt</authors><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph generative models are a highly active branch of machine learning. Given
the steady development of new models of ever-increasing complexity, it is
necessary to provide a principled way to evaluate and compare them. In this
paper, we enumerate the desirable criteria for comparison metrics, discuss the
development of such metrics, and provide a comparison of their respective
expressive power. We perform a systematic evaluation of the main metrics in use
today, highlighting some of the challenges and pitfalls researchers
inadvertently can run into. We then describe a collection of suitable metrics,
give recommendations as to their practical suitability, and analyse their
behaviour on synthetically generated perturbed graphs as well as on recently
proposed graph generative models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01099</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01099</id><submitter>Lukas Burgholzer</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:04:56 GMT</date><size>120kb</size><source_type>D</source_type></version><title>Towards Verification of Dynamic Quantum Circuits</title><authors>Lukas Burgholzer and Robert Wille</authors><categories>quant-ph cs.ET</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computers are reaching a level where interactions between classical
and quantum computations can happen in real-time. This marks the advent of a
new, broader class of quantum circuits: dynamic quantum circuits. They offer a
broader range of available computing primitives that lead to new challenges for
design tasks such as simulation, compilation, and verification. Due to the
non-unitary nature of dynamic circuit primitives, existing techniques for these
tasks are no longer applicable in an out-of-the-box fashion. In this work, we
discuss the resulting consequences for quantum circuit verification and present
first ideas for corresponding automatic methods. More precisely, we propose two
different schemes that eventually allow to treat the involved circuits as if
they were not dynamic at all. By this, we provide a basis for applying existing
techniques for quantum circuit verification to this broader class of circuits.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01100</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01100</id><submitter>Michel Pohl</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:07:31 GMT</date><size>26175kb</size><source_type>D</source_type></version><title>Prediction of the Position of External Markers Using a Recurrent Neural
  Network Trained With Unbiased Online Recurrent Optimization for Safe Lung
  Cancer Radiotherapy</title><authors>Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi and
  Ritu Bhusal Chhatkuli</authors><categories>eess.IV cs.CV cs.LG cs.NE</categories><comments>20 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  During lung cancer radiotherapy, the position of infrared reflective objects
on the chest can be recorded to estimate the tumor location. However,
radiotherapy systems usually have a latency inherent to robot control
limitations that impedes the radiation delivery precision. Not taking this
phenomenon into account may cause unwanted damage to healthy tissues and lead
to side effects such as radiation pneumonitis. In this research, we use nine
observation records of the three-dimensional position of three external markers
on the chest and abdomen of healthy individuals breathing during intervals from
73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the
recorded trajectories range from 6mm to 40mm in the superior-inferior
direction. We forecast the location of each marker simultaneously with a
horizon value (the time interval in advance for which the prediction is made)
between 0.1s and 2.0s, using a recurrent neural network (RNN) trained with
unbiased online recurrent optimization (UORO). We compare its performance with
an RNN trained with real-time recurrent learning, least mean squares (LMS), and
offline linear regression. Training and cross-validation are performed during
the first minute of each sequence. On average, UORO achieves the lowest
root-mean-square (RMS) and maximum error, equal respectively to 1.3mm and
8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core
i9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon
values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,
and UORO for horizon values greater than 0.6s.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01101</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01101</id><submitter>Gilad Yehudai</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:09:55 GMT</date><size>288kb</size><source_type>D</source_type></version><title>Learning a Single Neuron with Bias Using Gradient Descent</title><authors>Gal Vardi, Gilad Yehudai, Ohad Shamir</authors><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We theoretically study the fundamental problem of learning a single neuron
with a bias term ($\mathbf{x} \mapsto \sigma(&lt;\mathbf{w},\mathbf{x}&gt; + b)$) in
the realizable setting with the ReLU activation, using gradient descent.
Perhaps surprisingly, we show that this is a significantly different and more
challenging problem than the bias-less case (which was the focus of previous
works on single neurons), both in terms of the optimization geometry as well as
the ability of gradient methods to succeed in some scenarios. We provide a
detailed study of this problem, characterizing the critical points of the
objective, demonstrating failure cases, and providing positive convergence
guarantees under different sets of assumptions. To prove our results, we
develop some tools which may be of independent interest, and improve previous
results on learning single neurons.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01105</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01105</id><submitter>Sebastin Santy</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:12:59 GMT</date><size>130kb</size><source_type>D</source_type></version><title>Use of Formal Ethical Reviews in NLP Literature: Historical Trends and
  Current Practices</title><authors>Sebastin Santy, Anku Rani, Monojit Choudhury</authors><categories>cs.CL</categories><comments>Accepted at ACL 2021 Findings (7 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ethical aspects of research in language technologies have received much
attention recently. It is a standard practice to get a study involving human
subjects reviewed and approved by a professional ethics committee/board of the
institution. How commonly do we see mention of ethical approvals in NLP
research? What types of research or aspects of studies are usually subject to
such reviews? With the rising concerns and discourse around the ethics of NLP,
do we also observe a rise in formal ethical reviews of NLP studies? And, if so,
would this imply that there is a heightened awareness of ethical issues that
was previously lacking? We aim to address these questions by conducting a
detailed quantitative and qualitative analysis of the ACL Anthology, as well as
comparing the trends in our field to those of other related disciplines, such
as cognitive science, machine learning, data mining, and systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01108</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01108</id><submitter>Fabien Dufoulon</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:18:57 GMT</date><size>316kb</size><source_type>D</source_type></version><title>Efficient Deterministic Leader Election for Programmable Matter</title><authors>Fabien Dufoulon (Technion - Israel Institute of Technology), Shay
  Kutten (Technion - Israel Institute of Technology) and William K. Moses Jr.
  (University of Houston)</authors><categories>cs.DC cs.DS</categories><comments>PODC 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  It was suggested that a programmable matter system (composed of multiple
computationally weak mobile particles) should remain connected at all times
since otherwise, reconnection is difficult and may be impossible. At the same
time, it was not clear that allowing the system to disconnect carried a
significant advantage in terms of time complexity. We demonstrate for a
fundamental task, that of leader election, an algorithm where the system
disconnects and then reconnects automatically in a non-trivial way (particles
can move far away from their former neighbors and later reconnect to others).
Moreover, the runtime of the temporarily disconnecting deterministic leader
election algorithm is linear in the diameter. Hence, the disconnecting --
reconnecting algorithm is as fast as previous randomized algorithms. When
comparing to previous deterministic algorithms, we note that some of the
previous work assumed weaker schedulers. Still, the runtime of all the previous
deterministic algorithms that did not assume special shapes of the particle
system (shapes with no holes) was at least quadratic in $n$, where $n$ is the
number of particles in the system. (Moreover, the new algorithm is even faster
in some parameters than the deterministic algorithms that did assume special
initial shapes.)
  Since leader election is an important module in algorithms for various other
tasks, the presented algorithm can be useful for speeding up other algorithms
under the assumption of a strong scheduler. This leaves open the question: &quot;can
a deterministic algorithm be as fast as the randomized ones also under weaker
schedulers?&quot;
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01109</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01109</id><submitter>Pritam Anand Dr.</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:19:28 GMT</date><size>3629kb</size><source_type>D</source_type></version><title>Improvement over Pinball Loss Support Vector Machine</title><authors>Pritam Anand, Reshma Rastogi and Suresh Chandra</authors><categories>cs.LG stat.ML</categories><comments>The numerical results presented in this paper can be regenerated by
  the code available at https://github.com/ltpritamanand/UnifiedPinSVM/ . We
  hope that our this work will let the researchers to use the correct
  formulation of Pin-SVM model in future and improve the predictions across
  different domain of technologies</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, there have been several papers that discuss the extension of the
Pinball loss Support Vector Machine (Pin-SVM) model, originally proposed by
Huang et al.,[1][2]. Pin-SVM classifier deals with the pinball loss function,
which has been defined in terms of the parameter $\tau$. The parameter $\tau$
can take values in $[ -1,1]$. The existing Pin-SVM model requires to solve the
same optimization problem for all values of $\tau$ in $[ -1,1]$. In this paper,
we improve the existing Pin-SVM model for the binary classification task. At
first, we note that there is major difficulty in Pin-SVM model (Huang et al.
[1]) for $ -1 \leq \tau &lt; 0$. Specifically, we show that the Pin-SVM model
requires the solution of different optimization problem for $ -1 \leq \tau &lt;
0$. We further propose a unified model termed as Unified Pin-SVM which results
in a QPP valid for all $-1\leq \tau \leq 1$ and hence more convenient to use.
The proposed Unified Pin-SVM model can obtain a significant improvement in
accuracy over the existing Pin-SVM model which has also been empirically
justified by extensive numerical experiments with real-world datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01110</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01110</id><submitter>Efi Psomopoulou</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:20:19 GMT</date><size>27878kb</size><source_type>D</source_type></version><title>A robust controller for stable 3D pinching using tactile sensing</title><authors>Efi Psomopoulou, Nicholas Pestell, Fotios Papadopoulos, John Lloyd,
  Zoe Doulgeri, Nathan F. Lepora</authors><categories>cs.RO cs.SY eess.SY</categories><comments>8 pages, 10 figures, 1 appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a controller for stable grasping of unknown-shaped
objects by two robotic fingers with tactile fingertips. The grasp is stabilised
by rolling the fingertips on the contact surface and applying a desired
grasping force to reach an equilibrium state. The validation is both in
simulation and on a fully-actuated robot hand (the Shadow Modular Grasper)
fitted with custom-built optical tactile sensors (based on the BRL TacTip). The
controller requires the orientations of the contact surfaces, which are
estimated by regressing a deep convolutional neural network over the tactile
images. Overall, the grasp system is demonstrated to achieve stable equilibrium
poses on a range of objects varying in shape and softness, with the system
being robust to perturbations and measurement errors. This approach also has
promise to extend beyond grasping to stable in-hand object manipulation with
multiple fingers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01111</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01111</id><submitter>Wei Sun</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:23:16 GMT</date><size>742kb</size><source_type>D</source_type></version><title>Deep Learning based Full-reference and No-reference Quality Assessment
  Models for Compressed UGC Videos</title><authors>Wei Sun and Tao Wang and Xiongkuo Min and Fuwang Yi and Guangtao Zhai</authors><categories>eess.IV cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a deep learning based video quality assessment
(VQA) framework to evaluate the quality of the compressed user's generated
content (UGC) videos. The proposed VQA framework consists of three modules, the
feature extraction module, the quality regression module, and the quality
pooling module. For the feature extraction module, we fuse the features from
intermediate layers of the convolutional neural network (CNN) network into
final quality-aware feature representation, which enables the model to make
full use of visual information from low-level to high-level. Specifically, the
structure and texture similarities of feature maps extracted from all
intermediate layers are calculated as the feature representation for the full
reference (FR) VQA model, and the global mean and standard deviation of the
final feature maps fused by intermediate feature maps are calculated as the
feature representation for the no reference (NR) VQA model. For the quality
regression module, we use the fully connected (FC) layer to regress the
quality-aware features into frame-level scores. Finally, a
subjectively-inspired temporal pooling strategy is adopted to pool frame-level
scores into the video-level score. The proposed model achieves the best
performance among the state-of-the-art FR and NR VQA models on the Compressed
UGC VQA database and also achieves pretty good performance on the in-the-wild
UGC VQA databases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01112</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01112</id><submitter>Chen Zhang</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:23:18 GMT</date><size>223kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 07:21:35 GMT</date><size>223kb</size><source_type>D</source_type></version><title>DynaEval: Unifying Turn and Dialogue Level Evaluation</title><authors>Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas
  Friedrichs, Grandee Lee, Haizhou Li</authors><categories>cs.CL</categories><comments>ACL-IJCNLP 2021 (Main conference, Long paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dialogue is essentially a multi-turn interaction among interlocutors.
Effective evaluation metrics should reflect the dynamics of such interaction.
Existing automatic metrics are focused very much on the turn-level quality,
while ignoring such dynamics. To this end, we propose DynaEval, a unified
automatic evaluation framework which is not only capable of performing
turn-level evaluation, but also holistically considers the quality of the
entire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted
to model a dialogue in totality, where the graph nodes denote each individual
utterance and the edges represent the dependency between pairs of utterances. A
contrastive loss is then applied to distinguish well-formed dialogues from
carefully constructed negative samples. Experiments show that DynaEval
significantly outperforms the state-of-the-art dialogue coherence model, and
correlates strongly with human judgements across multiple dialogue evaluation
aspects at both turn and dialogue level.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01114</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01114</id><submitter>Yohann Rioual</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:23:47 GMT</date><size>296kb</size><source_type>D</source_type></version><title>Design and Comparison of Reward Functions in Reinforcement Learning for
  Energy Management of Sensor Nodes</title><authors>Yohann Rioual (1), Yannick Le Moullec (2), Johann Laurent (1), Muhidul
  Islam Khan (2) and Jean-Philippe Diguet (3) ((1) Lab-STICC, University
  Bretagne Sud, (2) Thomas Johann Seebeck Department of Electronics, Tallinn
  University of Technology, (3) IRL CNRS CROSSING)</authors><categories>eess.SY cs.LG cs.SY</categories><comments>13 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Interest in remote monitoring has grown thanks to recent advancements in
Internet-of-Things (IoT) paradigms. New applications have emerged, using small
devices called sensor nodes capable of collecting data from the environment and
processing it. However, more and more data are processed and transmitted with
longer operational periods. At the same, the battery technologies have not
improved fast enough to cope with these increasing needs. This makes the energy
consumption issue increasingly challenging and thus, miniaturized energy
harvesting devices have emerged to complement traditional energy sources.
Nevertheless, the harvested energy fluctuates significantly during the node
operation, increasing uncertainty in actually available energy resources.
Recently, approaches in energy management have been developed, in particular
using reinforcement learning approaches. However, in reinforcement learning,
the algorithm's performance relies greatly on the reward function. In this
paper, we present two contributions. First, we explore five different reward
functions to identify the most suitable variables to use in such functions to
obtain the desired behaviour. Experiments were conducted using the Q-learning
algorithm to adjust the energy consumption depending on the energy harvested.
Results with the five reward functions illustrate how the choice thereof
impacts the energy consumption of the node. Secondly, we propose two additional
reward functions able to find the compromise between energy consumption and a
node performance using a non-fixed balancing parameter. Our simulation results
show that the proposed reward functions adjust the node's performance depending
on the battery level and reduce the learning time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01115</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01115</id><submitter>Chukri Soueidi</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:27:23 GMT</date><size>337kb</size><source_type>D</source_type></version><title>Efficient and Expressive Bytecode-Level Instrumentation for Java
  Programs</title><authors>Chukri Soueidi, Marius Monnier, Ali Kassem, Yli\`es Falcone (Univ.
  Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Grenoble, France)</authors><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient and expressive tool for the instrumentation of Java
programs at the bytecode-level. BISM (Bytecode-Level Instrumentation for
Software Monitoring) is a light-weight Java bytecode instrumentation tool that
features an expressive high-level control-flow-aware instrumentation language.
The language is inspired by the aspect-oriented programming paradigm in
modularizing instrumentation into separate transformers, that encapsulate
joinpoint selection and advice inlining. BISM allows capturing joinpoints
ranging from bytecode instructions to methods execution and provides
comprehensive static and dynamic context information. It runs in two
instrumentation modes: build-time and load-time. BISM also provides a mechanism
to compose transformers and automatically detect their collision in the base
program. Transformers in a composition can control the visibility of their
advice and other instructions from the base program. We show several example
applications for BISM and demonstrate its effectiveness using three
experiments: a security scenario, a financial transaction system, and a general
runtime verification case. The results show that BISM instrumentation incurs
low runtime and memory overheads.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01116</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01116</id><submitter>Vladimir Shpilrain</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:27:33 GMT</date><size>34kb</size><source_type>D</source_type></version><title>MOBS (Matrices Over Bit Strings) public key exchange</title><authors>Nael Rahman and Vladimir Shpilrain</authors><categories>cs.CR math.GR</categories><comments>7 pages</comments><msc-class>94A60, 15B33, 20H25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use matrices over bit strings as platforms for Diffie-Hellman-like public
key exchange protocols. When multiplying matrices like that, we use Boolean OR
operation on bit strings in place of addition and Boolean AND operation in
place of multiplication. As a result, (1) computations with these matrices are
very efficient; (2) standard methods of attacking Diffie-Hellman-like protocols
are not applicable.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01121</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01121</id><submitter>Motonobu Kanagawa</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:45:49 GMT</date><size>50kb</size></version><title>Connections and Equivalences between the Nystr\&quot;om Method and Sparse
  Variational Gaussian Processes</title><authors>Veit Wild, Motonobu Kanagawa, Dino Sejdinovic</authors><categories>stat.ML cs.LG math.ST stat.ME stat.TH</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the connections between sparse approximation methods for
making kernel methods and Gaussian processes (GPs) scalable to massive data,
focusing on the Nystr\&quot;om method and the Sparse Variational Gaussian Processes
(SVGP). While sparse approximation methods for GPs and kernel methods share
some algebraic similarities, the literature lacks a deep understanding of how
and why they are related. This is a possible obstacle for the communications
between the GP and kernel communities, making it difficult to transfer results
from one side to the other. Our motivation is to remove this possible obstacle,
by clarifying the connections between the sparse approximations for GPs and
kernel methods. In this work, we study the two popular approaches, the
Nystr\&quot;om and SVGP approximations, in the context of a regression problem, and
establish various connections and equivalences between them. In particular, we
provide an RKHS interpretation of the SVGP approximation, and show that the
Evidence Lower Bound of the SVGP contains the objective function of the
Nystr\&quot;om approximation, revealing the origin of the algebraic equivalence
between the two approaches. We also study recently established convergence
results for the SVGP and how they are related to the approximation quality of
the Nystr\&quot;om method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01122</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01122</id><submitter>Xin-Long Luo</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:46:53 GMT</date><size>62kb</size><source_type>D</source_type></version><title>Tikhonov regularization continuation methods and the trust-region
  updating strategy for linearly equality-constrained optimization problems</title><authors>Xin-long Luo and Hang Xiao</authors><categories>math.NA cs.NA math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:2101.07055,
  arXiv:2012.14808</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper considers the Tikhonv regularization continuation method and the
trust-region updating strategy for the linearly equality-constrained
optimization problem (Trcmtr). Moreover, in order to improve its computational
efficiency and robustness, the new method uses the switching preconditioned
technique. That is to say, the new method uses the L-BFGS method as the
preconditioned technique to improve its computational efficiency in the
well-posed phase. Otherwise, it uses the inverse of the regularization
two-sided projected Hessian matrix as the pre-conditioner to improve its
robustness. Numerical results also show that the new method is more robust and
faster than the traditional optimization method such as the sequential
quadratic programming (SQP), the alternating direction method of multipliers
(ADMM) and the latest continuation method (Ptctr). The computational time of
the new method is about one fifth of that of SQP for the large-scale problem.
Finally, the global convergence analysis of the new method is also given.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01124</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01124</id><submitter>Jun Liu</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:48:15 GMT</date><size>3848kb</size><source_type>D</source_type></version><title>Opening the Black Box of Deep Neural Networks in Physical Layer
  Communication</title><authors>Jun Liu, Kai Mei, Dongtang Ma and Jibo Wei</authors><categories>eess.SP cs.IT cs.LG math.IT</categories><comments>5 pages, 5 figures, submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Network (DNN)-based physical layer techniques are attracting
considerable interest due to their potential to enhance communication systems.
However, most studies in the physical layer have tended to focus on the
implement of DNN but not to theoretically understand how does a DNN work in a
communication system. In this letter, we aim to quantitatively analyse why DNNs
can achieve comparable performance in the physical layer comparing with
traditional techniques and its cost in terms of computational complexity. We
further investigate and also experimentally validate how information is flown
in a DNN-based communication system under the information theoretic concepts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01127</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01127</id><submitter>Chun-Hao Chang</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:48:29 GMT</date><size>28765kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 06:14:35 GMT</date><size>26440kb</size><source_type>D</source_type></version><title>Towards Robust Classification Model by Counterfactual and Invariant Data
  Generation</title><authors>Chun-Hao Chang, George Alexandru Adam, Anna Goldenberg</authors><categories>cs.CV cs.AI</categories><comments>Accepted in 2021 CVPR</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the success of machine learning applications in science, industry,
and society in general, many approaches are known to be non-robust, often
relying on spurious correlations to make predictions. Spuriousness occurs when
some features correlate with labels but are not causal; relying on such
features prevents models from generalizing to unseen environments where such
correlations break. In this work, we focus on image classification and propose
two data generation processes to reduce spuriousness. Given human annotations
of the subset of the features responsible (causal) for the labels (e.g.
bounding boxes), we modify this causal set to generate a surrogate image that
no longer has the same label (i.e. a counterfactual image). We also alter
non-causal features to generate images still recognized as the original labels,
which helps to learn a model invariant to these features. In several
challenging datasets, our data generations outperform state-of-the-art methods
in accuracy when spurious correlations break, and increase the saliency focus
on causal features providing better explanations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01128</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01128</id><submitter>Meyer Scetbon</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:50:56 GMT</date><size>5562kb</size><source_type>D</source_type></version><title>Linear-Time Gromov Wasserstein Distances using Low Rank Couplings and
  Costs</title><authors>Meyer Scetbon, Gabriel Peyr\'e, Marco Cuturi</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ability to compare and align related datasets living in heterogeneous
spaces plays an increasingly important role in machine learning. The
Gromov-Wasserstein (GW) formalism can help tackle this problem. Its main goal
is to seek an assignment (more generally a coupling matrix) that can register
points across otherwise incomparable datasets. As a non-convex and quadratic
generalization of optimal transport (OT), GW is NP-hard. Yet, heuristics are
known to work reasonably well in practice, the state of the art approach being
to solve a sequence of nested regularized OT problems. While popular, that
heuristic remains too costly to scale, with cubic complexity in the number of
samples $n$. We show in this paper how a recent variant of the Sinkhorn
algorithm can substantially speed up the resolution of GW. That variant
restricts the set of admissible couplings to those admitting a low rank
factorization as the product of two sub-couplings. By updating alternatively
each sub-coupling, our algorithm computes a stationary point of the problem in
quadratic time with respect to the number of samples. When cost matrices have
themselves low rank, our algorithm has time complexity $\mathcal{O}(n)$. We
demonstrate the efficiency of our method on simulated and real data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01131</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01131</id><submitter>Lee Livsey</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:57:31 GMT</date><size>486kb</size></version><title>Performance and Usability of Visual and Verbal Verification of
  Word-based Key Fingerprints</title><authors>Lee Livsey, Helen Petrie, Siamak F. Shahandashti, Aidan Fray</authors><categories>cs.HC cs.CR</categories><comments>This is an accepted manuscript to appear in the proceedings of the
  15th International Symposium on Human Aspects of Information Security &amp;
  Assurance, HAISA 2021</comments><msc-class>68M25</msc-class><acm-class>H.1.2</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The security of messaging applications against person-in-the-middle attacks
relies on the authenticity of the exchanged keys. For users unable to meet in
person, a manual key fingerprint verification is necessary to ascertain key
authenticity. Such fingerprints can be exchanged visually or verbally, and it
is not clear in which condition users perform best. This paper reports the
results of a 62-participant study that investigated differences in performance
and perceived usability of visual and verbal comparisons of word-based key
fingerprints, and the influence of the individual's cognitive learning style.
The results show visual comparisons to be more effective against non-security
critical errors and are perceived to provide increased confidence, yet
participants perceive verbal comparisons to be easier and require less mental
effort. Besides, limited evidence was found on the influence of the
individual's learning style on their performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01132</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01132</id><submitter>Benoit Dufumier</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:00:35 GMT</date><size>7498kb</size><source_type>D</source_type></version><title>Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data
  Augmentation and Deep Ensemble Learning</title><authors>Benoit Dufumier, Pietro Gori, Ilaria Battaglia, Julie Victor, Antoine
  Grigis, Edouard Duchesnay</authors><categories>cs.CV cs.LG eess.IV</categories><comments>17 pages, 6 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Deep Learning (DL) and specifically CNN models have become a de facto method
for a wide range of vision tasks, outperforming traditional machine learning
(ML) methods. Consequently, they drew a lot of attention in the neuroimaging
field in particular for phenotype prediction or computer-aided diagnosis.
However, most of the current studies often deal with small single-site cohorts,
along with a specific pre-processing pipeline and custom CNN architectures,
which make them difficult to compare to. We propose an extensive benchmark of
recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data
augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM)
pre-processing and quasi-raw images. Experiments were conducted on a large
multi-site 3D brain anatomical MRI data-set comprising N=10k scans on 3
challenging tasks: age prediction, sex classification, and schizophrenia
diagnosis. We found that all models provide significantly better predictions
with VBM images than quasi-raw data. This finding evolved as the training set
approaches 10k samples where quasi-raw data almost reach the performance of
VBM. Moreover, we showed that linear models perform comparably with SOTA CNN on
VBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter
version that we proposed, provide a good compromise in terms of performance in
all data regime. Therefore, we suggest to employ them as the architectures by
default. Critically, we also showed that current CNN are still very biased
towards the acquisition site, even when trained with N=10k multi-site images.
In this context, VBM pre-processing provides an efficient way to limit this
site effect. Surprisingly, we did not find any clear benefit from data
augmentation techniques. Finally, we proved that deep ensemble learning is well
suited to re-calibrate big CNN models without sacrificing performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01134</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01134</id><submitter>Wei Liao</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:05:24 GMT</date><size>737kb</size><source_type>D</source_type></version><title>Smooth Q-learning: Accelerate Convergence of Q-learning Using Similarity</title><authors>Wei Liao and Xiaohui Wei and Jizhou Lai</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An improvement of Q-learning is proposed in this paper. It is different from
classic Q-learning in that the similarity between different states and actions
is considered in the proposed method. During the training, a new updating
mechanism is used, in which the Q value of the similar state-action pairs are
updated synchronously. The proposed method can be used in combination with both
tabular Q-learning function and deep Q-learning. And the results of numerical
examples illustrate that compared to the classic Q-learning, the proposed
method has a significantly better performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01135</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01135</id><submitter>Noemie Perivier</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:05:34 GMT</date><size>913kb</size></version><title>MNL-Bandit with Knapsacks</title><authors>Abdellah Aznag, Vineet Goyal and Noemie Perivier</authors><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dynamic assortment selection problem where a seller has a fixed
inventory of $N$ substitutable products and faces an unknown demand that
arrives sequentially over $T$ periods. In each period, the seller needs to
decide on the assortment of products (of cardinality at most $K$) to offer to
the customers. The customer's response follows an unknown multinomial logit
model (MNL) with parameters $v$. The goal of the seller is to maximize the
total expected revenue given the fixed initial inventory of $N$ products. We
give a policy that achieves a regret of $\tilde O\left(K \sqrt{K N T}\left(1 +
\frac{\sqrt{v_{\max}}}{q_{\min}}\text{OPT}\right) \right)$ under a mild
assumption on the model parameters. In particular, our policy achieves a
near-optimal $\tilde O(\sqrt{T})$ regret in the large inventory setting.
  Our policy builds upon the UCB-based approach for MNL-bandit without
inventory constraints in [1] and addresses the inventory constraints through an
exponentially sized LP for which we present a tractable approximation while
keeping the $\tilde O(\sqrt{T})$ regret bound.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01138</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01138</id><submitter>Stephan Thaler</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:10:43 GMT</date><size>9116kb</size><source_type>AD</source_type></version><title>Learning neural network potentials from experimental data via
  Differentiable Trajectory Reweighting</title><authors>Stephan Thaler and Julija Zavadlav</authors><categories>physics.chem-ph cs.LG physics.comp-ph</categories><acm-class>J.2; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In molecular dynamics (MD), neural network (NN) potentials trained bottom-up
on quantum mechanical data have seen tremendous success recently. Top-down
approaches that learn NN potentials directly from experimental data have
received less attention, typically facing numerical and computational
challenges when backpropagating through MD simulations. We present the
Differentiable Trajectory Reweighting (DiffTRe) method, which bypasses
differentiation through the MD simulation for time-independent observables.
Leveraging thermodynamic perturbation theory, we avoid exploding gradients and
achieve around 2 orders of magnitude speed-up in gradient computation for
top-down learning. We show effectiveness of DiffTRe in learning NN potentials
for an atomistic model of diamond and a coarse-grained model of water based on
diverse experimental observables including thermodynamic, structural and
mechanical properties. Importantly, DiffTRe also generalizes bottom-up
structural coarse-graining methods such as iterative Boltzmann inversion to
arbitrary potentials. The presented method constitutes an important milestone
towards enriching NN potentials with experimental data, particularly when
accurate bottom-up data is unavailable.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01139</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01139</id><submitter>Luc Waeijen</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:15:50 GMT</date><size>344kb</size><source_type>D</source_type></version><title>How Flexible is Your Computing System</title><authors>Shihua Huang, Luc Waeijen, Henk Corporaal</authors><categories>cs.AR</categories><comments>Partial preprint pending peer review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In literature computer architectures are frequently claimed to be highly
flexible, typically implying there exist trade-offs between flexibility and
performance or energy efficiency. Processor flexibility, however, is not very
sharply defined, and as such these claims can not be validated, nor can such
hypothetical relations be fully understood and exploited in the design of
computing systems. This paper is an attempt to introduce scientific rigour to
the notion of flexibility in computing systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01140</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01140</id><submitter>Georgy Meshcheryakov</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:18:03 GMT</date><size>237kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 10:20:12 GMT</date><size>237kb</size><source_type>D</source_type></version><title>semopy 2: A Structural Equation Modeling Package with Random Effects in
  Python</title><authors>Georgy Meshcheryakov, Anna A. Igolkina, Maria G. Samsonova</authors><categories>stat.AP cs.MS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Structural Equation Modeling (SEM) is an umbrella term that includes numerous
multivariate statistical techniques that are employed throughout a plethora of
research areas, ranging from social to natural sciences. Until recently, SEM
software was either commercial or restricted to niche languages, and the lack
of SEM packages compatible with more mainstream programming languages was dire.
To combat that, we introduced a Python package semopy 1 that surpassed other
state-of-the-art software in terms of performance and estimation accuracy. Yet,
it was lacking in functionality and its usage was burdened with unnecessary
boilerplate code. Here, we introduce a complete overhaul of semopy that
improves upon the previous results and comes with lots of new capabilities.
Furthermore, we propose a novel SEM model that combines in itself a notion of
random effects from linear mixed models (LMMs) to model numerous phenomena,
such as spatial data, time series or population stratification in genetics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01143</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01143</id><submitter>Leonardo Zepeda-N\'u\~nez</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:30:28 GMT</date><size>7479kb</size><source_type>D</source_type></version><title>Accurate and Robust Deep Learning Framework for Solving Wave-Based
  Inverse Problems in the Super-Resolution Regime</title><authors>Matthew Li, Laurent Demanet, Leonardo Zepeda-N\'u\~nez</authors><categories>math.NA cs.LG cs.NA stat.ML</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an end-to-end deep learning framework that comprehensively solves
the inverse wave scattering problem across all length scales. Our framework
consists of the newly introduced wide-band butterfly network coupled with a
simple training procedure that dynamically injects noise during training. While
our trained network provides competitive results in classical imaging regimes,
most notably it also succeeds in the super-resolution regime where other
comparable methods fail. This encompasses both (i) reconstruction of scatterers
with sub-wavelength geometric features, and (ii) accurate imaging when two or
more scatterers are separated by less than the classical diffraction limit. We
demonstrate these properties are retained even in the presence of strong noise
and extend to scatterers not previously seen in the training set. In addition,
our network is straightforward to train requiring no restarts and has an online
runtime that is an order of magnitude faster than optimization-based
algorithms. We perform experiments with a variety of wave scattering mediums
and we demonstrate that our proposed framework outperforms both classical
inversion and competing network architectures that specialize in oscillatory
wave scattering data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01144</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01144</id><submitter>Chujie Zheng</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:30:43 GMT</date><size>4731kb</size><source_type>D</source_type></version><title>Towards Emotional Support Dialog Systems</title><authors>Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou
  Yu, Yong Jiang, Minlie Huang</authors><categories>cs.CL</categories><comments>Accepted to ACL 2021 (Long Paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotional support is a crucial ability for many conversation scenarios,
including social interactions, mental health support, and customer service
chats. Following reasonable procedures and using various support skills can
help to effectively provide support. However, due to the lack of a
well-designed task and corpora of effective emotional support conversations,
research on building emotional support into dialog systems remains untouched.
In this paper, we define the Emotional Support Conversation (ESC) task and
propose an ESC Framework, which is grounded on the Helping Skills Theory. We
construct an Emotion Support Conversation dataset (ESConv) with rich annotation
(especially support strategy) in a help-seeker and supporter mode. To ensure a
corpus of high-quality conversations that provide examples of effective
emotional support, we take extensive effort to design training tutorials for
supporters and several mechanisms for quality control during data collection.
Finally, we evaluate state-of-the-art dialog models with respect to the ability
to provide emotional support. Our results show the importance of support
strategies in providing effective emotional support and the utility of ESConv
in training more emotional support systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01146</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01146</id><submitter>Ajitabh Kumar</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:34:50 GMT</date><size>611kb</size><source_type>D</source_type></version><title>Multi-stage, multi-swarm PSO for joint optimization of well placement
  and control</title><authors>Ajitabh Kumar</authors><categories>cs.NE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Evolutionary optimization algorithms, including particle swarm optimization
(PSO), have been successfully applied in oil industry for production planning
and control. Such optimization studies are quite challenging due to large
number of decision variables, production scenarios, and subsurface
uncertainties. In this work, a multi-stage, multi-swarm PSO (MS2PSO) is
proposed to fix certain issues with canonical PSO algorithm such as premature
convergence, excessive influence of global best solution, and oscillation.
  Multiple experiments are conducted using Olympus benchmark to compare the
efficacy of algorithms. Canonical PSO hyperparameters are first tuned to
prioritize exploration in early phase and exploitation in late phase. Next, a
two-stage multi-swarm PSO (2SPSO) is used where multiple-swarms of the first
stage collapse into a single swarm in the second stage. Finally, MS2PSO with
multiple stages and multiple swarms is used in which swarms recursively
collapse after each stage. Multiple swarm strategy ensures that diversity is
retained within the population and multiple modes are explored. Staging ensures
that local optima found during initial stage does not lead to premature
convergence. Optimization test case comprises of 90 control variables and a
twenty year period of flow simulation. It is observed that different algorithm
designs have their own benefits and drawbacks. Multiple swarms and stages help
algorithm to move away from local optima, but at the same time they may also
necessitate larger number of iterations for convergence. Both 2SPSO and MS2PSO
are found to be helpful for problems with high dimensions and multiple modes
where greater degree of exploration is desired.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01148</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01148</id><submitter>Amit Nanavati</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:39:34 GMT</date><size>1130kb</size><source_type>D</source_type></version><title>Your Tribe Decides Your Vibe: Analyzing Local Popularity in the US
  Patent Citation Network</title><authors>Nishit Narang, Manoj Kumar Ganji and Amit Anil Nanavati</authors><categories>cs.DL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many networks, the indegree of a vertex is a measure of its popularity.
Past research has studied indegree distributions treating the network as a
whole. In the US Patent citation network (USPCN), patents are classified into
categories and subcategories. A natural question arises: How do patents gather
their popularity from various (sub)categories? We analyse local indegree
distributions to answer this question.
  The citation (indegree) of a patent within the same category indicates its
internal popularity, while a cross-category citation indicates its external
popularity. We analyze the internal and external indegree distributions at each
level of USPCN hierarchy to learn how the internal and external popularity of
patents varies across (sub)categories.
  We find that all (sub)categories have local preferences that decide internal
and external patents' popularities. Different patents are popular in different
groups: Groups C1, C2 and C3 may not agree on popular patents in C1. In
general, patent popularity appears to be a highly local phenomenon with
subcategories (not even categories) deciding their own popular patents
independent of the other (sub)categories.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01149</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01149</id><submitter>Ho-Hsiang Wu</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:39:42 GMT</date><size>2268kb</size><source_type>D</source_type></version><title>Exploring modality-agnostic representations for music classification</title><authors>Ho-Hsiang Wu, Magdalena Fuentes, and Juan P. Bello</authors><categories>cs.SD cs.IR eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Music information is often conveyed or recorded across multiple data
modalities including but not limited to audio, images, text and scores.
However, music information retrieval research has almost exclusively focused on
single modality recognition, requiring development of separate models for each
modality. Some multi-modal works require multiple coexisting modalities given
to the model as inputs, constraining the use of these models to the few cases
where data from all modalities are available. To the best of our knowledge, no
existing model has the ability to take inputs from varying modalities, e.g.
images or sounds, and classify them into unified music categories. We explore
the use of cross-modal retrieval as a pretext task to learn modality-agnostic
representations, which can then be used as inputs to classifiers that are
independent of modality. We select instrument classification as an example task
for our study as both visual and audio components provide relevant semantic
information. We train music instrument classifiers that can take both images or
sounds as input, and perform comparably to sound-only or image-only
classifiers. Furthermore, we explore the case when there is limited labeled
data for a given modality, and the impact in performance by using labeled data
from other modalities. We are able to achieve almost 70% of best performing
system in a zero-shot setting. We provide a detailed analysis of experimental
results to understand the potential and limitations of the approach, and
discuss future steps towards modality-agnostic classifiers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01151</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01151</id><submitter>Johan Bjorck</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:41:02 GMT</date><size>2326kb</size><source_type>D</source_type></version><title>Towards Deeper Deep Reinforcement Learning</title><authors>Johan Bjorck, Carla P. Gomes, Kilian Q. Weinberger</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In computer vision and natural language processing, innovations in model
architecture that lead to increases in model capacity have reliably translated
into gains in performance. In stark contrast with this trend, state-of-the-art
reinforcement learning (RL) algorithms often use only small MLPs, and gains in
performance typically originate from algorithmic innovations. It is natural to
hypothesize that small datasets in RL necessitate simple models to avoid
overfitting; however, this hypothesis is untested. In this paper we investigate
how RL agents are affected by exchanging the small MLPs with larger modern
networks with skip connections and normalization, focusing specifically on soft
actor-critic (SAC) algorithms. We verify, empirically, that na\&quot;ively adopting
such architectures leads to instabilities and poor performance, likely
contributing to the popularity of simple models in practice. However, we show
that dataset size is not the limiting factor, and instead argue that intrinsic
instability from the actor in SAC taking gradients through the critic is the
culprit. We demonstrate that a simple smoothing method can mitigate this issue,
which enables stable training with large modern architectures. After smoothing,
larger models yield dramatic performance improvements for state-of-the-art
agents -- suggesting that more &quot;easy&quot; gains may be had by focusing on model
architectures in addition to algorithmic innovations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01153</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01153</id><submitter>Oliver Urbann</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:43:25 GMT</date><size>212kb</size><source_type>D</source_type></version><title>Online and Real-Time Tracking in a Surveillance Scenario</title><authors>Oliver Urbann, Oliver Bredtmann, Maximilian Otten, Jan-Philip Richter,
  Thilo Bauer, David Zibriczky</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach for tracking in a surveillance scenario.
Typical aspects for this scenario are a 24/7 operation with a static camera
mounted above the height of a human with many objects or people. The Multiple
Object Tracking Benchmark 20 (MOT20) reflects this scenario best. We can show
that our approach is real-time capable on this benchmark and outperforms all
other real-time capable approaches in HOTA, MOTA, and IDF1. We achieve this by
contributing a fast Siamese network reformulated for linear runtime (instead of
quadratic) to generate fingerprints from detections. Thus, it is possible to
associate the detections to Kalman filters based on multiple tracking specific
ratings: Cosine similarity of fingerprints, Intersection over Union, and pixel
distance ratio in the image.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01154</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01154</id><submitter>Stjepan Gro\v{s}</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:44:59 GMT</date><size>897kb</size><source_type>D</source_type></version><title>Controlled Update of Software Components using Concurrent Exection of
  Patched and Unpatched Versions</title><authors>Stjepan Gro\v{s}, Ivan Kova\v{c}evi\'c, Ivan Dujmi\'c, Matej
  Petrinovi\'c</authors><categories>cs.CR cs.SE</categories><comments>9 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Software patching is a common method of removing vulnerabilities in software
components to make IT systems more secure. However, there are many cases where
software patching is not possible due to the critical nature of the
application, especially when the vendor providing the application guarantees
correct operation only in a specific configuration. In this paper, we propose a
method to solve this problem. The idea is to run unpatched and patched
application instances concurrently, with the unpatched one having complete
control and the output of the patched one being used only for comparison, to
watch for differences that are consequences of introduced bugs. To test this
idea, we developed a system that allows us to run web applications in parallel
and tested three web applications. The experiments have shown that the idea is
promising for web applications from the technical side. Furthermore, we discuss
the potential limitations of this system and the idea in general, how long two
instances should run in order to be able to claim with some probability that
the patched version has not introduced any new bugs, other potential use cases
of the proposed system where two application instances run concurrently, and
finally the potential uses of this system with different types of applications,
such as SCADA systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01157</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01157</id><submitter>Zuoguang Wang</submitter><version version="v1"><date>Fri, 28 May 2021 12:58:24 GMT</date><size>4431kb</size></version><title>Social Engineering in Cybersecurity: A Domain Ontology and Knowledge
  Graph Application Examples</title><authors>Zuoguang Wang, Hongsong Zhu, Peipei Liu and Limin Sun</authors><categories>cs.CR</categories><comments>Z. Wang, H. Zhu, P. Liu and L. Sun. &quot;Social Engineering in
  Cybersecurity: A Domain Ontology and Knowledge Graph Application Examples.&quot;
  Cybersecurity 4(1), 2021. doi: 10.1186/s42400-021-00094-6</comments><doi>10.1186/s42400-021-00094-6</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Social engineering has posed a serious threat to cyberspace security. To
protect against social engineering attacks, a fundamental work is to know what
constitutes social engineering. This paper first develops a domain ontology of
social engineering in cybersecurity and conducts ontology evaluation by its
knowledge graph application. The domain ontology defines 11 concepts of core
entities that significantly constitute or affect social engineering domain,
together with 22 kinds of relations describing how these entities related to
each other. It provides a formal and explicit knowledge schema to understand,
analyze, reuse and share domain knowledge of social engineering. Furthermore,
this paper builds a knowledge graph based on 15 social engineering attack
incidents and scenarios. 7 knowledge graph application examples (in 6 analysis
patterns) demonstrate that the ontology together with knowledge graph is useful
to 1) understand and analyze social engineering attack scenario and incident,
2) find the top ranked social engineering threat elements (e.g. the most
exploited human vulnerabilities and most used attack mediums), 3) find
potential social engineering threats to victims, 4) find potential targets for
social engineering attackers, 5) find potential attack paths from specific
attacker to specific target, and 6) analyze the same origin attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01161</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01161</id><submitter>Manuel Chakravarty</submitter><version version="v1"><date>Wed, 2 Jun 2021 13:55:05 GMT</date><size>78kb</size></version><title>Babel Fees via Limited Liabilities</title><authors>Manuel M. T. Chakravarty and Nikos Karayannidis and Aggelos Kiayias
  and Michael Peyton Jones and Polina Vinogradova</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Custom currencies (ERC-20) on Ethereum are wildly popular, but they are
second class to the primary currency Ether. Custom currencies are more complex
and more expensive to handle than the primary currency as their accounting is
not natively performed by the underlying ledger, but instead in user-defined
contract code. Furthermore, and quite importantly, transaction fees can only be
paid in Ether.
  In this paper, we focus on being able to pay transaction fees in custom
currencies. We achieve this by way of a mechanism permitting short term
liabilities to pay transaction fees in conjunction with offers of custom
currencies to compensate for those liabilities. This enables block producers to
accept custom currencies in exchange for settling liabilities of transactions
that they process.
  We present formal ledger rules to handle liabilities together with the
concept of babel fees to pay transaction fees in custom currencies. We also
discuss how clients can determine what fees they have to pay, and we present a
solution to the knapsack problem variant that block producers have to solve in
the presence of babel fees to optimise their profits.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01167</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01167</id><submitter>Ishani Mondal</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:03:06 GMT</date><size>5403kb</size><source_type>D</source_type></version><title>End-to-End NLP Knowledge Graph Construction</title><authors>Ishani Mondal, Yufang Hou and Charles Jochim</authors><categories>cs.CL</categories><comments>Accepted in ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper studies the end-to-end construction of an NLP Knowledge Graph (KG)
from scientific papers. We focus on extracting four types of relations:
evaluatedOn between tasks and datasets, evaluatedBy between tasks and
evaluation metrics, as well as coreferent and related relations between the
same type of entities. For instance, F1-score is coreferent with F-measure. We
introduce novel methods for each of these relation types and apply our final
framework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a
large-scale KG, which can facilitate automatically constructing scientific
leaderboards for the NLP community. The results of our experiments indicate
that the resulting KG contains high-quality information.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01170</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01170</id><submitter>Anthony Rios</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:10:28 GMT</date><size>11302kb</size><source_type>D</source_type></version><title>Detecting Bot-Generated Text by Characterizing Linguistic Accommodation
  in Human-Bot Interactions</title><authors>Paras Bhatt and Anthony Rios</authors><categories>cs.CL</categories><comments>13 pages, to be published in Findings of ACL-IJCNLP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Language generation models' democratization benefits many domains, from
answering health-related questions to enhancing education by providing
AI-driven tutoring services. However, language generation models'
democratization also makes it easier to generate human-like text at-scale for
nefarious activities, from spreading misinformation to targeting specific
groups with hate speech. Thus, it is essential to understand how people
interact with bots and develop methods to detect bot-generated text. This paper
shows that bot-generated text detection methods are more robust across datasets
and models if we use information about how people respond to it rather than
using the bot's text directly. We also analyze linguistic alignment, providing
insight into differences between human-human and human-bot conversations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01171</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01171</id><submitter>P. Christopher Staecker</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:10:46 GMT</date><size>33kb</size></version><title>Digital homotopy relations and digital homology theories</title><authors>P. Christopher Staecker</authors><categories>math.AT cs.CV math.GN</categories><comments>arXiv admin note: substantial text overlap with arXiv:1907.00473,
  arXiv:1903.00706</comments><msc-class>55P10, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove results relating to two homotopy relations and four
homology theories developed in the topology of digital images.
  We introduce a new type of homotopy relation for digitally continuous
functions which we call &quot;strong homotopy.&quot; Both digital homotopy and strong
homotopy are natural digitizations of classical topological homotopy: the
difference between them is analogous to the difference between digital
4-adjacency and 8-adjacency in the plane.
  We also consider four different digital homology theories: a simplicial
homology theory by Arslan et al which is the homology of the clique complex, a
singular simplicial homology theory by D. W. Lee, a cubical homology theory by
Jamil and Ali, and a new kind of cubical homology for digital images with
$c_1$-adjacency which is easily computed, and generalizes a construction by
Karaca \&amp; Ege. We show that the two simplicial homology theories are isomorphic
to each other, but distinct from the two cubical theories.
  We also show that homotopic maps have the same induced homomorphisms in the
cubical homology theory, and strong homotopic maps additionally have the same
induced homomorphisms in the simplicial theory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01173</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01173</id><submitter>Yuto Nakashima</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:11:12 GMT</date><size>69kb</size><source_type>D</source_type></version><title>On the approximation ratio of LZ-End to LZ77</title><authors>Takumi Ideue, Takuya Mieno, Mitsuru Funakoshi, Yuto Nakashima,
  Shunsuke Inenaga, Masayuki Takeda</authors><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of Lempel-Ziv factorizations is a well-studied string structure. The
LZ-End factorization is a member of the family that achieved faster extraction
of any substrings (Kreft &amp; Navarro, TCS 2013). One of the interests for LZ-End
factorizations is the possible difference between the size of LZ-End and LZ77
factorizations. They also showed families of strings where the approximation
ratio of the number of LZ-End phrases to the number of LZ77 phrases
asymptotically approaches 2. However, the alphabet size of these strings is
unbounded. In this paper, we analyze the LZ-End factorization of the
period-doubling sequence. We also show that the approximation ratio for the
period-doubling sequence asymptotically approaches 2 for the binary alphabet.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01174</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01174</id><submitter>Peter Hansbo</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:11:27 GMT</date><size>318kb</size><source_type>D</source_type></version><title>Nitsche's Finite Element Method for Model Coupling in Elasticity</title><authors>Peter Hansbo and Mats G. Larson</authors><categories>math.NA cs.NA</categories><comments>11 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop a Nitsche finite element method for a model of Euler--Bernoulli
beams with axial stiffness embedded in a two--dimensional elastic bulk domain.
The beams have their own displacement fields, and the elastic subdomains
created by the beam network are triangulated independently and are coupled to
the beams weakly by use of Nitsche's method in the framework of hybridization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01175</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01175</id><submitter>Sarah Meng Li</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:11:53 GMT</date><size>29kb</size></version><title>Generators and Relations for the Group $\mathrm{O}_n(\mathbb{Z}[1/2])$</title><authors>Sarah Meng Li, Neil J. Ross, Peter Selinger</authors><categories>quant-ph cs.ET cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a finite presentation by generators and relations for the group
$\mathrm{O}_n(\mathbb{Z}[1/2])$ of $n$-dimensional orthogonal matrices with
entries in $\mathbb{Z}[1/2]$. We then obtain a similar presentation for the
group of $n$-dimensional orthogonal matrices of the form $M/\sqrt{2}{}^k$,
where $k$ is a nonnegative integer and $M$ is an integer matrix. Both groups
arise in the study of quantum circuits. In particular, when the dimension is a
power of $2$, the elements of the latter group are precisely the unitary
matrices that can be represented by a quantum circuit over the universal gate
set consisting of the Toffoli gate, the Hadamard gate, and the computational
ancilla.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01176</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01176</id><submitter>Hossein Monshizadeh Naeen</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:14:38 GMT</date><size>899kb</size></version><title>Hybrid Ensemble optimized algorithm based on Genetic Programming for
  imbalanced data classification</title><authors>Maliheh Roknizadeh, Hossein Monshizadeh Naeen</authors><categories>cs.LG cs.NE</categories><comments>11 pages, 4 Tables, 7 Figures Accepted in Twelfth International
  Conference on Information Technology, Computer and Telecommunications</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the most significant current discussions in the field of data mining
is classifying imbalanced data. In recent years, several ways are proposed such
as algorithm level (internal) approaches, data level (external) techniques, and
cost-sensitive methods. Although extensive research has been carried out on
imbalanced data classification, however, several unsolved challenges remain
such as no attention to the importance of samples to balance, determine the
appropriate number of classifiers, and no optimization of classifiers in the
combination of classifiers. The purpose of this paper is to improve the
efficiency of the ensemble method in the sampling of training data sets,
especially in the minority class, and to determine better basic classifiers for
combining classifiers than existing methods. We proposed a hybrid ensemble
algorithm based on Genetic Programming (GP) for two classes of imbalanced data
classification. In this study uses historical data from UCI Machine Learning
Repository to assess minority classes in imbalanced datasets. The performance
of our proposed algorithm is evaluated by Rapid-miner studio v.7.5.
Experimental results show the performance of the proposed method on the
specified data sets in the size of the training set shows 40% and 50% better
accuracy than other dimensions of the minority class prediction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01177</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01177</id><submitter>Nicolas Skatchkovsky</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:14:47 GMT</date><size>808kb</size><source_type>D</source_type></version><title>Learning to Time-Decode in Spiking Neural Networks Through the
  Information Bottleneck</title><authors>Nicolas Skatchkovsky, Osvaldo Simeone, Hyeryung Jang</authors><categories>cs.NE cs.AI cs.IT math.IT</categories><comments>Under review for conference publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the key challenges in training Spiking Neural Networks (SNNs) is that
target outputs typically come in the form of natural signals, such as labels
for classification or images for generative models, and need to be encoded into
spikes. This is done by handcrafting target spiking signals, which in turn
implicitly fixes the mechanisms used to decode spikes into natural signals,
e.g., rate decoding. The arbitrary choice of target signals and decoding rule
generally impairs the capacity of the SNN to encode and process information in
the timing of spikes. To address this problem, this work introduces a hybrid
variational autoencoder architecture, consisting of an encoding SNN and a
decoding Artificial Neural Network (ANN). The role of the decoding ANN is to
learn how to best convert the spiking signals output by the SNN into the target
natural signal. A novel end-to-end learning rule is introduced that optimizes a
directed information bottleneck training criterion via surrogate gradients. We
demonstrate the applicability of the technique in an experimental settings on
various tasks, including real-life datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01178</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01178</id><submitter>Danila Rukhovich</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:20:24 GMT</date><size>27496kb</size><source_type>D</source_type></version><title>ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View
  General-Purpose 3D Object Detection</title><authors>Danila Rukhovich, Anna Vorontsova, Anton Konushin</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the task of multi-view RGB-based 3D object
detection as an end-to-end optimization problem. To address this problem, we
propose ImVoxelNet, a novel fully convolutional method of 3D object detection
based on monocular or multi-view RGB images. The number of monocular images in
each multi-view input can variate during training and inference; actually, this
number might be unique for each multi-view input. ImVoxelNet successfully
handles both indoor and outdoor scenes, which makes it general-purpose.
Specifically, it achieves state-of-the-art results in car detection on KITTI
(monocular) and nuScenes (multi-view) benchmarks among all methods that accept
RGB images. Moreover, it surpasses existing RGB-based 3D object detection
methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark
for multi-view 3D object detection. The source code and the trained models are
available at \url{https://github.com/saic-vul/imvoxelnet}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01182</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01182</id><submitter>Matthias Gro{\ss}</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:26:26 GMT</date><size>115kb</size></version><title>Automating Speedrun Routing: Overview and Vision</title><authors>Matthias Gro{\ss}, Dietlind Z\&quot;uhlke, Boris Naujoks</authors><categories>cs.NE</categories><comments>8 pages, submitted to IEEE Conference on Games 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Speedrunning in general means to play a video game fast, i.e. using all means
at one's disposal to achieve a given goal in the least amount of time possible.
To do so, a speedrun must be planned in advance, or routed, as it is referred
to by the community. This paper focuses on discovering challenges and defining
models needed when trying to approach the problem of routing algorithmically.
It provides an overview of relevant speedrunning literature, extracting vital
information and formulating criticism. Important categorizations are pointed
out and a nomenclature is build to support professional discussion. Different
concepts of graph representations are presented and their potential is
discussed with regard to solving the speedrun routing optimization problem.
Visions both for problem modeling as well as solving are presented and assessed
regarding suitability and expected challenges. This results in a vision of
potential solutions and what will be addressed in the future.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01183</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01183</id><submitter>Sara Rajaee</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:26:37 GMT</date><size>777kb</size><source_type>D</source_type></version><title>A Cluster-based Approach for Improving Isotropy in Contextual Embedding
  Space</title><authors>Sara Rajaee and Mohammad Taher Pilehvar</authors><categories>cs.CL</categories><comments>To appear in ACL 2021 main conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The representation degeneration problem in Contextual Word Representations
(CWRs) hurts the expressiveness of the embedding space by forming an
anisotropic cone where even unrelated words have excessively positive
correlations. Existing techniques for tackling this issue require a learning
process to re-train models with additional objectives and mostly employ a
global assessment to study isotropy. Our quantitative analysis over isotropy
shows that a local assessment could be more accurate due to the clustered
structure of CWRs. Based on this observation, we propose a local cluster-based
method to address the degeneration issue in contextual embedding spaces. We
show that in clusters including punctuations and stop words, local dominant
directions encode structural information, removing which can improve CWRs
performance on semantic tasks. Moreover, we find that tense information in verb
representations dominates sense semantics. We show that removing dominant
directions of verb representations can transform the space to better suit
semantic applications. Our experiments demonstrate that the proposed
cluster-based method can mitigate the degeneration problem on multiple tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01184</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01184</id><submitter>Matthew Daggitt Dr</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:27:19 GMT</date><size>702kb</size><source_type>D</source_type></version><title>Formally Verified Convergence of Policy-Rich DBF Routing Protocols</title><authors>Matthew L. Daggitt, Timothy G. Griffin</authors><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we present new general convergence results about the behaviour
of Distributed Bellman-Ford (DBF) family of routing protocols, which includes
distance-vector protocols (e.g. RIP) and path-vector protocols (e.g. BGP).
  First, we propose a new algebraic model for abstract routing problems which
has fewer primitives than previous models and can represent more expressive
policy languages. The new model is also the first to allow concurrent reasoning
about distance-vector and path-vector protocols.
  Second, we explicitly demonstrate how DBF routing protocols are instances of
a larger class of asynchronous iterative algorithms, for which there already
exist powerful results about convergence. These results allow us to build upon
conditions previously shown by Sobrinho to be sufficient and necessary for the
convergence of path-vector protocols and generalise and strengthen them in
various ways: we show that, with a minor modification, they also apply to
distance-vector protocols; we prove they guarantee that the final routing
solution reached is unique, thereby eliminating the possibility of anomalies
such as BGP wedgies; we relax the model of asynchronous communication, showing
that the results still hold if routing messages can be lost, reordered, and
duplicated.
  Thirdly, our model and our accompanying theoretical results have been fully
formalised in the Agda theorem prover. The resulting library is a powerful tool
for quickly prototyping and formally verifying new policy languages. As an
example, we formally verify the correctness of a policy language with many of
the features of BGP including communities, conditional policy, path-inflation
and route filtering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01186</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01186</id><submitter>Dvir Ginzburg</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:29:35 GMT</date><size>6075kb</size><source_type>D</source_type></version><title>Self-Supervised Document Similarity Ranking via Contextualized Language
  Models and Hierarchical Inference</title><authors>Dvir Ginzburg and Itzik Malkiel and Oren Barkan and Avi Caciularu and
  Noam Koenigstein</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel model for the problem of ranking a collection of documents
according to their semantic similarity to a source (query) document. While the
problem of document-to-document similarity ranking has been studied, most
modern methods are limited to relatively short documents or rely on the
existence of &quot;ground-truth&quot; similarity labels. Yet, in most common real-world
cases, similarity ranking is an unsupervised problem as similarity labels are
unavailable. Moreover, an ideal model should not be restricted by documents'
length. Hence, we introduce SDR, a self-supervised method for document
similarity that can be applied to documents of arbitrary length. Importantly,
SDR can be effectively applied to extremely long documents, exceeding the 4,096
maximal token limits of Longformer. Extensive evaluations on large document
datasets show that SDR significantly outperforms its alternatives across all
metrics. To accelerate future research on unlabeled long document similarity
ranking, and as an additional contribution to the community, we herein publish
two human-annotated test sets of long documents similarity evaluation. The SDR
code and datasets are publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01190</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01190</id><submitter>Yuto Nakashima</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:32:55 GMT</date><size>8kb</size></version><title>Counting Lyndon Subsequences</title><authors>Ryo Hirakawa, Yuto Nakashima, Shunsuke Inenaga, Masayuki Takeda</authors><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting substrings/subsequences that preserve some property (e.g.,
palindromes, squares) is an important mathematical interest in stringology.
Recently, Glen et al. studied the number of Lyndon factors in a string. A
string $w = uv$ is called a Lyndon word if it is the lexicographically smallest
among all of its conjugates $vu$. In this paper, we consider a more general
problem &quot;counting Lyndon subsequences&quot;. We show (1) the maximum total number of
Lyndon subsequences in a string, (2) the expected total number of Lyndon
subsequences in a string, (3) the expected number of distinct Lyndon
subsequences in a string.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01191</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01191</id><submitter>Jiasheng Si</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:33:12 GMT</date><size>336kb</size><source_type>D</source_type></version><title>Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact
  Verification</title><authors>Jiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, Yulan He</authors><categories>cs.CL cs.AI</categories><comments>Accepted by ACL2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fact verification is a challenging task that requires simultaneously
reasoning and aggregating over multiple retrieved pieces of evidence to
evaluate the truthfulness of a claim. Existing approaches typically (i) explore
the semantic interaction between the claim and evidence at different
granularity levels but fail to capture their topical consistency during the
reasoning process, which we believe is crucial for verification; (ii) aggregate
multiple pieces of evidence equally without considering their implicit stances
to the claim, thereby introducing spurious information. To alleviate the above
issues, we propose a novel topic-aware evidence reasoning and stance-aware
aggregation model for more accurate fact verification, with the following four
key properties: 1) checking topical consistency between the claim and evidence;
2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring
semantic similarity between the global topic information and the semantic
representation of evidence; 4) aggregating evidence based on their implicit
stances to the claim. Extensive experiments conducted on the two benchmark
datasets demonstrate the superiority of the proposed model over several
state-of-the-art approaches for fact verification. The source code can be
obtained from https://github.com/jasenchn/TARSA.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01195</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01195</id><submitter>Tuhin Chakrabarty Mr</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:37:32 GMT</date><size>42kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 14:20:10 GMT</date><size>42kb</size></version><title>Figurative Language in Recognizing Textual Entailment</title><authors>Tuhin Chakrabarty, Debanjan Ghosh, Adam Poliak, Smaranda Muresan</authors><categories>cs.CL cs.AI</categories><comments>ACL 2021 (Findings)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a collection of recognizing textual entailment (RTE) datasets
focused on figurative language. We leverage five existing datasets annotated
for a variety of figurative language -- simile, metaphor, and irony -- and
frame them into over 12,500 RTE examples.We evaluate how well state-of-the-art
models trained on popular RTE datasets capture different aspects of figurative
language. Our results and analyses indicate that these models might not
sufficiently capture figurative language, struggling to perform pragmatic
inference and reasoning about world knowledge. Ultimately, our datasets provide
a challenging testbed for evaluating RTE models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01199</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01199</id><submitter>Qingqing Cao</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:43:51 GMT</date><size>597kb</size><source_type>D</source_type></version><title>IrEne: Interpretable Energy Prediction for Transformers</title><authors>Qingqing Cao, Yash Kumar Lal, Harsh Trivedi, Aruna Balasubramanian,
  Niranjan Balasubramanian</authors><categories>cs.CL</categories><comments>ACL 2021 camera ready</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Existing software-based energy measurements of NLP models are not accurate
because they do not consider the complex interactions between energy
consumption and model execution. We present IrEne, an interpretable and
extensible energy prediction system that accurately predicts the inference
energy consumption of a wide range of Transformer-based NLP models. IrEne
constructs a model tree graph that breaks down the NLP model into modules that
are further broken down into low-level machine learning (ML) primitives. IrEne
predicts the inference energy consumption of the ML primitives as a function of
generalizable features and fine-grained runtime resource usage. IrEne then
aggregates these low-level predictions recursively to predict the energy of
each module and finally of the entire model. Experiments across multiple
Transformer models show IrEne predicts inference energy consumption of
transformer models with an error of under 7% compared to the ground truth. In
contrast, existing energy models see an error of over 50%. We also show how
IrEne can be used to conduct energy bottleneck analysis and to easily evaluate
the energy impact of different architectural choices. We release the code and
data at https://github.com/StonyBrookNLP/irene.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01200</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01200</id><submitter>Karel J.  in 't Hout</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:44:55 GMT</date><size>63kb</size></version><title>Numerical valuation of American basket options via partial differential
  complementarity problems</title><authors>Karel in 't Hout and Jacob Snoeijer</authors><categories>math.NA cs.CE cs.NA q-fin.CP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the principal component analysis based approach introduced by
Reisinger &amp; Wittum (2007) and the comonotonic approach considered by Hanbali &amp;
Linders (2019) for the approximation of American basket option values via
multidimensional partial differential complementarity problems (PDCPs). Both
approximation approaches require the solution of just a limited number of
low-dimensional PDCPs. It is demonstrated by ample numerical experiments that
they define approximations that lie close to each other. Next, an efficient
discretisation of the pertinent PDCPs is presented that leads to a favourable
convergence behaviour.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01202</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01202</id><submitter>Pierre Marion</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:46:40 GMT</date><size>1100kb</size><source_type>D</source_type></version><title>Framing RNN as a kernel method: A neural ODE approach</title><authors>Adeline Fermanian, Pierre Marion, Jean-Philippe Vert, G\'erard Biau</authors><categories>stat.ML cs.LG</categories><comments>32 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on the interpretation of a recurrent neural network (RNN) as a
continuous-time neural differential equation, we show, under appropriate
conditions, that the solution of a RNN can be viewed as a linear function of a
specific feature set of the input sequence, known as the signature. This
connection allows us to frame a RNN as a kernel method in a suitable
reproducing kernel Hilbert space. As a consequence, we obtain theoretical
guarantees on generalization and stability for a large class of recurrent
networks. Our results are illustrated on simulated datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01205</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01205</id><submitter>Niklas Hedegaard Arent Mr</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:48:18 GMT</date><size>3604kb</size><source_type>D</source_type></version><title>Refined method to extract frequency-noise components of lasers by
  delayed self-heterodyne</title><authors>Niklas Hedegaard Arent, M\'onica Brusatori Far, Nicolas Volet</authors><categories>physics.optics cs.SY eess.SP eess.SY physics.ins-det</categories><comments>7 pages, 7 figures. To be published in Journal of Lightwave
  Technology. Measurements and data analysis carried out by Niklas Arent and
  Monica Brusatori Far. Nicolas acted as supervisor on this work</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An essential metric to quantify the stability of a laser is its frequency
noise (FN). This metric yields information on the linewidth and on noise
components which limit its usage for high precision purposes such as coherent
communication. Its experimental determination relies on challenging optical
phase measurements, for which dedicated commercial instruments have been
developed. In contrast, this work presents a simple and cost-effective method
for extracting FN features employing a delayed self-heterodyne (DSH) setup.
Using delay lengths much shorter than the coherence length of the laser, the
DSH trace reveals a correspondence with the FN power spectral density (PSD)
measured with commercial instruments. Results are found for multiple lasers,
with discrepancies in intense dither tone frequencies below 0.2 percent
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01207</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01207</id><submitter>Forrest Davis</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:52:11 GMT</date><size>546kb</size><source_type>D</source_type></version><title>Uncovering Constraint-Based Behavior in Neural Models via Targeted
  Fine-Tuning</title><authors>Forrest Davis and Marten van Schijndel</authors><categories>cs.CL</categories><comments>Proceedings of 59th Annual Meeting of the Association for
  Computational Linguistics (ACL 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing body of literature has focused on detailing the linguistic
knowledge embedded in large, pretrained language models. Existing work has
shown that non-linguistic biases in models can drive model behavior away from
linguistic generalizations. We hypothesized that competing linguistic processes
within a language, rather than just non-linguistic model biases, could obscure
underlying linguistic knowledge. We tested this claim by exploring a single
phenomenon in four languages: English, Chinese, Spanish, and Italian. While
human behavior has been found to be similar across languages, we find
cross-linguistic variation in model behavior. We show that competing processes
in a language act as constraints on model behavior and demonstrate that
targeted fine-tuning can re-weight the learned constraints, uncovering
otherwise dormant linguistic knowledge in models. Our results suggest that
models need to learn both the linguistic constraints in a language and their
relative ranking, with mismatches in either producing non-human-like behavior.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01210</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01210</id><submitter>Arie Cattan</submitter><version version="v1"><date>Wed, 2 Jun 2021 14:56:28 GMT</date><size>5308kb</size><source_type>D</source_type></version><title>Cross-document Coreference Resolution over Predicted Mentions</title><authors>Arie Cattan, Alon Eirew, Gabriel Stanovsky, Mandar Joshi, Ido Dagan</authors><categories>cs.CL</categories><comments>Findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Coreference resolution has been mostly investigated within a single document
scope, showing impressive progress in recent years based on end-to-end models.
However, the more challenging task of cross-document (CD) coreference
resolution remained relatively under-explored, with the few recent models
applied only to gold mentions. Here, we introduce the first end-to-end model
for CD coreference resolution from raw text, which extends the prominent model
for within-document coreference to the CD setting. Our model achieves
competitive results for event and entity coreference resolution on gold
mentions. More importantly, we set first baseline results, on the standard ECB+
dataset, for CD coreference resolution over predicted mentions. Further, our
model is simpler and more efficient than recent CD coreference resolution
systems, while not using any external resources.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01215</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01215</id><submitter>Talha Bin Masood</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:07:02 GMT</date><size>14380kb</size><source_type>D</source_type></version><title>Visual Analysis of Electronic Densities and Transitions in Molecules</title><authors>Talha Bin Masood, Signe Sidwall Thygesen, Mathieu Linares, Alexei I.
  Abrikosov, Vijay Natarajan, Ingrid Hotz</authors><categories>cs.HC cs.CG physics.chem-ph</categories><comments>15 pages, 9 figures, To appear in EuroVis 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The study of electronic transitions within a molecule connected to the
absorption or emission of light is a common task in the process of the design
of new materials. The transitions are complex quantum mechanical processes and
a detailed analysis requires a breakdown of these processes into components
that can be interpreted via characteristic chemical properties. We approach
these tasks by providing a detailed analysis of the electron density field.
This entails methods to quantify and visualize electron localization and
transfer from molecular subgroups combining spatial and abstract
representations. The core of our method uses geometric segmentation of the
electronic density field coupled with a graph-theoretic formulation of charge
transfer between molecular subgroups. The design of the methods has been guided
by the goal of providing a generic and objective analysis following fundamental
concepts. We illustrate the proposed approach using several case studies
involving the study of electronic transitions in different molecular systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01216</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01216</id><submitter>Manuel Hau{\ss}mann</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:09:20 GMT</date><size>798kb</size><source_type>D</source_type></version><title>Evidential Turing Processes</title><authors>Melih Kandemir, Abdullah Akg\&quot;ul, Manuel Haussmann, Gozde Unal</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A probabilistic classifier with reliable predictive uncertainties i) fits
successfully to the target domain data, ii) provides calibrated class
probabilities in difficult regions of the target domain (e.g. class overlap),
and iii) accurately identifies queries coming out of the target domain and
reject them. We introduce an original combination of evidential deep learning,
neural processes, and neural Turing machines capable of providing all three
essential properties mentioned above for total uncertainty quantification. We
observe our method on three image classification benchmarks and two neural net
architectures to consistently give competitive or superior scores with respect
to multiple uncertainty quantification metrics against state-of-the-art methods
explicitly tailored to one or a few of them. Our unified solution delivers an
implementation-friendly and computationally efficient recipe for safety
clearance and provides intellectual economy to an investigation of algorithmic
roots of epistemic awareness in deep neural nets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01217</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01217</id><submitter>Bo Peng</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:10:13 GMT</date><size>1875kb</size><source_type>D</source_type></version><title>DFGC 2021: A DeepFake Game Competition</title><authors>Bo Peng, Hongxing Fan, Wei Wang, Jing Dong, Yuezun Li, Siwei Lyu, Qi
  Li, Zhenan Sun, Han Chen, Baoying Chen, Yanjie Hu, Shenghai Luo, Junrui
  Huang, Yutong Yao, Boyuan Liu, Hefei Ling, Guosheng Zhang, Zhiliang Xu,
  Changtao Miao, Changlei Lu, Shan He, Xiaoyan Wu, Wanyi Zhuang</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a summary of the DFGC 2021 competition. DeepFake
technology is developing fast, and realistic face-swaps are increasingly
deceiving and hard to detect. At the same time, DeepFake detection methods are
also improving. There is a two-party game between DeepFake creators and
detectors. This competition provides a common platform for benchmarking the
adversarial game between current state-of-the-art DeepFake creation and
detection methods. In this paper, we present the organization, results and top
solutions of this competition and also share our insights obtained during this
event. We also release the DFGC-21 testing dataset collected from our
participants to further benefit the research community.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01221</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01221</id><submitter>Xiang Yue</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:15:10 GMT</date><size>3348kb</size><source_type>D</source_type></version><title>Differential Privacy for Text Analytics via Natural Text Sanitization</title><authors>Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li, Huan Sun and Sherman
  S. M. Chow</authors><categories>cs.CL cs.CR</categories><comments>ACL-ICJNLP'21 Findings; The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texts convey sophisticated knowledge. However, texts also convey sensitive
information. Despite the success of general-purpose language models and
domain-specific mechanisms with differential privacy (DP), existing text
sanitization mechanisms still provide low utility, as cursed by the
high-dimensional text representation. The companion issue of utilizing
sanitized texts for downstream analytics is also under-explored. This paper
takes a direct approach to text sanitization. Our insight is to consider both
sensitivity and similarity via our new local DP notion. The sanitized texts
also contribute to our sanitization-aware pretraining and fine-tuning, enabling
privacy-preserving natural language processing over the BERT language model
with promising utility. Surprisingly, the high utility does not boost up the
success rate of inference attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01223</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01223</id><submitter>Hang Yan</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:19:23 GMT</date><size>245kb</size><source_type>D</source_type></version><title>A Unified Generative Framework for Various NER Subtasks</title><authors>Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang and Xipeng Qiu</authors><categories>cs.CL</categories><comments>Accepted in the main conference of ACL-IJCNLP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named Entity Recognition (NER) is the task of identifying spans that
represent entities in sentences. Whether the entity spans are nested or
discontinuous, the NER task can be categorized into the flat NER, nested NER,
and discontinuous NER subtasks. These subtasks have been mainly solved by the
token-level sequence labelling or span-level classification. However, these
solutions can hardly tackle the three kinds of NER subtasks concurrently. To
that end, we propose to formulate the NER subtasks as an entity span sequence
generation task, which can be solved by a unified sequence-to-sequence
(Seq2Seq) framework. Based on our unified framework, we can leverage the
pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the
special design of the tagging schema or ways to enumerate spans. We exploit
three types of entity representations to linearize entities into a sequence.
Our proposed framework is easy-to-implement and achieves state-of-the-art
(SoTA) or near SoTA performance on eight English NER datasets, including two
flat NER datasets, three nested NER datasets, and three discontinuous NER
datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01225</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01225</id><submitter>Anish Pradhan</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:20:41 GMT</date><size>442kb</size><source_type>D</source_type></version><title>Intelligent Surface Optimization in Terahertz under Two Manifestations
  of Molecular Re-radiation</title><authors>Anish Pradhan, J. Kartheek Devineni, Harpreet S. Dhillon, Andreas F.
  Molisch</authors><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The operation of Terahertz (THz) communication can be significantly impacted
by the interaction between the transmitted wave and the molecules in the
atmosphere. In particular, it has been observed experimentally that the signal
undergoes not only molecular absorption, but also molecular re-radiation. Two
extreme modeling assumptions are prevalent in the literature, where the
re-radiated energy is modeled in the first as additive Gaussian noise and in
the second as a scattered component strongly correlated to the actual signal.
Since the exact characterization is still an open problem, we provide in this
paper the first comparative study of the performance of a reconfigurable
intelligent surface (RIS) assisted THz system under these two extreme models of
re-radiation. In particular, we employ an RIS to overcome the large pathloss by
creating a virtual line-of-sight (LOS) path. We then develop an optimization
framework for this setup and utilize the block-coordinate descent (BCD) method
to iteratively optimize both RIS configuration vector and receive beamforming
weight resulting in significant throughput gains for the user of interest
compared to random RIS configurations. As expected, our results reveal that
better throughput is achieved under the scattering assumption for the molecular
re-radiation than the noise assumption.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01226</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01226</id><submitter>Xiaokang Chen</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:21:56 GMT</date><size>2808kb</size><source_type>D</source_type></version><title>Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision</title><authors>Xiaokang Chen, Yuhui Yuan, Gang Zeng, Jingdong Wang</authors><categories>cs.CV</categories><comments>Accepted by CVPR 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the semi-supervised semantic segmentation problem via
exploring both labeled data and extra unlabeled data. We propose a novel
consistency regularization approach, called cross pseudo supervision (CPS). Our
approach imposes the consistency on two segmentation networks perturbed with
different initialization for the same input image. The pseudo one-hot label
map, output from one perturbed segmentation network, is used to supervise the
other segmentation network with the standard cross-entropy loss, and vice
versa. The CPS consistency has two roles: encourage high similarity between the
predictions of two perturbed networks for the same input image, and expand
training data by using the unlabeled data with pseudo labels. Experiment
results show that our approach achieves the state-of-the-art semi-supervised
segmentation performance on Cityscapes and PASCAL VOC 2012.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01227</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01227</id><submitter>Jayadev Billa</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:23:34 GMT</date><size>46kb</size></version><title>Improving low-resource ASR performance with untranscribed out-of-domain
  data</title><authors>Jayadev Billa</authors><categories>cs.CL cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semi-supervised training (SST) is a common approach to leverage
untranscribed/unlabeled speech data to improve automatic speech recognition
performance in low-resource languages. However, if the available unlabeled
speech is mismatched to the target domain, SST is not as effective, and in many
cases performs worse than the original system. In this paper, we address the
issue of low-resource ASR when only untranscribed out-of-domain speech data is
readily available in the target language. Specifically, we look to improve
performance on conversational/telephony speech (target domain) using web
resources, in particular YouTube data, which more closely resembles
news/topical broadcast data. Leveraging SST, we show that while in some cases
simply pooling the out-of-domain data with the training data lowers word error
rate (WER), in all cases, we see improvements if we train first with the
out-of-domain data and then fine-tune the resulting model with the original
training data. Using 2000 hours of speed perturbed YouTube audio in each target
language, with semi-supervised transcripts, we show improvements on multiple
languages/data sets, of up to 16.3% relative improvement in WER over the
baseline systems and up to 7.4% relative improvement in WER over a system that
simply pools the out-of-domain data with the training data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01228</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01228</id><submitter>Kevin Stowe</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:27:05 GMT</date><size>7385kb</size><source_type>D</source_type></version><title>Metaphor Generation with Conceptual Mappings</title><authors>Kevin Stowe, Tuhin Chakrabarty, Nanyun Peng, Smaranda Muresan, Iryna
  Gurevych</authors><categories>cs.CL</categories><comments>13 pages, 3 figures, to be published in the Joint Conference of the
  59th Annual Meeting of the Association for Computational Linguistics and the
  11th International Joint Conference on Natural Language Processing
  (ACL-IJCNLP 2021)</comments><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Generating metaphors is a difficult task as it requires understanding nuanced
relationships between abstract concepts. In this paper, we aim to generate a
metaphoric sentence given a literal expression by replacing relevant verbs.
Guided by conceptual metaphor theory, we propose to control the generation
process by encoding conceptual mappings between cognitive domains to generate
meaningful metaphoric expressions. To achieve this, we develop two methods: 1)
using FrameNet-based embeddings to learn mappings between domains and applying
them at the lexical level (CM-Lex), and 2) deriving source/target pairs to
train a controlled seq-to-seq generation model (CM-BART). We assess our methods
through automatic and human evaluation for basic metaphoricity and conceptual
metaphor presence. We show that the unsupervised CM-Lex model is competitive
with recent deep learning metaphor generation systems, and CM-BART outperforms
all other models both in automatic and human evaluations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01229</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01229</id><submitter>Tatsuki Kuribayashi</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:27:29 GMT</date><size>378kb</size><source_type>D</source_type></version><title>Lower Perplexity is Not Always Human-Like</title><authors>Tatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki
  Asahara, Kentaro Inui</authors><categories>cs.CL</categories><comments>Accepted by ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In computational psycholinguistics, various language models have been
evaluated against human reading behavior (e.g., eye movement) to build
human-like computational models. However, most previous efforts have focused
almost exclusively on English, despite the recent trend towards linguistic
universal within the general community. In order to fill the gap, this paper
investigates whether the established results in computational psycholinguistics
can be generalized across languages. Specifically, we re-examine an established
generalization -- the lower perplexity a language model has, the more
human-like the language model is -- in Japanese with typologically different
structures from English. Our experiments demonstrate that this established
generalization exhibits a surprising lack of universality; namely, lower
perplexity is not always human-like. Moreover, this discrepancy between English
and Japanese is further explored from the perspective of (non-)uniform
information density. Overall, our results suggest that a cross-lingual
evaluation will be necessary to construct human-like computational models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01232</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01232</id><submitter>Kiran Sharma Dr.</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:30:41 GMT</date><size>5443kb</size><source_type>D</source_type></version><title>A weighted unified informetrics based on Scopus and WoS</title><authors>Parul Khurana, Geetha Ganesan, Gulshan Kumar, Kiran Sharma</authors><categories>cs.DL cs.IR</categories><comments>22 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Numerous indexing databases keep track of the number of publications,
citations, etc. in order to maintain the progress of science and individual.
However, the choice of journals and articles varies among these indexing
databases, hence the number of citations and h-index varies. There is no common
platform exists that can provide a single count for the number of publications,
citations, h-index, etc. To overcome this limitation, we have proposed a
weighted unified informetrics, named &quot;conflate&quot;. The proposed system takes into
account the input from multiple indexing databases and generates a single
output. Here, we have used the data from Scopus and WoS to generate a conflate
dataset. Further, a comparative analysis of conflate has been performed with
Scopus and WoS at three levels: author, organization, and journal. Finally, a
mapping is proposed between research publications and distributed ledger
technology in order to provide a transparent and distributed view to its
stakeholders.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01236</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01236</id><submitter>Darryl Hill</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:36:50 GMT</date><size>609kb</size><source_type>D</source_type></version><title>Improved Spanning on Theta-5</title><authors>Prosenjit Bose (1), Darryl Hill (1), Aur\'elien Ooms ((1) Carleton
  University)</authors><categories>cs.CG</categories><comments>21 pages, 30 figures, to be published in the proceedings of WADS
  2021, The 17th Algorithms and Data Structures Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an upper bound of $\frac{
  \sin\left(\frac{3\pi}{10}\right)
  }{
  \sin\left(\frac{2\pi}{5}\right)-\sin\left(\frac{3\pi}{10}\right)
  }
  &lt;5.70$ on the spanning ratio of $\Theta_5$-graphs, improving on the previous
best known upper bound of $9.96$ [Bose, Morin, van Renssen, and Verdonschot.
The Theta-5-graph is a spanner. Computational Geometry, 2015.]
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01240</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01240</id><submitter>Uri Kirstein Mr.</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:43:12 GMT</date><size>128kb</size><source_type>D</source_type></version><title>Phoenix: A Formally Verified Regenerating Vault</title><authors>Uri Kirstein, Shelly Grossman, Michael Mirkin, James Wilcox, Ittay
  Eyal, Mooly Sagiv</authors><categories>cs.CR cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An attacker that gains access to a cryptocurrency user's private keys can
perform any operation in her stead. Due to the decentralized nature of most
cryptocurrencies, no entity can revert those operations. This is a central
challenge for decentralized systems, illustrated by numerous high-profile
heists. Vault contracts reduce this risk by introducing artificial delay on
operations, allowing abortion by the contract owner during the delay. However,
the theft of a key still renders the vault unusable and puts funds at risk.
  We introduce Phoenix, a novel contract architecture that allows the user to
restore its security properties after key loss. Phoenix takes advantage of
users' ability to store keys in easily-available but less secure storage
(tier-two) as well as more secure storage that is harder to access (tier-one).
Unlike previous solutions, the user can restore Phoenix security after the
theft of tier-two keys and does not lose funds despite losing keys in either
tier. Phoenix also introduces a mechanism to reduce the damage an attacker can
cause in case of a tier-one compromise.
  We formally specify Phoenix's required behavior and provide a prototype
implementation of Phoenix as an Ethereum contract. Since such an implementation
is highly sensitive and vulnerable to subtle bugs, we apply a formal
verification tool to prove specific code properties and identify faults. We
highlight a bug identified by the tool that could be exploited by an attacker
to compromise Phoenix. After fixing the bug, the tool proved the low-level
executable code's correctness.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01242</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01242</id><submitter>Ferdinando Fioretto</submitter><version version="v1"><date>Wed, 2 Jun 2021 15:46:27 GMT</date><size>2345kb</size><source_type>D</source_type></version><title>A Privacy-Preserving and Trustable Multi-agent Learning Framework</title><authors>Anudit Nagar, Cuong Tran, Ferdinando Fioretto</authors><categories>cs.LG cs.AI cs.CR</categories><comments>This paper is an extended version of Reference [32]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed multi-agent learning enables agents to cooperatively train a
model without requiring to share their datasets. While this setting ensures
some level of privacy, it has been shown that, even when data is not directly
shared, the training process is vulnerable to privacy attacks including data
reconstruction and model inversion attacks. Additionally, malicious agents that
train on inverted labels or random data, may arbitrarily weaken the accuracy of
the global model. This paper addresses these challenges and presents
Privacy-preserving and trustable Distributed Learning (PT-DL), a fully
decentralized framework that relies on Differential Privacy to guarantee strong
privacy protections of the agents' data, and Ethereum smart contracts to ensure
trustability. The paper shows that PT-DL is resilient up to a 50% collusion
attack, with high probability, in a malicious trust model and the experimental
evaluation illustrates the benefits of the proposed model as a
privacy-preserving and trustable distributed multi-agent learning system on
several classification tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01250</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01250</id><submitter>Dmitrii Kosarev</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:01:33 GMT</date><size>48kb</size></version><title>Generic Programming with Combinators and Objects</title><authors>Dmitrii Kosarev and Dmitry Boulytchev</authors><categories>cs.PL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a generic programming framework for OCAML which makes it possible
to implement extensible transformations for a large scale of type definitions.
Our framework makes use of objectoriented features of OCAML, utilising late
binding to override the default behaviour of generated transformations. The
support for polymorphic variant types complements the ability to describe
composable data types with the ability to implement composable transformations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01251</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01251</id><submitter>Vishal Vinod</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:05:24 GMT</date><size>266kb</size><source_type>D</source_type></version><title>Multilingual Medical Question Answering and Information Retrieval for
  Rural Health Intelligence Access</title><authors>Vishal Vinod, Susmit Agrawal, Vipul Gaurav, Pallavi R, Savita
  Choudhary</authors><categories>cs.CL cs.IR</categories><journal-ref>ICLR 2021 Workshop</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In rural regions of several developing countries, access to quality
healthcare, medical infrastructure, and professional diagnosis is largely
unavailable. Many of these regions are gradually gaining access to internet
infrastructure, although not with a strong enough connection to allow for
sustained communication with a medical practitioner. Several deaths resulting
from this lack of medical access, absence of patient's previous health records,
and the unavailability of information in indigenous languages can be easily
prevented. In this paper, we describe an approach leveraging the phenomenal
progress in Machine Learning and NLP (Natural Language Processing) techniques
to design a model that is low-resource, multilingual, and a preliminary
first-point-of-contact medical assistant. Our contribution includes defining
the NLP pipeline required for named-entity-recognition, language-agnostic
sentence embedding, natural language translation, information retrieval,
question answering, and generative pre-training for final query processing. We
obtain promising results for this pipeline and preliminary results for EHR
(Electronic Health Record) analysis with text summarization for medical
practitioners to peruse for their diagnosis. Through this NLP pipeline, we aim
to provide preliminary medical information to the user and do not claim to
supplant diagnosis from qualified medical practitioners. Using the input from
subject matter experts, we have compiled a large corpus to pre-train and
fine-tune our BioBERT based NLP model for the specific tasks. We expect recent
advances in NLP architectures, several of which are efficient and
privacy-preserving models, to further the impact of our solution and improve on
individual task performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01254</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01254</id><submitter>Tim Weninger PhD</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:07:32 GMT</date><size>515kb</size><source_type>D</source_type></version><title>Survey Equivalence: A Procedure for Measuring Classifier Accuracy
  Against Human Labels</title><authors>Paul Resnick, Yuqing Kong, Grant Schoenebeck, Tim Weninger</authors><categories>cs.LG cs.HC cs.MA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In many classification tasks, the ground truth is either noisy or subjective.
Examples include: which of two alternative paper titles is better? is this
comment toxic? what is the political leaning of this news article? We refer to
such tasks as survey settings because the ground truth is defined through a
survey of one or more human raters. In survey settings, conventional
measurements of classifier accuracy such as precision, recall, and
cross-entropy confound the quality of the classifier with the level of
agreement among human raters. Thus, they have no meaningful interpretation on
their own. We describe a procedure that, given a dataset with predictions from
a classifier and K ratings per item, rescales any accuracy measure into one
that has an intuitive interpretation. The key insight is to score the
classifier not against the best proxy for the ground truth, such as a majority
vote of the raters, but against a single human rater at a time. That score can
be compared to other predictors' scores, in particular predictors created by
combining labels from several other human raters. The survey equivalence of any
classifier is the minimum number of raters needed to produce the same expected
score as that found for the classifier.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01255</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01255</id><submitter>Subhanshu Gupta</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:09:15 GMT</date><size>3660kb</size><source_type>D</source_type></version><title>A 4-Element 800MHz-BW 29mW True-Time-Delay Spatial Signal Processor
  Enabling Fast Beam-Training with Data Communications</title><authors>Chung-Ching Lin, Chase Puglisi, Veljko Boljanovic, Soumen Mohapatra,
  Han Yan, Erfan Ghaderi, Deukhyoun Heo, Danijela Cabric, Subhanshu Gupta</authors><categories>eess.SP cs.SY eess.SY</categories><comments>to be presented at the IEEE European Solid-State Circuits Conference
  in September 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Spatial signal processors (SSP) for emerging millimeter-wave wireless
networks are critically dependent on link discovery. To avoid loss in
communication, mobile devices need to locate narrow directional beams with
millisecond latency. In this work, we demonstrate a true-time-delay (TTD) array
with digitally reconfigurable delay elements enabling both fast beam-training
at the receiver with wideband data communications. In beam-training mode, large
delay-bandwidth products are implemented to accelerate beam training using
frequency-dependent probing beams. In data communications mode, precise beam
alignment is achieved to mitigate spatial effects during beam-forming for
wideband signals. The 4-element switched-capacitor based time-interleaved array
uses a compact closed-loop integrator for signal combining with the delay
compensation implemented in the clock domain to achieve high precision and
large delay range. Prototyped in TSMC 65nm CMOS, the TTD SSP successfully
demonstrates unique frequency-to-angle mapping with 3.8ns maximum delay and
800MHz bandwidth in the beam-training mode. In the data communications mode,
nearly 12dB uniform beamforming gain is achieved from 80MHz to 800MHz. The TTD
SSP consumes 29mW at 1V supply achieving 122MB/s with 16-QAM at 9.8% EVM.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01257</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01257</id><submitter>Alexey Naumov</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:10:37 GMT</date><size>70kb</size></version><title>Tight High Probability Bounds for Linear Stochastic Approximation with
  Fixed Stepsize</title><authors>Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin
  Scaman, Hoi-To Wai</authors><categories>stat.ML cs.LG math.PR math.ST stat.TH</categories><comments>21 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides a non-asymptotic analysis of linear stochastic
approximation (LSA) algorithms with fixed stepsize. This family of methods
arises in many machine learning tasks and is used to obtain approximate
solutions of a linear system $\bar{A}\theta = \bar{b}$ for which $\bar{A}$ and
$\bar{b}$ can only be accessed through random estimates $\{({\bf A}_n, {\bf
b}_n): n \in \mathbb{N}^*\}$. Our analysis is based on new results regarding
moments and high probability bounds for products of matrices which are shown to
be tight. We derive high probability bounds on the performance of LSA under
weaker conditions on the sequence $\{({\bf A}_n, {\bf b}_n): n \in
\mathbb{N}^*\}$ than previous works. However, in contrast, we establish
polynomial concentration bounds with order depending on the stepsize. We show
that our conclusions cannot be improved without additional assumptions on the
sequence of random matrices $\{{\bf A}_n: n \in \mathbb{N}^*\}$, and in
particular that no Gaussian or exponential high probability bounds can hold.
Finally, we pay a particular attention to establishing bounds with sharp order
with respect to the number of iterations and the stepsize and whose leading
terms contain the covariance matrices appearing in the central limit theorems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01258</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01258</id><submitter>Xingyu Zhao</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:10:46 GMT</date><size>1246kb</size><source_type>D</source_type></version><title>Assessing the Reliability of Deep Learning Classifiers Through
  Robustness Evaluation and Operational Profiles</title><authors>Xingyu Zhao, Wei Huang, Alec Banks, Victoria Cox, David Flynn, Sven
  Schewe, Xiaowei Huang</authors><categories>cs.LG</categories><comments>Accepted by the AISafety'21 Workshop at IJCAI-21. To appear in a
  volume of CEUR Workshop Proceedings</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The utilisation of Deep Learning (DL) is advancing into increasingly more
sophisticated applications. While it shows great potential to provide
transformational capabilities, DL also raises new challenges regarding its
reliability in critical functions. In this paper, we present a model-agnostic
reliability assessment method for DL classifiers, based on evidence from
robustness evaluation and the operational profile (OP) of a given application.
We partition the input space into small cells and then &quot;assemble&quot; their
robustness (to the ground truth) according to the OP, where estimators on the
cells' robustness and OPs are provided. Reliability estimates in terms of the
probability of misclassification per input (pmi) can be derived together with
confidence levels. A prototype tool is demonstrated with simplified case
studies. Model assumptions and extension to real-world applications are also
discussed. While our model easily uncovers the inherent difficulties of
assessing the DL dependability (e.g. lack of data with ground truth and
scalability issues), we provide preliminary/compromised solutions to advance in
this research direction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01260</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01260</id><submitter>Patrick Rubin-Delanchy Dr</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:11:33 GMT</date><size>8026kb</size><source_type>D</source_type></version><title>Matrix factorisation and the interpretation of geodesic distance</title><authors>Nick Whiteley, Annie Gray and Patrick Rubin-Delanchy</authors><categories>stat.ML cs.LG</categories><comments>28 pages; 13 figures</comments><msc-class>62G05, 62H20, 62H12, 62H30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given a graph or similarity matrix, we consider the problem of recovering a
notion of true distance between the nodes, and so their true positions. Through
new insights into the manifold geometry underlying a generic latent position
model, we show that this can be accomplished in two steps: matrix
factorisation, followed by nonlinear dimension reduction. This combination is
effective because the point cloud obtained in the first step lives close to a
manifold in which latent distance is encoded as geodesic distance. Hence, a
nonlinear dimension reduction tool, approximating geodesic distance, can
recover the latent positions, up to a simple transformation. We give a detailed
account of the case where spectral embedding is used, followed by Isomap, and
provide encouraging experimental evidence for other combinations of techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01262</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01262</id><submitter>Thomas Haubner</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:14:43 GMT</date><size>132kb</size></version><title>End-To-End Deep Learning-Based Adaptation Control for Frequency-Domain
  Adaptive System Identification</title><authors>Thomas Haubner and Andreas Brendel and Walter Kellermann</authors><categories>eess.AS cs.SD eess.SP</categories><doi>10.13140/RG.2.2.14071.14242</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel end-to-end deep learning-based adaptation control
algorithm for frequency-domain adaptive system identification. The proposed
method exploits a deep neural network to map observed signal features to
corresponding step-sizes which control the filter adaptation. The parameters of
the network are optimized in an end-to-end fashion by minimizing the average
system distance of the adaptive filter. This avoids the need of explicit signal
power spectral density estimation as required for model-based adaptation
control and further auxiliary mechanisms to deal with model inaccuracies. The
proposed algorithm achieves fast convergence and robust steady-state
performance for scenarios characterized by non-white and non-stationary noise
signals, time-varying environment changes and additional model inaccuracies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01263</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01263</id><submitter>Chiyu Song</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:14:51 GMT</date><size>236kb</size><source_type>D</source_type></version><title>Global-Selector: A New Benchmark Dataset and Model Architecture for
  Multi-turn Response Selection</title><authors>Chiyu Song, Hongliang He, Huachuan Qiu, Haofei Yu, Zhenzhong Lan</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an essential component of dialogue systems, multi-turn response selection
aims to pick out the optimal response among a set of candidates to improve the
dialogue fluency. In this paper, we investigate three problems of current
response selection approaches, especially for generation-based conversational
agents: (i) Existing approaches are often formulated as a sentence scoring
problem, which does not consider relationships between responses. (ii) Existing
models tend to select undesirable candidates that have large overlaps with the
dialogue history. (iii) Negative instances in training are mainly constructed
by random sampling from the corpus, whereas generated candidates in practice
typically have a closer distribution. To address the above problems, we create
a new dataset called ConvAI2+ and propose a new response selector called
Global-Selector. Experimental results show that Global-Selector trained on
ConvAI2+ have noticeable improvements in both accuracy and inference speed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01266</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01266</id><submitter>Leonardo Fanzeres</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:20:43 GMT</date><size>2050kb</size></version><title>Sound-to-Imagination: Unsupervised Crossmodal Translation Using Deep
  Dense Network Architecture</title><authors>Leonardo A. Fanzeres and Climent Nadeu</authors><categories>cs.SD cs.GR cs.MM eess.AS eess.IV</categories><msc-class>68T07, 68T20</msc-class><acm-class>I.3.3; H.5.5; H.5.1; I.2.0</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The motivation of our research is to develop a sound-to-image (S2I)
translation system for enabling a human receiver to visually infer the
occurrence of sound related events. We expect the computer to 'imagine' the
scene from the captured sound, generating original images that picture the
sound emitting source. Previous studies on similar topics opted for simplified
approaches using data with low content diversity and/or strong supervision.
Differently, we propose to perform unsupervised S2I translation using thousands
of distinct and unknown scenes, with slightly pre-cleaned data, just enough to
guarantee aural-visual semantic coherence. To that end, we employ conditional
generative adversarial networks (GANs) with a deep densely connected generator.
Besides, we implemented a moving-average adversarial loss to address GANs
training instability. Though the specified S2I translation problem is quite
challenging, we were able to generalize the translator model enough to obtain
more than 14%, in average, of interpretable and semantically coherent images
translated from unknown sounds. Additionally, we present a solution using
informativity classifiers to perform quantitative evaluation of S2I
translation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01269</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01269</id><submitter>Soujanya Poria</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:21:38 GMT</date><size>6398kb</size><source_type>D</source_type></version><title>More Identifiable yet Equally Performant Transformers for Text
  Classification</title><authors>Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria, Eduard Hovy</authors><categories>cs.CL</categories><comments>ACL 2021</comments><journal-ref>ACL 2021</journal-ref><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Interpretability is an important aspect of the trustworthiness of a model's
predictions. Transformer's predictions are widely explained by the attention
weights, i.e., a probability distribution generated at its self-attention unit
(head). Current empirical studies provide shreds of evidence that attention
weights are not explanations by proving that they are not unique. A recent
study showed theoretical justifications to this observation by proving the
non-identifiability of attention weights. For a given input to a head and its
output, if the attention weights generated in it are unique, we call the
weights identifiable. In this work, we provide deeper theoretical analysis and
empirical observations on the identifiability of attention weights. Ignored in
the previous works, we find the attention weights are more identifiable than we
currently perceive by uncovering the hidden role of the key vector. However,
the weights are still prone to be non-unique attentions that make them unfit
for interpretation. To tackle this issue, we provide a variant of the encoder
layer that decouples the relationship between key and value vector and provides
identifiable weights up to the desired length of the input. We prove the
applicability of such variations by providing empirical justifications on
varied text classification tasks. The implementations are available at
https://github.com/declare-lab/identifiable-transformers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01271</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01271</id><submitter>Jonathan Dumas</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:28:10 GMT</date><size>150kb</size><source_type>D</source_type></version><title>Deep learning-based multi-output quantile forecasting of PV generation</title><authors>Jonathan Dumas, Colin Cointe, Xavier Fettweis, Bertrand Corn\'elusse</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper develops probabilistic PV forecasters by taking advantage of
recent breakthroughs in deep learning. It tailored forecasting tool, named
encoder-decoder, is implemented to compute intraday multi-output PV quantiles
forecasts to efficiently capture the time correlation. The models are trained
using quantile regression, a non-parametric approach that assumes no prior
knowledge of the probabilistic forecasting distribution. The case study is
composed of PV production monitored on-site at the University of Li\`ege
(ULi\`ege), Belgium. The weather forecasts from the regional climate model
provided by the Laboratory of Climatology are used as inputs of the deep
learning models. The forecast quality is quantitatively assessed by the
continuous ranked probability and interval scores. The results indicate this
architecture improves the forecast quality and is computationally efficient to
be incorporated in an intraday decision-making tool for robust optimization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01272</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01272</id><submitter>Tao Wang</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:33:43 GMT</date><size>1062kb</size></version><title>Grasp stability prediction with time series data based on STFT and LSTM</title><authors>Tao Wang and Frank Kirchner</authors><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  With an increasing demand for robots, robotic grasping will has a more
important role in future applications. This paper takes grasp stability
prediction as the key technology for grasping and tries to solve the problem
with time series data inputs including the force and pressure data. Widely
applied to more fields to predict unstable grasping with time series data,
algorithms can significantly promote the application of artificial intelligence
in traditional industries. This research investigates models that combine
short-time Fourier transform (STFT) and long short-term memory (LSTM) and then
tested generalizability with dexterous hand and suction cup gripper. The
experiments suggest good results for grasp stability prediction with the force
data and the generalized results in the pressure data. Among the 4 models,
(Data + STFT) &amp; LSTM delivers the best performance. We plan to perform more
work on grasp stability prediction, generalize the findings to different types
of sensors, and apply the grasp stability prediction in more grasping use cases
in real life.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01273</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01273</id><submitter>Wenlong Tian</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:34:07 GMT</date><size>619kb</size><source_type>D</source_type></version><title>Chunk Content is not Enough: Chunk-Context Aware Resemblance Detection
  for Deduplication Delta Compression</title><authors>Xuming Ye, Xiaoye Xue, Wenlong Tian, Zhiyong Xu, Weijun Xiao, Ruixuan
  Li</authors><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the growing popularity of cloud storage, removing duplicated data across
users is getting more critical for service providers to reduce costs. Recently,
Data resemblance detection is a novel technology to detect redundancy among
similarity. It extracts feature from each chunk content and treat chunks with
high similarity as candidates for removing redundancy. However, popular
resemblance methods such as &quot;N-transform&quot; and &quot;Finesse&quot; use only the chunk data
for feature extraction. A minor modification on the data chunk could seriously
deteriorate its capability for resemblance detection. In this paper, we
proposes a novel chunk-context aware resemblance detection algorithm, called
CARD, to mitigate this issue. CARD introduces a BP-Neural network-based
chunk-context aware model, and uses N-sub-chunk shingles-based initial feature
extraction strategy. It effectively integrates each data chunk content's
internal structure with the context information for feature extraction, the
impact of small changes in data chunks is significantly reduced. To evaluate
its performance, we implement a CARD prototype and conduct extensive
experiments using real-world data sets. The results show that CARD can detect
up to 75.03% more redundant data and accelerate the resemblance detection
operations by 5.6 to 17.8 times faster compared with the state-of-the-art
resemblance detection approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01275</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01275</id><submitter>Heather Switzer</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:37:05 GMT</date><size>3433kb</size><source_type>D</source_type></version><title>Probing for the Trace Estimation of a Permuted Matrix Inverse
  Corresponding to a Lattice Displacement</title><authors>Heather Switzer, Andreas Stathopoulos, Eloy Romero, Jesse Laeuchli,
  Kostas Orginos</authors><categories>hep-lat cs.NA math.NA</categories><msc-class>05B20, 15A15, 65C05, 65F50, 68R10, 81V05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probing is a general technique that is used to reduce the variance of the
Hutchinson stochastic estimator for the trace of the inverse of a large, sparse
matrix $A$. The variance of the estimator is the sum of the squares of the
off-diagonal elements of $A^{-1}$. Therefore, this technique computes probing
vectors that when used in the estimator they annihilate the largest
off-diagonal elements. For matrices that display decay of the magnitude of
$|A^{-1}_{ij}|$ with the graph distance between nodes $i$ and $j$, this is
achieved through graph coloring of increasing powers $A^p$. Equivalently, when
a matrix stems from a lattice discretization, it is computationally beneficial
to find a distance-$p$ coloring of the lattice.
  In this work, we study probing for the more general problem of computing the
trace of a permutation of $A^{-1}$, say $PA^{-1}$, motivated from Lattice QCD
where we need to construct &quot;disconnected diagrams&quot; to extract flavor-separated
Generalized Parton functions. In Lattice QCD, where the matrix has a 4D
toroidal lattice structure, these non-local operators correspond to a $PA^{-1}$
where $P$ is the permutation relating to some displacement in one or more
dimensions. We focus on a single dimension displacement ($k$) but our methods
are general. We show that probing on $A^p$ or $(PA)^p$ do not annihilate the
largest magnitude elements. To resolve this issue, our displacement-based
probing works on $PA^p$ using a new coloring scheme that works directly on
appropriately displaced neighborhoods on the lattice. We prove lower bounds on
the number of colors needed, and study the effect of this scheme on variance
reduction, both theoretically and experimentally on a real-world Lattice QCD
calculation. We achieve orders of magnitude speedup over the un-probed or the
naively probed methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01277</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01277</id><submitter>Pierre Gutierrez</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:37:20 GMT</date><size>10808kb</size><source_type>D</source_type></version><title>Data augmentation and pre-trained networks for extremely low data
  regimes unsupervised visual inspection</title><authors>Pierre Gutierrez, Antoine Cordier, Tha\&quot;is Caldeira, Th\'eophile
  Sautory</authors><categories>cs.CV cs.AI cs.LG stat.ML</categories><comments>16 pages, 8 figures, 9 tables, SPIE proceedings of Optical Metrology
  conference (https://spie.org/conferences-and-exhibitions/optical-metrology)</comments><acm-class>I.2.10; I.4.6; I.4.8; I.4.9; I.5</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The use of deep features coming from pre-trained neural networks for
unsupervised anomaly detection purposes has recently gathered momentum in the
computer vision field. In particular, industrial inspection applications can
take advantage of such features, as demonstrated by the multiple successes of
related methods on the MVTec Anomaly Detection (MVTec AD) dataset. These
methods make use of neural networks pre-trained on auxiliary classification
tasks such as ImageNet. However, to our knowledge, no comparative study of
robustness to the low data regimes between these approaches has been conducted
yet. For quality inspection applications, the handling of limited sample sizes
may be crucial as large quantities of images are not available for small
series. In this work, we aim to compare three approaches based on deep
pre-trained features when varying the quantity of available data in MVTec AD:
KNN, Mahalanobis, and PaDiM. We show that although these methods are mostly
robust to small sample sizes, they still can benefit greatly from using data
augmentation in the original image space, which allows to deal with very small
production runs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01282</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01282</id><submitter>Patrick Rubin-Delanchy Dr</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:43:43 GMT</date><size>385kb</size><source_type>D</source_type></version><title>Spectral embedding for dynamic networks with stability guarantees</title><authors>Ian Gallagher, Andrew Jones and Patrick Rubin-Delanchy</authors><categories>stat.ML cs.LG</categories><comments>16 pages, 4 figures</comments><msc-class>62M10, 62H30, 62G99</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of embedding a dynamic network, to obtain
time-evolving vector representations of each node, which can then be used to
describe the changes in behaviour of a single node, one or more communities, or
the entire graph. Given this open-ended remit, we wish to guarantee stability
in the spatio-temporal positioning of the nodes: assigning the same position,
up to noise, to nodes behaving similarly at a given time (cross-sectional
stability) and a constant position, up to noise, to a single node behaving
similarly across different times (longitudinal stability). These properties are
defined formally within a generic dynamic latent position model. By showing how
this model can be recast as a multilayer random dot product graph, we
demonstrate that unfolded adjacency spectral embedding satisfies both stability
conditions, allowing, for example, spatio-temporal clustering under the dynamic
stochastic block model. We also show how alternative methods, such as omnibus,
independent or time-averaged spectral embedding, lack one or the other form of
stability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01285</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01285</id><submitter>Ian Gemp</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:44:54 GMT</date><size>14800kb</size><source_type>D</source_type></version><title>Sample-based Approximation of Nash in Large Many-Player Games via
  Gradient Descent</title><authors>Ian Gemp, Rahul Savani, Marc Lanctot, Yoram Bachrach, Thomas Anthony,
  Richard Everett, Andrea Tacchetti, Tom Eccles, J\'anos Kram\'ar</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nash equilibrium is a central concept in game theory. Several Nash solvers
exist, yet none scale to normal-form games with many actions and many players,
especially those with payoff tensors too big to be stored in memory. In this
work, we propose an approach that iteratively improves an approximation to a
Nash equilibrium through joint play. It accomplishes this by tracing a
previously established homotopy which connects instances of the game defined
with decaying levels of entropy regularization. To encourage iterates to remain
near this path, we efficiently minimize \emph{average deviation incentive} via
stochastic gradient descent, intelligently sampling entries in the payoff
tensor as needed. This process can also be viewed as constructing and reacting
to a polymatrix approximation to the game. In these ways, our proposed
approach, \emph{average deviation incentive descent with adaptive sampling}
(ADIDAS), is most similar to three classical approaches, namely homotopy-type,
Lyapunov, and iterative polymatrix solvers. We demonstrate through experiments
the ability of this approach to approximate a Nash equilibrium in normal-form
games with as many as seven players and twenty one actions (over one trillion
outcomes) that are orders of magnitude larger than those possible with prior
algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01286</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01286</id><submitter>Homa Nikbakht</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:45:30 GMT</date><size>20kb</size></version><title>Cooperative Encoding and Decoding of Mixed Delay Traffic under
  Random-User Activity</title><authors>Homa Nikbakht, Mich\`ele Wigger, Shlomo Shamai (Shitz), Jean-Marie
  Gorce</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper analyses the multiplexing gain (MG) achievable over Wyner's
symmetric network with random user activity and random arrival of mixed-delay
traffic. The mixed-delay traffic is composed of delay-tolerant traffic and
delay-sensitive traffic where only the former can benefit from transmitter and
receiver cooperation since the latter is subject to stringent decoding delays.
The total number of cooperation rounds at transmitter and receiver sides is
limited to $\D$ rounds. We derive inner and outer bounds on the MG region. In
the limit as $\D\to \infty$, the bounds coincide and the results show that
transmitting delay-sensitive messages does not cause any penalty on the sum MG.
For finite $\D$ our bounds are still close and prove that the penalty caused by
delay-sensitive transmissions is small.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01288</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01288</id><submitter>Charlotte Frenkel</submitter><version version="v1"><date>Wed, 2 Jun 2021 16:51:45 GMT</date><size>1663kb</size><source_type>D</source_type></version><title>Bottom-Up and Top-Down Neural Processing Systems Design: Neuromorphic
  Intelligence as the Convergence of Natural and Artificial Intelligence</title><authors>Charlotte Frenkel, David Bol, Giacomo Indiveri</authors><categories>cs.NE cs.AI cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Moore's law has driven exponential computing power expectations, its
nearing end calls for new avenues for improving the overall system performance.
One of these avenues is the exploration of new alternative brain-inspired
computing architectures that promise to achieve the flexibility and
computational efficiency of biological neural processing systems. Within this
context, neuromorphic intelligence represents a paradigm shift in computing
based on the implementation of spiking neural network architectures tightly
co-locating processing and memory. In this paper, we provide a comprehensive
overview of the field, highlighting the different levels of granularity present
in existing silicon implementations, comparing approaches that aim at
replicating natural intelligence (bottom-up) versus those that aim at solving
practical artificial intelligence applications (top-down), and assessing the
benefits of the different circuit design styles used to achieve these goals.
First, we present the analog, mixed-signal and digital circuit design styles,
identifying the boundary between processing and memory through time
multiplexing, in-memory computation and novel devices. Next, we highlight the
key tradeoffs for each of the bottom-up and top-down approaches, survey their
silicon implementations, and carry out detailed comparative analyses to extract
design guidelines. Finally, we identify both necessary synergies and missing
elements required to achieve a competitive advantage for neuromorphic edge
computing over conventional machine-learning accelerators, and outline the key
elements for a framework toward neuromorphic intelligence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01300</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01300</id><submitter>Tao Qi</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:15:57 GMT</date><size>1618kb</size><source_type>D</source_type></version><title>PP-Rec: News Recommendation with Personalized User Interest and
  Time-aware News Popularity</title><authors>Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang</authors><categories>cs.IR</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalized news recommendation methods are widely used in online news
services. These methods usually recommend news based on the matching between
news content and user interest inferred from historical behaviors. However,
these methods usually have difficulties in making accurate recommendations to
cold-start users, and tend to recommend similar news with those users have
read. In general, popular news usually contain important information and can
attract users with different interests. Besides, they are usually diverse in
content and topic. Thus, in this paper we propose to incorporate news
popularity information to alleviate the cold-start and diversity problems for
personalized news recommendation. In our method, the ranking score for
recommending a candidate news to a target user is the combination of a
personalized matching score and a news popularity score. The former is used to
capture the personalized user interest in news. The latter is used to measure
time-aware popularity of candidate news, which is predicted based on news
content, recency, and real-time CTR using a unified framework. Besides, we
propose a popularity-aware user encoder to eliminate the popularity bias in
user behaviors for accurate interest modeling. Experiments on two real-world
datasets show our method can effectively improve the accuracy and diversity for
news recommendation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01309</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01309</id><submitter>Qiaohao Liang</submitter><version version="v1"><date>Sun, 23 May 2021 22:04:07 GMT</date><size>3209kb</size><source_type>D</source_type></version><title>Benchmarking the Performance of Bayesian Optimization across Multiple
  Experimental Materials Science Domains</title><authors>Qiaohao Liang, Aldair E. Gongora, Zekun Ren, Armi Tiihonen, Zhe Liu,
  Shijing Sun, James R. Deneault, Daniil Bash, Flore Mekki-Berrada, Saif A.
  Khan, Kedar Hippalgaonkar, Benji Maruyama, Keith A. Brown, John Fisher III,
  and Tonio Buonassisi</authors><categories>cond-mat.mtrl-sci cs.LG physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of machine learning (ML) for materials optimization, active
learning algorithms, such as Bayesian Optimization (BO), have been leveraged
for guiding autonomous and high-throughput experimentation systems. However,
very few studies have evaluated the efficiency of BO as a general optimization
algorithm across a broad range of experimental materials science domains. In
this work, we evaluate the performance of BO algorithms with a collection of
surrogate model and acquisition function pairs across five diverse experimental
materials systems, namely carbon nanotube polymer blends, silver nanoparticles,
lead-halide perovskites, as well as additively manufactured polymer structures
and shapes. By defining acceleration and enhancement metrics for general
materials optimization objectives, we find that for surrogate model selection,
Gaussian Process (GP) with anisotropic kernels (automatic relevance detection,
ARD) and Random Forests (RF) have comparable performance and both outperform
the commonly used GP without ARD. We discuss the implicit distributional
assumptions of RF and GP, and the benefits of using GP with anisotropic kernels
in detail. We provide practical insights for experimentalists on surrogate
model selection of BO during materials optimization campaigns.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01315</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01315</id><submitter>Jing Ma</submitter><version version="v1"><date>Sat, 29 May 2021 00:40:24 GMT</date><size>7565kb</size><source_type>D</source_type></version><title>Assessing the Causal Impact of COVID-19 Related Policies on Outbreak
  Dynamics: A Case Study in the US</title><authors>Jing Ma, Yushun Dong, Zheng Huang, Daniel Mietchen, Jundong Li</authors><categories>cs.LG cs.CY stat.ME</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To mitigate the spread of COVID-19 pandemic, decision-makers and public
authorities have announced various non-pharmaceutical policies. Analyzing the
causal impact of these policies in reducing the spread of COVID-19 is important
for future policy-making. The main challenge here is the existence of
unobserved confounders (e.g., vigilance of residents). Besides, as the
confounders may be time-varying during COVID-19 (e.g., vigilance of residents
changes in the course of the pandemic), it is even more difficult to capture
them. In this paper, we study the problem of assessing the causal effects of
different COVID-19 related policies on the outbreak dynamics in different
counties at any given time period. To this end, we integrate data about
different COVID-19 related policies (treatment) and outbreak dynamics (outcome)
for different United States counties over time and analyze them with respect to
variables that can infer the confounders, including the covariates of different
counties, their relational information and historical information. Based on
these data, we develop a neural network based causal effect estimation
framework which leverages above information in observational data and learns
the representations of time-varying (unobserved) confounders. In this way, it
enables us to quantify the causal impact of policies at different
granularities, ranging from a category of policies with a certain goal to a
specific policy type in this category. Besides, experimental results also
indicate the effectiveness of our proposed framework in capturing the
confounders for quantifying the causal impact of different policies. More
specifically, compared with several baseline methods, our framework captures
the outbreak dynamics more accurately, and our assessment of policies is more
consistent with existing epidemiological studies of COVID-19.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01317</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01317</id><submitter>Yichen Jiang</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:32:33 GMT</date><size>270kb</size><source_type>D</source_type></version><title>Enriching Transformers with Structured Tensor-Product Representations
  for Abstractive Summarization</title><authors>Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha
  Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, Jianfeng
  Gao</authors><categories>cs.CL cs.AI cs.LG</categories><comments>NAACL 2021 (14 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstractive summarization, the task of generating a concise summary of input
documents, requires: (1) reasoning over the source document to determine the
salient pieces of information scattered across the long document, and (2)
composing a cohesive text by reconstructing these salient facts into a shorter
summary that faithfully reflects the complex relations connecting these facts.
In this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019), an architecture
that enriches the original Transformer (Vaswani et al., 2017) with the
explicitly compositional Tensor Product Representation (TPR), for the task of
abstractive summarization. The key feature of our model is a structural bias
that we introduce by encoding two separate representations for each token to
represent the syntactic structure (with role vectors) and semantic content
(with filler vectors) separately. The model then binds the role and filler
vectors into the TPR as the layer output. We argue that the structured
intermediate representations enable the model to take better control of the
contents (salient facts) and structures (the syntax that connects the facts)
when generating the summary. Empirically, we show that our TP-TRANSFORMER
outperforms the Transformer and the original TP-TRANSFORMER significantly on
several abstractive summarization datasets based on both automatic and human
evaluations. On several syntactic and semantic probing tasks, we demonstrate
the emergent structural information in the role vectors and improved syntactic
interpretability in the TPR layer outputs. Code and models are available at
https://github.com/jiangycTarheel/TPT-Summ.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01325</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01325</id><submitter>David Lindner</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:38:10 GMT</date><size>11868kb</size><source_type>D</source_type></version><title>Addressing the Long-term Impact of ML Decisions via Policy Regret</title><authors>David Lindner and Hoda Heidari and Andreas Krause</authors><categories>cs.LG cs.AI cs.CY stat.ML</categories><comments>Accepted to IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning (ML) increasingly informs the allocation of opportunities to
individuals and communities in areas such as lending, education, employment,
and beyond. Such decisions often impact their subjects' future characteristics
and capabilities in an a priori unknown fashion. The decision-maker, therefore,
faces exploration-exploitation dilemmas akin to those in multi-armed bandits.
Following prior work, we model communities as arms. To capture the long-term
effects of ML-based allocation decisions, we study a setting in which the
reward from each arm evolves every time the decision-maker pulls that arm. We
focus on reward functions that are initially increasing in the number of pulls
but may become (and remain) decreasing after a certain point. We argue that an
acceptable sequential allocation of opportunities must take an arm's potential
for growth into account. We capture these considerations through the notion of
policy regret, a much stronger notion than the often-studied external regret,
and present an algorithm with provably sub-linear policy regret for
sufficiently long time horizons. We empirically compare our algorithm with
several baselines and find that it consistently outperforms them, in particular
for long time horizons.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01326</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01326</id><submitter>Joseph Chen</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:38:22 GMT</date><size>567kb</size></version><title>Robust Voxelization and Visualization by Improved Tetrahedral Mesh
  Generation</title><authors>Joseph Chen, Ko-Wei Tai, Wen-Chin Chen, and Ming Ouhyoung</authors><categories>cs.GR</categories><comments>11 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When obtaining interior 3D voxel data from triangular meshes, most existing
methods fail to handle low quality meshes which happens to take up a big
portion on the internet. In this work we present a robust voxelization method
that is based on tetrahedral mesh generation within a user defined error bound.
Comparing to other tetrahedral mesh generation methods, our method produces
much higher quality tetrahedral meshes as the intermediate outcome, which
allows us to utilize a faster voxelization algorithm that is based on a
stronger assumption. We show the results comparing to various methods including
the state-of-the-art. Our contribution includes a framework which takes
triangular mesh as an input and produces voxelized data, a proof to an unproved
algorithm that performs better than the state-of-the-art, and various
experiments including parallelization built on the GPU and CPU. We further
tested our method on various dataset including Princeton ModelNet and Thingi10k
to show the robustness of the framework, where near 100% availability is
achieved, while others can only achieve around 50%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01329</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01329</id><submitter>Giacomo Indiveri</submitter><version version="v1"><date>Sun, 30 May 2021 20:12:27 GMT</date><size>356kb</size><source_type>D</source_type></version><title>Introducing &quot;Neuromorphic Computing and Engineering&quot;</title><authors>Giacomo Indiveri</authors><categories>cs.DC cs.AR cs.CE cs.NE q-bio.NC</categories><comments>NCE Editorial</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard nature of computing is currently being challenged by a range of
problems that start to hinder technological progress. One of the strategies
being proposed to address some of these problems is to develop novel
brain-inspired processing methods and technologies, and apply them to a wide
range of application scenarios. This is an extremely challenging endeavor that
requires researchers in multiple disciplines to combine their efforts and
co-design at the same time the processing methods, the supporting computing
architectures, and their underlying technologies. The journal ``Neuromorphic
Computing and Engineering'' (NCE) has been launched to support this new
community in this effort and provide a forum and repository for presenting and
discussing its latest advances. Through close collaboration with our colleagues
on the editorial team, the scope and characteristics of NCE have been designed
to ensure it serves a growing transdisciplinary and dynamic community across
academia and industry.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01331</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01331</id><submitter>Tao Chen</submitter><version version="v1"><date>Mon, 31 May 2021 03:03:53 GMT</date><size>1634kb</size><source_type>D</source_type></version><title>Multi-Objectivizing Software Configuration Tuning (for a single
  performance concern)</title><authors>Tao Chen and Miqing Li</authors><categories>cs.DC cs.AI</categories><comments>13 pages, 7 figures, 4 tables. In Proceedings of the 29th ACM Joint
  European Software Engineering Conference and Symposium on the Foundations of
  Software Engineering (ESEC/FSE'21), 2021</comments><doi>10.1145/3468264.3468555</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Automatically tuning software configuration for optimizing a single
performance attribute (e.g., minimizing latency) is not trivial, due to the
nature of the configuration systems (e.g., complex landscape and expensive
measurement). To deal with the problem, existing work has been focusing on
developing various effective optimizers. However, a prominent issue that all
these optimizers need to take care of is how to avoid the search being trapped
in local optima -- a hard nut to crack for software configuration tuning due to
its rugged and sparse landscape, and neighboring configurations tending to
behave very differently. Overcoming such in an expensive measurement setting is
even more challenging. In this paper, we take a different perspective to tackle
this issue. Instead of focusing on improving the optimizer, we work on the
level of optimization model. We do this by proposing a meta
multi-objectivization model (MMO) that considers an auxiliary performance
objective (e.g., throughput in addition to latency). What makes this model
unique is that we do not optimize the auxiliary performance objective, but
rather use it to make similarly-performing while different configurations less
comparable (i.e. Pareto nondominated to each other), thus preventing the search
from being trapped in local optima.
  Experiments on eight real-world software systems/environments with diverse
performance attributes reveal that our MMO model is statistically more
effective than state-of-the-art single-objective counterparts in overcoming
local optima (up to 42% gain), while using as low as 24% of their measurements
to achieve the same (or better) performance result.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01335</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01335</id><submitter>Tianchu Ji</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:45:47 GMT</date><size>2909kb</size><source_type>D</source_type></version><title>On the Distribution, Sparsity, and Inference-time Quantization of
  Attention Values in Transformers</title><authors>Tianchu Ji, Shraddhan Jain, Michael Ferdman, Peter Milder, H. Andrew
  Schwartz, Niranjan Balasubramanian</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How much information do NLP tasks really need from a transformer's attention
mechanism at application-time (inference)? From recent work, we know that there
is sparsity in transformers and that the floating-points within its computation
can be discretized to fewer values with minimal loss to task accuracies.
However, this requires retraining or even creating entirely new models, both of
which can be expensive and carbon-emitting. Focused on optimizations that do
not require training, we systematically study the full range of typical
attention values necessary. This informs the design of an inference-time
quantization technique using both pruning and log-scaled mapping which produces
only a few (e.g. $2^3$) unique values. Over the tasks of question answering and
sentiment analysis, we find nearly 80% of attention values can be pruned to
zeros with minimal ($&lt; 1.0\%$) relative loss in accuracy. We use this pruning
technique in conjunction with quantizing the attention values to only a 3-bit
format, without retraining, resulting in only a 0.8% accuracy reduction on
question answering with fine-tuned RoBERTa.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01336</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01336</id><submitter>Huanyu Zhang</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:45:47 GMT</date><size>34kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 04:40:12 GMT</date><size>390kb</size></version><title>Improved Rates for Differentially Private Stochastic Convex Optimization
  with Heavy-Tailed Data</title><authors>Gautam Kamath, Xingtu Liu, Huanyu Zhang</authors><categories>cs.LG cs.CR cs.DS math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study stochastic convex optimization with heavy-tailed data under the
constraint of differential privacy. Most prior work on this problem is
restricted to the case where the loss function is Lipschitz. Instead, as
introduced by Wang, Xiao, Devadas, and Xu, we study general convex loss
functions with the assumption that the distribution of gradients has bounded
$k$-th moments. We provide improved upper bounds on the excess population risk
under approximate differential privacy of
$\tilde{O}\left(\sqrt{\frac{d}{n}}+\left(\frac{d}{\epsilon
n}\right)^{\frac{k-1}{k}}\right)$ and
$\tilde{O}\left(\frac{d}{n}+\left(\frac{d}{\epsilon
n}\right)^{\frac{2k-2}{k}}\right)$ for convex and strongly convex loss
functions, respectively. We also prove nearly-matching lower bounds under the
constraint of pure differential privacy, giving strong evidence that our bounds
are tight.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01340</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01340</id><submitter>Tim Roughgarden</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:48:32 GMT</date><size>49kb</size></version><title>Transaction Fee Mechanism Design</title><authors>Tim Roughgarden</authors><categories>cs.CR cs.DC cs.DS cs.GT econ.TH</categories><comments>Appears in the 22nd ACM Conference on Economics and Computation (EC
  '21). This conference paper is derived from Sections 2, 4, 5, 6, and 8 of the
  longer general-audience report published as arXiv:2012.00854</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand for blockchains such as Bitcoin and Ethereum is far larger than
supply, necessitating a mechanism that selects a subset of transactions to
include &quot;on-chain&quot; from the pool of all pending transactions. EIP-1559 is a
proposal to make several tightly coupled changes to the Ethereum blockchain's
transaction fee mechanism, including the introduction of variable-size blocks
and a burned base fee that rises and falls with demand. These changes are
slated for deployment in Ethereum's &quot;London fork,&quot; scheduled for late
summer~2021, at which point it will be the biggest economic change made to a
major blockchain to date.
  The first goal of this paper is to formalize the problem of designing a
transaction fee mechanism, taking into account the many idiosyncrasies of the
blockchain setting (ranging from off-chain collusion between miners and users
to the ease of money-burning). The second goal is to situate the specific
mechanism proposed in EIP-1559 in this framework and rigorously interrogate its
game-theoretic properties. The third goal is to suggest competing designs that
offer alternative sets of trade-offs. The final goal is to highlight research
opportunities for the EC community that could help shape the future of
blockchain transaction fee mechanisms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01342</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01342</id><submitter>Gowthami Somepalli</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:51:05 GMT</date><size>2787kb</size><source_type>D</source_type></version><title>SAINT: Improved Neural Networks for Tabular Data via Row Attention and
  Contrastive Pre-Training</title><authors>Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss,
  Tom Goldstein</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Tabular data underpins numerous high-impact applications of machine learning
from fraud detection to genomics and healthcare. Classical approaches to
solving tabular problems, such as gradient boosting and random forests, are
widely used by practitioners. However, recent deep learning methods have
achieved a degree of performance competitive with popular techniques. We devise
a hybrid deep learning approach to solving tabular data problems. Our method,
SAINT, performs attention over both rows and columns, and it includes an
enhanced embedding method. We also study a new contrastive self-supervised
pre-training method for use when labels are scarce. SAINT consistently improves
performance over previous deep learning methods, and it even outperforms
gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on
average over a variety of benchmark tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01343</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01343</id><submitter>Toni Mancini</submitter><version version="v1"><date>Tue, 1 Jun 2021 00:20:59 GMT</date><size>2740kb</size><source_type>D</source_type></version><title>Reconciling interoperability with efficient Verification and Validation
  within open source simulation environments</title><authors>Stefano Sinisi and Vadim Alimguzhin and Toni Mancini and Enrico Tronci</authors><categories>cs.SE cs.SY eess.SY q-bio.QM</categories><comments>Abridged abstract. This article is 29 pages long</comments><journal-ref>Simulation Modelling Practice and Theory, 109, 102277 (2021)</journal-ref><doi>10.1016/j.simpat.2021.102277</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A Cyber-Physical System (CPS) comprises physical as well as software
subsystems. Simulation-based approaches are typically used to support design
and Verification and Validation (V&amp;V) of CPSs in several domains such as:
aerospace, defence, automotive, smart grid and healthcare. Accordingly, many
simulation-based tools are available, and this poses huge interoperability
challenges. To overcome them, in 2010 the Functional Mock-up Interface (FMI)
was proposed as an open standard to support both Model Exchange (ME) and
Co-Simulation (CS). Models adhering to such a standard are called Functional
Mock-up Units (FMUs). FMUs play an essential role in defining complex CPSs
through, e.g., the SSP standard. Simulation-based V&amp;V of CPSs typically
requires exploring different scenarios (i.e., exogenous CPS input sequences),
many of them showing a shared prefix. Accordingly, the simulator state at the
end of a shared prefix is saved and then restored and used as a start state for
the simulation of the next scenario. In this context, an important FMI feature
is the capability to save and restore the internal FMU state on demand.
Unfortunately, the implementation of this feature is not mandatory and it is
available only within some commercial software. This motivates developing such
a feature for open-source CPS simulation environments. In this paper, we focus
on JModelica, an open-source modelling and simulation environment for CPSs
defined in the Modelica language. We describe how we have endowed JModelica
with our open-source implementation of the FMI 2.0 functions to save and
restore internal states of FMUs for ME. Furthermore, we present results
evaluating, through 934 benchmark models, correctness and efficiency of our
extended JModelica. Our results show that simulation-based V&amp;V is, on average,
22 times faster with our get/set functionality than without it.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01344</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01344</id><submitter>Yuan Gao</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:52:52 GMT</date><size>1873kb</size><source_type>D</source_type></version><title>Random walk approximation for irreversible drift-diffusion process on
  manifold: ergodicity, unconditional stability and convergence</title><authors>Yuan Gao, Jian-Guo Liu</authors><categories>math.NA cs.NA physics.data-an</categories><comments>28 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Irreversible drift-diffusion processes are very common in biochemical
reactions. They have a non-equilibrium stationary state (invariant measure)
which does not satisfy detailed balance. For the corresponding Fokker-Planck
equation on a closed manifold, via Voronoi tessellation, we propose two upwind
finite volume schemes with or without the information of the invariant measure.
Both two schemes enjoy stochastic $Q$-matrix structures and can be decomposed
as a gradient flow part and a Hamiltonian flow part, which enable us to prove
unconditional stability, ergodicity and error estimates. Based on two upwind
schemes, several numerical examples - including sampling accelerated by a
mixture flow, image transformations and simulations for stochastic model of
chaotic system - are conducted. These two structure-preserving schemes also
give a natural random walk approximation for a generic irreversible
drift-diffusion process on a manifold. Thus they can be adapted to
manifold-related computations induced from high dimensional molecular dynamics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01345</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01345</id><submitter>Lili Chen</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:53:39 GMT</date><size>551kb</size><source_type>D</source_type></version><title>Decision Transformer: Reinforcement Learning via Sequence Modeling</title><authors>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover,
  Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch</authors><categories>cs.LG cs.AI</categories><comments>First two authors contributed equally. Last two authors advised
  equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01350</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01350</id><submitter>Joao Marques-Silva</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:55:41 GMT</date><size>47kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 08:15:16 GMT</date><size>47kb</size></version><title>On Efficiently Explaining Graph-Based Classifiers</title><authors>Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, Joao Marques-Silva</authors><categories>cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent work has shown that not only decision trees (DTs) may not be
interpretable but also proposed a polynomial-time algorithm for computing one
PI-explanation of a DT. This paper shows that for a wide range of classifiers,
globally referred to as decision graphs, and which include decision trees and
binary decision diagrams, but also their multi-valued variants, there exist
polynomial-time algorithms for computing one PI-explanation. In addition, the
paper also proposes a polynomial-time algorithm for computing one contrastive
explanation. These novel algorithms build on explanation graphs (XpG's). XpG's
denote a graph representation that enables both theoretical and practically
efficient computation of explanations for decision graphs. Furthermore, the
paper pro- poses a practically efficient solution for the enumeration of
explanations, and studies the complexity of deciding whether a given feature is
included in some explanation. For the concrete case of decision trees, the
paper shows that the set of all contrastive explanations can be enumerated in
polynomial time. Finally, the experimental results validate the practical
applicability of the algorithms proposed in the paper on a wide range of
publicly available benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01351</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01351</id><submitter>Weiyi Xie</submitter><version version="v1"><date>Tue, 1 Jun 2021 11:24:48 GMT</date><size>5868kb</size><source_type>D</source_type></version><title>Deep Clustering Activation Maps for Emphysema Subtyping</title><authors>Weiyi Xie, Colin Jacobs, Bram van Ginneken</authors><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a deep learning clustering method that exploits dense features
from a segmentation network for emphysema subtyping from computed tomography
(CT) scans. Using dense features enables high-resolution visualization of image
regions corresponding to the cluster assignment via dense clustering activation
maps (dCAMs). This approach provides model interpretability. We evaluated
clustering results on 500 subjects from the COPDGenestudy, where radiologists
manually annotated emphysema sub-types according to their visual CT assessment.
We achieved a 43% unsupervised clustering accuracy, outperforming our baseline
at 41% and yielding results comparable to supervised classification at 45%. The
proposed method also offers a better cluster formation than the baseline,
achieving0.54 in silhouette coefficient and 0.55 in David-Bouldin scores.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01352</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01352</id><submitter>Ahmed Qureshi</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:56:27 GMT</date><size>25159kb</size><source_type>D</source_type></version><title>NeRP: Neural Rearrangement Planning for Unknown Objects</title><authors>Ahmed H. Qureshi, Arsalan Mousavian, Chris Paxton, Michael C. Yip, and
  Dieter Fox</authors><categories>cs.RO cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots will be expected to manipulate a wide variety of objects in complex
and arbitrary ways as they become more widely used in human environments. As
such, the rearrangement of objects has been noted to be an important benchmark
for AI capabilities in recent years. We propose NeRP (Neural Rearrangement
Planning), a deep learning based approach for multi-step neural object
rearrangement planning which works with never-before-seen objects, that is
trained on simulation data, and generalizes to the real world. We compare NeRP
to several naive and model-based baselines, demonstrating that our approach is
measurably better and can efficiently arrange unseen objects in fewer steps and
with less planning time. Finally, we demonstrate it on several challenging
rearrangement problems in the real world.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01354</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01354</id><submitter>Swarnadeep Saha</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:58:35 GMT</date><size>629kb</size><source_type>D</source_type></version><title>multiPRover: Generating Multiple Proofs for Improved Interpretability in
  Rule Reasoning</title><authors>Swarnadeep Saha, Prateek Yadav, Mohit Bansal</authors><categories>cs.CL cs.AI cs.LG cs.LO</categories><comments>NAACL 2021 (16 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on a type of linguistic formal reasoning where the goal is to reason
over explicit knowledge in the form of natural language facts and rules (Clark
et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such
reasoning by answering a question and also generating a proof graph that
explains the answer. However, compositional reasoning is not always unique and
there may be multiple ways of reaching the correct answer. Thus, in our work,
we address a new and challenging problem of generating multiple proof graphs
for reasoning over natural language rule-bases. Each proof provides a different
rationale for the answer, thereby improving the interpretability of such
reasoning systems. In order to jointly learn from all proof graphs and exploit
the correlations between multiple proofs for a question, we pose this task as a
set generation problem over structured output spaces where each proof is
represented as a directed graph. We propose two variants of a proof-set
generation model, multiPRover. Our first model, Multilabel-multiPRover,
generates a set of proofs via multi-label classification and implicit
conditioning between the proofs; while the second model, Iterative-multiPRover,
generates proofs iteratively by explicitly conditioning on the previously
generated proofs. Experiments on multiple synthetic, zero-shot, and
human-paraphrased datasets reveal that both multiPRover models significantly
outperform PRover on datasets containing multiple gold proofs.
Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios
where all examples have single correct proofs. It also generalizes better to
questions requiring higher depths of reasoning where multiple proofs are more
frequent. Our code and models are publicly available at
https://github.com/swarnaHub/multiPRover
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01355</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01355</id><submitter>Weihua Zhou</submitter><version version="v1"><date>Tue, 1 Jun 2021 15:49:31 GMT</date><size>690kb</size></version><title>A method using deep learning to discover new predictors of CRT response
  from mechanical dyssynchrony on gated SPECT MPI</title><authors>Zhuo He, Xinwei Zhang, Chen Zhao, Zhiyong Qian, Yao Wang, Xiaofeng
  Hou, Jiangang Zou, Weihua Zhou</authors><categories>physics.med-ph cs.AI</categories><comments>10 pages, 3 figures, will submit to journal of nuclear cardiology</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Background. Studies have shown that the conventional left ventricular
mechanical dyssynchrony (LVMD) parameters have their own statistical
limitations. The purpose of this study is to extract new LVMD parameters from
the phase analysis of gated SPECT MPI by deep learning to help CRT patient
selection. Methods. One hundred and three patients who underwent rest gated
SPECT MPI were enrolled in this study. CRT response was defined as a decrease
in left ventricular end-systolic volume (LVESV) &gt;= 15% at 6 +- 1 month follow
up. Autoencoder (AE), an unsupervised deep learning method, was trained by the
raw LV systolic phase polar maps to extract new LVMD parameters, called
AE-based LVMD parameters. Correlation analysis was used to explain the
relationships between new parameters with conventional LVMD parameters.
Univariate and multivariate analyses were used to establish a multivariate
model for predicting CRT response. Results. Complete data were obtained in 102
patients, 44.1% of them were classified as CRT responders. AE-based LVMD
parameter was significant in the univariate (OR 1.24, 95% CI 1.07 - 1.44, P =
0.006) and multivariate analyses (OR 1.03, 95% CI 1.01 - 1.06, P = 0.006).
Moreover, it had incremental value over PSD (AUC 0.72 vs. 0.63, LH 8.06, P =
0.005) and PBW (AUC 0.72 vs. 0.64, LH 7.87, P = 0.005), combined with
significant clinic characteristics, including LVEF and gender. Conclusions. The
new LVMD parameters extracted by autoencoder from the baseline gated SPECT MPI
has the potential to improve the prediction of CRT response.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01357</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01357</id><submitter>Valentin De Bortoli</submitter><version version="v1"><date>Tue, 1 Jun 2021 17:34:27 GMT</date><size>10112kb</size><source_type>D</source_type></version><title>Diffusion Schr\&quot;odinger Bridge with Applications to Score-Based
  Generative Modeling</title><authors>Valentin De Bortoli, James Thornton, Jeremy Heng, Arnaud Doucet</authors><categories>stat.ML cs.LG math.PR</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Progressively applying Gaussian noise transforms complex data distributions
to approximately Gaussian. Reversing this dynamic defines a generative model.
When the forward noising process is given by a Stochastic Differential Equation
(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the
associated reverse-time SDE may be estimated using score-matching. A limitation
of this approach is that the forward-time SDE must be run for a sufficiently
long time for the final distribution to be approximately Gaussian. In contrast,
solving the Schr\&quot;odinger Bridge problem (SB), i.e. an entropy-regularized
optimal transport problem on path spaces, yields diffusions which generate
samples from the data distribution in finite time. We present Diffusion SB
(DSB), an original approximation of the Iterative Proportional Fitting (IPF)
procedure to solve the SB problem, and provide theoretical analysis along with
generative modeling experiments. The first DSB iteration recovers the
methodology proposed by Song et al. (2021), with the flexibility of using
shorter time intervals, as subsequent DSB iterations reduce the discrepancy
between the final-time marginal of the forward (resp. backward) SDE with
respect to the prior (resp. data) distribution. Beyond generative modeling, DSB
offers a widely applicable computational optimal transport tool as the
continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,
2013).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01364</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01364</id><submitter>Jong-Chyi Su</submitter><version version="v1"><date>Wed, 2 Jun 2021 17:59:41 GMT</date><size>532kb</size><source_type>D</source_type></version><title>The Semi-Supervised iNaturalist Challenge at the FGVC8 Workshop</title><authors>Jong-Chyi Su and Subhransu Maji</authors><categories>cs.CV cs.LG</categories><comments>Tech report for Semi-iNat 2021 challenge, competition page:
  https://github.com/cvl-umass/semi-inat-2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semi-iNat is a challenging dataset for semi-supervised classification with a
long-tailed distribution of classes, fine-grained categories, and domain shifts
between labeled and unlabeled data. This dataset is behind the second iteration
of the semi-supervised recognition challenge to be held at the FGVC8 workshop
at CVPR 2021. Different from the previous one, this dataset (i) includes images
of species from different kingdoms in the natural taxonomy, (ii) is at a larger
scale --- with 810 in-class and 1629 out-of-class species for a total of 330k
images, and (iii) does not provide in/out-of-class labels, but provides coarse
taxonomic labels (kingdom and phylum) for the unlabeled images. This document
describes baseline results and the details of the dataset which is available
here: \url{https://github.com/cvl-umass/semi-inat-2021}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01367</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01367</id><submitter>David Coimbra</submitter><version version="v1"><date>Tue, 1 Jun 2021 21:18:23 GMT</date><size>444kb</size><source_type>D</source_type></version><title>On using distributed representations of source code for the detection of
  C security vulnerabilities</title><authors>David Coimbra, Sofia Reis, Rui Abreu, Corina P\u{a}s\u{a}reanu, Hakan
  Erdogmus</authors><categories>cs.CR cs.AI cs.LG cs.PL cs.SE</categories><comments>Submitted to DX 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents an evaluation of the code representation model Code2vec
when trained on the task of detecting security vulnerabilities in C source
code. We leverage the open-source library astminer to extract path-contexts
from the abstract syntax trees of a corpus of labeled C functions. Code2vec is
trained on the resulting path-contexts with the task of classifying a function
as vulnerable or non-vulnerable. Using the CodeXGLUE benchmark, we show that
the accuracy of Code2vec for this task is comparable to simple
transformer-based methods such as pre-trained RoBERTa, and outperforms more
naive NLP-based methods. We achieved an accuracy of 61.43% while maintaining
low computational requirements relative to larger models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01370</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01370</id><submitter>Muhammad Usman</submitter><version version="v1"><date>Wed, 2 Jun 2021 08:27:12 GMT</date><size>1631kb</size><source_type>D</source_type></version><title>q-RBFNN:A Quantum Calculus-based RBF Neural Network</title><authors>Syed Saiq Hussain, Muhammad Usman, Taha Hasan Masood Siddique, Imran
  Naseem, Roberto Togneri, Mohammed Bennamoun</authors><categories>cs.LG math.QA</categories><comments>Article is under review. This is a preprint version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research a novel stochastic gradient descent based learning approach
for the radial basis function neural networks (RBFNN) is proposed. The proposed
method is based on the q-gradient which is also known as Jackson derivative. In
contrast to the conventional gradient, which finds the tangent, the q-gradient
finds the secant of the function and takes larger steps towards the optimal
solution. The proposed $q$-RBFNN is analyzed for its convergence performance in
the context of least square algorithm. In particular, a closed form expression
of the Wiener solution is obtained, and stability bounds of the learning rate
(step-size) is derived. The analytical results are validated through computer
simulation. Additionally, we propose an adaptive technique for the time-varying
$q$-parameter to improve convergence speed with no trade-offs in the steady
state performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01382</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01382</id><submitter>Matthias C. Caro</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:00:04 GMT</date><size>38kb</size></version><title>Undecidability of Learnability</title><authors>Matthias C. Caro</authors><categories>cs.CC cs.LG math.LO</categories><comments>21 pages (main body) + 7 pages (reference and appendix); 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning researchers and practitioners steadily enlarge the multitude
of successful learning models. They achieve this through in-depth theoretical
analyses and experiential heuristics. However, there is no known
general-purpose procedure for rigorously evaluating whether newly proposed
models indeed successfully learn from data. We show that such a procedure
cannot exist. For PAC binary classification, uniform and universal online
learning, and exact learning through teacher-learner interactions, learnability
is in general undecidable, both in the sense of independence of the axioms in a
formal system and in the sense of uncomputability. Our proofs proceed via
computable constructions of function classes that encode the consistency
problem for formal systems and the halting problem for Turing machines into
complexity measures that characterize learnability. Our work shows that
undecidability appears in the theoretical foundations of machine learning:
There is no one-size-fits-all algorithm for deciding whether a machine learning
model can be successful. We cannot in general automatize the process of
assessing new learning models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01388</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01388</id><submitter>Thomas Hubregtsen</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:00:10 GMT</date><size>29kb</size></version><title>Single-component gradient rules for variational quantum algorithms</title><authors>Thomas Hubregtsen, Frederik Wilde, Shozab Qasim, Jens Eisert</authors><categories>quant-ph cs.LG</categories><comments>9 pages, 0 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many near-term quantum computing algorithms are conceived as variational
quantum algorithms, in which parameterized quantum circuits are optimized in a
hybrid quantum-classical setup. Examples are variational quantum eigensolvers,
quantum approximate optimization algorithms as well as various algorithms in
the context of quantum-assisted machine learning. A common bottleneck of any
such algorithm is constituted by the optimization of the variational
parameters. A popular set of optimization methods work on the estimate of the
gradient, obtained by means of circuit evaluations. We will refer to the way in
which one can combine these circuit evaluations as gradient rules. This work
provides a comprehensive picture of the family of gradient rules that vary
parameters of quantum gates individually. The most prominent known members of
this family are the parameter shift rule and the finite differences method. To
unite this family, we propose a generalized parameter shift rule that expresses
all members of the aforementioned family as special cases, and discuss how all
of these can be seen as providing access to a linear combination of exact
first- and second-order derivatives. We further prove that a parameter shift
rule with one non-shifted evaluation and only one shifted circuit evaluation
can not exist does not exist, and introduce a novel perspective for approaching
new gradient rules.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01399</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01399</id><submitter>J. Walker Orr</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:04:53 GMT</date><size>138kb</size><source_type>D</source_type></version><title>Automatic Assessment of the Design Quality of Python Programs with
  Personalized Feedback</title><authors>J. Walker Orr, Nathaniel Russell</authors><categories>cs.SE cs.AI cs.LG</categories><msc-class>68T99</msc-class><acm-class>K.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The assessment of program functionality can generally be accomplished with
straight-forward unit tests. However, assessing the design quality of a program
is a much more difficult and nuanced problem. Design quality is an important
consideration since it affects the readability and maintainability of programs.
Assessing design quality and giving personalized feedback is very time
consuming task for instructors and teaching assistants. This limits the scale
of giving personalized feedback to small class settings. Further, design
quality is nuanced and is difficult to concisely express as a set of rules. For
these reasons, we propose a neural network model to both automatically assess
the design of a program and provide personalized feedback to guide students on
how to make corrections. The model's effectiveness is evaluated on a corpus of
student programs written in Python. The model has an accuracy rate from 83.67%
to 94.27%, depending on the dataset, when predicting design scores as compared
to historical instructor assessment. Finally, we present a study where students
tried to improve the design of their programs based on the personalized
feedback produced by the model. Students who participated in the study improved
their program design scores by 19.58%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01400</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01400</id><submitter>Mari Ganesh Kumar</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:08:27 GMT</date><size>605kb</size><source_type>D</source_type></version><title>Dual Script E2E framework for Multilingual and Code-Switching ASR</title><authors>Mari Ganesh Kumar, Jom Kuriakose, Anand Thyagachandran, Arun Kumar A,
  Ashish Seth, Lodagala Durga Prasad, Saish Jaiswal, Anusha Prakash, Hema
  Murthy</authors><categories>eess.AS cs.LG cs.SD</categories><comments>Accepted for publication at Interspeech 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  India is home to multiple languages, and training automatic speech
recognition (ASR) systems for languages is challenging. Over time, each
language has adopted words from other languages, such as English, leading to
code-mixing. Most Indian languages also have their own unique scripts, which
poses a major limitation in training multilingual and code-switching ASR
systems.
  Inspired by results in text-to-speech synthesis, in this work, we use an
in-house rule-based phoneme-level common label set (CLS) representation to
train multilingual and code-switching ASR for Indian languages. We propose two
end-to-end (E2E) ASR systems. In the first system, the E2E model is trained on
the CLS representation, and we use a novel data-driven back-end to recover the
native language script. In the second system, we propose a modification to the
E2E model, wherein the CLS representation and the native language characters
are used simultaneously for training. We show our results on the multilingual
and code-switching tasks of the Indic ASR Challenge 2021. Our best results
achieve 6% and 5% improvement (approx) in word error rate over the baseline
system for the multilingual and code-switching tasks, respectively, on the
challenge development data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01401</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01401</id><submitter>Peng Gao</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:09:11 GMT</date><size>12015kb</size><source_type>D</source_type></version><title>Container: Context Aggregation Network</title><authors>Peng Gao, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, Aniruddha
  Kembhavi</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) are ubiquitous in computer vision, with
a myriad of effective and efficient variations. Recently, Transformers --
originally introduced in natural language processing -- have been increasingly
adopted in computer vision. While early adopters continue to employ CNN
backbones, the latest networks are end-to-end CNN-free Transformer solutions. A
recent surprising finding shows that a simple MLP based solution without any
traditional convolutional or Transformer components can produce effective
visual representations. While CNNs, Transformers and MLP-Mixers may be
considered as completely disparate architectures, we provide a unified view
showing that they are in fact special cases of a more general method to
aggregate spatial context in a neural network stack. We present the \model
(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head
context aggregation that can exploit long-range interactions \emph{a la}
Transformers while still exploiting the inductive bias of the local convolution
operation leading to faster convergence speeds, often seen in CNNs. In contrast
to Transformer-based methods that do not scale well to downstream tasks that
rely on larger input image resolutions, our efficient network, named
\modellight, can be employed in object detection and instance segmentation
networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive
detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large
improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50
backbone with a comparable compute and parameter size. Our method also achieves
promising results on self-supervised learning compared to DeiT on the DINO
framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01404</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01404</id><submitter>Jongwook Choi</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:12:26 GMT</date><size>6127kb</size><source_type>D</source_type></version><title>Variational Empowerment as Representation Learning for Goal-Based
  Reinforcement Learning</title><authors>Jongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, Shixiang
  Shane Gu</authors><categories>cs.LG cs.AI</categories><comments>Accepted at International Conference on Machine Learning (ICML) 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to reach goal states and learning diverse skills through mutual
information (MI) maximization have been proposed as principled frameworks for
self-supervised reinforcement learning, allowing agents to acquire broadly
applicable multitask policies with minimal reward engineering. Starting from a
simple observation that the standard goal-conditioned RL (GCRL) is encapsulated
by the optimization objective of variational empowerment, we discuss how GCRL
and MI-based RL can be generalized into a single family of methods, which we
name variational GCRL (VGCRL), interpreting variational MI maximization, or
variational empowerment, as representation learning methods that acquire
functionally-aware state representations for goal reaching. This novel
perspective allows us to: (1) derive simple but unexplored variants of GCRL to
study how adding small representation capacity can already expand its
capabilities; (2) investigate how discriminator function capacity and
smoothness determine the quality of discovered skills, or latent goals, through
modifying latent dimensionality and applying spectral normalization; (3) adapt
techniques such as hindsight experience replay (HER) from GCRL to MI-based RL;
and lastly, (4) propose a novel evaluation metric, named latent goal reaching
(LGR), for comparing empowerment algorithms with different choices of latent
dimensionality and discriminator parameterization. Through principled
mathematical derivations and careful experimental studies, our work lays a
novel foundation from which to evaluate, analyze, and develop representation
learning techniques in goal-based RL.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01410</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01410</id><submitter>Prasanna Sattigeri</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:29:04 GMT</date><size>693kb</size><source_type>D</source_type></version><title>Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and
  Communicating the Uncertainty of AI</title><authors>Soumya Ghosh, Q. Vera Liao, Karthikeyan Natesan Ramamurthy, Jiri
  Navratil, Prasanna Sattigeri, Kush R. Varshney, Yunfeng Zhang</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an open source Python toolkit named Uncertainty
Quantification 360 (UQ360) for the uncertainty quantification of AI models. The
goal of this toolkit is twofold: firstly, to provide a broad range of
capabilities to streamline, and hopefully foster the common practices of
quantifying, evaluating, improving, and communicating uncertainty in the AI
application development lifecycle; secondly, to disseminate the latest research
and educational materials for uncertainty quantification in machine learning,
and encourage further exploration of its utility and connections to other
pillars of trustworthy AI such as fairness and explainability. Beyond the
Python package (\url{https://github.com/IBM/UQ360}), we have developed an
interactive experience (\url{http://uq360.mybluemix.net}) and guidance
materials as educational tools to aid researchers and developers in producing
and communicating high-quality uncertainties in an effective manner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01413</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01413</id><submitter>Anthony Caterini</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:30:39 GMT</date><size>361kb</size><source_type>D</source_type></version><title>Rectangular Flows for Manifold Learning</title><authors>Anthony L. Caterini, Gabriel Loaiza-Ganem, Geoff Pleiss, John P.
  Cunningham</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normalizing flows are invertible neural networks with tractable
change-of-volume terms, which allows optimization of their parameters to be
efficiently performed via maximum likelihood. However, data of interest is
typically assumed to live in some (often unknown) low-dimensional manifold
embedded in high-dimensional ambient space. The result is a modelling mismatch
since -- by construction -- the invertibility requirement implies
high-dimensional support of the learned distribution. Injective flows, mapping
from low- to high-dimensional space, aim to fix this discrepancy by learning
distributions on manifolds, but the resulting volume-change term becomes more
challenging to evaluate. Current approaches either avoid computing this term
entirely using various heuristics, or assume the manifold is known beforehand
and therefore are not widely applicable. Instead, we propose two methods to
tractably calculate the gradient of this term with respect to the parameters of
the model, relying on careful use of automatic differentiation and techniques
from numerical linear algebra. Both approaches perform end-to-end nonlinear
manifold learning and density estimation for data projected onto this manifold.
We study the trade-offs between our proposed methods, empirically verify that
we outperform approaches ignoring the volume-change term by more accurately
learning manifolds and the corresponding distributions on them, and show
promising results on out-of-distribution detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01414</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01414</id><submitter>Daniel Gratzer</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:32:21 GMT</date><size>51kb</size><source_type>D</source_type></version><title>Normalization for multimodal type theory</title><authors>Daniel Gratzer</authors><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the conversion problem for multimodal type theory (MTT) by
characterizing the normal forms of the type theory and proving normalization.
Normalization follows from a novel adaptation of Sterling's Synthetic Tait
Computability which generalizes the framework to accommodate a type theory with
modalities and multiple modes. As a corollary of our main result, we reduce the
conversion problem of MTT to the conversion problem of its mode theory and show
the injectivity of type constructors. Finally, we conclude that MTT enjoys
decidable type-checking when instantiated with a decidable mode theory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01415</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01415</id><submitter>Wen-Chin Huang</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:41:03 GMT</date><size>383kb</size><source_type>D</source_type></version><title>A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker
  Identity in Dysarthric Voice Conversion</title><authors>Wen-Chin Huang, Kazuhiro Kobayashi, Yu-Huai Peng, Ching-Feng Liu, Yu
  Tsao, Hsin-Min Wang, Tomoki Toda</authors><categories>cs.SD cs.CL eess.AS</categories><comments>Accepted to Interspeech 2021. 5 pages, 3 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a new paradigm for maintaining speaker identity in dysarthric
voice conversion (DVC). The poor quality of dysarthric speech can be greatly
improved by statistical VC, but as the normal speech utterances of a dysarthria
patient are nearly impossible to collect, previous work failed to recover the
individuality of the patient. In light of this, we suggest a novel, two-stage
approach for DVC, which is highly flexible in that no normal speech of the
patient is required. First, a powerful parallel sequence-to-sequence model
converts the input dysarthric speech into a normal speech of a reference
speaker as an intermediate product, and a nonparallel, frame-wise VC model
realized with a variational autoencoder then converts the speaker identity of
the reference speech back to that of the patient while assumed to be capable of
preserving the enhanced quality. We investigate several design options.
Experimental evaluation results demonstrate the potential of our approach to
improving the quality of the dysarthric speech while maintaining the speaker
identity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01416</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01416</id><submitter>Absalom Ezugwu</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:41:56 GMT</date><size>2448kb</size></version><title>Ebola Optimization Search Algorithm (EOSA): A new metaheuristic
  algorithm based on the propagation model of Ebola virus disease</title><authors>Olaide N. Oyelade and Absalom E. Ezugwu</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Ebola virus and the disease in effect tend to randomly move individuals
in the population around susceptible, infected, quarantined, hospitalized,
recovered, and dead sub-population. Motivated by the effectiveness in
propagating the disease through the virus, a new bio-inspired and
population-based optimization algorithm is proposed. This paper presents a
novel metaheuristic algorithm named Ebola optimization algorithm (EOSA). To
correctly achieve this, this study models the propagation mechanism of the
Ebola virus disease, emphasising all consistent states of the propagation. The
model was further represented using a mathematical model based on first-order
differential equations. After that, the combined propagation and mathematical
models were adapted for developing the new metaheuristic algorithm. To evaluate
the proposed method's performance and capability compared with other
optimization methods, the underlying propagation and mathematical models were
first investigated to determine how they successfully simulate the EVD.
Furthermore, two sets of benchmark functions consisting of forty-seven (47)
classical and over thirty (30) constrained IEEE CEC-2017 benchmark functions
are investigated numerically. The results indicate that the performance of the
proposed algorithm is competitive with other state-of-the-art optimization
methods based on scalability analysis, convergence analysis, and sensitivity
analysis. Extensive simulation results indicate that the EOSA outperforms other
state-of-the-art popular metaheuristic optimization algorithms such as the
Particle Swarm Optimization Algorithm (PSO), Genetic Algorithm (GA), and
Artificial Bee Colony Algorithm (ABC) on some shifted, high dimensional and
large search range problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01420</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01420</id><submitter>Amin Karbasi</submitter><version version="v1"><date>Wed, 2 Jun 2021 18:51:57 GMT</date><size>3635kb</size><source_type>D</source_type></version><title>Parallelizing Thompson Sampling</title><authors>Amin Karbasi, Vahab Mirrokni, Mohammad Shadravan</authors><categories>cs.LG cs.AI math.OC stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  How can we make use of information parallelism in online decision making
problems while efficiently balancing the exploration-exploitation trade-off? In
this paper, we introduce a batch Thompson Sampling framework for two canonical
online decision making problems, namely, stochastic multi-arm bandit and linear
contextual bandit with finitely many arms. Over a time horizon $T$, our
\textit{batch} Thompson Sampling policy achieves the same (asymptotic) regret
bound of a fully sequential one while carrying out only $O(\log T)$ batch
queries. To achieve this exponential reduction, i.e., reducing the number of
interactions from $T$ to $O(\log T)$, our batch policy dynamically determines
the duration of each batch in order to balance the exploration-exploitation
trade-off. We also demonstrate experimentally that dynamic batch allocation
dramatically outperforms natural baselines such as static batch allocations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01423</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01423</id><submitter>Henry Kvinge</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:07:27 GMT</date><size>1333kb</size><source_type>D</source_type></version><title>One Representation to Rule Them All: Identifying Out-of-Support Examples
  in Few-shot Learning with Generic Representations</title><authors>Henry Kvinge, Scott Howland, Nico Courts, Lauren A. Phillips, John
  Buckheit, Zachary New, Elliott Skomski, Jung H. Lee, Sandeep Tiwari, Jessica
  Hibler, Courtney D. Corley, Nathan O. Hodas</authors><categories>cs.LG cs.AI cs.CV math.MG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of few-shot learning has made remarkable strides in developing
powerful models that can operate in the small data regime. Nearly all of these
methods assume every unlabeled instance encountered will belong to a handful of
known classes for which one has examples. This can be problematic for
real-world use cases where one routinely finds 'none-of-the-above' examples. In
this paper we describe this challenge of identifying what we term
'out-of-support' (OOS) examples. We describe how this problem is subtly
different from out-of-distribution detection and describe a new method of
identifying OOS examples within the Prototypical Networks framework using a
fixed point which we call the generic representation. We show that our method
outperforms other existing approaches in the literature as well as other
approaches that we propose in this paper. Finally, we investigate how the use
of such a generic point affects the geometry of a model's feature space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01424</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01424</id><submitter>Marcella Cornia</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:11:21 GMT</date><size>245kb</size><source_type>D</source_type></version><title>Learning to Select: A Fully Attentive Approach for Novel Object
  Captioning</title><authors>Marco Cagrandi, Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi,
  Rita Cucchiara</authors><categories>cs.CV cs.CL</categories><comments>ICMR 2021</comments><doi>10.1145/3460426.3463587</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image captioning models have lately shown impressive results when applied to
standard datasets. Switching to real-life scenarios, however, constitutes a
challenge due to the larger variety of visual concepts which are not covered in
existing training sets. For this reason, novel object captioning (NOC) has
recently emerged as a paradigm to test captioning models on objects which are
unseen during the training phase. In this paper, we present a novel approach
for NOC that learns to select the most relevant objects of an image, regardless
of their adherence to the training set, and to constrain the generative process
of a language model accordingly. Our architecture is fully-attentive and
end-to-end trainable, also when incorporating constraints. We perform
experiments on the held-out COCO dataset, where we demonstrate improvements
over the state of the art, both in terms of adaptability to novel objects and
caption quality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01425</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01425</id><submitter>Enmao Diao</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:12:03 GMT</date><size>3972kb</size><source_type>D</source_type></version><title>Gradient Assisted Learning</title><authors>Enmao Diao, Jie Ding, Vahid Tarokh</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed settings, collaborations between different entities, such as
financial institutions, medical centers, and retail markets, are crucial to
providing improved service and performance. However, the underlying entities
may have little interest in sharing their private data, proprietary models, and
objective functions. These privacy requirements have created new challenges for
collaboration. In this work, we propose Gradient Assisted Learning (GAL), a new
method for various entities to assist each other in supervised learning tasks
without sharing data, models, and objective functions. In this framework, all
participants collaboratively optimize the aggregate of local loss functions,
and each participant autonomously builds its own model by iteratively fitting
the gradients of the objective function. Experimental studies demonstrate that
Gradient Assisted Learning can achieve performance close to centralized
learning when all data, models, and objective functions are fully disclosed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01428</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01428</id><submitter>Zenglin Shi</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:15:34 GMT</date><size>31564kb</size><source_type>D</source_type></version><title>Unsharp Mask Guided Filtering</title><authors>Zenglin Shi, Yunlu Chen, Efstratios Gavves, Pascal Mettes, and Cees
  G.M. Snoek</authors><categories>cs.CV</categories><comments>IEEE Transactions on Image Processing, 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The goal of this paper is guided image filtering, which emphasizes the
importance of structure transfer during filtering by means of an additional
guidance image. Where classical guided filters transfer structures using
hand-designed functions, recent guided filters have been considerably advanced
through parametric learning of deep networks. The state-of-the-art leverages
deep networks to estimate the two core coefficients of the guided filter. In
this work, we posit that simultaneously estimating both coefficients is
suboptimal, resulting in halo artifacts and structure inconsistencies. Inspired
by unsharp masking, a classical technique for edge enhancement that requires
only a single coefficient, we propose a new and simplified formulation of the
guided filter. Our formulation enjoys a filtering prior from a low-pass filter
and enables explicit structure transfer by estimating a single coefficient.
Based on our proposed formulation, we introduce a successive guided filtering
network, which provides multiple filtering results from a single network,
allowing for a trade-off between accuracy and efficiency. Extensive ablations,
comparisons and analysis show the effectiveness and efficiency of our
formulation and network, resulting in state-of-the-art results across filtering
tasks like upsampling, denoising, and cross-modality filtering. Code is
available at \url{https://github.com/shizenglin/Unsharp-Mask-Guided-Filtering}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01429</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01429</id><submitter>Gabriel Peyr\'e</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:18:22 GMT</date><size>18719kb</size><source_type>D</source_type></version><title>Smooth Bilevel Programming for Sparse Regularization</title><authors>Clarice Poon and Gabriel Peyr\'e</authors><categories>stat.ML cs.LG math.OC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Iteratively reweighted least square (IRLS) is a popular approach to solve
sparsity-enforcing regression problems in machine learning. State of the art
approaches are more efficient but typically rely on specific coordinate pruning
schemes. In this work, we show how a surprisingly simple reparametrization of
IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is
able to achieve top performances on a wide range of sparsity (such as Lasso,
group Lasso and trace norm regularizations), regularization strength (including
hard constraints), and design matrices (ranging from correlated designs to
differential operators). Similarly to IRLS, our method only involves linear
systems resolutions, but in sharp contrast, corresponds to the minimization of
a smooth function. Despite being non-convex, we show that there is no spurious
minima and that saddle points are &quot;ridable&quot;, so that there always exists a
descent direction. We thus advocate for the use of a BFGS quasi-Newton solver,
which makes our approach simple, robust and efficient. We perform a numerical
benchmark of the convergence speed of our algorithm against state of the art
solvers for Lasso, group Lasso, trace norm and linearly constrained problems.
These results highlight the versatility of our approach, removing the need to
use different solvers depending on the specificity of the ML problem under
study.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01432</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01432</id><submitter>Enmao Diao</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:22:26 GMT</date><size>1592kb</size><source_type>D</source_type></version><title>SemiFL: Communication Efficient Semi-Supervised Federated Learning with
  Unlabeled Clients</title><authors>Enmao Diao, Jie Ding, Vahid Tarokh</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated Learning allows training machine learning models by using the
computation and private data resources of a large number of distributed clients
such as smartphones and IoT devices. Most existing works on Federated Learning
(FL) assume the clients have ground-truth labels. However, in many practical
scenarios, clients may be unable to label task-specific data, e.g., due to lack
of expertise. In this work, we consider a server that hosts a labeled dataset,
and wishes to leverage clients with unlabeled data for supervised learning. We
propose a new Federated Learning framework referred to as SemiFL in order to
address the problem of Semi-Supervised Federated Learning (SSFL). In SemiFL,
clients have completely unlabeled data, while the server has a small amount of
labeled data. SemiFL is communication efficient since it separates the training
of server-side supervised data and client-side unsupervised data. We
demonstrate various efficient strategies of SemiFL that enhance learning
performance. Extensive empirical evaluations demonstrate that our communication
efficient method can significantly improve the performance of a labeled server
with unlabeled clients. Moreover, we demonstrate that SemiFL can outperform
many existing FL results trained with fully supervised data, and perform
competitively with the state-of-the-art centralized Semi-Supervised Learning
(SSL) methods. For instance, in standard communication efficient scenarios, our
method can perform 93% accuracy on the CIFAR10 dataset with only 4000 labeled
samples at the server. Such accuracy is only 2% away from the result trained
from 50000 fully labeled data, and it improves about 30% upon existing SSFL
methods in the communication efficient setting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01434</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01434</id><submitter>Xihan Bian</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:31:27 GMT</date><size>1038kb</size><source_type>D</source_type></version><title>Robot in a China Shop: Using Reinforcement Learning for
  Location-Specific Navigation Behaviour</title><authors>Xihan Bian and Oscar Mendez and Simon Hadfield</authors><categories>cs.RO cs.LG</categories><comments>Published at ICRA 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots need to be able to work in multiple different environments. Even when
performing similar tasks, different behaviour should be deployed to best fit
the current environment. In this paper, We propose a new approach to
navigation, where it is treated as a multi-task learning problem. This enables
the robot to learn to behave differently in visual navigation tasks for
different environments while also learning shared expertise across
environments. We evaluated our approach in both simulated environments as well
as real-world data. Our method allows our system to converge with a 26%
reduction in training time, while also increasing accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01435</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01435</id><submitter>Tarik A. Rashid</submitter><version version="v1"><date>Fri, 14 May 2021 20:04:04 GMT</date><size>1355kb</size></version><title>Real-Time COVID-19 Diagnosis from X-Ray Images Using Deep CNN and
  Extreme Learning Machines Stabilized by Chimp Optimization Algorithm</title><authors>Hu Tianqing, Mohammad Khishe, Mokhtar Mohammadi, Gholam-Reza Parvizi,
  Sarkhel H. Taher Karim, Tarik A. Rashid</authors><categories>eess.IV cs.NE</categories><comments>17 pages. arXiv admin note: text overlap with arXiv:2105.14192</comments><journal-ref>Biomedical Signal Processing and Control, 2021</journal-ref><doi>10.1016/j.bspc.2021.102764</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Real-time detection of COVID-19 using radiological images has gained priority
due to the increasing demand for fast diagnosis of COVID-19 cases. This paper
introduces a novel two-phase approach for classifying chest X-ray images. Deep
Learning (DL) methods fail to cover these aspects since training and
fine-tuning the model's parameters consume much time. In this approach, the
first phase comes to train a deep CNN working as a feature extractor, and the
second phase comes to use Extreme Learning Machines (ELMs) for real-time
detection. The main drawback of ELMs is to meet the need of a large number of
hidden-layer nodes to gain a reliable and accurate detector in applying image
processing since the detective performance remarkably depends on the setting of
initial weights and biases. Therefore, this paper uses Chimp Optimization
Algorithm (ChOA) to improve results and increase the reliability of the network
while maintaining real-time capability. The designed detector is to be
benchmarked on the COVID-Xray-5k and COVIDetectioNet datasets, and the results
are verified by comparing it with the classic DCNN, Genetic Algorithm optimized
ELM (GA-ELM), Cuckoo Search optimized ELM (CS-ELM), and Whale Optimization
Algorithm optimized ELM (WOA-ELM). The proposed approach outperforms other
comparative benchmarks with 98.25% and 99.11% as ultimate accuracy on the
COVID-Xray-5k and COVIDetectioNet datasets, respectively, and it led relative
error to reduce as the amount of 1.75% and 1.01% as compared to a convolutional
CNN. More importantly, the time needed for training deep ChOA-ELM is only
0.9474 milliseconds, and the overall testing time for 3100 images is 2.937
seconds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01438</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01438</id><submitter>Sohini Roy</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:44:27 GMT</date><size>1331kb</size></version><title>A Leader-Follower Game Theoretic Approach to Arrest Cascading Failure in
  Smart Grid</title><authors>Sohini Roy, Arunabha Sen</authors><categories>cs.GT cs.NI</categories><comments>This paper is accepted for publication in American Journal of Science
  and Engineering. arXiv admin note: text overlap with arXiv:2101.08896</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Smart Grid System (SGS) is a joint network comprising the power and the
communication network. In this paper, the underlying
intra-and-interdependencies between entities for a given SGS is captured using
a dependency model called Modified Implicative Interdependency Model (MIIM)
[1]. Given an integer K, the K-contingency list problem gives the list of
K-most critical entities, failure of which maximizes the network damage at the
current time. The problem being NP complete [2] and owing to the higher running
time of the given Integer Linear Programming (ILP) based solution [3], a much
faster heuristic solution to generate an event driven self-updating
K-contingency list [4] is also given in this paper. Based on the contingency
lists obtained from both the solutions, this paper proposes an adaptive entity
hardening technique based on a leader-follower game theoretic approach that
arrests the cascading failure of entities in the SGS after an initial failure
of entities. The validation of the work is done by comparing the contingency
lists using both types of solutions, obtained for different K values using the
MIIM model on a smart grid of IEEE 14-Bus system with that obtained by
simulating the smart grid using a co-simulation system formed by MATPOWER and
Java Network Simulator (JNS). The K-contingency list obtained for a smart grid
of IEEE 14-Bus system also indicate that the network damage predicted by both
the ILP based solution and heuristic solution using MIIM are more realistic
compared to that obtained using another dependency model called Implicative
Interdependency Model (IIM) [2]. Advantage of using the MIIM based heuristic
solution is also shown in this paper when larger SGS of IEEE 118-Bus is
considered. Finally, it is shown how the adaptive hardening helps in improving
the network performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01439</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01439</id><submitter>Eduardo Perez-Pellitero</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:45:16 GMT</date><size>9559kb</size><source_type>D</source_type></version><title>NTIRE 2021 Challenge on High Dynamic Range Imaging: Dataset, Methods and
  Results</title><authors>Eduardo P\'erez-Pellitero and Sibi Catley-Chandar and Ale\v{s}
  Leonardis and Radu Timofte</authors><categories>cs.CV eess.IV</categories><comments>To appear in CVPRW 2021 (NTIRE)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews the first challenge on high-dynamic range (HDR) imaging
that was part of the New Trends in Image Restoration and Enhancement (NTIRE)
workshop, held in conjunction with CVPR 2021. This manuscript focuses on the
newly introduced dataset, the proposed methods and their results. The challenge
aims at estimating a HDR image from one or multiple respective low-dynamic
range (LDR) observations, which might suffer from under- or over-exposed
regions and different sources of noise. The challenge is composed by two
tracks: In Track 1 only a single LDR image is provided as input, whereas in
Track 2 three differently-exposed LDR images with inter-frame motion are
available. In both tracks, the ultimate goal is to achieve the best objective
HDR reconstruction in terms of PSNR with respect to a ground-truth image,
evaluated both directly and with a canonical tonemapping operation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01440</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01440</id><submitter>Biagio La Rosa</submitter><version version="v1"><date>Tue, 1 Jun 2021 07:24:19 GMT</date><size>7694kb</size><source_type>D</source_type></version><title>Memory Wrap: a Data-Efficient and Interpretable Extension to Image
  Classification Models</title><authors>Biagio La Rosa, Roberto Capobianco and Daniele Nardi</authors><categories>cs.LG</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to their black-box and data-hungry nature, deep learning techniques are
not yet widely adopted for real-world applications in critical domains, like
healthcare and justice. This paper presents Memory Wrap, a plug-and-play
extension to any image classification model. Memory Wrap improves both
data-efficiency and model interpretability, adopting a content-attention
mechanism between the input and some memories of past training samples. We show
that Memory Wrap outperforms standard classifiers when it learns from a limited
set of data, and it reaches comparable performance when it learns from the full
dataset. We discuss how its structure and content-attention mechanisms make
predictions interpretable, compared to standard classifiers. To this end, we
both show a method to build explanations by examples and counterfactuals, based
on the memory content, and how to exploit them to get insights about its
decision process. We test our approach on image classification tasks using
several architectures on three different datasets, namely CIFAR10, SVHN, and
CINIC10.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01441</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01441</id><submitter>Sabri Pllana</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:45:53 GMT</date><size>2652kb</size><source_type>D</source_type></version><title>Optimization of Heterogeneous Systems with AI Planning Heuristics and
  Machine Learning: A Performance and Energy Aware Approach</title><authors>Suejb Memeti and Sabri Pllana</authors><categories>cs.SE cs.AI cs.LG</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Heterogeneous computing systems provide high performance and energy
efficiency. However, to optimally utilize such systems, solutions that
distribute the work across host CPUs and accelerating devices are needed. In
this paper, we present a performance and energy aware approach that combines AI
planning heuristics for parameter space exploration with a machine learning
model for performance and energy evaluation to determine a near-optimal system
configuration. For data-parallel applications our approach determines a
near-optimal host-device distribution of work, number of processing units
required and the corresponding scheduling strategy. We evaluate our approach
for various heterogeneous systems accelerated with GPU or the Intel Xeon Phi.
The experimental results demonstrate that our approach finds a near-optimal
system configuration by evaluating only about 7% of reasonable configurations.
Furthermore, the performance per Joule estimation of system configurations
using our machine learning model is more than 1000x faster compared to the
system evaluation by program execution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01444</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01444</id><submitter>Joshua Feinglass</submitter><version version="v1"><date>Wed, 2 Jun 2021 19:58:20 GMT</date><size>7560kb</size></version><title>SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption
  Evaluation via Typicality Analysis</title><authors>Joshua Feinglass and Yezhou Yang</authors><categories>cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The open-ended nature of visual captioning makes it a challenging area for
evaluation. The majority of proposed models rely on specialized training to
improve human-correlation, resulting in limited adoption, generalizability, and
explainabilty. We introduce &quot;typicality&quot;, a new formulation of evaluation
rooted in information theory, which is uniquely suited for problems lacking a
definite ground truth. Typicality serves as our framework to develop a novel
semantic comparison, SPARCS, as well as referenceless fluency evaluation
metrics. Over the course of our analysis, two separate dimensions of fluency
naturally emerge: style, captured by metric SPURTS, and grammar, captured in
the form of grammatical outlier penalties. Through extensive experiments and
ablation studies on benchmark datasets, we show how these decomposed dimensions
of semantics and fluency provide greater system-level insight into captioner
differences. Our proposed metrics along with their combination, SMURF, achieve
state-of-the-art correlation with human judgment when compared with other
rule-based evaluation metrics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01446</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01446</id><submitter>Ashkan Ebadi</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:07:08 GMT</date><size>1983kb</size></version><title>Gender-Specific Patterns in the Artificial Intelligence Scientific
  Ecosystem</title><authors>Anahita Hajibabaei, Andrea Schiffauerova, Ashkan Ebadi</authors><categories>cs.SI</categories><comments>23 pages, 10 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gender disparity in science is one of the most focused debating points among
authorities and the scientific community. Over the last few decades, numerous
initiatives have endeavored to accelerate gender equity in academia and
research society. However, despite the ongoing efforts, gaps persist across the
world, and more measures need to be taken. Using social network analysis,
natural language processing, and machine learning, in this study, we
comprehensively analyzed gender-specific patterns in the highly
interdisciplinary and evolving field of artificial intelligence for the period
of 2000-2019. Our findings suggest an overall increasing rate of mixed-gender
collaborations. From the observed gender-specific collaborative patterns, the
existence of disciplinary homophily at both dyadic and team levels is
confirmed. However, a higher preference was observed for female researchers to
form homophilous collaborative links. Our core-periphery analysis indicated a
significant positive association between having diverse collaboration and
scientific performance and experience. We found evidence in support of
expecting the rise of new female superstar researchers in the artificial
intelligence field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01450</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01450</id><submitter>Ji Won Park</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:17:31 GMT</date><size>1155kb</size><source_type>D</source_type></version><title>Inferring Black Hole Properties from Astronomical Multivariate Time
  Series with Bayesian Attentive Neural Processes</title><authors>Ji Won Park, Ashley Villar, Yin Li, Yan-Fei Jiang, Shirley Ho, Joshua
  Yao-Yu Lin, Philip J. Marshall, Aaron Roodman</authors><categories>astro-ph.IM astro-ph.HE cs.LG</categories><comments>6 pages, 4 figures, 1 table, written for non-astronomers, submitted
  to the ICML 2021 Time Series and Uncertainty and Robustness in Deep Learning
  Workshops. Comments welcome!</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Among the most extreme objects in the Universe, active galactic nuclei (AGN)
are luminous centers of galaxies where a black hole feeds on surrounding
matter. The variability patterns of the light emitted by an AGN contain
information about the physical properties of the underlying black hole.
Upcoming telescopes will observe over 100 million AGN in multiple broadband
wavelengths, yielding a large sample of multivariate time series with long gaps
and irregular sampling. We present a method that reconstructs the AGN time
series and simultaneously infers the posterior probability density distribution
(PDF) over the physical quantities of the black hole, including its mass and
luminosity. We apply this method to a simulated dataset of 11,000 AGN and
report precision and accuracy of 0.4 dex and 0.3 dex in the inferred black hole
mass. This work is the first to address probabilistic time series
reconstruction and parameter inference for AGN in an end-to-end fashion.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01451</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01451</id><submitter>Richard Diehl Martinez</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:19:57 GMT</date><size>6061kb</size><source_type>D</source_type></version><title>Attention-based Contextual Language Model Adaptation for Speech
  Recognition</title><authors>Richard Diehl Martinez, Scott Novotney, Ivan Bulyko, Ariya Rastrow,
  Andreas Stolcke, Ankur Gandhe</authors><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Language modeling (LM) for automatic speech recognition (ASR) does not
usually incorporate utterance level contextual information. For some domains
like voice assistants, however, additional context, such as the time at which
an utterance was spoken, provides a rich input signal. We introduce an
attention mechanism for training neural speech recognition language models on
both text and non-linguistic contextual data. When applied to a large
de-identified dataset of utterances collected by a popular voice assistant
platform, our method reduces perplexity by 7.0% relative over a standard LM
that does not incorporate contextual information. When evaluated on utterances
extracted from the long tail of the dataset, our method improves perplexity by
9.0% relative over a standard LM and by over 2.8% relative when compared to a
state-of-the-art model for contextual LM.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01452</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01452</id><submitter>Steffen Eger</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:21:03 GMT</date><size>645kb</size><source_type>D</source_type></version><title>BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively
  Inspired Orthographic Adversarial Attacks</title><authors>Yannik Keller, Jan Mackensen, Steffen Eger</authors><categories>cs.CL cs.LG</categories><comments>Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial attacks expose important blind spots of deep learning systems.
While word- and sentence-level attack scenarios mostly deal with finding
semantic paraphrases of the input that fool NLP models, character-level attacks
typically insert typos into the input stream. It is commonly thought that these
are easier to defend via spelling correction modules. In this work, we show
that both a standard spellchecker and the approach of Pruthi et al. (2019),
which trains to defend against insertions, deletions and swaps, perform poorly
on the character-level benchmark recently proposed in Eger and Benz (2020)
which includes more challenging attacks such as visual and phonetic
perturbations and missing word segmentations. In contrast, we show that an
untrained iterative approach which combines context-independent character-level
information with context-dependent information from BERT's masked language
modeling can perform on par with human crowd-workers from Amazon Mechanical
Turk (AMT) supervised via 3-shot learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01459</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01459</id><submitter>Hamza Anwar</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:33:34 GMT</date><size>7507kb</size><source_type>D</source_type></version><title>Comprehensive Energy Footprint Benchmarking Algorithm for Electrified
  Powertrains</title><authors>Hamza Anwar, Aashrith Vishwanath, Apurva Chunodkar, Qadeer Ahmed</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Autonomy and electrification in automotive control systems have made
modern-day powertrains one of the most complex cyber-physical systems. This
paper presents a benchmark algorithm to quantify the performance of complex
automotive systems exhibiting mechanical, electrical, and thermal interactions
at various time-scales. Traditionally Dynamic Programming has been used for
benchmarking the performance, however, it fails to deliver results for system
with higher number of states and control lever due to curse of dimensionality.
We propose &quot;PS3&quot;, a three-step algorithm for mixed-integer nonlinear optimal
control problems with application to powertrain energy management. PS3 uses
pseudo-spectral collocation theory for highly accurate modeling of dynamics.
Based on the validated powertrain component models, we have addressed
simultaneous optimization of electrical (SOC), vehicular (eco-driving) and
thermal (after-treatment and battery temperatures) dynamics along with an
integer (gear and engine on/off) control and its corresponding (dwell-time)
constraints. PS3 is used to solve such large-scale powertrain problems having
fast and slow dynamic states, discontinuous behaviors, non-differentiable and
linearly interpolated 1-D and 2-D maps, as well as combinatorial constraints.
Five case study powertrain control problems are given to benchmark the accuracy
and computational effort against Dynamic Programming. Our analysis shows that
this algorithm does not scale computational burden as Dynamic Programming does,
and can handle highly complex interactions that occur in modern-day
powertrains, without compromising nonlinear and complex plant modeling.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01463</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01463</id><submitter>Hang Le</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:51:42 GMT</date><size>277kb</size><source_type>D</source_type></version><title>Lightweight Adapter Tuning for Multilingual Speech Translation</title><authors>Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, Laurent
  Besacier</authors><categories>cs.CL</categories><comments>Accepted at ACL-IJCNLP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Adapter modules were recently introduced as an efficient alternative to
fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters
of a model and injecting lightweight modules between layers, resulting in the
addition of only a small number of task-specific trainable parameters. While
adapter tuning was investigated for multilingual neural machine translation,
this paper proposes a comprehensive analysis of adapters for multilingual
speech translation (ST). Starting from different pre-trained models (a
multilingual ST trained on parallel data or a multilingual BART (mBART) trained
on non-parallel multilingual data), we show that adapters can be used to: (a)
efficiently specialize ST to specific language pairs with a low extra cost in
terms of parameters, and (b) transfer from an automatic speech recognition
(ASR) task and an mBART pre-trained model to a multilingual ST task.
Experiments show that adapter tuning offer competitive results to full
fine-tuning, while being much more parameter-efficient.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01465</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01465</id><submitter>Jieyu Zhao</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:57:58 GMT</date><size>12349kb</size><source_type>D</source_type></version><title>Ethical-Advice Taker: Do Language Models Understand Natural Language
  Interventions?</title><authors>Jieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and
  Kai-Wei Chang</authors><categories>cs.CL cs.AI cs.LG</categories><comments>9 pages, Findings of ACL-IJCNLP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible to use natural language to intervene in a model's behavior and
alter its prediction in a desired way? We investigate the effectiveness of
natural language interventions for reading-comprehension systems, studying this
in the context of social stereotypes. Specifically, we propose a new language
understanding task, Linguistic Ethical Interventions (LEI), where the goal is
to amend a question-answering (QA) model's unethical behavior by communicating
context-specific principles of ethics and equity to it. To this end, we build
upon recent methods for quantifying a system's social stereotypes, augmenting
them with different kinds of ethical interventions and the desired model
behavior under such interventions. Our zero-shot evaluation finds that even
today's powerful neural language models are extremely poor ethical-advice
takers, that is, they respond surprisingly little to ethical interventions even
though these interventions are stated as simple sentences. Few-shot learning
improves model behavior but remains far from the desired outcome, especially
when evaluated for various types of generalization. Our new task thus poses a
novel language understanding challenge for the community.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01467</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01467</id><submitter>Kamil Akhmetov</submitter><version version="v1"><date>Wed, 2 Jun 2021 20:58:24 GMT</date><size>15171kb</size><source_type>D</source_type></version><title>Domain Adaptation for Facial Expression Classifier via Domain
  Discrimination and Gradient Reversal</title><authors>Kamil Akhmetov</authors><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bringing empathy to a computerized system could significantly improve the
quality of human-computer communications, as soon as machines would be able to
understand customer intentions and better serve their needs. According to
different studies (Literature Review), visual information is one of the most
important channels of human interaction and contains significant behavioral
signals, that may be captured from facial expressions. Therefore, it is
consistent and natural that the research in the field of Facial Expression
Recognition (FER) has acquired increased interest over the past decade due to
having diverse application area including health-care, sociology, psychology,
driver-safety, virtual reality, cognitive sciences, security, entertainment,
marketing, etc. We propose a new architecture for the task of FER and examine
the impact of domain discrimination loss regularization on the learning
process. With regard to observations, including both classical training
conditions and unsupervised domain adaptation scenarios, important aspects of
the considered domain adaptation approach integration are traced. The results
may serve as a foundation for further research in the field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01474</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01474</id><submitter>Chengchun Shi</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:18:59 GMT</date><size>692kb</size><source_type>D</source_type></version><title>Testing Directed Acyclic Graph via Structural, Supervised and Generative
  Adversarial Learning</title><authors>Chengchun Shi, Yunzhe Zhou and Lexin Li</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a new hypothesis testing method for directed
acyclic graph (DAG). While there is a rich class of DAG estimation methods,
there is a relative paucity of DAG inference solutions. Moreover, the existing
methods often impose some specific model structures such as linear models or
additive models, and assume independent data observations. Our proposed test
instead allows the associations among the random variables to be nonlinear and
the data to be time-dependent. We build the test based on some highly flexible
neural networks learners. We establish the asymptotic guarantees of the test,
while allowing either the number of subjects or the number of time points for
each subject to diverge to infinity. We demonstrate the efficacy of the test
through simulations and a brain connectivity network analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01478</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01478</id><submitter>Fajri Koto</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:28:01 GMT</date><size>6864kb</size><source_type>D</source_type></version><title>Evaluating the Efficacy of Summarization Evaluation across Languages</title><authors>Fajri Koto and Jey Han Lau and Timothy Baldwin</authors><categories>cs.CL</categories><comments>Findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While automatic summarization evaluation methods developed for English are
routinely applied to other languages, this is the first attempt to
systematically quantify their panlinguistic efficacy. We take a summarization
corpus for eight different languages, and manually annotate generated summaries
for focus (precision) and coverage (recall). Based on this, we evaluate 19
summarization evaluation metrics, and find that using multilingual BERT within
BERTScore performs well across all languages, at a level above that for
English.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01481</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01481</id><submitter>Anne Marie Stupinski</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:35:53 GMT</date><size>16485kb</size><source_type>D</source_type></version><title>Quantifying language changes surrounding mental health on Twitter</title><authors>Anne Marie Stupinski, Thayer Alshaabi, Michael V. Arnold, Jane Lydia
  Adams, Joshua R. Minot, Matthew Price, Peter Sheridan Dodds, Christopher M.
  Danforth</authors><categories>physics.soc-ph cs.CL cs.SI</categories><comments>12 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mental health challenges are thought to afflict around 10% of the global
population each year, with many going untreated due to stigma and limited
access to services. Here, we explore trends in words and phrases related to
mental health through a collection of 1- , 2-, and 3-grams parsed from a data
stream of roughly 10% of all English tweets since 2012. We examine temporal
dynamics of mental health language, finding that the popularity of the phrase
'mental health' increased by nearly two orders of magnitude between 2012 and
2018. We observe that mentions of 'mental health' spike annually and reliably
due to mental health awareness campaigns, as well as unpredictably in response
to mass shootings, celebrities dying by suicide, and popular fictional stories
portraying suicide. We find that the level of positivity of messages containing
'mental health', while stable through the growth period, has declined recently.
Finally, we use the ratio of original tweets to retweets to quantify the
fraction of appearances of mental health language due to social amplification.
Since 2015, mentions of mental health have become increasingly due to retweets,
suggesting that stigma associated with discussion of mental health on Twitter
has diminished with time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01482</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01482</id><submitter>Christina Delimitrou</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:37:50 GMT</date><size>2987kb</size><source_type>D</source_type></version><title>Dagger: Accelerating RPCs in Cloud Microservices Through Tightly-Coupled
  Reconfigurable NICs</title><authors>Nikita Lazarev and Shaojie Xiang and Neil Adit and Zhiru Zhang and
  Christina Delimitrou</authors><categories>cs.AR cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ongoing shift of cloud services from monolithic designs to microservices
creates high demand for efficient and high performance datacenter networking
stacks, optimized for fine-grained workloads. Commodity networking systems
based on software stacks and peripheral NICs introduce high overheads when it
comes to delivering small messages.
  We present Dagger, a hardware acceleration fabric for cloud RPCs based on
FPGAs, where the accelerator is closely-coupled with the host processor over a
configurable memory interconnect. The three key design principle of Dagger are:
(1) offloading the entire RPC stack to an FPGA-based NIC, (2) leveraging memory
interconnects instead of PCIe buses as the interface with the host CPU, and (3)
making the acceleration fabric reconfigurable, so it can accommodate the
diverse needs of microservices. We show that the combination of these
principles significantly improves the efficiency and performance of cloud RPC
systems while preserving their generality. Dagger achieves 1.3-3.8x higher
per-core RPC throughput compared to both highly-optimized software stacks, and
systems using specialized RDMA adapters. It also scales up to 84 Mrps with 8
threads on 4 CPU cores, while maintaining state-of-the-art us-scale tail
latency. We also demonstrate that large third-party applications, like
memcached and MICA KVS, can be easily ported on Dagger with minimal changes to
their codebase, bringing their median and tail KVS access latency down to 2.8 -
3.5us and 5.4 - 7.8us, respectively. Finally, we show that Dagger is beneficial
for multi-tier end-to-end microservices with different threading models by
evaluating it using an 8-tier application implementing a flight check-in
service.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01483</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01483</id><submitter>Mazin Hnewa</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:50:25 GMT</date><size>16046kb</size><source_type>D</source_type></version><title>Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection</title><authors>Mazin Hnewa and Hayder Radha</authors><categories>cs.CV</categories><comments>accepted in 2021 IEEE International Conference on Image Processing
  (ICIP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The area of domain adaptation has been instrumental in addressing the domain
shift problem encountered by many applications. This problem arises due to the
difference between the distributions of source data used for training in
comparison with target data used during realistic testing scenarios. In this
paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)
framework that employs multiple domain adaptation paths and corresponding
domain classifiers at different scales of the recently introduced YOLOv4 object
detector to generate domain-invariant features. We train and test our proposed
method using popular datasets. Our experiments show significant improvements in
object detection performance when training YOLOv4 using the proposed MS-DAYOLO
and when tested on target data representing challenging weather conditions for
autonomous driving applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01484</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01484</id><submitter>Robert Harper</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:50:56 GMT</date><size>11kb</size></version><title>An Equational Logical Framework for Type Theories</title><authors>Robert Harper</authors><categories>math.LO cs.LO</categories><acm-class>F.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of intuitionistic type theories may be presented as equational
theories within a logical framework. This method was formulated by Per
Martin-L\&quot;{o}f in the mid-1980's and further developed by Uemura, who used it
to prove an initiality result for a class of models. Herein is presented a
logical framework for type theories that includes an extensional equality type
so that a type theory may be given by a signature of constants. The framework
is illustrated by a number of examples of type-theoretic concepts, including
identity and equality types, and a hierarchy of universes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01485</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01485</id><submitter>Chengliang Tang</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:52:05 GMT</date><size>173kb</size><source_type>D</source_type></version><title>Weakly Supervised Learning Creates a Fusion of Modeling Cultures</title><authors>Chengliang Tang, Gan Yuan, Tian Zheng</authors><categories>stat.ML cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past two decades have witnessed the great success of the algorithmic
modeling framework advocated by Breiman et al. (2001). Nevertheless, the
excellent prediction performance of these black-box models rely heavily on the
availability of strong supervision, i.e. a large set of accurate and exact
ground-truth labels. In practice, strong supervision can be unavailable or
expensive, which calls for modeling techniques under weak supervision. In this
comment, we summarize the key concepts in weakly supervised learning and
discuss some recent developments in the field. Using algorithmic modeling alone
under a weak supervision might lead to unstable and misleading results. A
promising direction would be integrating the data modeling culture into such a
framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01487</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01487</id><submitter>Aditya Kusupati</submitter><version version="v1"><date>Wed, 2 Jun 2021 21:57:52 GMT</date><size>16105kb</size><source_type>D</source_type></version><title>LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes</title><authors>Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani,
  Jae Sung Park, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi</authors><categories>cs.LG cs.CV</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning binary representations of instances and classes is a classical
problem with several high potential applications. In modern settings, the
compression of high-dimensional neural representations to low-dimensional
binary codes is a challenging task and often require large bit-codes to be
accurate. In this work, we propose a novel method for Learning Low-dimensional
binary Codes (LLC) for instances as well as classes. Our method does not
require any side-information, like annotated attributes or label meta-data, and
learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The
learnt codes are super-efficient while still ensuring nearly optimal
classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the
learnt codes capture intrinsically important features in the data, by
discovering an intuitive taxonomy over classes. We further quantitatively
measure the quality of our codes by applying it to the efficient image
retrieval as well as out-of-distribution (OOD) detection problems. For
ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit
HashNet using only 10 bits and also are as accurate as 10 dimensional real
representations. Finally, our learnt binary codes can perform OOD detection,
out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune
its threshold, while we require none. Code and pre-trained models are available
at https://github.com/RAIVNLab/LLC.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01488</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01488</id><submitter>Tanner Fiez</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:03:36 GMT</date><size>4644kb</size><source_type>D</source_type></version><title>Minimax Optimization with Smooth Algorithmic Adversaries</title><authors>Tanner Fiez, Chi Jin, Praneeth Netrapalli, Lillian J. Ratliff</authors><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers minimax optimization $\min_x \max_y f(x, y)$ in the
challenging setting where $f$ can be both nonconvex in $x$ and nonconcave in
$y$. Though such optimization problems arise in many machine learning paradigms
including training generative adversarial networks (GANs) and adversarially
robust models, many fundamental issues remain in theory, such as the absence of
efficiently computable optimality notions, and cyclic or diverging behavior of
existing algorithms. Our framework sprouts from the practical consideration
that under a computational budget, the max-player can not fully maximize
$f(x,\cdot)$ since nonconcave maximization is NP-hard in general. So, we
propose a new algorithm for the min-player to play against smooth algorithms
deployed by the adversary (i.e., the max-player) instead of against full
maximization. Our algorithm is guaranteed to make monotonic progress (thus
having no limit cycles), and to find an appropriate &quot;stationary point&quot; in a
polynomial number of iterations. Our framework covers practical settings where
the smooth algorithms deployed by the adversary are multi-step stochastic
gradient ascent, and its accelerated version. We further provide complementing
experiments that confirm our theoretical findings and demonstrate the
effectiveness of the proposed approach in practice.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01489</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01489</id><submitter>Ziyun Li</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:06:55 GMT</date><size>933kb</size><source_type>D</source_type></version><title>Not All Knowledge Is Created Equal</title><authors>Ziyun Li, Xinshao Wang, Haojin Yang, Di Hu, Neil M. Robertson, David
  A. Clifton, Christoph Meinel</authors><categories>cs.LG cs.AI cs.CV</categories><comments>Selective mutual knowledge distillation</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Mutual knowledge distillation (MKD) improves a model by distilling knowledge
from another model. However, not all knowledge is certain and correct,
especially under adverse conditions. For example, label noise usually leads to
less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge
misleads the learning rather than helps. This problem can be handled by two
aspects: (i) improving the reliability of a model where the knowledge is from
(i.e., knowledge source's reliability); (ii) selecting reliable knowledge for
distillation. In the literature, making a model more reliable is widely studied
while selective MKD receives little attention. Therefore, we focus on studying
selective MKD and highlight its importance in this work.
  Concretely, a generic MKD framework, Confident knowledge selection followed
by Mutual Distillation (CMD), is designed. The key component of CMD is a
generic knowledge selection formulation, making the selection threshold either
static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special
cases: zero knowledge and all knowledge, leading to a unified MKD framework. We
empirically find CMD-P performs better than CMD-S. The main reason is that a
model's knowledge upgrades and becomes confident as the training progresses.
  Extensive experiments are present to demonstrate the effectiveness of CMD and
thoroughly justify the design of CMD. For example, CMD-P obtains new
state-of-the-art results in robustness against label noise.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01490</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01490</id><submitter>Yuan Tian Miss</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:10:09 GMT</date><size>3718kb</size><source_type>D</source_type></version><title>What and How long: Prediction of Mobile App Engagement</title><authors>Yuan Tian, Ke Zhou, Dan Pelleg</authors><categories>cs.IR</categories><comments>38 pages, to appear in ACM Transactions on Information Systems 2021</comments><doi>10.1145/3464301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User engagement is crucial to the long-term success of a mobile app. Several
metrics, such as dwell time, have been used for measuring user engagement.
However, how to effectively predict user engagement in the context of mobile
apps is still an open research question. For example, do the mobile usage
contexts (e.g.,~time of day) in which users access mobile apps impact their
dwell time? Answers to such questions could help mobile operating system and
publishers to optimize advertising and service placement. In this paper, we
first conduct an empirical study for assessing how user characteristics,
temporal features, and the short/long-term contexts contribute to gains in
predicting users' app dwell time on the population level. The comprehensive
analysis is conducted on large app usage logs collected through a mobile
advertising company. The dataset covers more than 12K anonymous users and 1.3
million log events. Based on the analysis, we further investigate a novel
mobile app engagement prediction problem -- can we predict simultaneously what
app the user will use next and how long he/she will stay on that app? We
propose several strategies for this joint prediction problem and demonstrate
that our model can improve the performance significantly when compared with the
state-of-the-art baselines. Our work can help mobile system developers in
designing a better and more engagement-aware mobile app user experience.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01491</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01491</id><submitter>Christine Herlihy</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:12:39 GMT</date><size>5234kb</size><source_type>D</source_type></version><title>MedNLI Is Not Immune: Natural Language Inference Artifacts in the
  Clinical Domain</title><authors>Christine Herlihy and Rachel Rudinger</authors><categories>cs.CL cs.LG</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Crowdworker-constructed natural language inference (NLI) datasets have been
found to contain statistical artifacts associated with the annotation process
that allow hypothesis-only classifiers to achieve better-than-random
performance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018).
We investigate whether MedNLI, a physician-annotated dataset with premises
extracted from clinical notes, contains such artifacts (Romanov and Shivade,
2018). We find that entailed hypotheses contain generic versions of specific
concepts in the premise, as well as modifiers related to responsiveness,
duration, and probability. Neutral hypotheses feature conditions and behaviors
that co-occur with, or cause, the condition(s) in the premise. Contradiction
hypotheses feature explicit negation of the premise and implicit negation via
assertion of good health. Adversarial filtering demonstrates that performance
degrades when evaluated on the difficult subset. We provide partition
information and recommendations for alternative dataset construction strategies
for knowledge-intensive domains.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01492</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01492</id><submitter>Isaac Grosof</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:18:43 GMT</date><size>707kb</size><source_type>D</source_type></version><title>Nudge: Stochastically Improving upon FCFS</title><authors>Isaac Grosof, Kunhe Yang, Ziv Scully, Mor Harchol-Balter</authors><categories>cs.PF math.PR</categories><comments>29 pages, 4 figures. To appear in SIGMETRICS 2021</comments><doi>10.1145/3410220.3460102</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The First-Come First-Served (FCFS) scheduling policy is the most popular
scheduling algorithm used in practice. Furthermore, its usage is theoretically
validated: for light-tailed job size distributions, FCFS has weakly optimal
asymptotic tail of response time. But what if we don't just care about the
asymptotic tail? What if we also care about the 99th percentile of response
time, or the fraction of jobs that complete in under one second? Is FCFS still
best? Outside of the asymptotic regime, only loose bounds on the tail of FCFS
are known, and optimality is completely open.
  In this paper, we introduce a new policy, Nudge, which is the first policy to
provably stochastically improve upon FCFS. We prove that Nudge simultaneously
improves upon FCFS at every point along the tail, for light-tailed job size
distributions. As a result, Nudge outperforms FCFS for every moment and every
percentile of response time. Moreover, Nudge provides a multiplicative
improvement over FCFS in the asymptotic tail. This resolves a long-standing
open problem by showing that, counter to previous conjecture, FCFS is not
strongly asymptotically optimal.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01494</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01494</id><submitter>Shujian Zhang</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:22:52 GMT</date><size>404kb</size><source_type>D</source_type></version><title>Knowing More About Questions Can Help: Improving Calibration in Question
  Answering</title><authors>Shujian Zhang, Chengyue Gong, Eunsol Choi</authors><categories>cs.CL cs.AI</categories><comments>ACL 2021 (finding)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study calibration in question answering, estimating whether model
correctly predicts answer for each question. Unlike prior work which mainly
rely on the model's confidence score, our calibrator incorporates information
about the input example (e.g., question and the evidence context). Together
with data augmentation via back translation, our simple approach achieves 5-10%
gains in calibration accuracy on reading comprehension benchmarks. Furthermore,
we present the first calibration study in the open retrieval setting, comparing
the calibration accuracy of retrieval-based span prediction models and answer
generation models. Here again, our approach shows consistent gains over
calibrators relying on the model confidence. Our simple and efficient
calibrator can be easily adapted to many tasks and model architectures, showing
robust gains in all settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01496</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01496</id><submitter>Robin Weishaupt</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:33:09 GMT</date><size>43kb</size></version><title>Stability of Special Graph Classes</title><authors>Robin Weishaupt and J\&quot;org Rothe</authors><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frei et al. [6] showed that the problem to decide whether a graph is stable
with respect to some graph parameter under adding or removing either edges or
vertices is $\Theta_2^{\text{P}}$-complete. They studied the common graph
parameters $\alpha$ (independence number), $\beta$ (vertex cover number),
$\omega$ (clique number), and $\chi$ (chromatic number) for certain variants of
the stability problem. We follow their approach and provide a large number of
polynomial-time algorithms solving these problems for special graph classes,
namely for graphs without edges, complete graphs, paths, trees, forests,
bipartite graphs, and co-graphs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01497</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01497</id><submitter>Piyush Sharma</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:48:06 GMT</date><size>730kb</size></version><title>IoT Solutions with Multi-Sensor Fusion and Signal-Image Encoding for
  Secure Data Transfer and Decision Making</title><authors>Piyush K. Sharma, Mark Dennison, Adrienne Raglin</authors><categories>eess.SP cs.LG</categories><comments>Advances in Mass Data Analysis of Images and Signals in Artificial
  Intelligence and Pattern Recognition 15th International Conference, MDA 2020
  Amsterdam, The Netherlands, July 20-21, 2020.
  http://www.ibai-publishing.org/html/proceedings_2020/pdf/proceedings_book_MDA-AI&amp;PR_2020.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment of Internet of Things (IoT) devices and Data Fusion techniques
have gained popularity in public and government domains. This usually requires
capturing and consolidating data from multiple sources. As datasets do not
necessarily originate from identical sensors, fused data typically results in a
complex data problem. Because military is investigating how heterogeneous IoT
devices can aid processes and tasks, we investigate a multi-sensor approach.
Moreover, we propose a signal to image encoding approach to transform
information (signal) to integrate (fuse) data from IoT wearable devices to an
image which is invertible and easier to visualize supporting decision making.
Furthermore, we investigate the challenge of enabling an intelligent
identification and detection operation and demonstrate the feasibility of the
proposed Deep Learning and Anomaly Detection models that can support future
application that utilizes hand gesture data from wearable devices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01498</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01498</id><submitter>Caroline Wormell</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:56:18 GMT</date><size>555kb</size><source_type>D</source_type></version><title>Efficient computation of statistical properties of intermittent dynamics</title><authors>Caroline L. Wormell</authors><categories>math.DS cs.NA math.NA nlin.CD</categories><comments>Extract from PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intermittent maps of the interval are simple and widely-studied models for
chaos with slow mixing rates, but have been notoriously resistant to numerical
study. In this paper we present an effective framework to compute many ergodic
properties of these systems, in particular invariant measures and mean return
times. The framework combines three ingredients that each harness the smooth
structure of these systems' induced maps: Abel functions to compute the action
of the induced maps, Euler-Maclaurin summation to compute the pointwise action
of their transfer operators, and Chebyshev Galerkin discretisations to compute
the spectral data of the transfer operators. The combination of these
techniques allows one to obtain exponential convergence of estimates for
polynomially growing computational outlay, independent of the order of the
map's neutral fixed point. This enables numerical exploration of intermittent
dynamics in all parameter regimes, including in the infinite ergodic regime.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01499</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01499</id><submitter>Mina Khan</submitter><version version="v1"><date>Wed, 2 Jun 2021 22:58:47 GMT</date><size>31889kb</size><source_type>D</source_type></version><title>Personalizing Pre-trained Models</title><authors>Mina Khan, P Srivatsa, Advait Rane, Shriram Chenniappa, Asadali
  Hazariwala, and Pattie Maes</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-supervised or weakly supervised models trained on large-scale datasets
have shown sample-efficient transfer to diverse datasets in few-shot settings.
We consider how upstream pretrained models can be leveraged for downstream
few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP
PERsonalized) uses image representations from CLIP, a large-scale image
representation learning model trained using weak natural language supervision.
We developed a technique, called Multi-label Weight Imprinting (MWI), for
multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image
representations from CLIP. We evaluated CLIPPER on 10 single-label and 5
multi-label datasets. Our model shows robust and competitive performance, and
we set new benchmarks for few-shot, multi-label, and continual learning. Our
lightweight technique is also compute-efficient and enables privacy-preserving
applications as the data is not sent to the upstream model for fine-tuning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01501</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01501</id><submitter>Sahaana Suri</submitter><version version="v1"><date>Wed, 2 Jun 2021 23:02:26 GMT</date><size>1205kb</size><source_type>D</source_type></version><title>Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins</title><authors>Sahaana Suri, Ihab F. Ilyas, Christopher R\'e, Theodoros Rekatsinas</authors><categories>cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured data, or data that adheres to a pre-defined schema, can suffer
from fragmented context: information describing a single entity can be
scattered across multiple datasets or tables tailored for specific business
needs, with no explicit linking keys (e.g., primary key-foreign key
relationships or heuristic functions). Context enrichment, or rebuilding
fragmented context, using keyless joins is an implicit or explicit step in
machine learning (ML) pipelines over structured data sources. This process is
tedious, domain-specific, and lacks support in now-prevalent no-code ML systems
that let users create ML pipelines using just input data and high-level
configuration files. In response, we propose Ember, a system that abstracts and
automates keyless joins to generalize context enrichment. Our key insight is
that Ember can enable a general keyless join operator by constructing an index
populated with task-specific embeddings. Ember learns these embeddings by
leveraging Transformer-based representation learning techniques. We describe
our core architectural principles and operators when developing Ember, and
empirically demonstrate that Ember allows users to develop no-code pipelines
for five domains, including search, recommendation and question answering, and
can exceed alternatives by up to 39% recall, with as little as a single line
configuration change.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01503</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01503</id><submitter>Garrick Cabour</submitter><version version="v1"><date>Wed, 2 Jun 2021 23:17:29 GMT</date><size>603kb</size><source_type>D</source_type></version><title>Towards an Explanation Space to Align Humans and Explainable-AI Teamwork</title><authors>Garrick Cabour, Andr\'es Morales, \'Elise Ledoux, Samuel Bassetto</authors><categories>cs.AI cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Providing meaningful and actionable explanations to end-users is a
fundamental prerequisite for implementing explainable intelligent systems in
the real world. Explainability is a situated interaction between a user and the
AI system rather than being static design principles. The content of
explanations is context-dependent and must be defined by evidence about the
user and its context. This paper seeks to operationalize this concept by
proposing a formative architecture that defines the explanation space from a
user-inspired perspective. The architecture comprises five intertwined
components to outline explanation requirements for a task: (1) the end-users
mental models, (2) the end-users cognitive process, (3) the user interface, (4)
the human-explainer agent, and the (5) agent process. We first define each
component of the architecture. Then we present the Abstracted Explanation
Space, a modeling tool that aggregates the architecture's components to support
designers in systematically aligning explanations with the end-users work
practices, needs, and goals. It guides the specifications of what needs to be
explained (content - end-users mental model), why this explanation is necessary
(context - end-users cognitive process), to delimit how to explain it (format -
human-explainer agent and user interface), and when should the explanations be
given. We then exemplify the tool's use in an ongoing case study in the
aircraft maintenance domain. Finally, we discuss possible contributions of the
tool, known limitations/areas for improvement, and future work to be done.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01504</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01504</id><submitter>Paul McLachlan</submitter><version version="v1"><date>Wed, 2 Jun 2021 23:18:11 GMT</date><size>3723kb</size><source_type>D</source_type></version><title>DeepCompress: Efficient Point Cloud Geometry Compression</title><authors>Ryan Killea, Yun Li, Saeed Bastani, Paul McLachlan</authors><categories>cs.CV cs.GR cs.LG eess.IV</categories><comments>13 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Point clouds are a basic data type that is increasingly of interest as 3D
content becomes more ubiquitous. Applications using point clouds include
virtual, augmented, and mixed reality and autonomous driving. We propose a more
efficient deep learning-based encoder architecture for point clouds compression
that incorporates principles from established 3D object detection and image
compression architectures. Through an ablation study, we show that
incorporating the learned activation function from Computational Efficient
Neural Image Compression (CENIC) and designing more parameter-efficient
convolutional blocks yields dramatic gains in efficiency and performance. Our
proposed architecture incorporates Generalized Divisive Normalization
activations and propose a spatially separable InceptionV4-inspired block. We
then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized
Full Bodies dataset to evaluate our model's performance. Our proposed
modifications outperform the baseline approaches by a small margin in terms of
Bjontegard delta rate and PSNR values, yet reduces necessary encoder
convolution operations by 8 percent and reduces total encoder parameters by 20
percent. Our proposed architecture, when considered on its own, has a small
penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit
rate in Point to Plane Distance for the same peak signal-to-noise ratio.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01505</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01505</id><submitter>Peihao Zhu</submitter><version version="v1"><date>Wed, 2 Jun 2021 23:20:43 GMT</date><size>21551kb</size><source_type>D</source_type></version><title>Barbershop: GAN-based Image Compositing using Segmentation Masks</title><authors>Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka</authors><categories>cs.CV cs.GR</categories><comments>Project page: https://zpdesu.github.io/Barbershop/ Video:
  https://youtu.be/ZU-yrAvoJfQ</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Seamlessly blending features from multiple images is extremely challenging
because of complex relationships in lighting, geometry, and partial occlusion
which cause coupling between different parts of the image. Even though recent
work on GANs enables synthesis of realistic hair or faces, it remains difficult
to combine them into a single, coherent, and plausible image rather than a
disjointed set of image patches. We present a novel solution to image blending,
particularly for the problem of hairstyle transfer, based on GAN-inversion. We
propose a novel latent space for image blending which is better at preserving
detail and encoding spatial information, and propose a new GAN-embedding
algorithm which is able to slightly modify images to conform to a common
segmentation mask. Our novel representation enables the transfer of the visual
properties from multiple reference images including specific details such as
moles and wrinkles, and because we do image blending in a latent-space we are
able to synthesize images that are coherent. Our approach avoids blending
artifacts present in other approaches and finds a globally consistent image.
Our results demonstrate a significant improvement over the current state of the
art in a user study, with users preferring our blending solution over 95
percent of the time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01506</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01506</id><submitter>Matthew A. Wright</submitter><version version="v1"><date>Wed, 2 Jun 2021 23:24:06 GMT</date><size>39kb</size><source_type>D</source_type></version><title>Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel
  Machines</title><authors>Matthew A. Wright, Joseph E. Gonzalez</authors><categories>cs.LG stat.ML</categories><comments>Work in progress, comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite their ubiquity in core AI fields like natural language processing,
the mechanics of deep attention-based neural networks like the Transformer
model are not fully understood. In this article, we present a new perspective
towards understanding how Transformers work. In particular, we show that the
&quot;dot-product attention&quot; that is the core of the Transformer's operation can be
characterized as a kernel learning method on a pair of Banach spaces. In
particular, the Transformer's kernel is characterized as having an infinite
feature dimension. Along the way we consider an extension of the standard
kernel learning problem to a binary setting, where data come from two input
domains and a response is defined for every cross-domain pair. We prove a new
representer theorem for these binary kernel machines with non-Mercer
(indefinite, asymmetric) kernels (implying that the functions learned are
elements of reproducing kernel Banach spaces rather than Hilbert spaces), and
also prove a new universal approximation theorem showing that the Transformer
calculation can learn any binary non-Mercer reproducing kernel Banach space
pair. We experiment with new kernels in Transformers, and obtain results that
suggest the infinite dimensionality of the standard Transformer kernel is
partially responsible for its performance. This paper's results provide a new
theoretical understanding of a very important but poorly understood model in
modern machine~learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01513</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01513</id><submitter>Salman Ahmadi</submitter><version version="v1"><date>Thu, 3 Jun 2021 00:17:50 GMT</date><size>762kb</size></version><title>Granger Causality from Quantized Measurements</title><authors>Salman Ahmadi, Girish N. Nair, Erik Weyer</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  An approach is proposed for inferring Granger causality between jointly
stationary, Gaussian signals from quantized data. First, a necessary and
sufficient rank criterion for the equality of two conditional Gaussian
distributions is proved. Assuming a partial finite-order Markov property,
conditions are then derived under which Granger causality between them can be
reliably inferred from the second order moments of the quantized processes. A
necessary and sufficient condition is proposed for Granger causality inference
under binary quantization. Furthermore, sufficient conditions are introduced to
infer Granger causality between jointly Gaussian signals through measurements
quantized via non-uniform, uniform or high resolution quantizers. This approach
does not require the statistics of the underlying Gaussian signals to be
estimated, or a system model to be identified. No assumptions are made on the
identifiability of the jointly Gaussian random processes through the quantized
observations. The effectiveness of the proposed method is illustrated by
simulation results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01515</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01515</id><submitter>Apoorv Saxena</submitter><version version="v1"><date>Thu, 3 Jun 2021 00:45:07 GMT</date><size>11991kb</size><source_type>D</source_type></version><title>Question Answering Over Temporal Knowledge Graphs</title><authors>Apoorv Saxena, Soumen Chakrabarti and Partha Talukdar</authors><categories>cs.LG</categories><comments>ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by
providing temporal scopes (start and end times) on each edge in the KG. While
Question Answering over KG (KGQA) has received some attention from the research
community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored
area. Lack of broad coverage datasets has been another factor limiting progress
in this area. We address this challenge by presenting CRONQUESTIONS, the
largest known Temporal KGQA dataset, clearly stratified into buckets of
structural complexity. CRONQUESTIONS expands the only known previous dataset by
a factor of 340x. We find that various state-of-the-art KGQA methods fall far
short of the desired performance on this new dataset. In response, we also
propose CRONKGQA, a transformer-based solution that exploits recent advances in
Temporal KG embeddings, and achieves performance superior to all baselines,
with an increase of 120% in accuracy over the next best performing method.
Through extensive experiments, we give detailed insights into the workings of
CRONKGQA, as well as situations where significant further improvements appear
possible. In addition to the dataset, we have released our code as well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01516</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01516</id><submitter>Taisuke Kobayashi</submitter><version version="v1"><date>Thu, 3 Jun 2021 00:47:12 GMT</date><size>1856kb</size><source_type>D</source_type></version><title>Hyperbolically-Discounted Reinforcement Learning on Reward-Punishment
  Framework</title><authors>Taisuke Kobayashi</authors><categories>cs.LG</categories><comments>2 pages, 1 figure, presented as Paper Abstracts in ICDL-EPIROB2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new reinforcement learning with hyperbolic discounting.
Combining a new temporal difference error with the hyperbolic discounting in
recursive manner and reward-punishment framework, a new scheme to learn the
optimal policy is derived. In simulations, it is found that the proposal
outperforms the standard reinforcement learning, although the performance
depends on the design of reward and punishment. In addition, the averages of
discount factors w.r.t. reward and punishment are different from each other,
like a sign effect in animal behaviors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01518</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01518</id><submitter>Jiacheng Xu</submitter><version version="v1"><date>Thu, 3 Jun 2021 00:54:16 GMT</date><size>1103kb</size><source_type>D</source_type></version><title>Dissecting Generation Modes for Abstractive Summarization Models via
  Ablation and Attribution</title><authors>Jiacheng Xu and Greg Durrett</authors><categories>cs.CL</categories><comments>ACL 2021; 16 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the prominence of neural abstractive summarization models, we know
little about how they actually form summaries and how to understand where their
decisions come from. We propose a two-step method to interpret summarization
model decisions. We first analyze the model's behavior by ablating the full
model to categorize each decoder decision into one of several generation modes:
roughly, is the model behaving like a language model, is it relying heavily on
the input, or is it somewhere in between? After isolating decisions that do
depend on the input, we explore interpreting these decisions using several
different attribution methods. We compare these techniques based on their
ability to select content and reconstruct the model's predicted token from
perturbations of the input, thus revealing whether highlighted attributions are
truly important for the generation of the next token. While this machinery can
be broadly useful even beyond summarization, we specifically demonstrate its
capability to identify phrases the summarization model has memorized and
determine where in the training pipeline this memorization happened, as well as
study complex generation phenomena like sentence fusion on a per-instance
basis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01524</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01524</id><submitter>Robyn Brooks</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:11:36 GMT</date><size>1183kb</size><source_type>D</source_type></version><title>Combinatorial Conditions for Directed Collapsing</title><authors>Robin Belton, Robyn Brooks, Stefania Ebli, Lisbeth Fajstrup, Brittany
  Terese Fasy, Nicole Sanderson, and Elizabeth Vidaurre</authors><categories>math.AT cs.CG</categories><comments>22 pages, 11 figures</comments><msc-class>68Q85, 57Q10</msc-class><acm-class>D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this article is to study directed collapsibility of directed
Euclidean cubical complexes. One application of this is in the nontrivial task
of verifying the execution of concurrent programs. The classical definition of
collapsibility involves certain conditions on a pair of cubes of the complex.
The direction of the space can be taken into account by requiring that the past
links of vertices remain homotopy equivalent after collapsing. We call this
type of collapse a link-preserving directed collapse. In this paper, we give
combinatorially equivalent conditions for preserving the topology of the links,
allowing for the implementation of an algorithm for collapsing a directed
Euclidean cubical complex. Furthermore, we give conditions for when
link-preserving directed collapses preserve the contractability and
connectedness of directed path spaces, as well as examples when link-preserving
directed collapses do not preserve the number of connected components of the
path space between the minimum and a given vertex.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01526</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01526</id><submitter>George Boateng</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:15:41 GMT</date><size>193kb</size><source_type>D</source_type></version><title>&quot;You made me feel this way&quot;: Investigating Partners' Influence in
  Predicting Emotions in Couples' Conflict Interactions using Speech Data</title><authors>George Boateng, Peter Hilpert, Guy Bodenmann, Mona Neysari, Tobias
  Kowatsch</authors><categories>cs.CL</categories><comments>5 pages, Under review at ICMI 2021</comments><acm-class>J.4</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  How romantic partners interact with each other during a conflict influences
how they feel at the end of the interaction and is predictive of whether the
partners stay together in the long term. Hence understanding the emotions of
each partner is important. Yet current approaches that are used include
self-reports which are burdensome and hence limit the frequency of this data
collection. Automatic emotion prediction could address this challenge. Insights
from psychology research indicate that partners' behaviors influence each
other's emotions in conflict interaction and hence, the behavior of both
partners could be considered to better predict each partner's emotion. However,
it is yet to be investigated how doing so compares to only using each partner's
own behavior in terms of emotion prediction performance. In this work, we used
BERT to extract linguistic features (i.e., what partners said) and openSMILE to
extract paralinguistic features (i.e., how they said it) from a data set of 368
German-speaking Swiss couples (N = 736 individuals) which were videotaped
during an 8-minutes conflict interaction in the laboratory. Based on those
features, we trained machine learning models to predict if partners feel
positive or negative after the conflict interaction. Our results show that
including the behavior of the other partner improves the prediction
performance. Furthermore, for men, considering how their female partners spoke
is most important and for women considering what their male partner said is
most important in getting better prediction performance. This work is a step
towards automatically recognizing each partners' emotion based on the behavior
of both, which would enable a better understanding of couples in research,
therapy, and the real world.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01528</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01528</id><submitter>Derek Hansen</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:19:01 GMT</date><size>644kb</size><source_type>D</source_type></version><title>Normalizing Flows for Knockoff-free Controlled Feature Selection</title><authors>Derek Hansen, Brian Manzo, Jeffrey Regier</authors><categories>stat.ML cs.LG</categories><comments>17 pages, 4 figures. Under review at Neurips 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of controlled feature selection is to discover the features a
response depends on while limiting the proportion of false discoveries to a
predefined level. Recently, multiple methods have been proposed that use deep
learning to generate knockoffs for controlled feature selection through the
Model-X knockoff framework. We demonstrate, however, that these methods often
fail to control the false discovery rate (FDR). There are two reasons for this
shortcoming. First, these methods often learn inaccurate models of features.
Second, the &quot;swap&quot; property, which is required for knockoffs to be valid, is
often not well enforced. We propose a new procedure called FlowSelect that
remedies both of these problems. To more accurately model the features,
FlowSelect uses normalizing flows, the state-of-the-art method for density
estimation. To circumvent the need to enforce the swap property, FlowSelect
uses a novel MCMC-based procedure to directly compute p-values for each
feature. Asymptotically, FlowSelect controls the FDR exactly. Empirically,
FlowSelect controls the FDR well on both synthetic and semi-synthetic
benchmarks, whereas competing knockoff-based approaches fail to do so.
FlowSelect also demonstrates greater power on these benchmarks. Additionally,
using data from a genome-wide association study of soybeans, FlowSelect
correctly infers the genetic variants associated with specific soybean traits.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01532</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01532</id><submitter>Ang Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:29:29 GMT</date><size>17449kb</size><source_type>D</source_type></version><title>Noise Doesn't Lie: Towards Universal Detection of Deep Inpainting</title><authors>Ang Li, Qiuhong Ke, Xingjun Ma, Haiqin Weng, Zhiyuan Zong, Feng Xue,
  Rui Zhang</authors><categories>cs.CV</categories><comments>Accepted by IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep image inpainting aims to restore damaged or missing regions in an image
with realistic contents. While having a wide range of applications such as
object removal and image recovery, deep inpainting techniques also have the
risk of being manipulated for image forgery. A promising countermeasure against
such forgeries is deep inpainting detection, which aims to locate the inpainted
regions in an image. In this paper, we make the first attempt towards universal
detection of deep inpainting, where the detection network can generalize well
when detecting different deep inpainting methods. To this end, we first propose
a novel data generation approach to generate a universal training dataset,
which imitates the noise discrepancies exist in real versus inpainted image
contents to train universal detectors. We then design a Noise-Image
Cross-fusion Network (NIX-Net) to effectively exploit the discriminative
information contained in both the images and their noise patterns. We
empirically show, on multiple benchmark datasets, that our approach outperforms
existing detection methods by a large margin and generalize well to unseen deep
inpainting techniques. Our universal training dataset can also significantly
boost the generalizability of existing detection methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01534</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01534</id><submitter>Xun Yang</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:33:26 GMT</date><size>3070kb</size><source_type>D</source_type></version><title>Deconfounded Video Moment Retrieval with Causal Intervention</title><authors>Xun Yang, Fuli Feng, Wei Ji, Meng Wang, Tat-Seng Chua</authors><categories>cs.CV</categories><comments>This work has been accepted by SIGIR 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We tackle the task of video moment retrieval (VMR), which aims to localize a
specific moment in a video according to a textual query. Existing methods
primarily model the matching relationship between query and moment by complex
cross-modal interactions. Despite their effectiveness, current models mostly
exploit dataset biases while ignoring the video content, thus leading to poor
generalizability. We argue that the issue is caused by the hidden confounder in
VMR, {i.e., temporal location of moments}, that spuriously correlates the model
input and prediction. How to design robust matching models against the temporal
location biases is crucial but, as far as we know, has not been studied yet for
VMR.
  To fill the research gap, we propose a causality-inspired VMR framework that
builds structural causal model to capture the true effect of query and video
content on the prediction. Specifically, we develop a Deconfounded Cross-modal
Matching (DCM) method to remove the confounding effects of moment location. It
first disentangles moment representation to infer the core feature of visual
content, and then applies causal intervention on the disentangled multimodal
input based on backdoor adjustment, which forces the model to fairly
incorporate each possible location of the target into consideration. Extensive
experiments clearly show that our approach can achieve significant improvement
over the state-of-the-art methods in terms of both accuracy and generalization
(Codes:
\color{blue}{\url{https://github.com/Xun-Yang/Causal_Video_Moment_Retrieval}}
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01536</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01536</id><submitter>George Boateng</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:37:59 GMT</date><size>2852kb</size><source_type>D</source_type></version><title>BERT meets LIWC: Exploring State-of-the-Art Language Models for
  Predicting Communication Behavior in Couples' Conflict Interactions</title><authors>Jacopo Biggiogera, George Boateng, Peter Hilpert, Matthew Vowels, Guy
  Bodenmann, Mona Neysari, Fridtjof Nussbeck, Tobias Kowatsch</authors><categories>cs.CL</categories><comments>6 pages. Under review at ICMI 2021</comments><acm-class>J.4</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Many processes in psychology are complex, such as dyadic interactions between
two interacting partners (e.g. patient-therapist, intimate relationship
partners). Nevertheless, many basic questions about interactions are difficult
to investigate because dyadic processes can be within a person and between
partners, they are based on multimodal aspects of behavior and unfold rapidly.
Current analyses are mainly based on the behavioral coding method, whereby
human coders annotate behavior based on a coding schema. But coding is
labor-intensive, expensive, slow, focuses on few modalities. Current approaches
in psychology use LIWC for analyzing couples' interactions. However, advances
in natural language processing such as BERT could enable the development of
systems to potentially automate behavioral coding, which in turn could
substantially improve psychological research. In this work, we train machine
learning models to automatically predict positive and negative communication
behavioral codes of 368 German-speaking Swiss couples during an 8-minute
conflict interaction on a fine-grained scale (10-seconds sequences) using
linguistic features and paralinguistic features derived with openSMILE. Our
results show that both simpler TF-IDF features as well as more complex BERT
features performed better than LIWC, and that adding paralinguistic features
did not improve the performance. These results suggest it might be time to
consider modern alternatives to LIWC, the de facto linguistic features in
psychology, for prediction tasks in couples research. This work is a further
step towards the automated coding of couples' behavior which could enhance
couple research and therapy, and be utilized for other dyadic interactions as
well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01538</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01538</id><submitter>Alexander Matyasko</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:45:48 GMT</date><size>6208kb</size><source_type>D</source_type></version><title>PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack</title><authors>Alexander Matyasko, Lap-Pui Chau</authors><categories>cs.LG cs.CR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art deep neural networks are sensitive to small input
perturbations. Since the discovery of this intriguing vulnerability, many
defence methods have been proposed that attempt to improve robustness to
adversarial noise. Fast and accurate attacks are required to compare various
defence methods. However, evaluating adversarial robustness has proven to be
extremely challenging. Existing norm minimisation adversarial attacks require
thousands of iterations (e.g. Carlini &amp; Wagner attack), are limited to the
specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results
(e.g. Brendel &amp; Bethge attack). On the other hand, PGD attack, which is fast,
general and accurate, ignores the norm minimisation penalty and solves a
simpler perturbation-constrained problem. In this work, we introduce a fast,
general and accurate adversarial attack that optimises the original non-convex
constrained minimisation problem. We interpret optimising the Lagrangian of the
adversarial attack optimisation problem as a two-player game: the first player
minimises the Lagrangian wrt the adversarial noise; the second player maximises
the Lagrangian wrt the regularisation penalty. Our attack algorithm
simultaneously optimises primal and dual variables to find the minimal
adversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation,
such as $l_{\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual
proximal gradient descent attack. We show in the experiments that our attack
outperforms current state-of-the-art $l_{\infty}$-, $l_2$-, $l_1$-, and
$l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against
unregularised and adversarially trained models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01540</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01540</id><submitter>Xuezhe Ma</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:47:26 GMT</date><size>667kb</size><source_type>D</source_type></version><title>Luna: Linear Unified Nested Attention</title><authors>Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao
  Ma, Luke Zettlemoyer</authors><categories>cs.LG cs.CL</categories><comments>Preprint. 2 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quadratic computational and memory complexities of the Transformer's
attention mechanism have limited its scalability for modeling long sequences.
In this paper, we propose Luna, a linear unified nested attention mechanism
that approximates softmax attention with two nested linear attention functions,
yielding only linear (as opposed to quadratic) time and space complexity.
Specifically, with the first attention function, Luna packs the input sequence
into a sequence of fixed length. Then, the packed sequence is unpacked using
the second attention function. As compared to a more traditional attention
mechanism, Luna introduces an additional sequence with a fixed length as input
and an additional corresponding output, which allows Luna to perform attention
operation linearly, while also storing adequate contextual information. We
perform extensive evaluations on three benchmarks of sequence modeling tasks:
long-context sequence modeling, neural machine translation and masked language
modeling for large-scale pretraining. Competitive or even better experimental
results demonstrate both the effectiveness and efficiency of Luna compared to a
variety
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01541</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01541</id><submitter>Jia-Chen Gu</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:49:12 GMT</date><size>842kb</size><source_type>D</source_type></version><title>MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation
  Understanding</title><authors>Jia-Chen Gu, Chongyang Tao, Zhen-Hua Ling, Can Xu, Xiubo Geng, Daxin
  Jiang</authors><categories>cs.CL</categories><comments>Accepted by ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, various neural models for multi-party conversation (MPC) have
achieved impressive improvements on a variety of tasks such as addressee
recognition, speaker identification and response prediction. However, these
existing methods on MPC usually represent interlocutors and utterances
individually and ignore the inherent complicated structure in MPC which may
provide crucial interlocutor and utterance semantics and would enhance the
conversation understanding process. To this end, we present MPC-BERT, a
pre-trained model for MPC understanding that considers learning who says what
to whom in a unified model with several elaborated self-supervised tasks.
Particularly, these tasks can be generally categorized into (1) interlocutor
structure modeling including reply-to utterance recognition, identical speaker
searching and pointer consistency distinction, and (2) utterance semantics
modeling including masked shared utterance restoration and shared node
detection. We evaluate MPC-BERT on three downstream tasks including addressee
recognition, speaker identification and response selection. Experimental
results show that MPC-BERT outperforms previous methods by large margins and
achieves new state-of-the-art performance on all three downstream tasks at two
benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01543</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01543</id><submitter>Yue Gong</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:58:24 GMT</date><size>10548kb</size><source_type>D</source_type></version><title>Niffler: A Reference Architecture and System Implementation for View
  Discovery over Pathless Table Collections by Example</title><authors>Yue Gong, Zhiru Zhu, Sainyam Galhotra, Raul Castro Fernandez</authors><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying a project-join view (PJ-view) over collections of tables is the
first step of many data management projects, e.g., assembling a dataset to feed
into a business intelligence tool, creating a training dataset to fit a machine
learning model, and more. When the table collections are large and lack join
information--such as when combining databases, or on data lakes--query by
example (QBE) systems can help identify relevant data, but they are designed
under the assumption that join information is available in the schema, and do
not perform well on pathless table collections that do not have join path
information.
  We present a reference architecture that explicitly divides the end-to-end
problem of discovering PJ-views over pathless table collections into a human
and a technical problem. We then present Niffler, a system built to address the
technical problem. We introduce algorithms for the main components of Niffler,
including a signal generation component that helps reduce the size of the
candidate views that may be large due to errors and ambiguity in both the data
and input queries. We evaluate Niffler on real datasets to demonstrate the
effectiveness of the new engine in discovering PJ-views over pathless table
collections.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01544</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01544</id><submitter>Hong-Yu Zhou</submitter><version version="v1"><date>Thu, 3 Jun 2021 01:59:50 GMT</date><size>8611kb</size><source_type>D</source_type></version><title>SSMD: Semi-Supervised Medical Image Detection with Adaptive Consistency
  and Heterogeneous Perturbation</title><authors>Hong-Yu Zhou, Chengdi Wang, Haofeng Li, Gang Wang, Shu Zhang, Weimin
  Li, Yizhou Yu</authors><categories>cs.CV</categories><comments>Accepted by Medical Image Analysis</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semi-Supervised classification and segmentation methods have been widely
investigated in medical image analysis. Both approaches can improve the
performance of fully-supervised methods with additional unlabeled data.
However, as a fundamental task, semi-supervised object detection has not gained
enough attention in the field of medical image analysis. In this paper, we
propose a novel Semi-Supervised Medical image Detector (SSMD). The motivation
behind SSMD is to provide free yet effective supervision for unlabeled data, by
regularizing the predictions at each position to be consistent. To achieve the
above idea, we develop a novel adaptive consistency cost function to regularize
different components in the predictions. Moreover, we introduce heterogeneous
perturbation strategies that work in both feature space and image space, so
that the proposed detector is promising to produce powerful image
representations and robust predictions. Extensive experimental results show
that the proposed SSMD achieves the state-of-the-art performance at a wide
range of settings. We also demonstrate the strength of each proposed module
with comprehensive ablation studies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01548</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01548</id><submitter>Xiangning Chen</submitter><version version="v1"><date>Thu, 3 Jun 2021 02:08:03 GMT</date><size>3062kb</size><source_type>D</source_type></version><title>When Vision Transformers Outperform ResNets without Pretraining or
  Strong Data Augmentations</title><authors>Xiangning Chen, Cho-Jui Hsieh, Boqing Gong</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pretraining and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pretraining or strong data
augmentations. They also possess more perceptive attention maps.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01551</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01551</id><submitter>Ya Zhou</submitter><version version="v1"><date>Thu, 3 Jun 2021 02:35:13 GMT</date><size>1054kb</size></version><title>Matching-Theory-Based Multi-User Cooperative Computing Framework</title><authors>Ya Zhou, Guopeng Zhang, Kezhi Wang and Kun Yang</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a matching theory based multi-user cooperative
framework to minimize the overall energy consumption of all the user equipments
(UEs), where the UEs can be classified into the following roles: resource
demander (RD), resource provider (RP), and standalone UE (SU). We first
determine the role of the UE by leveraging the roommate matching method. Then,
by adopting college admission based algorithm, we divide the UEs into multiple
cooperation groups, each consisting of one RP and multiple RDs. Next, we
propose the rotation swap operation to further improve the performance without
deteriorating the system stability. Finally, we present an effective task
offloading algorithm to minimize the energy consumption of all the cooperation
groups. The simulation results verify the effectiveness of the proposed scheme.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01553</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01553</id><submitter>Peng-Shuai Wang</submitter><version version="v1"><date>Thu, 3 Jun 2021 02:37:47 GMT</date><size>5331kb</size><source_type>D</source_type></version><title>Spline Positional Encoding for Learning 3D Implicit Signed Distance
  Fields</title><authors>Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, Xin Tong</authors><categories>cs.CV cs.GR</categories><comments>Accepted by IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilayer perceptrons (MLPs) have been successfully used to represent 3D
shapes implicitly and compactly, by mapping 3D coordinates to the corresponding
signed distance values or occupancy values. In this paper, we propose a novel
positional encoding scheme, called Spline Positional Encoding, to map the input
coordinates to a high dimensional space before passing them to MLPs, for
helping to recover 3D signed distance fields with fine-scale geometric details
from unorganized 3D point clouds. We verified the superiority of our approach
over other positional encoding schemes on tasks of 3D shape reconstruction from
input point clouds and shape space learning. The efficacy of our approach
extended to image reconstruction is also demonstrated and evaluated.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01555</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01555</id><submitter>Jekaterina Novikova Dr.</submitter><version version="v1"><date>Thu, 3 Jun 2021 02:44:40 GMT</date><size>32kb</size></version><title>Comparing Acoustic-based Approaches for Alzheimer's Disease Detection</title><authors>Aparna Balagopalan, Jekaterina Novikova</authors><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted to INTERSPEECH 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the performance and generalizability of three
approaches for AD detection from speech on the recent ADReSSo challenge
dataset: 1) using conventional acoustic features 2) using novel pre-trained
acoustic embeddings 3) combining acoustic features and embeddings. We find that
while feature-based approaches have a higher precision, classification
approaches relying on the combination of embeddings and features prove to have
a higher, and more balanced performance across multiple metrics of performance.
Our best model, using such a combined approach, outperforms the acoustic
baseline in the challenge by 2.8\%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01559</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01559</id><submitter>Zhuoren Jiang</submitter><version version="v1"><date>Thu, 3 Jun 2021 02:57:08 GMT</date><size>4443kb</size><source_type>D</source_type></version><title>Adjacency List Oriented Relational Fact Extraction via Adaptive
  Multi-task Learning</title><authors>Fubang Zhao, Zhuoren Jiang, Yangyang Kang, Changlong Sun, Xiaozhong
  Liu</authors><categories>cs.CL</categories><comments>13 pages, 3 figures, accepted by findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Relational fact extraction aims to extract semantic triplets from
unstructured text. In this work, we show that all of the relational fact
extraction models can be organized according to a graph-oriented analytical
perspective. An efficient model, aDjacency lIst oRiented rElational faCT
(DIRECT), is proposed based on this analytical framework. To alleviate
challenges of error propagation and sub-task loss equilibrium, DIRECT employs a
novel adaptive multi-task learning strategy with dynamic sub-task loss
balancing. Extensive experiments are conducted on two benchmark datasets, and
results prove that the proposed model outperforms a series of state-of-the-art
(SoTA) models for relational triplet extraction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01560</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01560</id><submitter>Vijay Viswanathan</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:00:12 GMT</date><size>7826kb</size><source_type>D</source_type></version><title>CitationIE: Leveraging the Citation Graph for Scientific Information
  Extraction</title><authors>Vijay Viswanathan, Graham Neubig, Pengfei Liu</authors><categories>cs.DL cs.CL</categories><comments>ACL-IJCNLP 2021 camera-ready (long paper in main conference)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically extracting key information from scientific documents has the
potential to help scientists work more efficiently and accelerate the pace of
scientific progress. Prior work has considered extracting document-level entity
clusters and relations end-to-end from raw scientific text, which can improve
literature search and help identify methods and materials for a given problem.
Despite the importance of this task, most existing works on scientific
information extraction (SciIE) consider extraction solely based on the content
of an individual paper, without considering the paper's place in the broader
literature. In contrast to prior work, we augment our text representations by
leveraging a complementary source of document context: the citation graph of
referential links between citing and cited papers. On a test set of
English-language scientific documents, we show that simple ways of utilizing
the structure and content of the citation graph can each lead to significant
gains in different scientific information extraction tasks. When these tasks
are combined, we observe a sizable improvement in end-to-end information
extraction over the state-of-the-art, suggesting the potential for future work
along this direction. We release software tools to facilitate citation-aware
SciIE development.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01561</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01561</id><submitter>Cunxiang Wang</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:04:06 GMT</date><size>1232kb</size><source_type>D</source_type></version><title>Can Generative Pre-trained Language Models Serve as Knowledge Bases for
  Closed-book QA?</title><authors>Cunxiang Wang and Pai Liu and Yue Zhang</authors><categories>cs.CL</categories><comments>Accepted By ACL-IJCNLP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent work has investigated the interesting question using pre-trained
language models (PLMs) as knowledge bases for answering open questions.
However, existing work is limited in using small benchmarks with high
test-train overlaps. We construct a new dataset of closed-book QA using SQuAD,
and investigate the performance of BART. Experiments show that it is
challenging for BART to remember training facts in high precision, and also
challenging to answer closed-book questions even if relevant knowledge is
retained. Some promising directions are found, including decoupling the
knowledge memorizing process and the QA finetune process, forcing the model to
recall relevant knowledge when question answering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01562</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01562</id><submitter>Wang Xu</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:09:38 GMT</date><size>537kb</size><source_type>D</source_type></version><title>Discriminative Reasoning for Document-level Relation Extraction</title><authors>Wang Xu, Kehai Chen, Tiejun Zhao</authors><categories>cs.CL</categories><comments>11 pages, 4 figures, 5 tables. Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document-level relation extraction (DocRE) models generally use graph
networks to implicitly model the reasoning skill (i.e., pattern recognition,
logical reasoning, coreference reasoning, etc.) related to the relation between
one entity pair in a document. In this paper, we propose a novel discriminative
reasoning framework to explicitly model the paths of these reasoning skills
between each entity pair in this document. Thus, a discriminative reasoning
network is designed to estimate the relation probability distribution of
different reasoning paths based on the constructed graph and vectorized
document contexts for each entity pair, thereby recognizing their relation.
Experimental results show that our method outperforms the previous
state-of-the-art performance on the large-scale DocRE dataset. The code is
publicly available at https://github.com/xwjim/DRN.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01567</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01567</id><submitter>Jason Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:24:36 GMT</date><size>76kb</size><source_type>D</source_type></version><title>Deterministic Weighted Expander Decomposition in Almost-linear Time</title><authors>Jason Li, Thatchaphol Saranurak</authors><categories>cs.DS</categories><comments>15 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this note, we study the expander decomposition problem in a more general
setting where the input graph has positively weighted edges and nonnegative
demands on its vertices. We show how to extend the techniques of Chuzhoy et al.
(FOCS 2020) to this wider setting, obtaining a deterministic algorithm for the
problem in almost-linear time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01570</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01570</id><submitter>Xingzhi Guo</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:31:58 GMT</date><size>4180kb</size><source_type>D</source_type></version><title>Subset Node Representation Learning over Large Dynamic Graphs</title><authors>Xingzhi Guo, Baojian Zhou, Steven Skiena</authors><categories>cs.SI</categories><comments>9 pages + 2 pages supplement, accepted to 2021 ACM SIGKDD</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic graph representation learning is a task to learn node embeddings over
dynamic networks, and has many important applications, including knowledge
graphs, citation networks to social networks. Graphs of this type are usually
large-scale but only a small subset of vertices are related in downstream
tasks. Current methods are too expensive to this setting as the complexity is
at best linear-dependent on both the number of nodes and edges. In this paper,
we propose a new method, namely Dynamic Personalized PageRank Embedding
(\textsc{DynamicPPE}) for learning a target subset of node representations over
large-scale dynamic networks. Based on recent advances in local node embedding
and a novel computation of dynamic personalized PageRank vector (PPV),
\textsc{DynamicPPE} has two key ingredients: 1) the per-PPV complexity is
$\mathcal{O}(m \bar{d} / \epsilon)$ where $m,\bar{d}$, and $\epsilon$ are the
number of edges received, average degree, global precision error respectively.
Thus, the per-edge event update of a single node is only dependent on $\bar{d}$
in average; and 2) by using these high quality PPVs and hash kernels, the
learned embeddings have properties of both locality and global consistency.
These two make it possible to capture the evolution of graph structure
effectively. Experimental results demonstrate both the effectiveness and
efficiency of the proposed method over large-scale dynamic networks. We apply
\textsc{DynamicPPE} to capture the embedding change of Chinese cities in the
Wikipedia graph during this ongoing COVID-19 pandemic
(https://en.wikipedia.org/wiki/COVID-19_pandemic). Our results show that these
representations successfully encode the dynamics of the Wikipedia graph.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01573</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01573</id><submitter>Lei Liu</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:42:45 GMT</date><size>235kb</size><source_type>D</source_type></version><title>Irregularly Clipped Sparse Regression Codes</title><authors>Wencong Li, Lei Liu, Brian M. Kurkoski</authors><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, submitted to IEEE ITW 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recently, it was found that clipping can significantly improve the section
error rate (SER) performance of sparse regression (SR) codes if an optimal
clipping threshold is chosen. In this paper, we propose irregularly clipped SR
codes, where multiple clipping thresholds are applied to symbols according to a
distribution, to further improve the SER performance of SR codes. Orthogonal
approximate message passing (OAMP) algorithm is used for decoding. Using state
evolution, the distribution of irregular clipping thresholds is optimized to
minimize the SER of OAMP decoding. As a result, optimized irregularly clipped
SR codes achieve a better tradeoff between clipping distortion and noise
distortion than regularly clipped SR codes. Numerical results demonstrate that
irregularly clipped SR codes achieve 0.4 dB gain in signal-to-noise-ratio (SNR)
over regularly clipped SR codes at code length$\,\approx2.5\!\times\! 10^4$ and
SER$\,\approx10^{-5}$. We further show that irregularly clipped SR codes are
robust over a wide range of code rates.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01577</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01577</id><submitter>Honghao Wei</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:53:27 GMT</date><size>141kb</size></version><title>A Provably-Efficient Model-Free Algorithm for Constrained Markov
  Decision Processes</title><authors>Honghao Wei, Xin Liu, Lei Ying</authors><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first {\em model-free}, {\em simulator-free}
reinforcement learning algorithm for Constrained Markov Decision Processes
(CMDPs) with sublinear regret and zero constraint violation. The algorithm is
named Triple-Q because it has three key components: a Q-function (also called
action-value function) for the cumulative reward, a Q-function for the
cumulative utility for the constraint, and a virtual-Queue that
(over)-estimates the cumulative constraint violation. Under Triple-Q, at each
step, an action is chosen based on the pseudo-Q-value that is a combination of
the three Q values. The algorithm updates the reward and utility Q-values with
learning rates that depend on the visit counts to the corresponding (state,
action) pairs and are periodically reset. In the episodic CMDP setting,
Triple-Q achieves $\tilde{\cal O}\left(\frac{1 }{\delta}H^4
S^{\frac{1}{2}}A^{\frac{1}{2}}K^{\frac{4}{5}} \right)$ regret, where $K$ is the
total number of episodes, $H$ is the number of steps in each episode, $S$ is
the number of states, $A$ is the number of actions, and $\delta$ is Slater's
constant. Furthermore, Triple-Q guarantees zero constraint violation when $K$
is sufficiently large. Finally, the computational complexity of Triple-Q is
similar to SARSA for unconstrained MDPs and is computationally efficient.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01580</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01580</id><submitter>Yuchen Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 03:58:35 GMT</date><size>6310kb</size><source_type>D</source_type></version><title>The Limitations of Limited Context for Constituency Parsing</title><authors>Yuchen Li, Andrej Risteski</authors><categories>cs.CL cs.LG</categories><comments>To be published in ACL 2021 (https://2021.aclweb.org/) as a long
  paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating syntax into neural approaches in NLP has a multitude of
practical and scientific benefits. For instance, a language model that is
syntax-aware is likely to be able to produce better samples; even a
discriminative model like BERT with a syntax module could be used for core NLP
tasks like unsupervised syntactic parsing. Rapid progress in recent years was
arguably spurred on by the empirical success of the Parsing-Reading-Predict
architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM
of (Shen et al., 2019). Most notably, this is the first time neural approaches
were able to successfully perform unsupervised syntactic parsing (evaluated by
various metrics like F-1 score).
  However, even heuristic (much less fully mathematical) understanding of why
and when these architectures work is lagging severely behind. In this work, we
answer representational questions raised by the architectures in (Shen et al.,
2018a, 2019), as well as some transition-based syntax-aware language models
(Dyer et al., 2016): what kind of syntactic structure can current neural
approaches to syntax represent? Concretely, we ground this question in the
sandbox of probabilistic context-free-grammars (PCFGs), and identify a key
aspect of the representational power of these approaches: the amount and
directionality of context that the predictor has access to when forced to make
parsing decision. We show that with limited context (either bounded, or
unidirectional), there are PCFGs, for which these approaches cannot represent
the max-likelihood parse; conversely, if the context is unlimited, they can
represent the max-likelihood parse of any PCFG.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01581</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01581</id><submitter>William Timkey</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:03:15 GMT</date><size>799kb</size><source_type>D</source_type></version><title>To Point or Not to Point: Understanding How Abstractive Summarizers
  Paraphrase Text</title><authors>Matt Wilber, William Timkey, Marten Van Schijndel</authors><categories>cs.CL</categories><comments>Findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Abstractive neural summarization models have seen great improvements in
recent years, as shown by ROUGE scores of the generated summaries. But despite
these improved metrics, there is limited understanding of the strategies
different models employ, and how those strategies relate their understanding of
language. To understand this better, we run several experiments to characterize
how one popular abstractive model, the pointer-generator model of See et al.
(2017), uses its explicit copy/generation switch to control its level of
abstraction (generation) vs extraction (copying). On an extractive-biased
dataset, the model utilizes syntactic boundaries to truncate sentences that are
otherwise often copied verbatim. When we modify the copy/generation switch and
force the model to generate, only simple paraphrasing abilities are revealed
alongside factual inaccuracies and hallucinations. On an abstractive-biased
dataset, the model copies infrequently but shows similarly limited abstractive
abilities. In line with previous research, these results suggest that
abstractive summarization models lack the semantic understanding necessary to
generate paraphrases that are both abstractive and faithful to the source
document.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01583</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01583</id><submitter>Meng Jiang</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:07:26 GMT</date><size>445kb</size><source_type>D</source_type></version><title>Cross-Network Learning with Partially Aligned Graph Convolutional
  Networks</title><authors>Meng Jiang</authors><categories>cs.LG</categories><comments>9 pages, 8 figures, KDD 2021</comments><doi>10.1145/3447548.3467282</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graph neural networks have been widely used for learning representations of
nodes for many downstream tasks on graph data. Existing models were designed
for the nodes on a single graph, which would not be able to utilize information
across multiple graphs. The real world does have multiple graphs where the
nodes are often partially aligned. For examples, knowledge graphs share a
number of named entities though they may have different relation schema;
collaboration networks on publications and awarded projects share some
researcher nodes who are authors and investigators, respectively; people use
multiple web services, shopping, tweeting, rating movies, and some may register
the same email account across the platforms. In this paper, I propose partially
aligned graph convolutional networks to learn node representations across the
models. I investigate multiple methods (including model sharing,
regularization, and alignment reconstruction) as well as theoretical analysis
to positively transfer knowledge across the (small) set of partially aligned
nodes. Extensive experiments on real-world knowledge graphs and collaboration
networks show the superior performance of our proposed methods on relation
classification and link prediction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01586</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01586</id><submitter>Vardaan Pahuja</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:14:11 GMT</date><size>5751kb</size><source_type>D</source_type></version><title>A Systematic Investigation of KB-Text Embedding Alignment at Scale</title><authors>Vardaan Pahuja, Yu Gu, Wenhu Chen, Mehdi Bahrami, Lei Liu, Wei-Peng
  Chen and Yu Su</authors><categories>cs.CL cs.LG</categories><comments>Accepted to ACL-IJCNLP 2021. 11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge bases (KBs) and text often contain complementary knowledge: KBs
store structured knowledge that can support long range reasoning, while text
stores more comprehensive and timely knowledge in an unstructured way.
Separately embedding the individual knowledge sources into vector spaces has
demonstrated tremendous successes in encoding the respective knowledge, but how
to jointly embed and reason with both knowledge sources to fully leverage the
complementary information is still largely an open problem. We conduct a
large-scale, systematic investigation of aligning KB and text embeddings for
joint reasoning. We set up a novel evaluation framework with two evaluation
tasks, few-shot link prediction and analogical reasoning, and evaluate an array
of KB-text embedding alignment methods. We also demonstrate how such alignment
can infuse textual information into KB embeddings for more accurate link
prediction on emerging entities and events, using COVID-19 as a case study.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01587</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01587</id><submitter>Sai Munikoti</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:15:03 GMT</date><size>228kb</size></version><title>An Information Theoretic approach to identify Dominant Voltage
  Influencers for Unbalanced Distribution Systems</title><authors>Sai Munikoti, Mohammad Abujubbeh, Kumarsinh Jhala, Balasubramaniam
  Natarajan</authors><categories>eess.SY cs.SY</categories><comments>8 pages, 6 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Smart distribution grid with multiple renewable energy sources can experience
random voltage fluctuations due to variable generation, which may result in
voltage violations. Traditional voltage control algorithms are inadequate to
handle fast voltage variations. Therefore, new dynamic control methods are
being developed that can significantly benefit from the knowledge of dominant
voltage influencer (DVI) nodes. DVI nodes for a particular node of interest
refer to nodes that have a relatively high impact on the voltage fluctuations
at that node. Conventional power flow-based algorithms to identify DVI nodes
are computationally complex, which limits their use in real-time applications.
This paper proposes a novel information theoretic voltage influencing score
(VIS) that quantifies the voltage influencing capacity of nodes with
DERs/active loads in a three phase unbalanced distribution system. VIS is then
employed to rank the nodes and identify the DVI set. VIS is derived
analytically in a computationally efficient manner and its efficacy to identify
DVI nodes is validated using the IEEE 37-node test system. It is shown through
experiments that KL divergence and Bhattacharyya distance are effective
indicators of DVI nodes with an identifying accuracy of more than 90%. The
computation burden is also reduced by an order of 5, thus providing the
foundation for efficient voltage control.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01588</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01588</id><submitter>Vasilis Gkatzelis</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:17:08 GMT</date><size>30kb</size></version><title>Resource-Aware Cost-Sharing Mechanisms with Priors</title><authors>Vasilis Gkatzelis and Emmanouil Pountourakis and Alkmini Sgouritsa</authors><categories>cs.GT</categories><comments>To appear at ACM EC 2021 conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In a decentralized system with $m$ machines, we study the selfish scheduling
problem where each user strategically chooses which machine to use. Each
machine incurs a cost, which is a function of the total load assigned to it,
and some cost-sharing mechanism distributes this cost among the machine's
users. The users choose a machine aiming to minimize their own share of the
cost, so the cost-sharing mechanism induces a game among them. We approach this
problem from the perspective of a designer who can select which cost-sharing
mechanism to use, aiming to minimize the price of anarchy (PoA) of the induced
games.
  Recent work introduced the class of \emph{resource-aware} cost-sharing
mechanisms, whose decisions can depend on the set of machines in the system,
but are oblivious to the total number of users. These mechanisms can guarantee
low PoA bounds for instances where the cost functions of the machines are all
convex or concave, but can suffer from very high PoA for cost functions that
deviate from these families.
  In this paper we show that if we enhance the class of resource-aware
mechanisms with some prior information regarding the users, then they can
achieve low PoA for a much more general family of cost functions. We first show
that, as long as the mechanism knows just two of the participating users, then
it can assign special roles to them and ensure a constant PoA. We then extend
this idea to settings where the mechanism has access to the probability with
which each user is present in the system. For all these instances, we provide a
mechanism that achieves an expected PoA that is logarithmic in the expected
number of users.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01589</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01589</id><submitter>Hongyuan Diao</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:21:06 GMT</date><size>3650kb</size></version><title>The Emotion coding and Propagation based on improved Genetic algorithm</title><authors>Hongyuan Diao, Fuzhong Nian, Xuelong Yu, Xirui Liu and Xinhao Liu</authors><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational communication research on information has been prevalent in
recent years, as people are progressively inquisitive in social behavior and
public opinion. Nevertheless, it is of great significance to analyze the
direction of predominant sentiment from the sentiment communication
perspective. In this paper, the information emotion propagation model is
established by introducing revamp genetic algorithms into information emotion.
In the process of information dissemination, both the information emotions and
the network emotions are dynamic. For this model, the information emotions and
the network nodes emotions are quantified as binary codes. The convergence
effects, crossover and mutation algorithms are introduced. These factors all
act on the transmission process via dynamic propagation rate, and the improved
genetic algorithm also acts on the emotion transmission. In particular, the
latter two algorithms are different from the existing biological domain. Based
on the existing research results in other manuscripts, we perform simulation
described above on the hybrid network. The simulation results demonstrate that
the trend approximate to the actual data. As a result, our work can prove that
our proposed model is essentially consistent with the actual emotion
transmission phenomenon.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01590</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01590</id><submitter>Roberto Vega</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:22:43 GMT</date><size>985kb</size><source_type>D</source_type></version><title>SIMLR: Machine Learning inside the SIR model for COVID-19 Forecasting</title><authors>Roberto Vega, Leonardo Flores, Russell Greiner</authors><categories>cs.LG stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate forecasts of the number of newly infected people during an epidemic
are critical for making effective timely decisions. This paper addresses this
challenge using the SIMLR model, which incorporates machine learning (ML) into
the epidemiological SIR model. For each region, SIMLR tracks the changes in the
policies implemented at the government level, which it uses to estimate the
time-varying parameters of an SIR model for forecasting the number of new
infections 1- to 4-weeks in advance.It also forecasts the probability of
changes in those government policies at each of these future times, which is
essential for the longer-range forecasts. We applied SIMLR to data from regions
in Canada and in the United States,and show that its MAPE (mean average
percentage error) performance is as good as SOTA forecasting models, with the
added advantage of being an interpretable model. We expect that this approach
will be useful not only for forecasting COVID-19 infections, but also in
predicting the evolution of other infectious diseases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01592</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01592</id><submitter>Shuo Jiang</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:35:34 GMT</date><size>2070kb</size><source_type>D</source_type></version><title>Data-Driven Design-by-Analogy: State of the Art and Future Directions</title><authors>Shuo Jiang, Jie Hu, Kristin L. Wood, Jianxi Luo</authors><categories>cs.AI cs.CE</categories><comments>A Preprint Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design-by-Analogy (DbA) is a design methodology wherein new solutions,
opportunities or designs are generated in a target domain based on inspiration
drawn from a source domain; it can benefit designers in mitigating design
fixation and improving design ideation outcomes. Recently, the increasingly
available design databases and rapidly advancing data science and artificial
intelligence technologies have presented new opportunities for developing
data-driven methods and tools for DbA support. In this study, we survey
existing data-driven DbA studies and categorize individual studies according to
the data, methods, and applications in four categories, namely, analogy
encoding, retrieval, mapping, and evaluation. Based on both nuanced organic
review and structured analysis, this paper elucidates the state of the art of
data-driven DbA research to date and benchmarks it with the frontier of data
science and AI research to identify promising research opportunities and
directions for the field. Finally, we propose a future conceptual data-driven
DbA system that integrates all propositions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01594</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01594</id><submitter>Weisong Wen</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:38:54 GMT</date><size>924kb</size></version><title>Towards Robust GNSS Positioning and Real-time Kinematic Using Factor
  Graph Optimization</title><authors>Weisong Wen and Li-Ta Hsu</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global navigation satellite systems (GNSS) are one of the utterly popular
sources for providing globally referenced positioning for autonomous systems.
However, the performance of the GNSS positioning is significantly challenged in
urban canyons, due to the signal reflection and blockage from buildings. Given
the fact that the GNSS measurements are highly environmentally dependent and
time-correlated, the conventional filtering-based method for GNSS positioning
cannot simultaneously explore the time-correlation among historical
measurements. As a result, the filtering-based estimator is sensitive to
unexpected outlier measurements. In this paper, we present a factor graph-based
formulation for GNSS positioning and real-time kinematic (RTK). The formulated
factor graph framework effectively explores the time-correlation of
pseudorange, carrier-phase, and doppler measurements, and leads to the
non-minimal state estimation of the GNSS receiver. The feasibility of the
proposed method is evaluated using datasets collected in challenging urban
canyons of Hong Kong and significantly improved positioning accuracy is
obtained, compared with the filtering-based estimator.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01595</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01595</id><submitter>Shunsuke Inenaga</submitter><version version="v1"><date>Thu, 3 Jun 2021 04:53:23 GMT</date><size>1310kb</size><source_type>D</source_type></version><title>Position Heaps for Cartesian-tree Matching on Strings and Tries</title><authors>Akio Nishimoto, Noriki Fujisato, Yuto Nakashima, Shunsuke Inenaga</authors><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Cartesian-tree pattern matching is a recently introduced scheme of
pattern matching that detects fragments in a sequential data stream which have
a similar structure as a query pattern. Formally, Cartesian-tree pattern
matching seeks all substrings $S'$ of the text string $S$ such that the
Cartesian tree of $S'$ and that of a query pattern $P$ coincide. In this paper,
we present a new indexing structure for this problem called the Cartesian-tree
Position Heap (CPH). Let $n$ be the length of the input text string $S$, $m$
the length of a query pattern $P$, and $\sigma$ the alphabet size. We show that
the CPH of $S$, denoted $\mathsf{CPH}(S)$, supports pattern matching queries in
$O(m (\sigma + \log (\min\{h, m\})) + occ)$ time with $O(n)$ space, where $h$
is the height of the CPH and $occ$ is the number of pattern occurrences. We
show how to build $\mathsf{CPH}(S)$ in $O(n \log \sigma)$ time with $O(n)$
working space. Further, we extend the problem to the case where the text is a
labeled tree (i.e. a trie). Given a trie $T$ with $N$ nodes, we show that the
CPH of $T$, denoted $\mathsf{CPH}(T)$, supports pattern matching queries on the
trie in $O(m (\sigma^2 + \log (\min\{h, m\})) + occ)$ time with $O(N \sigma)$
space. We also show a construction algorithm for $\mathsf{CPH}(T)$ running in
$O(N \sigma)$ time and $O(N \sigma)$ working space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01596</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01596</id><submitter>Ho Hin Lee</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:01:11 GMT</date><size>4792kb</size><source_type>D</source_type></version><title>Attention-Guided Supervised Contrastive Learning for Semantic
  Segmentation</title><authors>Ho Hin Lee, Yucheng Tang, Qi Yang, Xin Yu, Shunxing Bao, Bennett A.
  Landman, Yuankai Huo</authors><categories>cs.CV cs.AI cs.LG</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contrastive learning has shown superior performance in embedding global and
spatial invariant features in computer vision (e.g., image classification).
However, its overall success of embedding local and spatial variant features is
still limited, especially for semantic segmentation. In a per-pixel prediction
task, more than one label can exist in a single image for segmentation (e.g.,
an image contains both cat, dog, and grass), thereby it is difficult to define
'positive' or 'negative' pairs in a canonical contrastive learning setting. In
this paper, we propose an attention-guided supervised contrastive learning
approach to highlight a single semantic object every time as the target. With
our design, the same image can be embedded to different semantic clusters with
semantic attention (i.e., coerce semantic masks) as an additional input
channel. To achieve such attention, a novel two-stage training strategy is
presented. We evaluate the proposed method on multi-organ medical image
segmentation task, as our major task, with both in-house data and BTCV 2015
datasets. Comparing with the supervised and semi-supervised training
state-of-the-art in the backbone of ResNet-50, our proposed pipeline yields
substantial improvement of 5.53% and 6.09% in Dice score for both medical image
segmentation cohorts respectively. The performance of the proposed method on
natural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%
substantial improvement.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01597</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01597</id><submitter>Kaushal Kumar Maurya</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:08:01 GMT</date><size>6947kb</size><source_type>D</source_type></version><title>ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language
  Generation</title><authors>Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano and
  Kumari Deepshikha</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted in Findings of ACL-IJCNLP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the recent advancement in NLP research, cross-lingual transfer for
natural language generation is relatively understudied. In this work, we
transfer supervision from high resource language (HRL) to multiple low-resource
languages (LRLs) for natural language generation (NLG). We consider four NLG
tasks (text summarization, question generation, news headline generation, and
distractor generation) and three syntactically diverse languages, i.e.,
English, Hindi, and Japanese. We propose an unsupervised cross-lingual language
generation framework (called ZmBART) that does not use any parallel or
pseudo-parallel/back-translated data. In this framework, we further pre-train
mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task
using monolingual data of three languages. The objective function of the
auxiliary task is close to the target tasks which enriches the multi-lingual
latent representation of mBART and provides good initialization for target
tasks. Then, this model is fine-tuned with task-specific supervised English
data and directly evaluated with low-resource languages in the Zero-shot
setting. To overcome catastrophic forgetting and spurious correlation issues,
we applied freezing model component and data argumentation approaches
respectively. This simple modeling approach gave us promising results.We
experimented with few-shot training (with 1000 supervised data points) which
boosted the model performance further. We performed several ablations and
cross-lingual transferability analyses to demonstrate the robustness of ZmBART.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01598</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01598</id><submitter>Son T. Luu</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:08:11 GMT</date><size>1133kb</size><source_type>D</source_type></version><title>Automatically Detecting Cyberbullying Comments on Online Game Forums</title><authors>Hanh Hong-Phuc Vo, Hieu Trung Tran, Son T. Luu</authors><categories>cs.CL</categories><comments>Accepted at RIVF 2021 Conference</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Online game forums are popular to most of game players. They use it to
communicate and discuss the strategy of the game, or even to make friends.
However, game forums also contain abusive and harassment speech, disturbing and
threatening players. Therefore, it is necessary to automatically detect and
remove cyberbullying comments to keep the game forum clean and friendly. We use
the Cyberbullying dataset collected from World of Warcraft (WoW) and League of
Legends (LoL) forums and train classification models to automatically detect
whether a comment of a player is abusive or not. The result obtains 82.69% of
macro F1-score for LoL forum and 83.86% of macro F1-score for WoW forum by the
Toxic-BERT model on the Cyberbullying dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01601</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01601</id><submitter>Jiao Sun</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:22:16 GMT</date><size>265kb</size><source_type>D</source_type></version><title>Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia</title><authors>Jiao Sun and Nanyun Peng</authors><categories>cs.CL cs.CY cs.LG</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human activities can be seen as sequences of events, which are crucial to
understanding societies. Disproportional event distribution for different
demographic groups can manifest and amplify social stereotypes, and potentially
jeopardize the ability of members in some groups to pursue certain goals. In
this paper, we present the first event-centric study of gender biases in a
Wikipedia corpus. To facilitate the study, we curate a corpus of career and
personal life descriptions with demographic information consisting of 7,854
fragments from 10,412 celebrities. Then we detect events with a
state-of-the-art event detection model, calibrate the results using
strategically generated templates, and extract events that have asymmetric
associations with genders. Our study discovers that the Wikipedia pages tend to
intermingle personal life events with professional events for females but not
for males, which calls for the awareness of the Wikipedia community to
formalize guidelines and train the editors to mind the implicit biases that
contributors carry. Our work also lays the foundation for future works on
quantifying and discovering event biases at the corpus level.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01603</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01603</id><submitter>Xianhang Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:35:43 GMT</date><size>4881kb</size><source_type>D</source_type></version><title>CT-Net: Channel Tensorization Network for Video Classification</title><authors>Kunchang Li, Xianhang Li, Yali Wang, Jun Wang and Yu Qiao</authors><categories>cs.CV</categories><comments>ICLR 2021. Code is available on https://github.com/Andy1621/CT-Net</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D convolution is powerful for video classification but often computationally
expensive, recent studies mainly focus on decomposing it on spatial-temporal
and/or channel dimensions. Unfortunately, most approaches fail to achieve a
preferable balance between convolutional efficiency and feature-interaction
sufficiency. For this reason, we propose a concise and novel Channel
Tensorization Network (CT-Net), by treating the channel dimension of input
feature as a multiplication of K sub-dimensions. On one hand, it naturally
factorizes convolution in a multiple dimension way, leading to a light
computation burden. On the other hand, it can effectively enhance feature
interaction from different channels, and progressively enlarge the 3D receptive
field of such interaction to boost classification accuracy. Furthermore, we
equip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to
exploit spatial, temporal and channel attention in a high-dimensional manner,
to improve the cooperative power of all the feature dimensions in our
CT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive
experiments are conducted on several challenging video benchmarks, e.g.,
Kinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of
recent SOTA approaches, in terms of accuracy and/or efficiency. The codes and
models will be available on https://github.com/Andy1621/CT-Net.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01604</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01604</id><submitter>Hyun-Jin Park</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:36:18 GMT</date><size>519kb</size><source_type>D</source_type></version><title>Noisy student-teacher training for robust keyword spotting</title><authors>Hyun-Jin Park, Pai Zhu, Ignacio Lopez Moreno, Niranjan Subrahmanya</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose self-training with noisy student-teacher approach for streaming
keyword spotting, that can utilize large-scale unlabeled data and aggressive
data augmentation. The proposed method applies aggressive data augmentation
(spectral augmentation) on the input of both student and teacher and utilize
unlabeled data at scale, which significantly boosts the accuracy of student
against challenging conditions. Such aggressive augmentation usually degrades
model performance when used with supervised training with hard-labeled data.
Experiments show that aggressive spec augmentation on baseline supervised
training method degrades accuracy, while the proposed self-training with noisy
student-teacher training improves accuracy of some difficult-conditioned test
sets by as much as 60%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01606</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01606</id><submitter>Yinpeng Dong</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:39:57 GMT</date><size>3074kb</size><source_type>D</source_type></version><title>Exploring Memorization in Adversarial Training</title><authors>Yinpeng Dong, Ke Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, Jun
  Zhu</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that deep learning models have a propensity for fitting the
entire training set even with random labels, which requires memorization of
every training sample. In this paper, we investigate the memorization effect in
adversarial training (AT) for promoting a deeper understanding of capacity,
convergence, generalization, and especially robust overfitting of adversarially
trained classifiers. We first demonstrate that deep networks have sufficient
capacity to memorize adversarial examples of training data with completely
random labels, but not all AT algorithms can converge under the extreme
circumstance. Our study of AT with random labels motivates further analyses on
the convergence and generalization of AT. We find that some AT methods suffer
from a gradient instability issue, and the recently suggested complexity
measures cannot explain robust generalization by considering models trained on
random labels. Furthermore, we identify a significant drawback of memorization
in AT that it could result in robust overfitting. We then propose a new
mitigation algorithm motivated by detailed memorization analyses. Extensive
experiments on various datasets validate the effectiveness of the proposed
method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01607</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01607</id><submitter>Satyapriya Krishna</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:45:21 GMT</date><size>9652kb</size><source_type>D</source_type></version><title>Grounding Complex Navigational Instructions Using Scene Graphs</title><authors>Michiel de Jong, Satyapriya Krishna, Anuva Agarwal</authors><categories>cs.LG cs.CL cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1706.07230 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Training a reinforcement learning agent to carry out natural language
instructions is limited by the available supervision, i.e. knowing when the
instruction has been carried out. We adapt the CLEVR visual question answering
dataset to generate complex natural language navigation instructions and
accompanying scene graphs, yielding an environment-agnostic supervised dataset.
To demonstrate the use of this data set, we map the scenes to the VizDoom
environment and use the architecture in \citet{gatedattention} to train an
agent to carry out these more complex language instructions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01608</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01608</id><submitter>Ethan Shi</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:52:58 GMT</date><size>8343kb</size><source_type>D</source_type></version><title>A Discussion On the Validity of Manifold Learning</title><authors>Dai Shi, Andi Han, Yi Guo, and Junbin Gao</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dimensionality reduction (DR) and manifold learning (ManL) have been applied
extensively in many machine learning tasks, including signal processing, speech
recognition, and neuroinformatics. However, the understanding of whether DR and
ManL models can generate valid learning results remains unclear. In this work,
we investigate the validity of learning results of some widely used DR and ManL
methods through the chart mapping function of a manifold. We identify a
fundamental problem of these methods: the mapping functions induced by these
methods violate the basic settings of manifolds, and hence they are not
learning manifold in the mathematical sense. To address this problem, we
provide a provably correct algorithm called fixed points Laplacian mapping
(FPLM), that has the geometric guarantee to find a valid manifold
representation (up to a homeomorphism). Combining one additional
condition(orientation preserving), we discuss a sufficient condition for an
algorithm to be bijective for any d-simplex decomposition result on a
d-manifold. However, constructing such a mapping function and its computational
method satisfying these conditions is still an open problem in mathematics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01609</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01609</id><submitter>Piji Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 05:56:57 GMT</date><size>185kb</size><source_type>D</source_type></version><title>Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese
  Grammatical Error Correction</title><authors>Piji Li and Shuming Shi</authors><categories>cs.CL cs.AI</categories><comments>Accepted in the main conference of ACL 2021. Code:
  https://github.com/lipiji/TtT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of Chinese Grammatical Error Correction (CGEC) and
present a new framework named Tail-to-Tail (\textbf{TtT}) non-autoregressive
sequence prediction to address the deep issues hidden in CGEC. Considering that
most tokens are correct and can be conveyed directly from source to target, and
the error positions can be estimated and corrected based on the bidirectional
context information, thus we employ a BERT-initialized Transformer Encoder as
the backbone model to conduct information modeling and conveying. Considering
that only relying on the same position substitution cannot handle the
variable-length correction cases, various operations such substitution,
deletion, insertion, and local paraphrasing are required jointly. Therefore, a
Conditional Random Fields (CRF) layer is stacked on the up tail to conduct
non-autoregressive sequence prediction by modeling the token dependencies.
Since most tokens are correct and easily to be predicted/conveyed to the
target, then the models may suffer from a severe class imbalance issue. To
alleviate this problem, focal loss penalty strategies are integrated into the
loss functions. Moreover, besides the typical fix-length error correction
datasets, we also construct a variable-length corpus to conduct experiments.
Experimental results on standard datasets, especially on the variable-length
datasets, demonstrate the effectiveness of TtT in terms of sentence-level
Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and
Correction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01613</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01613</id><submitter>Chun-Hao Chang</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:20:18 GMT</date><size>1481kb</size><source_type>D</source_type></version><title>NODE-GAM: Neural Generalized Additive Model for Interpretable Deep
  Learning</title><authors>Chun-Hao Chang, Rich Caruana, Anna Goldenberg</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deployment of machine learning models in real high-risk settings (e.g.
healthcare) often depends not only on model's accuracy but also on its
fairness, robustness and interpretability. Generalized Additive Models (GAMs)
have a long history of use in these high-risk domains, but lack desirable
features of deep learning such as differentiability and scalability. In this
work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that
scale well to large datasets, while remaining interpretable and accurate. We
show that our proposed models have comparable accuracy to other
non-interpretable models, and outperform other GAMs on large datasets. We also
show that our models are more accurate in self-supervised learning setting when
access to labeled data is limited.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01615</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01615</id><submitter>Quanyu Liao</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:25:04 GMT</date><size>4771kb</size><source_type>D</source_type></version><title>Imperceptible Adversarial Examples for Fake Image Detection</title><authors>Quanyu Liao, Yuezun Li, Xin Wang, Bin Kong, Bin Zhu, Siwei Lyu,
  Youbing Yin, Qi Song, Xi Wu</authors><categories>cs.CV cs.AI</categories><comments>Accepted by ICIP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fooling people with highly realistic fake images generated with Deepfake or
GANs brings a great social disturbance to our society. Many methods have been
proposed to detect fake images, but they are vulnerable to adversarial
perturbations -- intentionally designed noises that can lead to the wrong
prediction. Existing methods of attacking fake image detectors usually generate
adversarial perturbations to perturb almost the entire image. This is redundant
and increases the perceptibility of perturbations. In this paper, we propose a
novel method to disrupt the fake image detection by determining key pixels to a
fake image detector and attacking only the key pixels, which results in the
$L_0$ and the $L_2$ norms of adversarial perturbations much less than those of
existing works. Experiments on two public datasets with three fake image
detectors indicate that our proposed method achieves state-of-the-art
performance in both white-box and black-box attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01617</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01617</id><submitter>Pengfei Xie</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:36:38 GMT</date><size>6305kb</size><source_type>D</source_type></version><title>Improving the Transferability of Adversarial Examples with New Iteration
  Framework and Input Dropout</title><authors>Pengfei Xie, Linyuan Wang, Ruoxi Qin, Kai Qiao, Shuhao Shi, Guoen Hu,
  Bin Yan</authors><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01618</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01618</id><submitter>Quanyu Liao</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:38:15 GMT</date><size>3071kb</size><source_type>D</source_type></version><title>Transferable Adversarial Examples for Anchor Free Object Detection</title><authors>Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Bin Zhu, Youbing Yin, Qi
  Song, Xi Wu</authors><categories>cs.CV</categories><comments>Accepted as oral in ICME 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have been demonstrated to be vulnerable to adversarial
attacks: subtle perturbation can completely change prediction result. The
vulnerability has led to a surge of research in this direction, including
adversarial attacks on object detection networks. However, previous studies are
dedicated to attacking anchor-based object detectors. In this paper, we present
the first adversarial attack on anchor-free object detectors. It conducts
category-wise, instead of previously instance-wise, attacks on object
detectors, and leverages high-level semantic information to efficiently
generate transferable adversarial examples, which can also be transferred to
attack other object detectors, even anchor-based detectors such as Faster
R-CNN. Experimental results on two benchmark datasets demonstrate that our
proposed method achieves state-of-the-art performance and transferability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01621</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01621</id><submitter>Sergey Verbitskiy</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:45:19 GMT</date><size>144kb</size><source_type>D</source_type></version><title>ERANNs: Efficient Residual Audio Neural Networks for Audio Pattern
  Recognition</title><authors>Sergey Verbitskiy and Viacheslav Vyshegorodtsev</authors><categories>cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a new architecture of convolutional neural networks (CNNs) based
on ResNet for audio pattern recognition tasks. The main modification is
introducing a new hyper-parameter for decreasing temporal sizes of tensors with
increased stride sizes which we call &quot;the decreasing temporal size parameter&quot;.
Optimal values of this parameter decrease the number of multi-adds that make
the system faster. This approach not only decreases computational complexity
but it can save and even increase (for the AudioSet dataset) the performance
for audio pattern recognition tasks. This observation can be confirmed by
experiments on three datasets: the AudioSet dataset, the ESC-50 dataset, and
RAVDESS. Our best system achieves the state-of-the-art performance on the
AudioSet dataset with mAP of 0.450. We also transfer a model pre-trained on the
AudioSet dataset to the ESC-50 dataset and RAVDESS and obtain the
state-of-the-art results with accuracies of 0.961 and 0.748, respectively. We
call our system &quot;ERANN&quot; (Efficient Residual Audio Neural Network).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01623</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01623</id><submitter>Junyi Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:48:00 GMT</date><size>704kb</size><source_type>D</source_type></version><title>Few-shot Knowledge Graph-to-Text Generation with Pretrained Language
  Models</title><authors>Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing
  Yuan and Ji-Rong Wen</authors><categories>cs.CL</categories><comments>Accepted to ACL 2021 Findings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies how to automatically generate a natural language text that
describes the facts in knowledge graph (KG). Considering the few-shot setting,
we leverage the excellent capacities of pretrained language models (PLMs) in
language understanding and generation. We make three major technical
contributions, namely representation alignment for bridging the semantic gap
between KG encodings and PLMs, relation-biased KG linearization for deriving
better input representations, and multi-task learning for learning the
correspondence between KG and text. Extensive experiments on three benchmark
datasets have demonstrated the effectiveness of our model on KG-to-text
generation task. In particular, our model outperforms all comparison methods on
both fully-supervised and few-shot settings. Our code and datasets are
available at https://github.com/RUCAIBox/Few-Shot-KG2Text.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01624</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01624</id><submitter>Kumar Abhishek</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:49:44 GMT</date><size>3076kb</size><source_type>D</source_type></version><title>Sleeping Combinatorial Bandits</title><authors>Kumar Abhishek, Ganesh Ghalme, Sujit Gujar, Yadati Narahari</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we study an interesting combination of sleeping and
combinatorial stochastic bandits. In the mixed model studied here, at each
discrete time instant, an arbitrary \emph{availability set} is generated from a
fixed set of \emph{base} arms. An algorithm can select a subset of arms from
the \emph{availability set} (sleeping bandits) and receive the corresponding
reward along with semi-bandit feedback (combinatorial bandits).
  We adapt the well-known CUCB algorithm in the sleeping combinatorial bandits
setting and refer to it as \CSUCB. We prove -- under mild smoothness conditions
-- that the \CSUCB\ algorithm achieves an $O(\log (T))$ instance-dependent
regret guarantee. We further prove that (i) when the range of the rewards is
bounded, the regret guarantee of \CSUCB\ algorithm is $O(\sqrt{T \log (T)})$
and (ii) the instance-independent regret is $O(\sqrt[3]{T^2 \log(T)})$ in a
general setting. Our results are quite general and hold under general
environments -- such as non-additive reward functions, volatile arm
availability, a variable number of base-arms to be pulled -- arising in
practical applications. We validate the proven theoretical guarantees through
experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01625</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01625</id><submitter>Wanzheng Zhu</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:54:03 GMT</date><size>389kb</size><source_type>D</source_type></version><title>Generate, Prune, Select: A Pipeline for Counterspeech Generation against
  Online Hate Speech</title><authors>Wanzheng Zhu and Suma Bhat</authors><categories>cs.CL</categories><comments>The 59th Annual Meeting of the Association for Computational
  Linguistics and the 11th International Joint Conference on Natural Language
  Processing (ACL-IJCNLP): Findings</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Countermeasures to effectively fight the ever increasing hate speech online
without blocking freedom of speech is of great social interest. Natural
Language Generation (NLG), is uniquely capable of developing scalable
solutions. However, off-the-shelf NLG methods are primarily
sequence-to-sequence neural models and they are limited in that they generate
commonplace, repetitive and safe responses regardless of the hate speech (e.g.,
&quot;Please refrain from using such language.&quot;) or irrelevant responses, making
them ineffective for de-escalating hateful conversations. In this paper, we
design a three-module pipeline approach to effectively improve the diversity
and relevance. Our proposed pipeline first generates various counterspeech
candidates by a generative model to promote diversity, then filters the
ungrammatical ones using a BERT model, and finally selects the most relevant
counterspeech response using a novel retrieval-based method. Extensive
Experiments on three representative datasets demonstrate the efficacy of our
approach in generating diverse and relevant counterspeech.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01627</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01627</id><submitter>Jan Wolff</submitter><version version="v1"><date>Thu, 3 Jun 2021 06:56:09 GMT</date><size>56kb</size></version><title>Piercing the Veil: Designs to Support Information Literacy on Social
  Platforms</title><authors>Jan Wolff</authors><categories>cs.HC cs.SI</categories><comments>Originally submitted to and presented at CHI'21 Workshop on
  Technologies to Support Critical Thinking in an Age of Misinformation</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this position paper we approach problems concerning critical digital and
information literacy with ideas to provide more digestible explanations of
abstract concepts through interface design. In particular, we focus on social
media platforms where we see the possibility of counteracting the spread of
misinformation by providing users with more proficiency through our approaches.
We argue that the omnipresent trend to abstract away and hide information from
users via UI/UX design opposes their ability to self-learn. This leads us to
propose a different framework in which we unify elegant and simple interfaces
with nudges that promote a look behind the curtain. Such designs serve to
foster a deeper understanding of employed technologies and aim to increase the
critical assessment of content encountered on social platforms. Furthermore, we
consider users with an intermediary skill level to be largely ignored in
current approaches, as they are given no tools to broaden their knowledge
without consultation of expert material. The resulting stagnation is
exemplified by the tactics of misinformation campaigns, which exploit the
ensuing lack of information literacy and critical thinking. We propose an
approach to design that sufficiently emancipates users in both aspects by
promoting a look behind the abstraction of UI/UX so that an autonomous learning
process is given the chance to occur. Furthermore, we name ideas for future
research within this area that take our considerations into account.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01628</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01628</id><submitter>Jim de Groot</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:03:44 GMT</date><size>46kb</size><source_type>D</source_type></version><title>A Coalgebraic Approach to Dualities for Neighborhood Frames</title><authors>Guram Bezhanishvili, Nick Bezhanishvili, Jim de Groot</authors><categories>cs.LO math.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We develop a uniform coalgebraic approach to Thomason and J\'{o}nsson-Tarski
type dualities for various classes of neighborhood frames and neighborhood
algebras. In the first part of the paper we construct an endofunctor on the
category of complete and atomic Boolean algebras that is dual to the double
powerset functor on $\mathsf{Set}$. This allows us to show that Thomason
duality for neighborhood frames can be viewed as an algebra-coalgebra duality.
We generalize this approach to any class of algebras for an endofunctor
presented by one-step axioms in the language of infinitary modal logic. As a
consequence, we obtain a uniform approach to dualities for various classes of
neighborhood frames, including monotone neighborhood frames, pretopological
spaces, and topological spaces.
  In the second part of the paper we develop a coalgebraic approach to
J\'{o}nsson-Tarski duality for neighborhood algebras and descriptive
neighborhood frames. We introduce an analogue of the Vietoris endofunctor on
the category of Stone spaces and show that descriptive neighborhood frames are
isomorphic to coalgebras for this endofunctor. This allows us to obtain a
coalgebraic proof of the duality between descriptive neighborhood frames and
neighborhood algebras. Using one-step axioms in the language of finitary modal
logic, we restrict this duality to other classes of neighborhood algebras
studied in the literature, including monotone modal algebras and contingency
algebras.
  We conclude the paper by connecting the two types of dualities via canonical
extensions, and discuss when these extensions are functorial.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01629</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01629</id><submitter>Guillaume Le Moing</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:04:00 GMT</date><size>15785kb</size><source_type>D</source_type></version><title>Semantic Palette: Guiding Scene Generation with Class Proportions</title><authors>Guillaume Le Moing and Tuan-Hung Vu and Himalaya Jain and Patrick
  P\'erez and Matthieu Cord</authors><categories>cs.CV</categories><comments>Accepted to IEEE CVPR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the recent progress of generative adversarial networks (GANs) at
synthesizing photo-realistic images, producing complex urban scenes remains a
challenging problem. Previous works break down scene generation into two
consecutive phases: unconditional semantic layout synthesis and image synthesis
conditioned on layouts. In this work, we propose to condition layout generation
as well for higher semantic control: given a vector of class proportions, we
generate layouts with matching composition. To this end, we introduce a
conditional framework with novel architecture designs and learning objectives,
which effectively accommodates class proportions to guide the scene generation
process. The proposed architecture also allows partial layout editing with
interesting applications. Thanks to the semantic control, we can produce
layouts close to the real distribution, helping enhance the whole scene
generation process. On different metrics and urban scene benchmarks, our models
outperform existing baselines. Moreover, we demonstrate the merit of our
approach for data augmentation: semantic segmenters trained on real
layout-image pairs along with additional ones generated by our approach
outperform models only trained on real pairs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01632</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01632</id><submitter>Farhan Sadique</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:10:16 GMT</date><size>7260kb</size><source_type>D</source_type></version><title>Cybersecurity Information Exchange with Privacy (CYBEX-P) and TAHOE -- A
  Cyberthreat Language</title><authors>Farhan Sadique, Ignacio Astaburuaga, Raghav Kaul, Shamik Sengupta,
  Shahriar Badsha, James Schnebly, Adam Cassell, Jeff Springer, Nancy
  Latourrette and Sergiu M. Dascalu</authors><categories>cs.CR cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Cybersecurity information sharing (CIS) is envisioned to protect
organizations more effectively from advanced cyber attacks. However, a
completely automated CIS platform is not widely adopted. The major challenges
are: (1) the absence of a robust cyber threat language (CTL) and (2) the
concerns over data privacy. This work introduces Cybersecurity Information
Exchangewith Privacy (CYBEX-P), as a CIS framework, to tackle these challenges.
CYBEX-P allows organizations to share heterogeneous data with granular,
attribute based privacy control. It correlates the data to automatically
generate intuitive reports and defensive rules. To achieve such versatility, we
have developed TAHOE - a graph based CTL. TAHOE is a structure for
storing,sharing and analyzing threat data. It also intrinsically correlates the
data. We have further developed a universal Threat Data Query Language (TDQL).
In this paper, we propose the system architecture for CYBEX-P. We then discuss
its scalability and privacy features along with a use case of CYBEX-P providing
Infrastructure as a Service (IaaS). We further introduce TAHOE&amp; TDQL as better
alternatives to existing CTLs and formulate ThreatRank - an algorithm to detect
new malicious even
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01635</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01635</id><submitter>Venelin Kovatchev</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:12:00 GMT</date><size>67kb</size><source_type>D</source_type></version><title>Can vectors read minds better than experts? Comparing data augmentation
  strategies for the automated scoring of children's mindreading ability</title><authors>Venelin Kovatchev, Phillip Smith, Mark Lee, and Rory Devine</authors><categories>cs.CL cs.LG</categories><comments>The paper will be presented at ACL-IJCNLP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we implement and compare 7 different data augmentation
strategies for the task of automatic scoring of children's ability to
understand others' thoughts, feelings, and desires (or &quot;mindreading&quot;).
  We recruit in-domain experts to re-annotate augmented samples and determine
to what extent each strategy preserves the original rating. We also carry out
multiple experiments to measure how much each augmentation strategy improves
the performance of automatic scoring systems. To determine the capabilities of
automatic systems to generalize to unseen data, we create UK-MIND-20 - a new
corpus of children's performance on tests of mindreading, consisting of 10,320
question-answer pairs.
  We obtain a new state-of-the-art performance on the MIND-CA corpus, improving
macro-F1-score by 6 points. Results indicate that both the number of training
examples and the quality of the augmentation strategies affect the performance
of the systems. The task-specific augmentations generally outperform
task-agnostic augmentations. Automatic augmentations based on vectors (GloVe,
FastText) perform the worst.
  We find that systems trained on MIND-CA generalize well to UK-MIND-20. We
demonstrate that data augmentation strategies also improve the performance on
unseen data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01639</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01639</id><submitter>Chathura Gamage</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:20:30 GMT</date><size>8270kb</size><source_type>D</source_type></version><title>Deceptive Level Generation for Angry Birds</title><authors>Chathura Gamage, Matthew Stephenson, Vimukthini Pinto, Jochen Renz</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Angry Birds AI competition has been held over many years to encourage the
development of AI agents that can play Angry Birds game levels better than
human players. Many different agents with various approaches have been employed
over the competition's lifetime to solve this task. Even though the performance
of these agents has increased significantly over the past few years, they still
show major drawbacks in playing deceptive levels. This is because most of the
current agents try to identify the best next shot rather than planning an
effective sequence of shots. In order to encourage advancements in such agents,
we present an automated methodology to generate deceptive game levels for Angry
Birds. Even though there are many existing content generators for Angry Birds,
they do not focus on generating deceptive levels. In this paper, we propose a
procedure to generate deceptive levels for six deception categories that can
fool the state-of-the-art Angry Birds playing AI agents. Our results show that
generated deceptive levels exhibit similar characteristics of human-created
deceptive levels. Additionally, we define metrics to measure the stability,
solvability, and degree of deception of the generated levels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01642</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01642</id><submitter>Cheng Yang</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:22:48 GMT</date><size>366kb</size><source_type>D</source_type></version><title>Projection-free Graph-based Classifier Learning using Gershgorin Disc
  Perfect Alignment</title><authors>Cheng Yang, Gene Cheung, Wai-tian Tan, Guangtao Zhai</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In semi-supervised graph-based binary classifier learning, a subset of known
labels $\hat{x}_i$ are used to infer unknown labels, assuming that the label
signal $x$ is smooth with respect to a similarity graph specified by a
Laplacian matrix. When restricting labels $x_i$ to binary values, the problem
is NP-hard. While a conventional semi-definite programming (SDP) relaxation can
be solved in polynomial time using, for example, the alternating direction
method of multipliers (ADMM), the complexity of iteratively projecting a
candidate matrix $M$ onto the positive semi-definite (PSD) cone ($M \succeq 0$)
remains high. In this paper, leveraging a recent linear algebraic theory called
Gershgorin disc perfect alignment (GDPA), we propose a fast projection-free
method by solving a sequence of linear programs (LP) instead. Specifically, we
first recast the SDP relaxation to its SDP dual, where a feasible solution $H
\succeq 0$ can be interpreted as a Laplacian matrix corresponding to a balanced
signed graph sans the last node. To achieve graph balance, we split the last
node into two that respectively contain the original positive and negative
edges, resulting in a new Laplacian $\bar{H}$. We repose the SDP dual for
solution $\bar{H}$, then replace the PSD cone constraint $\bar{H} \succeq 0$
with linear constraints derived from GDPA -- sufficient conditions to ensure
$\bar{H}$ is PSD -- so that the optimization becomes an LP per iteration.
Finally, we extract predicted labels from our converged LP solution $\bar{H}$.
Experiments show that our algorithm enjoyed a $40\times$ speedup on average
over the next fastest scheme while retaining comparable label prediction
performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01644</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01644</id><submitter>Andrea Fronzetti Colladon PhD</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:25:26 GMT</date><size>324kb</size></version><title>Corporate core values and social responsibility: What really matters to
  whom</title><authors>M. A. Barchiesi, A. Fronzetti Colladon</authors><categories>cs.CL cs.SI physics.soc-ph</categories><acm-class>I.2.7; J.4; H.4.0</acm-class><journal-ref>Technological Forecasting and Social Change 170, 120907 (2021)</journal-ref><doi>10.1016/j.techfore.2021.120907</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study uses an innovative measure, the Semantic Brand Score, to assess
the interest of stakeholders in different company core values. Among others, we
focus on corporate social responsibility (CSR) core value statements, and on
the attention they receive from five categories of stakeholders (customers,
company communication teams, employees, associations and media). Combining big
data methods and tools of Social Network Analysis and Text Mining, we analyzed
about 58,000 Italian tweets and found that different stakeholders have
different prevailing interests. CSR gets much less attention than expected.
Core values related to customers and employees are in the foreground.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01645</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01645</id><submitter>Chuan-Ju Wang</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:25:26 GMT</date><size>37kb</size></version><title>R\'enyi Divergence in General Hidden Markov Models</title><authors>Cheng-Der Fuh, Su-Chi Fuh, Yuan-Chen Liu, and Chuan-Ju Wang</authors><categories>cs.IT math.IT</categories><comments>39 pages</comments><msc-class>ACM-class: E.4</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we examine the existence of the R\'enyi divergence between two
time invariant general hidden Markov models with arbitrary positive initial
distributions. By making use of a Markov chain representation of the
probability distribution for the general hidden Markov model and eigenvalue for
the associated Markovian operator, we obtain, under some regularity conditions,
convergence of the R\'enyi divergence. By using this device, we also
characterize the R\'enyi divergence, and obtain the Kullback-Leibler divergence
as {\alpha} \rightarrow 1 of the R\'enyi divergence. Several examples,
including the classical finite state hidden Markov models, Markov switching
models, and recurrent neural networks, are given for illustration. Moreover, we
develop a non-Monte Carlo method that computes the R\'enyi divergence of
two-state Markov switching models via the underlying invariant probability
measure, which is characterized by the Fredholm integral equation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01646</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01646</id><submitter>Carolina Urz\'ua-Torres</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:33:27 GMT</date><size>15kb</size></version><title>Towards coercive boundary element methods for the wave equation</title><authors>Olaf Steinbach, Carolina Urz\'ua-Torres, Marco Zank</authors><categories>math.NA cs.NA</categories><msc-class>35L05, 65R20</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this note, we discuss the ellipticity of the single layer boundary
integral operator for the wave equation in one space dimension. This result not
only generalizes the well-known ellipticity of the energetic boundary integral
formulation in $L^2$, but it also turns out to be a particular case of a recent
result on the inf-sup stability of boundary integral operators for the wave
equation. Instead of the time derivative in the energetic formulation, we use a
modified Hilbert transformation, which allows us to stay in Sobolev spaces of
the same order. This results in the applicability of standard boundary element
error estimates, which are confirmed by numerical results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01649</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01649</id><submitter>Xinyu Zuo</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:42:20 GMT</date><size>11697kb</size><source_type>D</source_type></version><title>LearnDA: Learnable Knowledge-Guided Data Augmentation for Event
  Causality Identification</title><authors>Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng and
  Yuguang Chen</authors><categories>cs.CL</categories><comments>Accepted to ACL 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Modern models for event causality identification (ECI) are mainly based on
supervised learning, which are prone to the data lacking problem.
Unfortunately, the existing NLP-related augmentation methods cannot directly
produce the available data required for this task. To solve the data lacking
problem, we introduce a new approach to augment training data for event
causality identification, by iteratively generating new examples and
classifying event causality in a dual learning framework. On the one hand, our
approach is knowledge-guided, which can leverage existing knowledge bases to
generate well-formed new sentences. On the other hand, our approach employs a
dual mechanism, which is a learnable augmentation framework and can
interactively adjust the generation process to generate task-related sentences.
Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show
that 1) our method can augment suitable task-related training data for ECI; 2)
our method outperforms previous methods on EventStoryLine and Causal-TimeBank
(+2.5 and +2.1 points on F1 value respectively).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01650</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01650</id><submitter>Gavin Suddrey</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:47:06 GMT</date><size>20373kb</size><source_type>D</source_type></version><title>Learning and Executing Re-usable Behaviour Trees from Natural Language
  Instruction</title><authors>Gavin Suddrey, Ben Talbot and Frederic Maire</authors><categories>cs.RO cs.AI cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Domestic and service robots have the potential to transform industries such
as health care and small-scale manufacturing, as well as the homes in which we
live. However, due to the overwhelming variety of tasks these robots will be
expected to complete, providing generic out-of-the-box solutions that meet the
needs of every possible user is clearly intractable. To address this problem,
robots must therefore not only be capable of learning how to complete novel
tasks at run-time, but the solutions to these tasks must also be informed by
the needs of the user. In this paper we demonstrate how behaviour trees, a well
established control architecture in the fields of gaming and robotics, can be
used in conjunction with natural language instruction to provide a robust and
modular control architecture for instructing autonomous agents to learn and
perform novel complex tasks. We also show how behaviour trees generated using
our approach can be generalised to novel scenarios, and can be re-used in
future learning episodes to create increasingly complex behaviours. We validate
this work against an existing corpus of natural language instructions,
demonstrate the application of our approach on both a simulated robot solving a
toy problem, as well as two distinct real-world robot platforms which,
respectively, complete a block sorting scenario, and a patrol scenario.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01651</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01651</id><submitter>Ada Diaconescu Dr.</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:48:13 GMT</date><size>1639kb</size><source_type>D</source_type></version><title>Timing Configurations Affect the Macro-Properties of Multi-Scale
  Feedback Systems</title><authors>Patricia Mellodge (1), Ada Diaconescu (2) and Louisa Jane Di Felice
  (3) ((1) University of Hartford, (2) Telecom Paris, LTCI, IPP, (3) Autonomous
  University of Barcelona)</authors><categories>eess.SY cs.SY nlin.AO</categories><comments>10 pages, 11 Figures, submitted to ACSOS 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Multi-scale feedback systems, where information cycles through micro- and
macro-scales leading to adaptation, are ubiquitous across domains, from animal
societies and human organisations to electric grids and neural networks.
Studies on the effects of timing on system properties are often domain
specific. The Multi-Scale Abstraction Feedbacks (MSAF) design pattern aims to
generalise the description and understanding of multi-scale systems where
feedback occurs across scales. We expand on MSAF to include timing
considerations. We then apply these considerations to two models: a
hierarchical oscillator (HO) and a hierarchical cellular automata (HCA).
Results show how (i) different timing configurations significantly affect
system macro-properties and (ii) different regions of time configurations can
lead to the same macro-properties. These results contribute to theory, while
also providing useful insights for designing and controlling such systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01654</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01654</id><submitter>Xinyu Zuo</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:50:50 GMT</date><size>11566kb</size><source_type>D</source_type></version><title>Improving Event Causality Identification via Self-Supervised
  Representation Learning on External Causal Statement</title><authors>Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng and
  Yuguang Chen</authors><categories>cs.CL</categories><comments>Accepted to Findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Current models for event causality identification (ECI) mainly adopt a
supervised framework, which heavily rely on labeled data for training.
Unfortunately, the scale of current annotated datasets is relatively limited,
which cannot provide sufficient support for models to capture useful indicators
from causal statements, especially for handing those new, unseen cases. To
alleviate this problem, we propose a novel approach, shortly named CauSeRL,
which leverages external causal statements for event causality identification.
First of all, we design a self-supervised framework to learn context-specific
causal patterns from external causal statements. Then, we adopt a contrastive
transfer strategy to incorporate the learned context-specific causal patterns
into the target ECI model. Experimental results show that our method
significantly outperforms previous methods on EventStoryLine and
Causal-TimeBank (+2.0 and +3.4 points on F1 value respectively).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01655</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01655</id><submitter>Lorenzo Steccanella</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:53:18 GMT</date><size>673kb</size><source_type>D</source_type></version><title>Hierarchical Representation Learning for Markov Decision Processes</title><authors>Lorenzo Steccanella, Simone Totaro, Anders Jonsson</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper we present a novel method for learning hierarchical
representations of Markov decision processes. Our method works by partitioning
the state space into subsets, and defines subtasks for performing transitions
between the partitions. We formulate the problem of partitioning the state
space as an optimization problem that can be solved using gradient descent
given a set of sampled trajectories, making our method suitable for
high-dimensional problems with large state spaces. We empirically validate the
method, by showing that it can successfully learn a useful hierarchical
representation in a navigation domain. Once learned, the hierarchical
representation can be used to solve different tasks in the given domain, thus
generalizing knowledge across tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01656</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01656</id><submitter>Yu Mitsuzumi</submitter><version version="v1"><date>Thu, 3 Jun 2021 07:55:18 GMT</date><size>3086kb</size><source_type>D</source_type></version><title>Generalized Domain Adaptation</title><authors>Yu Mitsuzumi, Go Irie, Daiki Ikami and Takashi Shibata</authors><categories>cs.CV</categories><comments>Accepted by CVPR 2021. Code is available at
  https://github.com/nttcslab/Generalized-Domain-Adaptation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many variants of unsupervised domain adaptation (UDA) problems have been
proposed and solved individually. Its side effect is that a method that works
for one variant is often ineffective for or not even applicable to another,
which has prevented practical applications. In this paper, we give a general
representation of UDA problems, named Generalized Domain Adaptation (GDA). GDA
covers the major variants as special cases, which allows us to organize them in
a comprehensive framework. Moreover, this generalization leads to a new
challenging setting where existing methods fail, such as when domain labels are
unknown, and class labels are only partially given to each domain. We propose a
novel approach to the new setting. The key to our approach is self-supervised
class-destructive learning, which enables the learning of class-invariant
representations and domain-adversarial classifiers without using any domain
labels. Extensive experiments using three benchmark datasets demonstrate that
our method outperforms the state-of-the-art UDA methods in the new setting and
that it is competitive in existing UDA variations as well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01660</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01660</id><submitter>Botao Hao</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:04:33 GMT</date><size>24kb</size></version><title>Bandit Phase Retrieval</title><authors>Tor Lattimore, Botao Hao</authors><categories>stat.ML cs.LG math.ST stat.ME stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study a bandit version of phase retrieval where the learner chooses
actions $(A_t)_{t=1}^n$ in the $d$-dimensional unit ball and the expected
reward is $\langle A_t, \theta_\star\rangle^2$ where $\theta_\star \in \mathbb
R^d$ is an unknown parameter vector. We prove that the minimax cumulative
regret in this problem is $\smash{\tilde \Theta(d \sqrt{n})}$, which improves
on the best known bounds by a factor of $\smash{\sqrt{d}}$. We also show that
the minimax simple regret is $\smash{\tilde \Theta(d / \sqrt{n})}$ and that
this is only achievable by an adaptive algorithm. Our analysis shows that an
apparently convincing heuristic for guessing lower bounds can be misleading and
that uniform bounds on the information ratio for information-directed sampling
are not sufficient for optimal regret.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01666</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01666</id><submitter>Kai-Hui Liang</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:16:25 GMT</date><size>3813kb</size><source_type>D</source_type></version><title>Discovering Chatbot's Self-Disclosure's Impact on User Trust, Affinity,
  and Recommendation Effectiveness</title><authors>Kai-Hui Liang, Weiyan Shi, Yoojung Oh, Jingwen Zhang, Zhou Yu</authors><categories>cs.CL cs.AI</categories><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, chatbots have been empowered to engage in social
conversations with humans and have the potential to elicit people to disclose
their personal experiences, opinions, and emotions. However, how and to what
extent people respond to chabots' self-disclosure remain less known. In this
work, we designed a social chatbot with three self-disclosure levels that
conducted small talks and provided relevant recommendations to people. 372
MTurk participants were randomized to one of the four groups with different
self-disclosure levels to converse with the chatbot on two topics, movies, and
COVID-19. We found that people's self-disclosure level was strongly reciprocal
to a chatbot's self-disclosure level. Chatbots' self-disclosure also positively
impacted engagement and users' perception of the bot and led to a more
effective recommendation such that participants enjoyed and agreed more with
the recommendations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01667</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01667</id><submitter>Juan Leon Alcazar</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:16:42 GMT</date><size>7813kb</size><source_type>D</source_type></version><title>APES: Audiovisual Person Search in Untrimmed Video</title><authors>Juan Leon Alcazar, Long Mai, Federico Perazzi, Joon-Young Lee, Pablo
  Arbelaez, Bernard Ghanem, and Fabian Caba Heilbron</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans are arguably one of the most important subjects in video streams, many
real-world applications such as video summarization or video editing workflows
often require the automatic search and retrieval of a person of interest.
Despite tremendous efforts in the person reidentification and retrieval
domains, few works have developed audiovisual search strategies. In this paper,
we present the Audiovisual Person Search dataset (APES), a new dataset composed
of untrimmed videos whose audio (voices) and visual (faces) streams are densely
annotated. APES contains over 1.9K identities labeled along 36 hours of video,
making it the largest dataset available for untrimmed audiovisual person
search. A key property of APES is that it includes dense temporal annotations
that link faces to speech segments of the same identity. To showcase the
potential of our new dataset, we propose an audiovisual baseline and benchmark
for person retrieval. Our study shows that modeling audiovisual cues benefits
the recognition of people's identities. To enable reproducibility and promote
future research, the dataset annotations and baseline code are available at:
https://github.com/fuankarion/audiovisual-person-search
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01671</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01671</id><submitter>Siyuan Niu</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:20:40 GMT</date><size>140kb</size><source_type>D</source_type></version><title>Analyzing crosstalk error in the NISQ era</title><authors>Siyuan Niu (LIRMM), Aida Todri-Sanial (LIRMM, CNRS)</authors><categories>cs.AR quant-ph</categories><proxy>ccsd</proxy><journal-ref>IEEE Computer Society Annual Symposium on VLSI 2021, Jul 2021,
  Tampa, Florida, United States</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noisy Intermediate-Scale Quantum (NISQ) hardware has unavoidable noises, and
crosstalk error is a significant error source. When multiple quantum operations
are executed simultaneously, the quantum state can be corrupted due to the
crosstalk between gates during simultaneous operations, decreasing the circuit
fidelity. In this work, we first report on several protocols for characterizing
crosstalk. Then, we discuss different crosstalk mitigation methods from the
hardware and software perspectives. Finally, we perform crosstalk injection
experiments on the IBM quantum device and demonstrate the fidelity improvement
with the crosstalk mitigation method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01674</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01674</id><submitter>Hao Liu</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:23:24 GMT</date><size>2452kb</size><source_type>D</source_type></version><title>JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale
  Online Inference at Baidu</title><authors>Hao Liu, Qian Gao, Jiang Li, Xiaochao Liao, Hao Xiong, Guangxing Chen,
  Wenlin Wang, Guobao Yang, Zhiwei Zha, Daxiang Dong, Dejing Dou, Haoyi Xiong</authors><categories>cs.IR cs.DC cs.LG</categories><comments>Accepted to SIGKDD 2021 applied data science track</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01678</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01678</id><submitter>Mingyi Liu</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:25:42 GMT</date><size>1321kb</size></version><title>Learning Representation over Dynamic Graph using Aggregation-Diffusion
  Mechanism</title><authors>Mingyi Liu and Zhiying Tu and Xiaofei Xu and Zhongjie Wang</authors><categories>cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation learning on graphs that evolve has recently received
significant attention due to its wide application scenarios, such as
bioinformatics, knowledge graphs, and social networks. The propagation of
information in graphs is important in learning dynamic graph representations,
and most of the existing methods achieve this by aggregation. However, relying
only on aggregation to propagate information in dynamic graphs can result in
delays in information propagation and thus affect the performance of the
method. To alleviate this problem, we propose an aggregation-diffusion (AD)
mechanism that actively propagates information to its neighbor by diffusion
after the node updates its embedding through the aggregation mechanism. In
experiments on two real-world datasets in the dynamic link prediction task, the
AD mechanism outperforms the baseline models that only use aggregation to
propagate information. We further conduct extensive experiments to discuss the
influence of different factors in the AD mechanism.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01680</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01680</id><submitter>Junyoung Park</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:29:17 GMT</date><size>2196kb</size><source_type>D</source_type></version><title>Convergent Graph Solvers</title><authors>Junyoung Park, Jinhyun Choo, Jinkyoo Park</authors><categories>cs.LG cs.AI</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the convergent graph solver (CGS), a deep learning method that
learns iterative mappings to predict the properties of a graph system at its
stationary state (fixed point) with guaranteed convergence. CGS systematically
computes the fixed points of a target graph system and decodes them to estimate
the stationary properties of the system without the prior knowledge of existing
solvers or intermediate solutions. The forward propagation of CGS proceeds in
three steps: (1) constructing the input dependent linear contracting iterative
maps, (2) computing the fixed-points of the linear maps, and (3) decoding the
fixed-points to estimate the properties. The contractivity of the constructed
linear maps guarantees the existence and uniqueness of the fixed points
following the Banach fixed point theorem. To train CGS efficiently, we also
derive a tractable analytical expression for its gradient by leveraging the
implicit function theorem. We evaluate the performance of CGS by applying it to
various network-analytic and graph benchmark problems. The results indicate
that CGS has competitive capabilities for predicting the stationary properties
of graph systems, irrespective of whether the target systems are linear or
non-linear. CGS also shows high performance for graph classification problems
where the existence or the meaning of a fixed point is hard to be clearly
defined, which highlights the potential of CGS as a general graph neural
network architecture.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01682</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01682</id><submitter>Olivier Sprangers</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:32:13 GMT</date><size>1471kb</size><source_type>D</source_type></version><title>Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic
  Regression</title><authors>Olivier Sprangers, Sebastian Schelter, Maarten de Rijke</authors><categories>cs.LG stat.ML</categories><doi>10.1145/3447548.3467278</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gradient Boosting Machines (GBM) are hugely popular for solving tabular data
problems. However, practitioners are not only interested in point predictions,
but also in probabilistic predictions in order to quantify the uncertainty of
the predictions. Creating such probabilistic predictions is difficult with
existing GBM-based solutions: they either require training multiple models or
they become too computationally expensive to be useful for large-scale
settings. We propose Probabilistic Gradient Boosting Machines (PGBM), a method
to create probabilistic predictions with a single ensemble of decision trees in
a computationally efficient manner. PGBM approximates the leaf weights in a
decision tree as a random variable, and approximates the mean and variance of
each sample in a dataset via stochastic tree ensemble update equations. These
learned moments allow us to subsequently sample from a specified distribution
after training. We empirically demonstrate the advantages of PGBM compared to
existing state-of-the-art methods: (i) PGBM enables probabilistic estimates
without compromising on point performance in a single model, (ii) PGBM learns
probabilistic estimates via a single model only (and without requiring
multi-parameter boosting), and thereby offers a speedup of up to several orders
of magnitude over existing state-of-the-art methods on large datasets, and
(iii) PGBM achieves accurate probabilistic estimates in tasks with complex
differentiable loss functions, such as hierarchical time series problems, where
we observed up to 10\% improvement in point forecasting performance and up to
300\% improvement in probabilistic forecasting performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01683</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01683</id><submitter>Udaya B Rongala</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:32:43 GMT</date><size>5747kb</size><source_type>D</source_type></version><title>Rich dynamics caused by known biological brain network features
  resulting in stateful networks</title><authors>Udaya B. Rongala and Henrik J\&quot;orntell</authors><categories>q-bio.NC cs.AI cs.NE</categories><comments>13 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mammalian brain could contain dense and sparse network connectivity
structures, including both excitatory and inhibitory neurons, but is without
any clearly defined output layer. The neurons have time constants, which mean
that the integrated network structure has state memory. The network structure
contains complex mutual interactions between the neurons under different
conditions, which depend on the internal state of the network. The internal
state can be defined as the distribution of activity across all individual
neurons across the network. Therefore, the state of a neuron/network becomes a
defining factor for how information is represented within the network. Towards
this study, we constructed a fully connected (with dense/sparse coding
strategies) recurrent network comprising of both excitatory and inhibitory
neurons, driven by pseudo-random inputs of varying frequencies. In this study
we assessed the impact of varying specific intrinsic parameters of the neurons
that enriched network state dynamics, such as initial neuron activity, amount
of inhibition in combination with thresholded neurons and conduction delays.
The impact was assessed by quantifying the changes in mutual interactions
between the neurons within the network for each given input. We found such
effects were more profound in sparsely connected networks than in densely
connected networks. However, also densely connected networks could make use of
such dynamic changes in the mutual interactions between neurons, as a given
input could induce multiple different network states.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01684</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01684</id><submitter>Susmita Bhaduri</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:38:22 GMT</date><size>28kb</size></version><title>Language Independent Speech Emotion and Non-invasive Early Detection of
  Neurocognitive Disorder</title><authors>Susmita Bhaduri, Anirban Bhaduri, Rajib Sarkar</authors><categories>cs.SD eess.AS q-bio.NC</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Emotions(like fear,anger,sadness,happiness etc.) are the fundamental features
of human behavior and governs his/her mental health. The subtlety of emotional
fluctuations can be examined through perturbation in conversations or speech.
Analysis of emotional state of a person from acoustical features of speech
signal leads to discovery of vital cues determining his or her mental health.
Hence, it's an important field of research in the area of Human Computer
Interaction(HCI). In a recent work we have shown that how the contrast in
Hurst-Exponent calculated from the non-stationary and nonlinear aspects of
&quot;angry&quot; and &quot;sad&quot; speech(spoken in English language) recordings in the
Toronto-Emotional-Speech-Set(TESS) can be used for early detection and
diagnosis of Alzheimer's Disease. In this work we have extended the work and
extracted Hurst-exponent for the speech-signals of similar emotions but spoken
in German language. It has been observed that the Hurst-exponent efficiently
segregates the contrasting emotions of &quot;anger&quot; and &quot;sadness&quot; in the speech
spoken in German language, in similar fashion it has been doing for English
speech. Hence it can be concluded that the Hurst-exponent can differentiate
among speech spoken out of different emotions in language-independent manner.
We propose algorithm for a language-independent application for early
non-invasive detection of various severe neurocognitive-disorders like
Alzheimer's Disease, MND(motor-neuron-disorder), ASD(autism-spectrum-disorder),
depression, suicidal-tendency etc. which is not possible with the state of the
art medical science.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01686</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01686</id><submitter>Ningyu Zhang</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:44:03 GMT</date><size>10190kb</size><source_type>D</source_type></version><title>AliCG: Fine-grained and Evolvable Conceptual Graph Construction for
  Semantic Search at Alibaba</title><authors>Ningyu Zhang, Qianghuai Jia, Shumin Deng, Xiang Chen, Hongbin Ye, Hui
  Chen, Huaixiao Tou, Gang Huang, Zhao Wang, Nengwei Hua, Huajun Chen</authors><categories>cs.AI</categories><comments>Accepted by KDD 2021 (Applied Data Science Track)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conceptual graphs, which is a particular type of Knowledge Graphs, play an
essential role in semantic search. Prior conceptual graph construction
approaches typically extract high-frequent, coarse-grained, and time-invariant
concepts from formal texts. In real applications, however, it is necessary to
extract less-frequent, fine-grained, and time-varying conceptual knowledge and
build taxonomy in an evolving manner. In this paper, we introduce an approach
to implementing and deploying the conceptual graph at Alibaba. Specifically, We
propose a framework called AliCG which is capable of a) extracting fine-grained
concepts by a novel bootstrapping with alignment consensus approach, b) mining
long-tail concepts with a novel low-resource phrase mining approach, c)
updating the graph dynamically via a concept distribution estimation method
based on implicit and explicit user behaviors. We have deployed the framework
at Alibaba UC Browser. Extensive offline evaluation as well as online A/B
testing demonstrate the efficacy of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01689</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01689</id><submitter>Mirco Planamente</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:46:43 GMT</date><size>27841kb</size><source_type>D</source_type></version><title>Cross-Domain First Person Audio-Visual Action Recognition through
  Relative Norm Alignment</title><authors>Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo</authors><categories>cs.CV</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First person action recognition is an increasingly researched topic because
of the growing popularity of wearable cameras. This is bringing to light
cross-domain issues that are yet to be addressed in this context. Indeed, the
information extracted from learned representations suffers from an intrinsic
environmental bias. This strongly affects the ability to generalize to unseen
scenarios, limiting the application of current methods in real settings where
trimmed labeled data are not available during training. In this work, we
propose to leverage over the intrinsic complementary nature of audio-visual
signals to learn a representation that works well on data seen during training,
while being able to generalize across different domains. To this end, we
introduce an audio-visual loss that aligns the contributions from the two
modalities by acting on the magnitude of their feature norm representations.
This new loss, plugged into a minimal multi-modal action recognition
architecture, leads to strong results in cross-domain first person action
recognition, as demonstrated by extensive experiments on the popular
EPIC-Kitchens dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01693</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01693</id><submitter>Th\'eophile Chaumont-Frelet</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:50:24 GMT</date><size>33kb</size></version><title>Bridging the Multiscale Hybrid-Mixed and Multiscale Hybrid High-Order
  methods</title><authors>T. Chaumont-Frelet and A. Ern and S. Lemaire and F. Valentin</authors><categories>math.NA cs.NA</categories><report-no>hal-03235525</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the equivalence between the Multiscale Hybrid-Mixed (MHM) and
the Multiscale Hybrid High-Order (MsHHO) methods for a variable diffusion
problem with piecewise polynomial source terms. Under the idealized assumption
that the local problems defining the multiscale basis functions are exactly
solved, we prove that the equivalence holds for general polytopal (coarse)
meshes and arbitrary approximation orders. Also, we leverage the interchange of
properties to perform a unified convergence analysis, as well as to improve on
both methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01695</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01695</id><submitter>Stephan Stahlschmidt</submitter><version version="v1"><date>Thu, 3 Jun 2021 08:53:26 GMT</date><size>833kb</size></version><title>From indexation policies through citation networks to normalized
  citation impacts: Web of Science, Scopus, and Dimensions as varying resonance
  chambers</title><authors>Stephan Stahlschmidt, Dimity Stephen</authors><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensions was introduced as an alternative bibliometric database to the
well-established Web of Science (WoS) and Scopus, however all three databases
have fundamental differences in coverage and content, resultant from their
owners' indexation philosophies. In light of these differences, we explore
here, using a citation network analysis and assessment of normalised citation
impact of &quot;duplicate&quot; publications, whether the three databases offer
structurally different perspectives of the bibliometric landscape or if they
are essentially homogenous substitutes. Our citation network analysis of core
and exclusive 2016-2018 publications revealed a large set of core publications
indexed in all three databases that are highly self-referential. In comparison,
each database selected a set of exclusive publications that appeared to hold
similarly low levels of relevance to the core set and to one another, with
slightly more internal communication between exclusive publications in Scopus
and Dimensions than WoS. Our comparison of normalised citations for 41,848
publications indexed in all three databases found that German sectors were
valuated as more impactful in Scopus and Dimensions compared to WoS,
particularly for sectors with an applied research focus. We conclude that the
databases do present structurally different perspectives, although Scopus and
Dimensions with their additional circle of applied research vary more from the
more base research-focused WoS than they do from one another.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01700</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01700</id><submitter>Neslihan Bayramoglu</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:03:31 GMT</date><size>26263kb</size><source_type>D</source_type></version><title>Machine Learning Based Texture Analysis of Patella from X-Rays for
  Detecting Patellofemoral Osteoarthritis</title><authors>Neslihan Bayramoglu, Miika T. Nieminen, Simo Saarakkala</authors><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective is to assess the ability of texture features for detecting
radiographic patellofemoral osteoarthritis (PFOA) from knee lateral view
radiographs. We used lateral view knee radiographs from MOST public use
datasets (n = 5507 knees). Patellar region-of-interest (ROI) was automatically
detected using landmark detection tool (BoneFinder). Hand-crafted features,
based on LocalBinary Patterns (LBP), were then extracted to describe the
patellar texture. First, a machine learning model (Gradient Boosting Machine)
was trained to detect radiographic PFOA from the LBP features. Furthermore, we
used end-to-end trained deep convolutional neural networks (CNNs) directly on
the texture patches for detecting the PFOA. The proposed classification models
were eventually compared with more conventional reference models that use
clinical assessments and participant characteristics such as age, sex, body
mass index(BMI), the total WOMAC score, and tibiofemoral Kellgren-Lawrence (KL)
grade. Atlas-guided visual assessment of PFOA status by expert readers provided
in the MOST public use datasets was used as a classification outcome for the
models. Performance of prediction models was assessed using the area under the
receiver operating characteristic curve (ROC AUC), the area under the
precision-recall (PR) curve-average precision (AP)-, and Brier score in the
stratified 5-fold cross validation setting.Of the 5507 knees, 953 (17.3%) had
PFOA. AUC and AP for the strongest reference model including age, sex, BMI,
WOMAC score, and tibiofemoral KL grade to predict PFOA were 0.817 and 0.487,
respectively. Textural ROI classification using CNN significantly improved the
prediction performance (ROC AUC= 0.889, AP= 0.714). We present the first study
that analyses patellar bone texture for diagnosing PFOA. Our results
demonstrates the potential of using texture features of patella to predict
PFOA.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01702</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01702</id><submitter>Hao Sun</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:06:25 GMT</date><size>1577kb</size><source_type>D</source_type></version><title>PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental
  Health Support</title><authors>Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, Minlie Huang</authors><categories>cs.CL</categories><comments>Accepted to Findings of ACL 2021 (Long Paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Great research interests have been attracted to devise AI services that are
able to provide mental health support. However, the lack of corpora is a main
obstacle to this research, particularly in Chinese language. In this paper, we
propose PsyQA, a Chinese dataset of psychological health support in the form of
question and answer pair. PsyQA is crawled from a Chinese mental health service
platform, and contains 22K questions and 56K long and well-structured answers.
Based on the psychological counseling theories, we annotate a portion of answer
texts with typical strategies for providing support, and further present
in-depth analysis of both lexical features and strategy patterns in the
counseling answers. We also evaluate the performance of generating counseling
answers with the generative pretrained models. Results show that utilizing
strategies enhances the fluency and helpfulness of generated answers, but there
is still a large space for future research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01703</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01703</id><submitter>Nirav Diwan</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:07:54 GMT</date><size>5127kb</size><source_type>D</source_type></version><title>Fingerprinting Fine-tuned Language Models in the Wild</title><authors>Nirav Diwan, Tanmoy Chakravorty, Zubair Shafiq</authors><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are concerns that the ability of language models (LMs) to generate high
quality synthetic text can be misused to launch spam, disinformation, or
propaganda. Therefore, the research community is actively working on developing
approaches to detect whether a given text is organic or synthetic. While this
is a useful first step, it is important to be able to further fingerprint the
author LM to attribute its origin. Prior work on fingerprinting LMs is limited
to attributing synthetic text generated by a handful (usually &lt; 10) of
pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad
of ways (e.g., on a domain-specific text corpus) before being used to generate
synthetic text. It is challenging to fingerprinting fine-tuned LMs because the
universe of fine-tuned LMs is much larger in realistic scenarios. To address
this challenge, we study the problem of large-scale fingerprinting of
fine-tuned LMs in the wild. Using a real-world dataset of synthetic text
generated by 108 different fine-tuned LMs, we conduct comprehensive experiments
to demonstrate the limitations of existing fingerprinting approaches. Our
results show that fine-tuning itself is the most effective in attributing the
synthetic text generated by fine-tuned LMs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01706</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01706</id><submitter>Saeid Hosseini</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:17:34 GMT</date><size>3395kb</size><source_type>D</source_type></version><title>EmoDNN: Understanding emotions from short texts through a deep neural
  network ensemble</title><authors>Sara Kamran, Raziyeh Zall, Mohammad Reza Kangavari, Saeid Hosseini,
  Sana Rahmani, and Wen Hua</authors><categories>cs.LG cs.CL cs.IR cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The latent knowledge in the emotions and the opinions of the individuals that
are manifested via social networks are crucial to numerous applications
including social management, dynamical processes, and public security.
Affective computing, as an interdisciplinary research field, linking artificial
intelligence to cognitive inference, is capable to exploit emotion-oriented
knowledge from brief contents. The textual contents convey hidden information
such as personality and cognition about corresponding authors that can
determine both correlations and variations between users. Emotion recognition
from brief contents should embrace the contrast between authors where the
differences in personality and cognition can be traced within emotional
expressions. To tackle this challenge, we devise a framework that, on the one
hand, infers latent individual aspects, from brief contents and, on the other
hand, presents a novel ensemble classifier equipped with dynamic dropout
convnets to extract emotions from textual context. To categorize short text
contents, our proposed method conjointly leverages cognitive factors and
exploits hidden information. We utilize the outcome vectors in a novel
embedding model to foster emotion-pertinent features that are collectively
assembled by lexicon inductions. Experimental results show that compared to
other competitors, our proposed model can achieve a higher performance in
recognizing emotion from noisy contents.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01708</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01708</id><submitter>Buliao Huang</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:24:58 GMT</date><size>4886kb</size><source_type>D</source_type></version><title>Semi-supervised Conditional Density Estimation for Imputation and
  Classification of Incomplete Instances</title><authors>Buliao Huang</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Incomplete instances with various missing attributes in many real-world
scenes have brought challenges to the classification task. There are some
missing values imputation methods to fill the missing values with substitute
values before classification. However, the separation between imputation and
classification may lead to inferior performance since label information are
ignored during imputation. Moreover, these imputation methods tend to
initialize these missing values with strong prior assumptions, while the
unreliability of such initialization is rarely considered. To tackle these
problems, a novel semi-supervised conditional normalizing flow (SSCFlow) is
proposed in this paper. SSCFlow explicitly utilizes the observed labels to
facilitate the imputation and classification simultaneously by employing a
semi-supervised algorithm to estimate the conditional probability density of
missing values. Moreover, SSCFlow takes the initialized missing values as
corrupted initial imputation and iteratively reconstructs their latent
representations with an overcomplete denoising autoencoder to approximate the
true conditional probability density of missing values. Experiments have been
conducted with real-world datasets to demonstrate the robustness and efficiency
of the proposed algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01709</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01709</id><submitter>Shuang Zeng</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:25:44 GMT</date><size>5628kb</size><source_type>D</source_type></version><title>SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level
  Relation Extraction</title><authors>Shuang Zeng, Yuting Wu and Baobao Chang</authors><categories>cs.CL cs.AI cs.LG</categories><comments>11 pages, 3 figures, 3 tables, Long paper accepted by Findings of
  ACL-IJCNLP 2021</comments><journal-ref>ACL-IJCNLP 2021</journal-ref><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Document-level relation extraction has attracted much attention in recent
years. It is usually formulated as a classification problem that predicts
relations for all entity pairs in the document. However, previous works
indiscriminately represent intra- and inter-sentential relations in the same
way, confounding the different patterns for predicting them. Besides, they
create a document graph and use paths between entities on the graph as clues
for logical reasoning. However, not all entity pairs can be connected with a
path and have the correct logical reasoning paths in their graph. Thus many
cases of logical reasoning cannot be covered. This paper proposes an effective
architecture, SIRE, to represent intra- and inter-sentential relations in
different ways. We design a new and straightforward form of logical reasoning
module that can cover more logical reasoning chains. Experiments on the public
datasets show SIRE outperforms the previous state-of-the-art methods. Further
analysis shows that our predictions are reliable and explainable. Our code is
available at https://github.com/DreamInvoker/SIRE.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01710</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01710</id><submitter>Milind Chabbi</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:27:37 GMT</date><size>2117kb</size><source_type>D</source_type></version><title>Optimistic Concurrency Control for Real-world Go Programs (Extended
  Version with Appendix)</title><authors>Zhizhou Zhang, Milind Chabbi, Adam Welc, Timothy Sherwood</authors><categories>cs.DC cs.PF cs.PL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a source-to-source transformation framework, GOCC, that consumes
lock-based pessimistic concurrency programs in the Go language and transforms
them into optimistic concurrency programs that use Hardware Transactional
Memory (HTM). The choice of the Go language is motivated by the fact that
concurrency is a first-class citizen in Go, and it is widely used in Go
programs. GOCC performs rich inter-procedural program analysis to detect and
filter lock-protected regions and performs AST-level code transformation of the
surrounding locks when profitable. Profitability is driven by both static
analyses of critical sections and dynamic analysis via execution profiles. A
custom HTM library, using perceptron, learns concurrency behavior and
dynamically decides whether to use HTM in the rewritten lock/unlock points.
Given the rich history of transactional memory research but its lack of
adoption in any industrial setting, we believe this workflow, which ultimately
produces source-code patches, is more apt for industry-scale adoption. Results
on widely adopted Go libraries and applications demonstrate significant (up to
10x) and scalable performance gains resulting from our automated transformation
while avoiding major performance regressions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01711</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01711</id><submitter>Michal Kazmierski</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:28:14 GMT</date><size>941kb</size><source_type>D</source_type></version><title>Lymph Node Graph Neural Networks for Cancer Metastasis Prediction</title><authors>Michal Kazmierski and Benjamin Haibe-Kains</authors><categories>cs.LG eess.IV q-bio.QM</categories><comments>9 pages, 3 figures. Preprint, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting outcomes, such as survival or metastasis for individual cancer
patients is a crucial component of precision oncology. Machine learning (ML)
offers a promising way to exploit rich multi-modal data, including clinical
information and imaging to learn predictors of disease trajectory and help
inform clinical decision making. In this paper, we present a novel graph-based
approach to incorporate imaging characteristics of existing cancer spread to
local lymph nodes (LNs) as well as their connectivity patterns in a prognostic
ML model. We trained an edge-gated Graph Convolutional Network (Gated-GCN) to
accurately predict the risk of distant metastasis (DM) by propagating
information across the LN graph with the aid of soft edge attention mechanism.
In a cohort of 1570 head and neck cancer patients, the Gated-GCN achieves AUROC
of 0.757 for 2-year DM classification and $C$-index of 0.725 for lifetime DM
risk prediction, outperforming current prognostic factors as well as previous
approaches based on aggregated LN features. We also explored the importance of
graph structure and individual lymph nodes through ablation experiments and
interpretability studies, highlighting the importance of considering individual
LN characteristics as well as the relationships between regions of cancer
spread.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01714</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01714</id><submitter>Xiao Zhang</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:34:17 GMT</date><size>1099kb</size></version><title>Optimization Variance: Exploring Generalization Properties of DNNs</title><authors>Xiao Zhang, Dongrui Wu, Haoyi Xiong, Bo Dai</authors><categories>cs.LG cs.AI</categories><comments>Work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike the conventional wisdom in statistical learning theory, the test error
of a deep neural network (DNN) often demonstrates double descent: as the model
complexity increases, it first follows a classical U-shaped curve and then
shows a second descent. Through bias-variance decomposition, recent studies
revealed that the bell-shaped variance is the major cause of model-wise double
descent (when the DNN is widened gradually). This paper investigates epoch-wise
double descent, i.e., the test error of a DNN also shows double descent as the
number of training epoches increases. By extending the bias-variance analysis
to epoch-wise double descent of the zero-one loss, we surprisingly find that
the variance itself, without the bias, varies consistently with the test error.
Inspired by this result, we propose a novel metric, optimization variance (OV),
to measure the diversity of model updates caused by the stochastic gradients of
random training batches drawn in the same iteration. OV can be estimated using
samples from the training set only but correlates well with the (unknown)
\emph{test} error, and hence early stopping may be achieved without using a
validation set.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01717</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01717</id><submitter>Marc Blanchon</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:40:08 GMT</date><size>17868kb</size><source_type>D</source_type></version><title>Towards urban scenes understanding through polarization cues</title><authors>Marc Blanchon, D\'esir\'e Sidib\'e, Olivier Morel, Ralph Seulin,
  Fabrice Meriaudeau</authors><categories>cs.CV</categories><comments>Submitted to Autonomous Robots - Special Issue on Unconventional
  Sensors in Robotics</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Autonomous robotics is critically affected by the robustness of its scene
understanding algorithms. We propose a two-axis pipeline based on polarization
indices to analyze dynamic urban scenes. As robots evolve in unknown
environments, they are prone to encountering specular obstacles. Usually,
specular phenomena are rarely taken into account by algorithms which causes
misinterpretations and erroneous estimates. By exploiting all the light
properties, systems can greatly increase their robustness to events. In
addition to the conventional photometric characteristics, we propose to include
polarization sensing.
  We demonstrate in this paper that the contribution of polarization
measurement increases both the performances of segmentation and the quality of
depth estimation. Our polarimetry-based approaches are compared here with other
state-of-the-art RGB-centric methods showing interest of using polarization
imaging.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01718</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01718</id><submitter>Hiroyasu Katsuno</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:42:16 GMT</date><size>979kb</size><source_type>D</source_type></version><title>Fast improvement of TEM image with low-dose electrons by deep learning</title><authors>Hiroyasu Katsuno, Yuki Kimura, Tomoya Yamazaki and Ichigaku Takigawa</authors><categories>eess.IV cs.CV</categories><comments>8 pages, 7 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Low-electron-dose observation is indispensable for observing various samples
using a transmission electron microscope; consequently, image processing has
been used to improve transmission electron microscopy (TEM) images. To apply
such image processing to in situ observations, we here apply a convolutional
neural network to TEM imaging. Using a dataset that includes short-exposure
images and long-exposure images, we develop a pipeline for processed
short-exposure images, based on end-to-end training. The quality of images
acquired with a total dose of approximately 5 e- per pixel becomes comparable
to that of images acquired with a total dose of approximately 1000 e- per
pixel. Because the conversion time is approximately 8 ms, in situ observation
at 125 fps is possible. This imaging technique enables in situ observation of
electron-beam-sensitive specimens.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01720</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01720</id><submitter>Micha{\l} Bosy</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:47:21 GMT</date><size>564kb</size><source_type>D</source_type></version><title>Hybrid coupling of finite element and boundary integral methods</title><authors>Timo Betcke and Micha{\l} Bosy and Erik Burman</authors><categories>math.NA cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we discuss a hybridised method for FEM-BEM coupling. The
coupling from both sides use a Nitsche type approach to couple to the trace
variable. This leads to a formulation that is robust and flexible with respect
to approximation spaces and can easily be combined as a building block with
other hybridised methods. Energy error estimates and the convergence of Jacobi
iterations are proved and the performance of the method is illustrated on some
computational examples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01721</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01721</id><submitter>Kuanqi Cai</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:47:28 GMT</date><size>1867kb</size></version><title>Curiosity-based Robot Navigation under Uncertainty in Crowded
  Environments</title><authors>Kuanqi Cai, Weinan Chen, Chaoqun Wang, Shuang Song, and Max Q.-H. Meng
  (Fellow, IEEE)</authors><categories>cs.RO</categories><comments>2021 IEEE International Conference on Robotics and Automation (ICRA)
  Workshop on Social Intelligence in Humans and Robots</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile robots have become more and more popular in our daily life. In
large-scale and crowded environments, how to navigate safely with localization
precision is a critical problem. To solve this problem, we proposed a
curiosity-based framework that can find an effective path with the
consideration of human comfort, localization uncertainty, crowds, and the
cost-to-go to the target. Three parts are involved in the proposed framework:
the distance assessment module, the curiosity gain of the information-rich
area, and the curiosity negative gain of crowded areas. The curiosity gain of
the information-rich area was proposed to provoke the robot to approach
localization referenced landmarks. To guarantee human comfort while coexisting
with robots, we propose curiosity gain of the spacious area to bypass the crowd
and maintain an appropriate distance between robots and humans. The evaluation
is conducted in an unstructured environment. The results show that our method
can find a feasible path, which can consider the localization uncertainty while
simultaneously avoiding the crowded area.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01722</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01722</id><submitter>Weijin Zhu</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:50:13 GMT</date><size>1537kb</size><source_type>D</source_type></version><title>GMAIR: Unsupervised Object Detection Based on Spatial Attention and
  Gaussian Mixture</title><authors>Weijin Zhu, Yao Shen, Linfeng Yu, Lizeth Patricia Aguirre Sanchez</authors><categories>cs.CV</categories><comments>15 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent studies on unsupervised object detection based on spatial attention
have achieved promising results. Models, such as AIR and SPAIR, output &quot;what&quot;
and &quot;where&quot; latent variables that represent the attributes and locations of
objects in a scene, respectively. Most of the previous studies concentrate on
the &quot;where&quot; localization performance; however, we claim that acquiring &quot;what&quot;
object attributes is also essential for representation learning. This paper
presents a framework, GMAIR, for unsupervised object detection. It incorporates
spatial attention and a Gaussian mixture in a unified deep generative model.
GMAIR can locate objects in a scene and simultaneously cluster them without
supervision. Furthermore, we analyze the &quot;what&quot; latent variables and clustering
process. Finally, we evaluate our model on MultiMNIST and Fruit2D datasets and
show that GMAIR achieves competitive results on localization and clustering
compared to state-of-the-art methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01723</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01723</id><submitter>Aur\'elien Bibaut</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:50:13 GMT</date><size>812kb</size><source_type>D</source_type></version><title>Risk Minimization from Adaptively Collected Data: Guarantees for
  Supervised and Policy Learning</title><authors>Aur\'elien Bibaut and Antoine Chambaz and Maria Dimakopoulou and
  Nathan Kallus and Mark van der Laan</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical risk minimization (ERM) is the workhorse of machine learning,
whether for classification and regression or for off-policy policy learning,
but its model-agnostic guarantees can fail when we use adaptively collected
data, such as the result of running a contextual bandit algorithm. We study a
generic importance sampling weighted ERM algorithm for using adaptively
collected data to minimize the average of a loss function over a hypothesis
class and provide first-of-their-kind generalization guarantees and fast
convergence rates. Our results are based on a new maximal inequality that
carefully leverages the importance sampling structure to obtain rates with the
right dependence on the exploration rate in the data. For regression, we
provide fast rates that leverage the strong convexity of squared-error loss.
For policy learning, we provide rate-optimal regret guarantees that close an
open gap in the existing literature whenever exploration decays to zero, as is
the case for bandit-collected data. An empirical investigation validates our
theory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01726</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01726</id><submitter>Ra\'ul Nozal</submitter><version version="v1"><date>Thu, 3 Jun 2021 09:56:01 GMT</date><size>557kb</size><source_type>D</source_type></version><title>Exploiting co-execution with oneAPI: heterogeneity from a modern
  perspective</title><authors>Ra\'ul Nozal and Jose Luis Bosque</authors><categories>cs.DC cs.PL</categories><comments>14 pages, 9 figures, conference</comments><acm-class>C.1.2; C.1.4; C.1.3; D.1.3; D.2.0; D.2.3; D.2.11; D.2.13; D.4.7;
  D.4.9; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming efficiently heterogeneous systems is a major challenge, due to
the complexity of their architectures. Intel oneAPI, a new and powerful
standards-based unified programming model, built on top of SYCL, addresses
these issues. In this paper, oneAPI is provided with co-execution strategies to
run the same kernel between different devices, enabling the exploitation of
static and dynamic policies. On top of that, static and dynamic load-balancing
algorithms are integrated and analyzed.
  This work evaluates the performance and energy efficiency for a well-known
set of regular and irregular HPC benchmarks, using an integrated GPU and CPU.
Experimental results show that co-execution is worthwhile when using dynamic
algorithms, improving efficiency even more when using unified shared memory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01729</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01729</id><submitter>Marc Zeller</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:02:01 GMT</date><size>897kb</size></version><title>DEIS: Dependability Engineering Innovation for Industrial CPS</title><authors>Erik Armengaud, Georg Macher, Alexander Massoner, Sebastian Frager,
  Rasmus Adler, Daniel Schneider, Simone Longo, Massimiliano Melis, Riccardo
  Groppo, Federica Villa, Padraig OLeary, Kevin Bambury, Finnegan Anita, Marc
  Zeller, Kai Hoefig, Yiannis Papadopoulos, Richard Hawkins, Tim Kelly</authors><categories>cs.SE</categories><doi>10.1007/978-3-319-66972-4_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The open and cooperative nature of Cyber-Physical Systems (CPS) poses new
challenges in assuring dependability. The DEIS project (Dependability
Engineering Innovation for automotive CPS. This project has received funding
from the European Union's Horizon 2020 research and innovation programme under
grant agreement No 732242, see http://www.deis-project.eu) addresses these
challenges by developing technologies that form a science of dependable system
integration. In the core of these technologies lies the concept of a Digital
Dependability Identity (DDI) of a component or system. DDIs are modular,
composable, and executable in the field facilitating (a) efficient synthesis of
component and system dependability information over the supply chain and (b)
effective evaluation of this information in-the-field for safe and secure
composition of highly distributed and autonomous CPS. The paper outlines the
DDI concept and opportunities for application in four industrial use cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01730</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01730</id><submitter>Erwin Jos\'e L\'opez Pulgar\'in Dr</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:07:55 GMT</date><size>7879kb</size><source_type>D</source_type></version><title>Drivers' Manoeuvre Modelling and Prediction for Safe HRI</title><authors>Erwin Jose Lopez Pulgarin, Guido Herrmann, Ute Leonards</authors><categories>cs.RO cs.HC cs.LG</categories><comments>Submitted to IEEE Transactions on Human-Machine Systems on
  17-Jul-2020</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  As autonomous machines such as robots and vehicles start performing tasks
involving human users, ensuring a safe interaction between them becomes an
important issue. Translating methods from human-robot interaction (HRI) studies
to the interaction between humans and other highly complex machines (e.g.
semi-autonomous vehicles) could help advance the use of those machines in
scenarios requiring human interaction. One method involves understanding human
intentions and decision-making to estimate the human's present and near-future
actions whilst interacting with a robot. This idea originates from the
psychological concept of Theory of Mind, which has been broadly explored for
robotics and recently for autonomous and semi-autonomous vehicles. In this
work, we explored how to predict human intentions before an action is performed
by combining data from human-motion, vehicle-state and human inputs (e.g.
steering wheel, pedals). A data-driven approach based on Recurrent Neural
Network models was used to classify the current driving manoeuvre and to
predict the future manoeuvre to be performed. A state-transition model was used
with a fixed set of manoeuvres to label data recorded during the trials for
real-time applications. Models were trained and tested using drivers of
different seat preferences, driving expertise and arm-length; precision and
recall metrics over 95% for manoeuvre identification and 86% for manoeuvre
prediction were achieved, with prediction time-windows of up to 1 second for
both known and unknown test subjects. Compared to our previous results,
performance improved and manoeuvre prediction was possible for unknown test
subjects without knowing the current manoeuvre.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01732</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01732</id><submitter>Ziqing Yang</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:18:43 GMT</date><size>262kb</size><source_type>D</source_type></version><title>Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer</title><authors>Ziqing Yang, Wentao Ma, Yiming Cui, Jiani Ye, Wanxiang Che, Shijin
  Wang</authors><categories>cs.CL</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilingual pre-trained models have achieved remarkable transfer performance
by pre-trained on rich kinds of languages. Most of the models such as mBERT are
pre-trained on unlabeled corpora. The static and contextual embeddings from the
models could not be aligned very well. In this paper, we aim to improve the
zero-shot cross-lingual transfer performance by aligning the embeddings better.
We propose a pre-training task named Alignment Language Model (AlignLM), which
uses the statistical alignment information as the prior knowledge to guide
bilingual word prediction. We evaluate our method on multilingual machine
reading comprehension and natural language interface tasks. The results show
AlignLM can improve the zero-shot performance significantly on MLQA and XNLI
datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01733</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01733</id><submitter>Arup Ratan Paul</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:20:29 GMT</date><size>5815kb</size></version><title>A Novel SEPIC-\'Cuk Based High Gain Solar Micro-Inverter for Grid
  Integration</title><authors>Arup Ratan Paul, Arghyadip Bhattacharya, Kishore Chatterjee</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Solar micro-inverters are becoming increasingly popular as they are modular,
and they posses the capability of extracting maximum available power from the
individual photovoltaic (PV) modules of a solar array. For realizing
micro-inverters single stage transformer-less topologies are preferred as they
offer better power evacuation efficacy. A SEPIC-\'Cuk based transformer-less
micro-inverter, having only one high frequency switch and four line frequency
switches, is proposed in this paper. The proposed converter can be employed to
interface a 35 V PV module to a 220 V single phase ac grid. As a very high gain
is required to be achieved for the converter, it is made to operate in
discontinuous conduction mode (DCM) for all possible operating conditions.
Since the ground of the each PV modules is connected to the ground of the
utility, there is no possibility of leakage current flow between the module and
the utility. Detailed simulation studies are carried out to ascertain the
efficacy of the proposed micro-inverter. A laboratory prototype of the inverter
is fabricated, and detailed experimental studies are carried out to confirm the
viability of the proposed scheme.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01735</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01735</id><submitter>\c{S}\&quot;ukr\&quot;u Ozan</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:23:58 GMT</date><size>773kb</size></version><title>Auto-tagging of Short Conversational Sentences using Transformer Methods</title><authors>D. Emre Ta\c{s}ar,\c{S}\&quot;ukr\&quot;u Ozan, Umut \&quot;Ozdil, M. Fatih Akca,
  O\u{g}uzhan \&quot;Olmez, Semih G\&quot;ul\&quot;um, Se\c{c}ilay Kutal, Ceren Belhan</authors><categories>cs.CL cs.LG</categories><comments>in Turkish language</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of categorizing short speech sentences according to their
semantic features with high accuracy is a subject studied in natural language
processing. In this study, a data set created with samples classified in 46
different categories was used. Examples consist of sentences taken from chat
conversations between a company's customer representatives and the company's
website visitors. The primary purpose is to automatically tag questions and
requests from visitors in the most accurate way for 46 predetermined categories
for use in a chat application to generate meaningful answers to the questions
asked by the website visitors. For this, different BERT models and one GPT-2
model, pre-trained in Turkish, were preferred. The classification performances
of the relevant models were analyzed in detail and reported accordingly.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01737</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01737</id><submitter>Idan Mehalel</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:33:49 GMT</date><size>86kb</size><source_type>D</source_type></version><title>Optimal sets of questions for Twenty Questions</title><authors>Yuval Filmus and Idan Mehalel</authors><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the distributional Twenty Questions game, Bob chooses a number $x$ from
$1$ to $n$ according to a distribution $\mu$, and Alice (who knows $\mu$)
attempts to identify $x$ using Yes/No questions, which Bob answers truthfully.
Her goal is to minimize the expected number of questions.
  The optimal strategy for the Twenty Questions game corresponds to a Huffman
code for $\mu$, yet this strategy could potentially uses all $2^n$ possible
questions. Dagan et al. constructed a set of $1.25^{n+o(n)}$ questions which
suffice to construct an optimal strategy for all $\mu$, and showed that this
number is optimal (up to sub-exponential factors) for infinitely many $n$.
  We determine the optimal size of such a set of questions for all $n$ (up to
sub-exponential factors), answering an open question of Dagan et al. In
addition, we generalize the results of Dagan et al. to the $d$-ary setting,
obtaining similar results with $1.25$ replaced by $1 + (d-1)/d^{d/(d-1)}$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01738</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01738</id><submitter>Amareshwara Sainadh Chamarthi</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:36:04 GMT</date><size>16481kb</size><source_type>D</source_type></version><title>Implicit gradients based novel finite volume scheme for compressible
  single and multi-component flows</title><authors>Amareshwara Sainadh Chamarthi, Steven H. Frankel, Abhishek Chintagunta</authors><categories>math.NA cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a novel approach to compute the numerical fluxes at the
cell boundaries in the finite volume approach. Explicit gradients used in
deriving the reconstruction polynomials are replaced by high-order gradients
computed by compact finite differences, referred to as implicit gradients in
this paper. The new finite volume scheme has superior dispersion and
dissipation properties in comparison to the compact reconstruction approach.
These implicit gradients are re-used in viscous flux computation and
post-processing, which further improves efficiency. A problem-independent shock
capturing approach via Boundary Variation Diminishing (BVD) algorithm is used
to suppress oscillations for the simulation of flows with shocks and material
interfaces. Several numerical test cases are carried out to verify the proposed
finite volume method's capability using the implicit gradient method for single
and multicomponent flows. Significant improvements are observed by computing
the gradients implicitly for the viscous flows.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01739</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01739</id><submitter>Aditya Jyoti Paul</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:40:54 GMT</date><size>1109kb</size></version><title>Advances in Classifying the Stages of Diabetic Retinopathy Using
  Convolutional Neural Networks in Low Memory Edge Devices</title><authors>Aditya Jyoti Paul</authors><categories>eess.IV cs.CV cs.LG</categories><comments>This paper is currently under review at IEEE MASCON 2021.
  http://ieeemascon.in</comments><msc-class>68T45, 68T10, 68T07, 68U10</msc-class><acm-class>I.2.10; I.4.8; I.5.1; J.3; I.4.1; K.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diabetic Retinopathy (DR) is a severe complication that may lead to retinal
vascular damage and is one of the leading causes of vision impairment and
blindness. DR broadly is classified into two stages - non-proliferative (NPDR),
where there are almost no symptoms, except a few microaneurysms, and
proliferative (PDR) involving a huge number of microaneurysms and hemorrhages,
soft and hard exudates, neo-vascularization, macular ischemia or a combination
of these, making it easier to detect. More specifically, DR is usually
classified into five levels, labeled 0-4, from 0 indicating no DR to 4 which is
most severe. This paper firstly presents a discussion on the risk factors of
the disease, then surveys the recent literature on the topic followed by
examining certain techniques which were found to be highly effective in
improving the prognosis accuracy. Finally, a convolutional neural network model
is proposed to detect all the stages of DR on a low-memory edge
microcontroller. The model has a size of just 5.9 MB, accuracy and F1 score
both of 94% and an inference speed of about 20 frames per second.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01741</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01741</id><submitter>David Mark Bossens</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:42:49 GMT</date><size>382kb</size><source_type>D</source_type></version><title>Lifetime policy reuse and the importance of task capacity</title><authors>David M. Bossens and Adam J. Sobey</authors><categories>cs.LG cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A long-standing challenge in artificial intelligence is lifelong learning. In
lifelong learning, many tasks are presented in sequence and learners must
efficiently transfer knowledge between tasks while avoiding catastrophic
forgetting over long lifetimes. On these problems, policy reuse and other
multi-policy reinforcement learning techniques can learn many tasks. However,
they can generate many temporary or permanent policies, resulting in memory
issues. Consequently, there is a need for lifetime-scalable methods that
continually refine a policy library of a pre-defined size. This paper presents
a first approach to lifetime-scalable policy reuse. To pre-select the number of
policies, a notion of task capacity, the maximal number of tasks that a policy
can accurately solve, is proposed. To evaluate lifetime policy reuse using this
method, two state-of-the-art single-actor base-learners are compared: 1) a
value-based reinforcement learner, Deep Q-Network (DQN) or Deep Recurrent
Q-Network (DRQN); and 2) an actor-critic reinforcement learner, Proximal Policy
Optimisation (PPO) with or without Long Short-Term Memory layer. By selecting
the number of policies based on task capacity, D(R)QN achieves near-optimal
performance with 6 policies in a 27-task MDP domain and 9 policies in an
18-task POMDP domain; with fewer policies, catastrophic forgetting and negative
transfer are observed. Due to slow, monotonic improvement, PPO requires fewer
policies, 1 policy for the 27-task domain and 4 policies for the 18-task
domain, but it learns the tasks with lower accuracy than D(R)QN. These findings
validate lifetime-scalable policy reuse and suggest using D(R)QN for larger and
PPO for smaller library sizes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01744</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01744</id><submitter>Ye Chao Bai</submitter><version version="v1"><date>Thu, 3 Jun 2021 10:49:48 GMT</date><size>7549kb</size><source_type>D</source_type></version><title>Multi-Scale Feature Aggregation by Cross-Scale Pixel-to-Region Relation
  Operation for Semantic Segmentation</title><authors>Yechao Bai, Ziyuan Huang, Lyuyu Shen, Hongliang Guo, Marcelo H. Ang Jr
  and Daniela Rus</authors><categories>cs.CV</categories><comments>Accepted to RA-L 2021</comments><doi>10.1109/LRA.2021.3086419</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting multi-scale features has shown great potential in tackling
semantic segmentation problems. The aggregation is commonly done with sum or
concatenation (concat) followed by convolutional (conv) layers. However, it
fully passes down the high-level context to the following hierarchy without
considering their interrelation. In this work, we aim to enable the low-level
feature to aggregate the complementary context from adjacent high-level feature
maps by a cross-scale pixel-to-region relation operation. We leverage
cross-scale context propagation to make the long-range dependency capturable
even by the high-resolution low-level features. To this end, we employ an
efficient feature pyramid network to obtain multi-scale features. We propose a
Relational Semantics Extractor (RSE) and Relational Semantics Propagator (RSP)
for context extraction and propagation respectively. Then we stack several RSP
into an RSP head to achieve the progressive top-down distribution of the
context. Experiment results on two challenging datasets Cityscapes and COCO
demonstrate that the RSP head performs competitively on both semantic
segmentation and panoptic segmentation with high efficiency. It outperforms
DeeplabV3 [1] by 0.7% with 75% fewer FLOPs (multiply-adds) in the semantic
segmentation task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01750</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01750</id><submitter>Shrisha Rao</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:01:06 GMT</date><size>2121kb</size><source_type>D</source_type></version><title>Modeling Influencer Marketing Campaigns In Social Networks</title><authors>Ronak Doshi and Ajay Ramesh Ranganathan and Shrisha Rao</authors><categories>cs.AI cs.SI</categories><comments>34 pages</comments><msc-class>68T42, 93A16</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In the present day, more than 3.8 billion people around the world actively
use social media. The effectiveness of social media in facilitating quick and
easy sharing of information has attracted brands and advertizers who wish to
use the platform to market products via the influencers in the network.
Influencers, owing to their massive popularity, provide a huge potential
customer base generating higher returns of investment in a very short period.
However, it is not straightforward to decide which influencers should be
selected for an advertizing campaign that can generate maximum returns with
minimum investment. In this work, we present an agent-based model (ABM) that
can simulate the dynamics of influencer advertizing campaigns in a variety of
scenarios and can help to discover the best influencer marketing strategy. Our
system is a probabilistic graph-based model that incorporates real-world
factors such as customers' interest in a product, customer behavior, the
willingness to pay, a brand's investment cap, influencers' engagement with
influence diffusion, and the nature of the product being advertized viz. luxury
and non-luxury.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01751</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01751</id><submitter>Sawan Kumar</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:02:36 GMT</date><size>82kb</size><source_type>D</source_type></version><title>Reordering Examples Helps during Priming-based Few-Shot Learning</title><authors>Sawan Kumar, Partha Talukdar</authors><categories>cs.CL</categories><comments>12 pages, 1 figure, Accepted to Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to learn from limited data, or few-shot learning, is a desirable
and often critical requirement for NLP systems. While many existing methods do
poorly at learning from a handful of examples, large pretrained language models
have recently been shown to be efficient few-shot learners. One approach to
few-shot learning, which does not require finetuning of model parameters, is to
augment the language model's input with priming text which is typically
constructed using task specific descriptions and examples. In this work, we
further explore priming-based few-shot learning, with focus on using examples
as prompts. We show that presenting examples in the right order is key for
generalization. We introduce PERO (Prompting with Examples in the Right Order),
where we formulate few-shot learning as search over the set of permutations of
the training examples. We show that PERO can learn to generalize efficiently
using as few as 10 examples, in contrast to existing approaches. While the
newline token is a natural choice for separating the examples in the prompt, we
show that learning a new separator token can potentially provide further gains
in performance. We demonstrate the effectiveness of the proposed method on the
tasks of sentiment classification, natural language inference and fact
retrieval. Finally, we analyze the learned prompts to reveal novel insights,
including the idea that two training examples in the right order alone can
provide competitive performance for sentiment classification and natural
language inference.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01760</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01760</id><submitter>Leyang Cui</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:29:43 GMT</date><size>587kb</size><source_type>D</source_type></version><title>Template-Based Named Entity Recognition Using BART</title><authors>Leyang Cui, Yu Wu, Jian Liu, Sen Yang, Yue Zhang</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a recent interest in investigating few-shot NER, where the
low-resource target domain has different label sets compared with a
resource-rich source domain. Existing methods use a similarity-based metric.
However, they cannot make full use of knowledge transfer in NER model
parameters. To address the issue, we propose a template-based method for NER,
treating NER as a language model ranking problem in a sequence-to-sequence
framework, where original sentences and statement templates filled by candidate
named entity span are regarded as the source sequence and the target sequence,
respectively. For inference, the model is required to classify each candidate
span based on the corresponding template scores. Our experiments demonstrate
that the proposed method achieves 92.55% F1 score on the CoNLL03 (rich-resource
task), and significantly better than fine-tuning BERT 10.88%, 15.34%, and
11.73% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS
(low-resource task), respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01761</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01761</id><submitter>Luo Luo</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:30:32 GMT</date><size>51kb</size><source_type>D</source_type></version><title>Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced
  Convex-Concave Minimax Optimization</title><authors>Luo Luo, Guangzeng Xie, Tong Zhang, Zhihua Zhang</authors><categories>math.OC cs.LG</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper considers stochastic first-order algorithms for convex-concave
minimax problems of the form $\min_{\bf x}\max_{\bf y}f(\bf x, \bf y)$, where
$f$ can be presented by the average of $n$ individual components which are
$L$-average smooth. For $\mu_x$-strongly-convex-$\mu_y$-strongly-concave
setting, we propose a new method which could find a $\varepsilon$-saddle point
of the problem in $\tilde{\mathcal O}
\big(\sqrt{n(\sqrt{n}+\kappa_x)(\sqrt{n}+\kappa_y)}\log(1/\varepsilon)\big)$
stochastic first-order complexity, where $\kappa_x\triangleq L/\mu_x$ and
$\kappa_y\triangleq L/\mu_y$. This upper bound is near optimal with respect to
$\varepsilon$, $n$, $\kappa_x$ and $\kappa_y$ simultaneously. In addition, the
algorithm is easily implemented and works well in practical. Our methods can be
extended to solve more general unbalanced convex-concave minimax problems and
the corresponding upper complexity bounds are also near optimal.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01763</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01763</id><submitter>Dmitry Kosolobov</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:32:50 GMT</date><size>84kb</size><source_type>D</source_type></version><title>Internal Shortest Absent Word Queries in Constant Time and Linear Space</title><authors>Golnaz Badkobeh, Panagiotis Charalampopoulos, Dmitry Kosolobov, and
  Solon P. Pissis</authors><categories>cs.DS</categories><comments>13 pages, 1 figure, 4 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given a string $T$ of length $n$ over an alphabet $\Sigma\subset
\{1,2,\ldots,n^{O(1)}\}$ of size $\sigma$, we are to preprocess $T$ so that
given a range $[i,j]$, we can return a representation of a shortest string over
$\Sigma$ that is absent in the fragment $T[i]\cdots T[j]$ of $T$. We present an
$O(n)$-space data structure that answers such queries in constant time and can
be constructed in $O(n\log_\sigma n)$ time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01764</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01764</id><submitter>Kezhou Lin</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:33:59 GMT</date><size>48kb</size><source_type>D</source_type></version><title>Less is More: Sparse Sampling for Dense Reaction Predictions</title><authors>Kezhou Lin and Xiaohan Wang and Zhedong Zheng and Linchao Zhu and Yi
  Yang</authors><categories>cs.CV</categories><comments>Code is available at:
  https://github.com/HenryLittle/EEV-Challenge-2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining viewer responses from videos can be useful for creators and
streaming platforms to analyze the video performance and improve the future
user experience. In this report, we present our method for 2021 Evoked
Expression from Videos Challenge. In particular, our model utilizes both audio
and image modalities as inputs to predict emotion changes of viewers. To model
long-range emotion changes, we use a GRU-based model to predict one sparse
signal with 1Hz. We observe that the emotion changes are smooth. Therefore, the
final dense prediction is obtained via linear interpolating the signal, which
is robust to the prediction fluctuation. Albeit simple, the proposed method has
achieved pearson's correlation score of 0.04430 on the final private test set.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01766</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01766</id><submitter>Vitaly Cheptsov</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:39:56 GMT</date><size>16kb</size></version><title>Dynamic Analysis of ARINC 653 RTOS with LLVM</title><authors>Vitaly Cheptsov, Alexey Khoroshilov</authors><categories>cs.SE</categories><comments>7 pages</comments><journal-ref>2018 Ivannikov Ispras Open Conference (ISPRAS), 2018, pp. 9-15</journal-ref><doi>10.1109/ISPRAS.2018.00009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing standards for airborne-embedded software systems impose a number of
requirements applicable to the software development cycle of hard real-time
operating systems found in modern aircraft. The measures taken are meant to
reduce the risks of undesired consequences, but have strongly varying costs.
Dynamic instrumentation and static analysis are common practices used to
automatically find software defects, from strictly non-conforming code
constructions to memory corruptions or invalid control flow. LLVM analyser and
sanitizer infrastructure, while regularly applied to general-purpose software,
originally was not thought to be introduced to heavily restricted environments.
In this paper we discuss the specifics of airborne systems with regards to
dynamic instrumentation and provide practical considerations to be taken into
account for the effective use of general-purpose instrumentation tools. We
bring a complete LLVM stack support to JetOS, a prospective onboard real-time
operating system currently being developed at ISP RAS in collaboration with
GosNIIAS. As an example, we port AddressSanitizer, MemorySanitizer, and
UndefinedBehaviorSanitizer and provide the details against the caveats on all
relevant sides: a sanitizer, a compiler, and an operating system. In addition
we suggest uninvolved optimisations and enhancements to the runtimes to
maximise the effects of the tools.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01768</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01768</id><submitter>Aman Nougrahiya</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:49:39 GMT</date><size>636kb</size><source_type>D</source_type></version><title>Homeostasis: Design and Implementation of a Self-Stabilizing Compiler</title><authors>Aman Nougrahiya, V. Krishna Nandivada</authors><categories>cs.PL</categories><comments>33 pages, 16 figures. Patent filed (application no: 202041054066).
  For associated code, see https://github.com/anonymousoopsla21/homeostasis</comments><acm-class>D.2.3; D.2.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Mainstream compilers perform a multitude of analyses and optimizations on the
given input program. Each analysis pass may generate a program-abstraction.
Each optimization pass is typically composed of multiple alternating phases of
inspection of program-abstractions and transformations of the program. Upon
transformation of a program, the program-abstractions generated by various
analysis passes may become inconsistent with the program's modified state.
Consequently, the downstream transformations may be considered unsafe until the
relevant program-abstractions are stabilized, i.e., the program-abstractions
are made consistent with the modified program. In general, the existing
compiler frameworks do not perform automated stabilization of the
program-abstractions and instead leave it to the optimization writer to deal
with the complex task of identifying the relevant program-abstractions to
stabilize, the points where the stabilization is to be performed, and the exact
procedure of stabilization. Similarly, adding new analyses becomes a challenge
as one has to understand which all existing optimizations may impact the newly
added program-abstractions. In this paper, we address these challenges by
providing the design and implementation of a novel generalized compiler-design
framework called Homeostasis.
  Homeostasis can be used to guarantee the trigger of automated stabilization
of relevant program-abstractions under every possible transformation of the
program. Interestingly, Homeostasis provides such guarantees not only for the
existing optimization passes but also for any future optimizations that may be
added to the framework. We have implemented our proposed ideas in the IMOP
compiler framework, for OpenMP C programs. We present an evaluation which shows
that Homeostasis is efficient and easy to use.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01770</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01770</id><submitter>Susanne Trick</submitter><version version="v1"><date>Thu, 3 Jun 2021 11:52:13 GMT</date><size>1161kb</size><source_type>D</source_type></version><title>A Normative Model of Classifier Fusion</title><authors>Susanne Trick, Constantin A. Rothkopf</authors><categories>cs.LG cs.AI stat.ML</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining the outputs of multiple classifiers or experts into a single
probabilistic classification is a fundamental task in machine learning with
broad applications from classifier fusion to expert opinion pooling. Here we
present a hierarchical Bayesian model of probabilistic classifier fusion based
on a new correlated Dirichlet distribution. This distribution explicitly models
positive correlations between marginally Dirichlet-distributed random vectors
thereby allowing normative modeling of correlations between base classifiers or
experts. The proposed model naturally accommodates the classic Independent
Opinion Pool and other independent fusion algorithms as special cases. It is
evaluated by uncertainty reduction and correctness of fusion on synthetic and
real-world data sets. We show that a change in performance of the fused
classifier due to uncertainty reduction can be Bayes optimal even for highly
correlated base classifiers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01777</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01777</id><submitter>Aaron Snoswell</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:00:38 GMT</date><size>1645kb</size><source_type>D</source_type></version><title>LiMIIRL: Lightweight Multiple-Intent Inverse Reinforcement Learning</title><authors>Aaron J. Snoswell, Surya P. N. Singh, Nan Ye</authors><categories>cs.LG cs.AI cs.RO</categories><comments>Under review for NeurIPS 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multiple-Intent Inverse Reinforcement Learning (MI-IRL) seeks to find a
reward function ensemble to rationalize demonstrations of different but
unlabelled intents. Within the popular expectation maximization (EM) framework
for learning probabilistic MI-IRL models, we present a warm-start strategy
based on up-front clustering of the demonstrations in feature space. Our
theoretical analysis shows that this warm-start solution produces a
near-optimal reward ensemble, provided the behavior modes satisfy mild
separation conditions. We also propose a MI-IRL performance metric that
generalizes the popular Expected Value Difference measure to directly assesses
learned rewards against the ground-truth reward ensemble. Our metric elegantly
addresses the difficulty of pairing up learned and ground truth rewards via a
min-cost flow formulation, and is efficiently computable. We also develop a
MI-IRL benchmark problem that allows for more comprehensive algorithmic
evaluations. On this problem, we find our MI-IRL warm-start strategy helps
avoid poor quality local minima reward ensembles, resulting in a significant
improvement in behavior clustering. Our extensive sensitivity analysis
demonstrates that the quality of the learned reward ensembles is improved under
various settings, including cases where our theoretical assumptions do not
necessarily hold. Finally, we demonstrate the effectiveness of our methods by
discovering distinct driving styles in a large real-world dataset of driver GPS
trajectories.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01779</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01779</id><submitter>Ying Lu</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:04:36 GMT</date><size>1096kb</size><source_type>D</source_type></version><title>Preparation of Many-body Ground States by Time Evolution with
  Variational Microscopic Magnetic Fields and Incomplete Interactions</title><authors>Ying Lu, Yue-Min Li, Peng-Fei Zhou and Shi-Ju Ran</authors><categories>quant-ph cond-mat.str-el cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State preparation is of fundamental importance in quantum physics, which can
be realized by constructing the quantum circuit as a unitary that transforms
the initial state to the target, or implementing a quantum control protocol to
evolve to the target state with a designed Hamiltonian. In this work, we study
the latter on quantum many-body systems by the time evolution with fixed
couplings and variational magnetic fields. In specific, we consider to prepare
the ground states of the Hamiltonians containing certain interactions that are
missing in the Hamiltonians for the time evolution. An optimization method is
proposed to optimize the magnetic fields by &quot;fine-graining&quot; the discretization
of time, in order to gain high precision and stability. The back propagation
technique is utilized to obtain the gradients of the fields against the
logarithmic fidelity. Our method is tested on preparing the ground state of
Heisenberg chain with the time evolution by the XY and Ising interactions, and
its performance surpasses two baseline methods that use local and global
optimization strategies, respectively. Our work can be applied and generalized
to other quantum models such as those defined on higher dimensional lattices.
It enlightens to reduce the complexity of the required interactions for
implementing quantum control or other tasks in quantum information and
computation by means of optimizing the magnetic fields.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01781</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01781</id><submitter>Ivan Heibi</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:09:41 GMT</date><size>5605kb</size></version><title>A protocol to gather, characterize and analyze incoming citations of
  retracted articles</title><authors>Ivan Heibi, Silvio Peroni</authors><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this article, we present a methodology which takes as input a collection
of retracted articles, gathers the entities citing them, characterizes such
entities according to multiple dimensions (disciplines, year of publication,
sentiment, etc.), and applies a quantitative and qualitative analysis on the
collected values. The methodology is composed of four phases: (1) identifying,
retrieving, and extracting basic metadata of the entities which have cited a
retracted article, (2) extracting and labeling additional features based on the
textual content of the citing entities, (3) building a descriptive statistical
summary based on the collected data, and finally (4) running a topic modeling
analysis. The goal of the methodology is to generate data and visualizations
that help understanding possible behaviors related to retraction cases. We
present the methodology in a structured step-by-step form following its four
phases, discuss its limits and possible workarounds, and list the planned
future improvements.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01782</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01782</id><submitter>Kodirjon Akhmedov</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:10:26 GMT</date><size>4135kb</size><source_type>D</source_type></version><title>Machine learning models for DOTA 2 outcomes prediction</title><authors>Kodirjon Akhmedov and Anh Huy Phan</authors><categories>cs.LG</categories><comments>11 pages, 12 figures, the paper will be published in IEEE
  Transactions on Games Journal</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Prediction of the real-time multiplayer online battle arena (MOBA) games'
match outcome is one of the most important and exciting tasks in Esports
analytical research. This research paper predominantly focuses on building
predictive machine and deep learning models to identify the outcome of the Dota
2 MOBA game using the new method of multi-forward steps predictions. Three
models were investigated and compared: Linear Regression (LR), Neural Networks
(NN), and a type of recurrent neural network Long Short-Term Memory (LSTM). In
order to achieve the goals, we developed a data collecting python server using
Game State Integration (GSI) to track the real-time data of the players. Once
the exploratory feature analysis and tuning hyper-parameters were done, our
models' experiments took place on different players with dissimilar backgrounds
of playing experiences. The achieved accuracy scores depend on the
multi-forward prediction parameters, which for the worse case in linear
regression 69\% but on average 82\%, while in the deep learning models hit the
utmost accuracy of prediction on average 88\% for NN, and 93\% for LSTM models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01784</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01784</id><submitter>Ben Green</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:16:08 GMT</date><size>379kb</size></version><title>The Contestation of Tech Ethics: A Sociotechnical Approach to Ethics and
  Technology in Action</title><authors>Ben Green</authors><categories>cs.CY cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent controversies related to topics such as fake news, privacy, and
algorithmic bias have prompted increased public scrutiny of digital
technologies and soul-searching among many of the people associated with their
development. In response, the tech industry, academia, civil society, and
governments have rapidly increased their attention to &quot;ethics&quot; in the design
and use of digital technologies (&quot;tech ethics&quot;). Yet almost as quickly as
ethics discourse has proliferated across the world of digital technologies, the
limitations of these approaches have also become apparent: tech ethics is vague
and toothless, is subsumed into corporate logics and incentives, and has a
myopic focus on individual engineers and technology design rather than on the
structures and cultures of technology production. As a result of these
limitations, many have grown skeptical of tech ethics and its proponents,
charging them with &quot;ethics-washing&quot;: promoting ethics research and discourse to
defuse criticism and government regulation without committing to ethical
behavior. By looking at how ethics has been taken up in both science and
business in superficial and depoliticizing ways, I recast tech ethics as a
terrain of contestation where the central fault line is not whether it is
desirable to be ethical, but what &quot;ethics&quot; entails and who gets to define it.
This framing highlights the significant limits of current approaches to tech
ethics and the importance of studying the formulation and real-world effects of
tech ethics. In order to identify and develop more rigorous strategies for
reforming digital technologies and the social relations that they mediate, I
describe a sociotechnical approach to tech ethics, one that reflexively applies
many of tech ethics' own lessons regarding digital technologies to tech ethics
itself.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01786</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01786</id><submitter>Charbel Merhej</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:18:26 GMT</date><size>1538kb</size><source_type>D</source_type></version><title>What Happened Next? Using Deep Learning to Value Defensive Actions in
  Football Event-Data</title><authors>Charbel Merhej, Ryan Beal, Sarvapali Ramchurn (University of
  Southampton), Tim Matthews (Sentient Sports)</authors><categories>cs.AI</categories><comments>10 pages, 7 figures, Proceedings of the 27th ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining (KDD '21), August 14--18, 2021, Virtual
  Event, Singapore</comments><doi>10.1145/3447548.3467090</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Objectively quantifying the value of player actions in football (soccer) is a
challenging problem. To date, studies in football analytics have mainly focused
on the attacking side of the game, while there has been less work on
event-driven metrics for valuing defensive actions (e.g., tackles and
interceptions). Therefore in this paper, we use deep learning techniques to
define a novel metric that values such defensive actions by studying the threat
of passages of play that preceded them. By doing so, we are able to value
defensive actions based on what they prevented from happening in the game. Our
Defensive Action Expected Threat (DAxT) model has been validated using
real-world event-data from the 2017/2018 and 2018/2019 English Premier League
seasons, and we combine our model outputs with additional features to derive an
overall rating of defensive ability for players. Overall, we find that our
model is able to predict the impact of defensive actions allowing us to better
value defenders using event-data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01789</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01789</id><submitter>Adriana Stan PhD</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:22:18 GMT</date><size>449kb</size><source_type>D</source_type></version><title>Speaker verification-derived loss and data augmentation for DNN-based
  multispeaker speech synthesis</title><authors>Beata Lorincz, Adriana Stan, Mircea Giurgiu</authors><categories>eess.AS cs.SD</categories><comments>Accepted at EUSIPCO 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Building multispeaker neural network-based text-to-speech synthesis systems
commonly relies on the availability of large amounts of high quality recordings
from each speaker and conditioning the training process on the speaker's
identity or on a learned representation of it. However, when little data is
available from each speaker, or the number of speakers is limited, the
multispeaker TTS can be hard to train and will result in poor speaker
similarity and naturalness.
  In order to address this issue, we explore two directions: forcing the
network to learn a better speaker identity representation by appending an
additional loss term; and augmenting the input data pertaining to each speaker
using waveform manipulation methods. We show that both methods are efficient
when evaluated with both objective and subjective measures. The additional loss
term aids the speaker similarity, while the data augmentation improves the
intelligibility of the multispeaker TTS system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01793</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01793</id><submitter>Quzhe Huang</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:29:40 GMT</date><size>5421kb</size><source_type>D</source_type></version><title>Three Sentences Are All You Need: Local Path Enhanced Document Relation
  Extraction</title><authors>Quzhe Huang, Shengqi Zhu, Yansong Feng, Yuan Ye, Yuxuan Lai, Dongyan
  Zhao</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document-level Relation Extraction (RE) is a more challenging task than
sentence RE as it often requires reasoning over multiple sentences. Yet, human
annotators usually use a small number of sentences to identify the relationship
between a given entity pair. In this paper, we present an embarrassingly simple
but effective method to heuristically select evidence sentences for
document-level RE, which can be easily combined with BiLSTM to achieve good
performance on benchmark datasets, even better than fancy graph neural network
based methods. We have released our code at
https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01797</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01797</id><submitter>Pengda Qin</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:36:01 GMT</date><size>4262kb</size><source_type>D</source_type></version><title>TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data</title><authors>Pengda Qin and Yuhong Li</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Among ubiquitous multimodal data in the real world, text is the modality
generated by human, while image reflects the physical world honestly. In a
visual understanding application, machines are expected to understand images
like human. Inspired by this, we propose a novel self-supervised learning
method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual
representations by fully utilizing the naturally-existing multimodal data. Our
core idea of self-supervised learning is to maximize the mutual information
between features extracted from multiple views of a shared context to a
rational degree. Different from previous methods which only consider multiple
views from a single modality, our work produces multiple views from different
modalities, and jointly optimizes the mutual information for features pairs of
intra-modality and inter-modality. Considering the information gap between
inter-modality features pairs from data noise, we adopt a \emph{ranking-based}
contrastive learning to optimize the mutual information. During evaluation, we
directly use the pre-trained visual representations to complete various image
classification tasks. Experimental results show that, TVDIM significantly
outperforms previous visual self-supervised methods when processing the same
set of images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01798</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01798</id><submitter>Mathias Niepert</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:42:21 GMT</date><size>1941kb</size><source_type>D</source_type></version><title>Implicit MLE: Backpropagating Through Discrete Exponential Family
  Distributions</title><authors>Mathias Niepert and Pasquale Minervini and Luca Franceschi</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Integrating discrete probability distributions and combinatorial optimization
problems into neural networks has numerous applications but poses several
challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a
framework for end-to-end learning of models combining discrete exponential
family distributions and differentiable neural components. I-MLE is widely
applicable: it only requires the ability to compute the most probable states;
and does not rely on smooth relaxations. The framework encompasses several
approaches, such as perturbation-based implicit differentiation and recent
methods to differentiate through black-box combinatorial solvers. We introduce
a novel class of noise distributions for approximating marginals via
perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood
estimation when used in some recently studied learning settings that involve
combinatorial solvers. Experiments on several datasets suggest that I-MLE is
competitive with and often outperforms existing approaches which rely on
problem-specific relaxations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01801</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01801</id><submitter>Ada Diaconescu Dr.</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:48:35 GMT</date><size>617kb</size></version><title>An Information-oriented Model of Multi-Scale (Feedback) Systems</title><authors>Ada Diaconescu (1), Louisa Jane Di Felice (2), Patricia Mellodge (3)
  ((1) Telecom Paris, LTCI, IPP, (2) Autonomous University of Barcelona, (3)
  Hartford University)</authors><categories>eess.SY cs.SY</categories><comments>7 pages, 3 Figures, presented at B-Series #5 online event, to be
  submitted to SISSY 2021 workshop (with IEEE ACSOS Intl. Conf.)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Multi-scale structures are prevalent in both natural and artificial systems,
as they can handle increasing complexity. Several terms are employed almost
interchangeably across various application domains to refer to the multi-scale
concept - e.g., hierarchy, holarchy, multi-level, multi-layer, nested,
embedded, micro-macro or coarse graining. While the concrete meanings behind
these terms may differ slightly, several core commonalities persist across all
cases. In this position paper we aim to highlight these common features of the
multi-scale concept, as a preliminary basis for a generic theory of multi-scale
systems. We discuss the concepts of scale and multi-scale systems in general,
and then of multi-scale feedback systems in particular, focusing on the role
played by information in such systems. Our long-term objective is to develop a
general theory of multi-scale feedback systems, applicable across all domains
dealing with complex systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01804</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01804</id><submitter>Haiyang Xu</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:50:26 GMT</date><size>655kb</size><source_type>D</source_type></version><title>E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual
  Learning</title><authors>Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming
  Xiao and Fei Huang</authors><categories>cs.CV cs.AI cs.CL</categories><journal-ref>ACL2021 main conference</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision-language pre-training (VLP) on large-scale image-text pairs has
achieved huge success for the cross-modal downstream tasks. The most existing
pre-training methods mainly adopt a two-step training procedure, which firstly
employs a pre-trained object detector to extract region-based visual features,
then concatenates the image representation and text embedding as the input of
Transformer to train. However, these methods face problems of using
task-specific visual representation of the specific object detector for generic
cross-modal understanding, and the computation inefficiency of two-stage
pipeline. In this paper, we propose the first end-to-end vision-language
pre-trained model for both V+L understanding and generation, namely E2E-VLP,
where we build a unified Transformer framework to jointly learn visual
representation, and semantic alignments between image and text. We incorporate
the tasks of object detection and image captioning into pre-training with a
unified Transformer encoder-decoder architecture for enhancing visual learning.
An extensive set of experiments have been conducted on well-established
vision-language downstream tasks to demonstrate the effectiveness of this novel
VLP paradigm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01805</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01805</id><submitter>Tiange Xiang</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:57:01 GMT</date><size>1822kb</size><source_type>D</source_type></version><title>Partial Graph Reasoning for Neural Network Regularization</title><authors>Tiange Xiang, Chaoyi Zhang, Yang Song, Siqi Liu, Hongliang Yuan,
  Weidong Cai</authors><categories>cs.LG cs.CV</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularizers helped deep neural networks prevent feature co-adaptations.
Dropout,as a commonly used regularization technique, stochastically disables
neuron ac-tivations during network optimization. However, such complete feature
disposal can affect the feature representation and network understanding.
Toward betterdescriptions of latent representations, we present DropGraph that
learns regularization function by constructing a stand-alone graph from the
backbone features. DropGraph first samples stochastic spatial feature vectors
and then incorporates graph reasoning methods to generate feature map
distortions. This add-on graph regularizes the network during training and can
be completely skipped during inference. We provide intuitions on the linkage
between graph reasoning andDropout with further discussions on how partial
graph reasoning method reduces feature correlations. To this end, we
extensively study the modeling of graphvertex dependencies and the utilization
of the graph for distorting backbone featuremaps. DropGraph was validated on
four tasks with a total of 7 different datasets.The experimental results show
that our method outperforms other state-of-the-art regularizers while leaving
the base model structure unmodified during inference.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01806</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01806</id><submitter>Dorcas Ofori-Boateng</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:58:04 GMT</date><size>839kb</size><source_type>D</source_type></version><title>Topological Anomaly Detection in Dynamic Multilayer Blockchain Networks</title><authors>Dorcas Ofori-Boateng, Ignacio Segovia Dominguez, Murat Kantarcioglu,
  Cuneyt G. Akcora, Yulia R. Gel</authors><categories>cs.CR math.AT stat.AP</categories><comments>26 pages, 6 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent surge of criminal activities with
cross-cryptocurrency trades, we introduce a new topological perspective to
structural anomaly detection in dynamic multilayer networks. We postulate that
anomalies in the underlying blockchain transaction graph that are composed of
multiple layers are likely to be also manifested in anomalous patterns of the
network shape properties. As such, we invoke the machinery of clique persistent
homology on graphs to systematically and efficiently track evolution of the
network shape and, as a result, to detect changes in the underlying network
topology and geometry. We develop a new persistence summary for multilayer
networks, called stacked persistence diagram, and prove its stability under
input data perturbations. We validate our new topological anomaly detection
framework in application to dynamic multilayer networks from the Ethereum
Blockchain and the Ripple Credit Network, and show that our stacked PD approach
substantially outperforms the state-of-art techniques, yielding up to 40% gains
in precision.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01808</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01808</id><submitter>Thierry Mora</submitter><version version="v1"><date>Thu, 3 Jun 2021 12:59:16 GMT</date><size>1089kb</size><source_type>D</source_type></version><title>MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood
  Inference from Sampled Trajectories</title><authors>Giulio Isacchini, Natanael Spisak, Armita Nourmohammad, Thierry Mora,
  Aleksandra M. Walczak</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation-based inference enables learning the parameters of a model even
when its likelihood cannot be computed in practice. One class of methods uses
data simulated with different parameters to infer an amortized estimator for
the likelihood-to-evidence ratio, or equivalently the posterior function. We
show that this approach can be formulated in terms of mutual information
maximization between model parameters and simulated data. We use this
equivalence to reinterpret existing approaches for amortized inference, and
propose two new methods that rely on lower bounds of the mutual information. We
apply our framework to the inference of parameters of stochastic processes and
chaotic dynamical systems from sampled trajectories, using artificial neural
networks for posterior prediction. Our approach provides a unified framework
that leverages the power of mutual information estimators for inference.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01809</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01809</id><submitter>Quzhe Huang</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:00:25 GMT</date><size>10509kb</size><source_type>D</source_type></version><title>Exploring Distantly-Labeled Rationales in Neural Network Models</title><authors>Quzhe Huang, Shengqi Zhu, Yansong Feng, Dongyan Zhao</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies strive to incorporate various human rationales into neural
networks to improve model performance, but few pay attention to the quality of
the rationales. Most existing methods distribute their models' focus to
distantly-labeled rationale words entirely and equally, while ignoring the
potential important non-rationale words and not distinguishing the importance
of different rationale words. In this paper, we propose two novel auxiliary
loss functions to make better use of distantly-labeled rationales, which
encourage models to maintain their focus on important words beyond labeled
rationales (PINs) and alleviate redundant training on non-helpful rationales
(NoIRs). Experiments on two representative classification tasks show that our
proposed methods can push a classification model to effectively learn crucial
clues from non-perfect rationales while maintaining the ability to spread its
focus to other unlabeled important words, thus significantly outperform
existing methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01810</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01810</id><submitter>Jiwei Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:00:28 GMT</date><size>259kb</size></version><title>Defending against Backdoor Attacks in Natural Language Generation</title><authors>Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei
  Li, Tianwei Zhang</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The frustratingly fragile nature of neural network models make current
natural language generation (NLG) systems prone to backdoor attacks and
generate malicious sequences that could be sexist or offensive. Unfortunately,
little effort has been invested to how backdoor attacks can affect current NLG
models and how to defend against these attacks. In this work, we investigate
this problem on two important NLG tasks, machine translation and dialogue
generation. By giving a formal definition for backdoor attack and defense, and
developing corresponding benchmarks, we design methods to attack NLG models,
which achieve high attack success to ask NLG models to generate malicious
sequences. To defend against these attacks, we propose to detect the attack
trigger by examining the effect of deleting or replacing certain words on the
generation outputs, which we find successful for certain types of attacks. We
will discuss the limitation of this work, and hope this work can raise the
awareness of backdoor risks concealed in deep NLG systems. (Code and data are
available at https://github.com/ShannonAI/backdoor_nlg.)
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01812</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01812</id><submitter>Adriana Stan PhD</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:04:59 GMT</date><size>2798kb</size><source_type>D</source_type></version><title>An objective evaluation of the effects of recording conditions and
  speaker characteristics in multi-speaker deep neural speech synthesis</title><authors>Beata Lorincz, Adriana Stan, Mircea Giurgiu</authors><categories>eess.AS cs.SD</categories><comments>Accepted at 25th International Conference on Knowledge-Based and
  Intelligent Information &amp; Engineering Systems (KES 2021)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Multi-speaker spoken datasets enable the creation of text-to-speech synthesis
(TTS) systems which can output several voice identities. The multi-speaker
(MSPK) scenario also enables the use of fewer training samples per speaker.
However, in the resulting acoustic model, not all speakers exhibit the same
synthetic quality, and some of the voice identities cannot be used at all.
  In this paper we evaluate the influence of the recording conditions, speaker
gender, and speaker particularities over the quality of the synthesised output
of a deep neural TTS architecture, namely Tacotron2. The evaluation is possible
due to the use of a large Romanian parallel spoken corpus containing over 81
hours of data. Within this setup, we also evaluate the influence of different
types of text representations: orthographic, phonetic, and phonetic extended
with syllable boundaries and lexical stress markings.
  We evaluate the results of the MSPK system using the objective measures of
equal error rate (EER) and word error rate (WER), and also look into the
distances between natural and synthesised t-SNE projections of the embeddings
computed by an accurate speaker verification network. The results show that
there is indeed a large correlation between the recording conditions and the
speaker's synthetic voice quality. The speaker gender does not influence the
output, and that extending the input text representation with syllable
boundaries and lexical stress information does not equally enhance the
generated audio across all speaker identities. The visualisation of the t-SNE
projections of the natural and synthesised speaker embeddings show that the
acoustic model shifts some of the speakers' neural representation, but not all
of them. As a result, these speakers have lower performances of the output
speech.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01813</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01813</id><submitter>Lizan Kivits</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:06:18 GMT</date><size>637kb</size><source_type>D</source_type></version><title>Identification of physical networks through structured polynomial models</title><authors>E.M.M. (Lizan) Kivits, Paul M.J. Van den Hof</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Physical dynamic networks most commonly consist of interconnections of
physical components that can be described by diffusive couplings. These
diffusive couplings imply that the cause-effect relationships in the
interconnections are symmetric and therefore physical dynamic networks can be
represented by undirected graphs. This paper shows how (prediction error)
identification methods developed for polynomial linear time-invariant systems
can be configured to consistently identify the parameters and the
interconnection structure of (undirected) physical networks. Further, a
multi-step least squares (convex) optimization algorithm is developed to solve
the nonconvex optimization problem that results from the identification method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01822</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01822</id><submitter>Ruben Brokkelkamp</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:17:19 GMT</date><size>29kb</size></version><title>Capturing Corruption with Hybrid Auctions: Social Welfare Loss in
  Multi-Unit Auctions</title><authors>Andries van Beek, Ruben Brokkelkamp and Guido Sch\&quot;afer</authors><categories>cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Corruption in auctions is a phenomenon that is theoretically still poorly
understood, despite the fact that it occurs rather frequently in practice. In
this paper, we initiate the study of the social welfare loss caused by a
corrupt auctioneer, both in the single-item and the multi-unit auction setting.
In our model, the auctioneer may collude with the winners of the auction by
letting them lower their bids in exchange for a fixed fraction $\gamma$ of the
surplus. As it turns out, this setting is equivalent to a $\gamma$-hybrid
auction in which the payments are a convex combination (parameterized by
$\gamma$) of the first-price and the second-price payments. Our goal is thus to
obtain a precise understanding of the (robust) price of anarchy of
$\gamma$-hybrid auctions. If no further restrictions are imposed on the bids,
we prove a bound on the robust POA which is tight (over the entire range of
$\gamma$) for the single-item and the multi-unit auction setting. On the other
hand, if the bids satisfy the no-overbidding assumption a more fine-grained
landscape of the price of anarchy emerges, depending on the auction setting and
the equilibrium notion. We derive tight bounds for single-item auctions up to
the correlated price of anarchy and for the pure price of anarchy in multi-unit
auctions. These results are complemented by nearly tight bounds on the coarse
correlated price of anarchy in both settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01826</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01826</id><submitter>Beren Millidge Mr</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:23:49 GMT</date><size>94kb</size><source_type>D</source_type></version><title>Towards a Mathematical Theory of Abstraction</title><authors>Beren Millidge</authors><categories>cs.AI stat.ML</categories><comments>03/06/21 initial upload</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the utility of well-chosen abstractions for understanding and
predicting the behaviour of complex systems is well appreciated, precisely what
an abstraction $\textit{is}$ has so far has largely eluded mathematical
formalization. In this paper, we aim to set out a mathematical theory of
abstraction. We provide a precise characterisation of what an abstraction is
and, perhaps more importantly, suggest how abstractions can be learnt directly
from data both for static datasets and for dynamical systems. We define an
abstraction to be a small set of `summaries' of a system which can be used to
answer a set of queries about the system or its behaviour. The difference
between the ground truth behaviour of the system on the queries and the
behaviour of the system predicted only by the abstraction provides a measure of
the `leakiness' of the abstraction which can be used as a loss function to
directly learn abstractions from data. Our approach can be considered a
generalization of classical statistics where we are not interested in
reconstructing `the data' in full, but are instead only concerned with
answering a set of arbitrary queries about the data. While highly theoretical,
our results have deep implications for statistical inference and machine
learning and could be used to develop explicit methods for learning precise
kinds of abstractions directly from data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01827</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01827</id><submitter>Roman Parovik</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:23:50 GMT</date><size>314kb</size><source_type>D</source_type></version><title>A Computer Program for the Numerical Analysis of Economic Cycles Within
  the Framework of the Dubovsky Generalized Model</title><authors>Danil Makarov and Roman Parovik</authors><categories>math.NA cs.NA cs.SC</categories><msc-class>37N30</msc-class><acm-class>G.1.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The article proposes a computer program for calculating economic crises
according to the generalized mathematical model of S.V. Dubovsky. This model is
represented by a system of ordinary nonlinear differential equations with
fractional derivatives in the sense of Gerasimov-Caputo with initial
conditions. Furthermore, according to a numerical algorithm based on an
explicit nonlocal finite-difference scheme, oscillograms and phase trajectories
were constructed. It is shown that changing the orders of fractional
derivatives in the model can give rise to various modes, for example, damped
modes with a steady-state amplitude. It is concluded that the orders of
fractional derivatives are responsible for the intensity of the process.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01830</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01830</id><submitter>Changhee Han Dr.</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:31:16 GMT</date><size>1758kb</size><source_type>D</source_type></version><title>Effort-free Automated Skeletal Abnormality Detection of Rat Fetuses on
  Whole-body Micro-CT Scans</title><authors>Akihiro Fukuda, Changhee Han, Kazumi Hakamada</authors><categories>eess.IV cs.CV cs.LG</categories><comments>5 pages, 5 figures, accepted to ICIP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning-based fast and quantitative automated screening plays a key
role in analyzing human bones on Computed Tomography (CT) scans. However,
despite the requirement in drug safety assessment, such research is rare on
animal fetus micro-CT scans due to its laborious data collection and
annotation. Therefore, we propose various bone feature engineering techniques
to thoroughly automate the skeletal localization/labeling/abnormality detection
of rat fetuses on whole-body micro-CT scans with minimum effort. Despite
limited training data of 49 fetuses, in skeletal labeling and abnormality
detection, we achieve accuracy of 0.900 and 0.810, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01834</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01834</id><submitter>Timoth\'ee Lesort</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:41:29 GMT</date><size>1319kb</size><source_type>D</source_type></version><title>Continual Learning in Deep Networks: an Analysis of the Last Layer</title><authors>Timoth\'ee Lesort, Thomas George, Irina Rish</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study how different output layer types of a deep neural network learn and
forget in continual learning settings. We describe the three factors affecting
catastrophic forgetting in the output layer: (1) weights modifications, (2)
interferences, and (3) projection drift. Our goal is to provide more insights
into how different types of output layers can address (1) and (2). We also
propose potential solutions and evaluate them on several benchmarks. We show
that the best-performing output layer type depends on the data distribution
drifts or the amount of data available. In particular, in some cases where a
standard linear layer would fail, it is sufficient to change the
parametrization and get significantly better performance while still training
with SGD. Our results and analysis shed light on the dynamics of the output
layer in continual learning scenarios and help select the best-suited output
layer for a given scenario.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01835</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01835</id><submitter>Pedro Carneiro Neto</submitter><version version="v1"><date>Wed, 2 Jun 2021 12:42:35 GMT</date><size>7874kb</size><source_type>D</source_type></version><title>Deep Learning Based Analysis of Prostate Cancer from MP-MRI</title><authors>Pedro C. Neto</authors><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The diagnosis of prostate cancer faces a problem with overdiagnosis that
leads to damaging side effects due to unnecessary treatment. Research has shown
that the use of multi-parametric magnetic resonance images to conduct biopsies
can drastically help to mitigate the overdiagnosis, thus reducing the side
effects on healthy patients. This study aims to investigate the use of deep
learning techniques to explore computer-aid diagnosis based on MRI as input.
Several diagnosis problems ranging from classification of lesions as being
clinically significant or not to the detection and segmentation of lesions are
addressed with deep learning based approaches.
  This thesis tackled two main problems regarding the diagnosis of prostate
cancer. Firstly, XmasNet was used to conduct two large experiments on the
classification of lesions. Secondly, detection and segmentation experiments
were conducted, first on the prostate and afterward on the prostate cancer
lesions. The former experiments explored the lesions through a two-dimensional
space, while the latter explored models to work with three-dimensional inputs.
For this task, the 3D models explored were the 3D U-Net and a pretrained 3D
ResNet-18. A rigorous analysis of all these problems was conducted with a total
of two networks, two cropping techniques, two resampling techniques, two crop
sizes, five input sizes and data augmentations experimented for lesion
classification. While for segmentation two models, two input sizes and data
augmentations were experimented. However, while the binary classification of
the clinical significance of lesions and the detection and segmentation of the
prostate already achieve the desired results (0.870 AUC and 0.915 dice score
respectively), the classification of the PIRADS score and the segmentation of
lesions still have a large margin to improve (0.664 accuracy and 0.690 dice
score respectively).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01836</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01836</id><submitter>Xiao Luo</submitter><version version="v1"><date>Wed, 2 Jun 2021 07:36:11 GMT</date><size>13567kb</size><source_type>D</source_type></version><title>DNA-GCN: Graph convolutional networks for predicting DNA-protein binding</title><authors>Yuhang Guo, Xiao Luo, Liang Chen and Minghua Deng</authors><categories>q-bio.GN cs.LG</categories><comments>10 pages, 3 figures</comments><journal-ref>In ICIC 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Predicting DNA-protein binding is an important and classic problem in
bioinformatics. Convolutional neural networks have outperformed conventional
methods in modeling the sequence specificity of DNA-protein binding. However,
none of the studies has utilized graph convolutional networks for motif
inference. In this work, we propose to use graph convolutional networks for
motif inference. We build a sequence k-mer graph for the whole dataset based on
k-mer co-occurrence and k-mer sequence relationship and then learn DNA Graph
Convolutional Network (DNA-GCN) for the whole dataset. Our DNA-GCN is
initialized with a one-hot representation for all nodes, and it then jointly
learns the embeddings for both k-mers and sequences, as supervised by the known
labels of sequences. We evaluate our model on 50 datasets from ENCODE. DNA-GCN
shows its competitive performance compared with the baseline model. Besides, we
analyze our model and design several different architectures to help fit
different datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01838</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01838</id><submitter>Jie Yang</submitter><version version="v1"><date>Wed, 2 Jun 2021 00:36:25 GMT</date><size>2085kb</size></version><title>Acoustic-based Object Detection for Pedestrian Using Smartphone</title><authors>Zi Wang, Jie Yang</authors><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Walking while using a smartphone is becoming a major pedestrian safety
concern as people may unknowingly bump into various obstacles that could lead
to severe injuries. In this paper, we propose ObstacleWatch, an acoustic-based
obstacle collision detection system to improve the safety of pedestrians who
are engaged in smartphone usage while walking. ObstacleWatch leverages the
advanced audio hardware of the smartphone to sense the surrounding obstacles
and infers fine-grained information about the frontal obstacle for collision
detection. In particular, our system emits well-designed inaudible beep signals
from the smartphone built-in speaker and listens to the reflections with the
stereo recording of the smartphone. By analyzing the reflected signals received
at two microphones, ObstacleWatch is able to extract fine-grained information
of the frontal obstacle including the distance, angle, and size for detecting
the possible collisions and to alert users. Our experimental evaluation under
two real-world environments with different types of phones and obstacles shows
that ObstacleWatch achieves over 92% accuracy in predicting obstacle collisions
with distance estimation errors at about 2 cm. Results also show that
ObstacleWatch is robust to different sizes of objects and is compatible with
different phone models with low energy consumption.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01840</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01840</id><submitter>Jie Yang</submitter><version version="v1"><date>Wed, 2 Jun 2021 00:32:40 GMT</date><size>1861kb</size></version><title>A Continuous Liveness Detection System for Text-independent Speaker
  Verification</title><authors>Linghan Zhang, Jie Yang</authors><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice authentication is drawing increasing attention and becomes an
attractive alternative to passwords for mobile authentication. Recent advances
in mobile technology further accelerate the adoption of voice biometrics in an
array of diverse mobile applications. However, recent studies show that voice
authentication is vulnerable to replay attacks, where an adversary can spoof a
voice authentication system using a pre-recorded voice sample collected from
the victim. In this paper, we propose VoiceLive, a liveness detection system
for both text-dependent and text-independent voice authentication on
smartphones. VoiceLive detects a live user by leveraging the user's unique
vocal system and the stereo recording of smartphones. In particular, utilizing
the built-in gyroscope, loudspeaker, and microphone, VoiceLive first measures
the smartphone's distance and angle from the user, then it captures the
position-specific time-difference-of-arrival (TDoA) changes in a sequence of
phoneme sounds to the two microphones of the phone, and uses such unique TDoA
dynamic which doesn't exist under replay attacks for liveness detection.
VoiceLive is practical as it doesn't require additional hardware but
two-channel stereo recording that is supported by virtually all smartphones.
Our experimental evaluation with 12 participants and different types of phones
shows that VoiceLive achieves over 99% detection accuracy at around 1% Equal
Error Rate (EER) on the text-dependent system and around 99% accuracy and 2%
EER on the text-independent one. Results also show that VoiceLive is robust to
different phone positions, i.e. the user is free to hold the smartphone with
distinct distances and angles.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01842</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01842</id><submitter>Joao Ramos</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:45:13 GMT</date><size>2182kb</size><source_type>D</source_type></version><title>The dynamic effect of mechanical losses of transmissions on the equation
  of motion of legged robots</title><authors>Youngwoo Sim and Joao Ramos</authors><categories>cs.RO</categories><comments>arXiv admin note: substantial text overlap with arXiv:2011.02506</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Industrial manipulators do not collapse under their own weight when powered
off due to the friction in their joints. Although these mechanism are effective
for stiff position control of pick-and-place, they are inappropriate for legged
robots that must rapidly regulate compliant interactions with the environment.
However, no metric exists to quantify the robot's performance degradation due
to mechanical losses in the actuators and transmissions. This paper provides a
fundamental formulation that uses the mechanical efficiency of transmissions to
quantify the effect of power losses in the mechanical transmissions on the
dynamics of a whole robotic system. We quantitatively demonstrate the intuitive
fact that the apparent inertia of the robots increase in the presence of joint
friction. We also show that robots that employ high gear ratio and low
efficiency transmissions can statically sustain more substantial external
loads. We expect that the framework presented here will provide the fundamental
tools for designing the next generation of legged robots that can effectively
interact with the world.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01847</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01847</id><submitter>Xiaohu Wu</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:51:54 GMT</date><size>143kb</size><source_type>D</source_type></version><title>Towards Cost-Optimal Policies for DAGs to Utilize IaaS Clouds with
  Online Learning</title><authors>Xiaohu Wu, Han Yu, Giuliano Casale, and Guanyu Gao</authors><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Premier cloud service providers (CSPs) offer two types of purchase options,
namely on-demand and spot instances, with time-varying features in availability
and price. Users like startups have to operate on a limited budget and
similarly others hope to reduce their costs. While interacting with a CSP,
central to their concerns is the process of cost-effectively utilizing
different purchase options possibly in addition to self-owned instances. A job
in data-intensive applications is typically represented by a directed acyclic
graph which can further be transformed into a chain of tasks. The key to
achieving cost efficiency is determining the allocation of a specific deadline
to each task, as well as the allocation of different types of instances to the
task. In this paper, we propose a framework that determines the optimal
allocation of deadlines to tasks. The framework also features an optimal policy
to determine the allocation of spot and on-demand instances in a predefined
time window, and a near-optimal policy for allocating self-owned instances. The
policies are designed to be parametric to support the usage of online learning
to infer the optimal values against the dynamics of cloud markets. Finally,
several intuitive heuristics are used as baselines to validate the cost
improvement brought by the proposed solutions. We show that the cost
improvement over the state-of-the-art is up to 24.87% when spot and on-demand
instances are considered and up to 59.05% when self-owned instances are
considered.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01850</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01850</id><submitter>Jian Xiang</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:54:59 GMT</date><size>64kb</size></version><title>Relational Analysis of Sensor Attacks on Cyber-Physical Systems</title><authors>Jian Xiang, Nathan Fulton, Stephen Chong</authors><categories>cs.CR</categories><comments>This is an extended version of the paper with the same title that
  appeared in the 2021 Computer Security Foundations Symposium</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cyber-physical systems, such as self-driving cars or autonomous aircraft,
must defend against attacks that target sensor hardware. Analyzing system
design can help engineers understand how a compromised sensor could impact the
system's behavior; however, designing security analyses for cyber-physical
systems is difficult due to their combination of discrete dynamics, continuous
dynamics, and nondeterminism.
  This paper contributes a framework for modeling and analyzing sensor attacks
on cyber-physical systems, using the formalism of hybrid programs. We formalize
and analyze two relational properties of a system's robustness. These
relational properties respectively express (1) whether a system's safety
property can be influenced by sensor attacks, and (2) whether a system's
high-integrity state can be affected by sensor attacks. We characterize these
relational properties by defining an equivalence relation between a system
under attack and the original unattacked system. That is, the system satisfies
the robustness properties if executions of the attacked system are
appropriately related to executions of the unattacked system.
  We present two techniques for reasoning about the equivalence relation and
thus proving the relational properties for a system. One proof technique
decomposes large proof obligations to smaller proof obligations. The other
proof technique adapts the self-composition technique from the literature on
secure information-flow, allowing us to reduce reasoning about the equivalence
of two systems to reasoning about properties of a single system. This technique
allows us to reuse existing tools for reasoning about properties of hybrid
programs, but is challenging due to the combination of discrete dynamics,
continuous dynamics, and nondeterminism.
  To evaluate, we present three case studies motivated by real design flaws in
existing cyber-physical systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01853</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01853</id><submitter>Amaury Pouly</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:57:28 GMT</date><size>26kb</size><source_type>D</source_type></version><title>On the Computation of the Algebraic Closure of Finitely Generated Groups
  of Matrices</title><authors>Klara Nosan, Amaury Pouly, Mahsa Shirmohammadi, James Worrell</authors><categories>cs.CC math.AG math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of computing the Zariski closure of a finitely
generated group of matrices. The Zariski closure was previously shown to be
computable by Derksen, Jeandel and Koiran, but the termination argument for
their algorithm appears not to yield any complexity bound. In this paper we
follow a different approach and obtain a bound on the degree of the polynomials
that define the closure. Our bound shows that the closure can be computed in
elementary time. We describe several applications of this result, e.g.,
concerning quantum automata and quantum universal gates. We also obtain an
upper bound on the length of a strictly increasing chain of linear algebraic
groups, all of which are generated over a fixed number field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01854</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01854</id><submitter>Ali Taghibakhshi</submitter><version version="v1"><date>Thu, 3 Jun 2021 13:57:32 GMT</date><size>2622kb</size><source_type>D</source_type></version><title>Optimization-Based Algebraic Multigrid Coarsening Using Reinforcement
  Learning</title><authors>Ali Taghibakhshi, Scott MacLachlan, Luke Olson, Matthew West</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large sparse linear systems of equations are ubiquitous in science and
engineering, such as those arising from discretizations of partial differential
equations. Algebraic multigrid (AMG) methods are one of the most common methods
of solving such linear systems, with an extensive body of underlying
mathematical theory. A system of linear equations defines a graph on the set of
unknowns and each level of a multigrid solver requires the selection of an
appropriate coarse graph along with restriction and interpolation operators
that map to and from the coarse representation. The efficiency of the multigrid
solver depends critically on this selection and many selection methods have
been developed over the years. Recently, it has been demonstrated that it is
possible to directly learn the AMG interpolation and restriction operators,
given a coarse graph selection. In this paper, we consider the complementary
problem of learning to coarsen graphs for a multigrid solver. We propose a
method using a reinforcement learning (RL) agent based on graph neural networks
(GNNs), which can learn to perform graph coarsening on small training graphs
and then be applied to unstructured large graphs. We demonstrate that this
method can produce better coarse graphs than existing algorithms, even as the
graph size increases and other properties of the graph are varied. We also
propose an efficient inference procedure for performing graph coarsening that
results in linear time complexity in graph size.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01858</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01858</id><submitter>Anders L{\o}land</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:01:21 GMT</date><size>764kb</size><source_type>D</source_type></version><title>Statistical embedding: Beyond principal components</title><authors>Dag Tj{\o}stheim and Martin Jullum and Anders L{\o}land</authors><categories>stat.ML cs.LG stat.ME</categories><msc-class>62-02, 62-07, 62H25, 62H30, 94-02, 94C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been an intense recent activity in embedding of very high
dimensional and nonlinear data structures, much of it in the data science and
machine learning literature. We survey this activity in four parts. In the
first part we cover nonlinear methods such as principal curves,
multidimensional scaling, local linear methods, ISOMAP, graph based methods and
kernel based methods. The second part is concerned with topological embedding
methods, in particular mapping topological properties into persistence
diagrams. Another type of data sets with a tremendous growth is very
high-dimensional network data. The task considered in part three is how to
embed such data in a vector space of moderate dimension to make the data
amenable to traditional techniques such as cluster and classification
techniques. The final part of the survey deals with embedding in
$\mathbb{R}^2$, which is visualization. Three methods are presented: $t$-SNE,
UMAP and LargeVis based on methods in parts one, two and three, respectively.
The methods are illustrated and compared on two simulated data sets; one
consisting of a triple of noisy Ranunculoid curves, and one consisting of
networks of increasing complexity and with two types of nodes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01860</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01860</id><submitter>Zhe Xu</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:02:45 GMT</date><size>669kb</size><source_type>D</source_type></version><title>Noisy Labels are Treasure: Mean-Teacher-Assisted Confident Learning for
  Hepatic Vessel Segmentation</title><authors>Zhe Xu, Donghuan Lu, Yixin Wang, Jie Luo, Jayender Jagadeesan, Kai Ma,
  Yefeng Zheng, Xiu Li</authors><categories>eess.IV cs.CV</categories><comments>11 pages, to appear in MICCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manually segmenting the hepatic vessels from Computer Tomography (CT) is far
more expertise-demanding and laborious than other structures due to the
low-contrast and complex morphology of vessels, resulting in the extreme lack
of high-quality labeled data. Without sufficient high-quality annotations, the
usual data-driven learning-based approaches struggle with deficient training.
On the other hand, directly introducing additional data with low-quality
annotations may confuse the network, leading to undesirable performance
degradation. To address this issue, we propose a novel mean-teacher-assisted
confident learning framework to robustly exploit the noisy labeled data for the
challenging hepatic vessel segmentation task. Specifically, with the adapted
confident learning assisted by a third party, i.e., the weight-averaged teacher
model, the noisy labels in the additional low-quality dataset can be
transformed from &quot;encumbrance&quot; to &quot;treasure&quot; via progressive pixel-wise
soft-correction, thus providing productive guidance. Extensive experiments
using two public datasets demonstrate the superiority of the proposed framework
as well as the effectiveness of each component.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01861</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01861</id><submitter>Yuma Kinoshita</submitter><version version="v1"><date>Tue, 1 Jun 2021 08:31:47 GMT</date><size>276kb</size><source_type>D</source_type></version><title>Separated-Spectral-Distribution Estimation Based on Bayesian Inference
  with Single RGB Camera</title><authors>Yuma Kinoshita and Hitoshi Kiya</authors><categories>eess.IV cs.CV cs.MM</categories><comments>to appear in IEEE ICIP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a novel method for separately estimating spectral
distributions from images captured by a typical RGB camera. The proposed method
allows us to separately estimate a spectral distribution of illumination,
reflectance, or camera sensitivity, while recent hyperspectral cameras are
limited to capturing a joint spectral distribution from a scene. In addition,
the use of Bayesian inference makes it possible to take into account prior
information of both spectral distributions and image noise as probability
distributions. As a result, the proposed method can estimate spectral
distributions in a unified way, and it can enhance the robustness of the
estimation against noise, which conventional spectral-distribution estimation
methods cannot. The use of Bayesian inference also enables us to obtain the
confidence of estimation results. In an experiment, the proposed method is
shown not only to outperform conventional estimation methods in terms of RMSE
but also to be robust against noise.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01862</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01862</id><submitter>Jesse Hagenaars</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:03:41 GMT</date><size>8584kb</size><source_type>D</source_type></version><title>Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural
  Networks</title><authors>Federico Paredes-Vall\'es, Jesse Hagenaars, Guido de Croon</authors><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic sensing and computing hold a promise for highly energy-efficient
and high-bandwidth-sensor processing. A major challenge for neuromorphic
computing is that learning algorithms for traditional artificial neural
networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due
to the discrete spikes and more complex neuronal dynamics. As a consequence,
SNNs have not yet been successfully applied to complex, large-scale tasks. In
this article, we focus on the self-supervised learning problem of optical flow
estimation from event-based camera inputs, and investigate the changes that are
necessary to the state-of-the-art ANN training pipeline in order to
successfully tackle it with SNNs. More specifically, we first modify the input
event representation to encode a much smaller time slice with minimal explicit
temporal information. Consequently, we make the network's neuronal dynamics and
recurrent connections responsible for integrating information over time.
Moreover, we reformulate the self-supervised loss function for event-based
optical flow to improve its convexity. We perform experiments with various
types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs,
we investigate the effects of elements such as parameter initialization and
optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We
find that initialization and surrogate gradient width play a crucial part in
enabling learning with sparse inputs, while the inclusion of adaptivity and
learnable neuronal parameters can improve performance. We show that the
performance of the proposed ANNs and SNNs are on par with that of the current
state-of-the-art ANNs trained in a self-supervised manner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01863</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01863</id><submitter>Yuming Jiang</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:40:36 GMT</date><size>10347kb</size><source_type>D</source_type></version><title>Robust Reference-based Super-Resolution via C2-Matching</title><authors>Yuming Jiang, Kelvin C.K. Chan, Xintao Wang, Chen Change Loy, Ziwei
  Liu</authors><categories>cs.CV cs.LG eess.IV</categories><comments>To appear in CVPR2021. The source code is available at
  https://github.com/yumingj/C2-Matching</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising
paradigm to enhance a low-resolution (LR) input image by introducing an
additional high-resolution (HR) reference image. Existing Ref-SR methods mostly
rely on implicit correspondence matching to borrow HR textures from reference
images to compensate for the information loss in input images. However,
performing local transfer is difficult because of two gaps between input and
reference images: the transformation gap (e.g. scale and rotation) and the
resolution gap (e.g. HR and LR). To tackle these challenges, we propose
C2-Matching in this work, which produces explicit robust matching crossing
transformation and resolution. 1) For the transformation gap, we propose a
contrastive correspondence network, which learns transformation-robust
correspondences using augmented views of the input image. 2) For the resolution
gap, we adopt a teacher-student correlation distillation, which distills
knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR
matching. 3) Finally, we design a dynamic aggregation module to address the
potential misalignment issue. In addition, to faithfully evaluate the
performance of Ref-SR under a realistic setting, we contribute the
Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario.
Extensive experiments demonstrate that our proposed C2-Matching significantly
outperforms state of the arts by over 1dB on the standard CUFED5 benchmark.
Notably, it also shows great generalizability on WR-SR dataset as well as
robustness across large scale and rotation transformations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01865</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01865</id><submitter>Taufiq Hasan</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:09:04 GMT</date><size>4933kb</size><source_type>D</source_type></version><title>Heart Sound Classification Considering Additive Noise and Convolutional
  Distortion</title><authors>Farhat Binte Azam, Md. Istiaq Ansari, Ian Mclane, Taufiq Hasan</authors><categories>cs.SD cs.AI eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cardiac auscultation is an essential point-of-care method used for the early
diagnosis of heart diseases. Automatic analysis of heart sounds for abnormality
detection is faced with the challenges of additive noise and sensor-dependent
degradation. This paper aims to develop methods to address the cardiac
abnormality detection problem when both types of distortions are present in the
cardiac auscultation sound. We first mathematically analyze the effect of
additive and convolutional noise on short-term filterbank-based features and a
Convolutional Neural Network (CNN) layer. Based on the analysis, we propose a
combination of linear and logarithmic spectrogram-image features. These 2D
features are provided as input to a residual CNN network (ResNet) for heart
sound abnormality detection. Experimental validation is performed on an
open-access heart sound abnormality detection dataset involving noisy
recordings obtained from multiple stethoscope sensors. The proposed method
achieves significantly improved results compared to the conventional
approaches, with an area under the ROC (receiver operating characteristics)
curve (AUC) of 91.36%, F-1 score of 84.09%, and Macc (mean of sensitivity and
specificity) of 85.08%. We also show that the proposed method shows the best
mean accuracy across different source domains including stethoscope and noise
variability, demonstrating its effectiveness in different recording conditions.
The proposed combination of linear and logarithmic features along with the
ResNet classifier effectively minimizes the impact of background noise and
sensor variability for classifying phonocardiogram (PCG) signals. The proposed
method paves the way towards developing computer-aided cardiac auscultation
systems in noisy environments using low-cost stethoscopes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01866</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01866</id><submitter>Hamidreza Kasaei</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:12:11 GMT</date><size>4349kb</size><source_type>D</source_type></version><title>Simultaneous Multi-View Object Recognition and Grasping in Open-Ended
  Domains</title><authors>Hamidreza Kasaei, Sha Luo, Remo Sasso, Mohammadreza Kasaei</authors><categories>cs.RO cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:2103.10997</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robot working in human-centric environments needs to know which kind of
objects exist in the scene, where they are, and how to grasp and manipulate
various objects in different situations to help humans in everyday tasks.
Therefore, object recognition and grasping are two key functionalities for such
robots. Most state-of-the-art tackles object recognition and grasping as two
separate problems while both use visual input. Furthermore, the knowledge of
the robot is fixed after the training phase. In such cases, if the robot faces
new object categories, it must retrain from scratch to incorporate new
information without catastrophic interference. To address this problem, we
propose a deep learning architecture with augmented memory capacities to handle
open-ended object recognition and grasping simultaneously. In particular, our
approach takes multi-views of an object as input and jointly estimates
pixel-wise grasp configuration as well as a deep scale- and rotation-invariant
representation as outputs. The obtained representation is then used for
open-ended object recognition through a meta-active learning technique. We
demonstrate the ability of our approach to grasp never-seen-before objects and
to rapidly learn new object categories using very few examples on-site in both
simulation and real-world settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01870</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01870</id><submitter>James Hsin-Yu Chiang</submitter><version version="v1"><date>Wed, 2 Jun 2021 10:32:05 GMT</date><size>128kb</size><source_type>D</source_type></version><title>Maximizing Extractable Value from Automated Market Makers</title><authors>Massimo Bartoletti, James Hsin-yu Chiang, Alberto Lluch-Lafuente</authors><categories>cs.CR cs.CE cs.FL cs.GT</categories><comments>12 pages. Under submission</comments><msc-class>68N30</msc-class><acm-class>I.6.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated Market Makers (AMMs) are decentralized applications that allow
users to exchange crypto-tokens without the need to find a matching exchange
order. AMMs are one of the most successful DeFi use case so far, as the main
AMM platforms UniSwap and Balancer process a daily volume of transactions worth
billions of dollars. Despite this success story, AMMs are well-known to suffer
from transaction-ordering issues: indeed, adversaries can frontrun user
transactions to increase their gain to the detriment of honest users. Being
specifically designated to arrange user transactions into blocks, miners can
easily play the role of adversary, by suitably selecting and ordering
transactions - and possibly inserting their own - to increase their gain. In
this paper we formally characterize rational miners as players which follow an
optimal strategy in the mining game. We identify relevant variants of the game,
corresponding to specific real-world constraints that a miner might have. We
devise effective procedures to construct solutions to mining game, both in its
most general form and in some relevant variants. Most notably, miners can
exploit these solutions to maximize the value extracted from user transactions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01871</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01871</id><submitter>Xin Tao</submitter><version version="v1"><date>Fri, 28 May 2021 10:31:51 GMT</date><size>1333kb</size><source_type>D</source_type></version><title>Short-term Maintenance Planning of Autonomous Trucks for Minimizing
  Economic Risk</title><authors>Xin Tao, Jonas M{\aa}rtensson, H{\aa}kan Warnquist, Anna Pernest{\aa}l</authors><categories>cs.AI cs.SY eess.SY</categories><comments>22 pages, 13 figures, journal, submitted to Reliability Engineering &amp;
  System Safety, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New autonomous driving technologies are emerging every day and some of them
have been commercially applied in the real world. While benefiting from these
technologies, autonomous trucks are facing new challenges in short-term
maintenance planning, which directly influences the truck operator's profit. In
this paper, we implement a vehicle health management system by addressing the
maintenance planning issues of autonomous trucks on a transport mission. We
also present a maintenance planning model using a risk-based decision-making
method, which identifies the maintenance decision with minimal economic risk of
the truck company. Both availability losses and maintenance costs are
considered when evaluating the economic risk. We demonstrate the proposed model
by numerical experiments illustrating real-world scenarios. In the experiments,
compared to three baseline methods, the expected economic risk of the proposed
method is reduced by up to $47\%$. We also conduct sensitivity analyses of
different model parameters. The analyses show that the economic risk
significantly decreases when the estimation accuracy of remaining useful life,
the maximal allowed time of delivery delay before order cancellation, or the
number of workshops increases. The experiment results contribute to identifying
future research and development attentions of autonomous trucks from an
economic perspective.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01872</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01872</id><submitter>Hiro Wakimura</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:18:57 GMT</date><size>34982kb</size></version><title>Symmetry-preserving enforcement of low-dissipation method based on
  boundary variation diminishing principle</title><authors>Hiro Wakimura (1), Shinichi Takagi (1) and Feng Xiao (1) ((1) Tokyo
  Institute of Technology)</authors><categories>math.NA cs.NA physics.comp-ph</categories><comments>32 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of high-order shock-capturing schemes, P$_n$T$_m$-BVD (Deng et al.,
J. Comp. Phys., 386:323-349, 2019; Comput. &amp; Fluids, 200:104433, 2020.)
schemes, have been devised to solve the Euler equations with substantially
reduced numerical dissipation, which enable high-resolution simulations to
resolve flow structures of wider range scales. In such simulations with low
dissipation, errors of round-off level might grow and contaminate the numerical
solutions. A typical example of such problems is the loss of symmetry in the
numerical solutions for physical problems of symmetric configurations even if
the schemes are mathematically in line with the symmetry rules. In this study,
the mechanisms of symmetry-breaking in a finite volume framework with the
P$_4$T$_2$-BVD reconstruction scheme are thoroughly examined. Particular
attention has been paid to remove the possible causes due to the lack of
associativity in floating-point arithmetic which is associated with round-off
errors. Modifications and new techniques are proposed to completely remove the
possible causes for symmetry breaking in different components of the
P$_4$T$_2$-BVD finite volume solver. Benchmark tests that have symmetric
solution structures are used to verify the proposed methods. The numerical
results demonstrate the perfect symmetric solution structures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01880</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01880</id><submitter>Peter Davies</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:25:46 GMT</date><size>105kb</size></version><title>Component Stability in Low-Space Massively Parallel Computation</title><authors>Artur Czumaj, Peter Davies, Merav Parter</authors><categories>cs.DS cs.DC</categories><comments>45 pages, to appear at PODC 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the power and limitations of component-stable algorithms in the
low-space model of Massively Parallel Computation (MPC). Recently Ghaffari,
Kuhn and Uitto (FOCS 2019) introduced the class of component-stable low-space
MPC algorithms, which are, informally, defined as algorithms for which the
outputs reported by the nodes in different connected components are required to
be independent. This very natural notion was introduced to capture most (if not
all) of the known efficient MPC algorithms to date, and it was the first
general class of MPC algorithms for which one can show non-trivial conditional
lower bounds. In this paper we enhance the framework of component-stable
algorithms and investigate its effect on the complexity of randomized and
deterministic low-space MPC. Our key contributions include:
  1) We revise and formalize the lifting approach of Ghaffari, Kuhn and Uitto.
This requires a very delicate amendment of the notion of component stability,
which allows us to fill in gaps in the earlier arguments.
  2) We also extend the framework to obtain conditional lower bounds for
deterministic algorithms and fine-grained lower bounds that depend on the
maximum degree $\Delta$.
  3) We demonstrate a collection of natural graph problems for which
non-component-stable algorithms break the conditional lower bound obtained for
component-stable algorithms. This implies that, for both deterministic and
randomized algorithms, component-stable algorithms are conditionally weaker
than the non-component-stable ones.
  Altogether our results imply that component-stability might limit the
computational power of the low-space MPC model, at least in certain contexts,
paving the way for improved upper bounds that escape the conditional lower
bound setting of Ghaffari, Kuhn, and Uitto.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01883</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01883</id><submitter>Xue Yang</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:29:19 GMT</date><size>10053kb</size><source_type>D</source_type></version><title>Learning High-Precision Bounding Box for Rotated Object Detection via
  Kullback-Leibler Divergence</title><authors>Xue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian,
  Junchi Yan</authors><categories>cs.CV cs.AI cs.LG</categories><comments>14 pages, 3 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing rotated object detectors are mostly inherited from the horizontal
detection paradigm, as the latter has evolved into a well-developed area.
However, these detectors are difficult to perform prominently in high-precision
detection due to the limitation of current regression loss design, especially
for objects with large aspect ratios. Taking the perspective that horizontal
detection is a special case for rotated object detection, in this paper, we are
motivated to change the design of rotation regression loss from induction
paradigm to deduction methodology, in terms of the relation between rotation
and horizontal detection. We show that one essential challenge is how to
modulate the coupled parameters in the rotation regression loss, as such the
estimated parameters can influence to each other during the dynamic joint
optimization, in an adaptive and synergetic way. Specifically, we first convert
the rotated bounding box into a 2-D Gaussian distribution, and then calculate
the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the
regression loss. By analyzing the gradient of each parameter, we show that KLD
(and its derivatives) can dynamically adjust the parameter gradients according
to the characteristics of the object. It will adjust the importance (gradient
weight) of the angle parameter according to the aspect ratio. This mechanism
can be vital for high-precision detection as a slight angle error would cause a
serious accuracy drop for large aspect ratios objects. More importantly, we
have proved that KLD is scale invariant. We further show that the KLD loss can
be degenerated into the popular $l_{n}$-norm loss for horizontal detection.
Experimental results on seven datasets using different detectors show its
consistent superiority, and codes are available at
https://github.com/yangxue0827/RotationDetection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01885</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01885</id><submitter>Ayushi Rastogi</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:31:37 GMT</date><size>66364kb</size><source_type>D</source_type></version><title>How does Software Change?</title><authors>Ayushi Rastogi, Georgios Gousios</authors><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software evolves with changes to its codebase over time. Internally, software
changes in response to decisions to include some code change into the codebase
and discard others. Explaining the mechanism of software evolution, this paper
presents a theory of software change. Our theory is grounded in multiple
evidence sources (e.g., GitHub documentation and relevant scientific
literature) relating to the pull-based development model in GitHub. The
resulting theory explains the influence of project-related core concepts (e.g.,
people and governance) as well as its ecosystem on the decision of software
change.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01890</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01890</id><submitter>Yixin Liu</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:34:17 GMT</date><size>5335kb</size><source_type>D</source_type></version><title>SimCLS: A Simple Framework for Contrastive Learning of Abstractive
  Summarization</title><authors>Yixin Liu, Pengfei Liu</authors><categories>cs.CL</categories><comments>Published as a short paper at ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a conceptually simple while empirically powerful
framework for abstractive summarization, SimCLS, which can bridge the gap
between the learning objective and evaluation metrics resulting from the
currently dominated sequence-to-sequence learning framework by formulating text
generation as a reference-free evaluation problem (i.e., quality estimation)
assisted by contrastive learning. Experimental results show that, with minor
modification over existing top-scoring systems, SimCLS can improve the
performance of existing top-performing models by a large margin. Particularly,
2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on
the CNN/DailyMail dataset, driving the state-of-the-art performance to a new
level. We have open-sourced our codes and results:
https://github.com/yixinL7/SimCLS. Results of our proposed models have been
deployed into ExplainaBoard platform, which allows researchers to understand
our systems in a more fine-grained way.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01893</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01893</id><submitter>Ilham Courie</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:36:23 GMT</date><size>3525kb</size><source_type>D</source_type></version><title>Worst-Case Pointing Performance Analysis for Large Flexible Spacecraft</title><authors>Ilham Courie, Francesco Sanfedino, Daniel Alazard</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tool, PELIB, developed in MATLAB /SIMULINK environment
to perform pointing performance analysis based on European pointing standards.
PELIB is designed as an extension of the Satellite Dynamics Toolbox (SDT),
which derives the Linear Fractional Transformation (LFT) models of flexible
space structures. The addition of PELIB will allow the users of SDT to perform
pointing performance analysis of real mission scenarios in the same environment
used for control synthesis. PELIB offers as well the possibility to take into
account uncertainties in the system. This feature represents an enhancement to
the current verification tools available in the European space industry
community by providing the worst-case pointing budget. The capabilities of
PELIB were demonstrated in a case study involving a spacecraft model with two
flexible solar arrays. Several error sources, as well as uncertain parameters,
were included in this model. The nominal performance has been investigated
using PELIB and compared with the current European reference tool. The
worst-case performance is also investigated with the new feature of PELIB to
obtain the worst-case performance budget
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01894</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01894</id><submitter>Merav Parter</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:38:18 GMT</date><size>671kb</size><source_type>D</source_type></version><title>Low-Congestion Shortcuts in Constant Diameter Graphs</title><authors>Shimon Kogan and Merav Parter</authors><categories>cs.DS cs.DC</categories><comments>To appear in PODC 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low congestion shortcuts, introduced by Ghaffari and Haeupler (SODA 2016),
provide a unified framework for global optimization problems in the congest
model of distributed computing. Roughly speaking, for a given graph $G$ and a
collection of vertex-disjoint connected subsets $S_1,\ldots, S_\ell \subseteq
V(G)$, $(c,d)$ low-congestion shortcuts augment each subgraph $G[S_i]$ with a
subgraph $H_i \subseteq G$ such that: (i) each edge appears on at most $c$
subgraphs (congestion bound), and (ii) the diameter of each subgraph $G[S_i]
\cup H_i$ is bounded by $d$ (dilation bound). It is desirable to compute
shortcuts of small congestion and dilation as these quantities capture the
round complexity of many global optimization problems in the congest model. For
$n$-vertex graphs with constant diameter $D=O(1)$, Das-Sarma et al. (STOC 2011,
SIAM J. Comput. 2012) presented an (implicit) shortcuts lower bound with (As
usual, $\tilde{O}()$ and $\tilde{\Omega}()$ hide poly-logarithmic factors.)
$c+d=\widetilde{\Omega}(n^{(D-2)/(2D-2)})$. A nearly matching upper bound,
however, was only recently obtained for $D \in \{3,4\}$ by Kitamura et al.
(DISC 2019).
  In this work, we resolve the long-standing complexity gap of shortcuts in
constant diameter graphs, originally posed by Lotker et al. (PODC 2001). We
present new shortcut constructions which match, up to poly-logarithmic terms,
the lower bounds of Das-Sarma et al. As a result, we provide improved and
existentially optimal algorithms for several network optimization tasks in
constant diameter graphs, including MST, $(1+\epsilon)$-approximate minimum
cuts and more.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01895</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01895</id><submitter>Abhinav Sinha</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:38:37 GMT</date><size>11498kb</size><source_type>D</source_type></version><title>Three-agent Time-constrained Cooperative Pursuit-Evasion</title><authors>Abhinav Sinha, Shashi Ranjan Kumar, Dwaipayan Mukherjee</authors><categories>eess.SY cs.MA cs.RO cs.SY math.OC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper considers a pursuit-evasion scenario among three agents -- an
evader, a pursuer, and a defender. We design cooperative guidance laws for the
evader and the defender team to safeguard the evader from an attacking pursuer.
Unlike differential games, optimal control formulations, and other heuristic
methods, we propose a novel perspective on designing effective nonlinear
feedback control laws for the evader-defender team using a time-constrained
guidance approach. The evader lures the pursuer on the collision course by
offering itself as bait. At the same time, the defender protects the evader
from the pursuer by exercising control over the engagement duration. Depending
on the nature of the mission, the defender may choose to take an aggressive or
defensive stance. Such consideration widens the applicability of the proposed
methods in various three-agent motion planning scenarios such as aircraft
defense, asset guarding, search and rescue, surveillance, and secure
transportation. We use a fixed-time sliding mode control strategy to design the
control laws for the evader-defender team and a nonlinear finite-time
disturbance observer to estimate the pursuer's maneuver. Finally, we present
simulations to demonstrate favorable performance under various engagement
geometries, thus vindicating the efficacy of the proposed designs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01896</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01896</id><submitter>Battula Balnarsaiah Mr</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:39:30 GMT</date><size>1892kb</size></version><title>Denoising and Optical and SAR Image Classifications Based on Feature
  Extraction and Sparse Representation</title><authors>Battula Balnarsaiah, G Rajitha</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical image data have been used by the Remote Sensing workforce to study
land use and cover since such data is easily interpretable. Synthetic Aperture
Radar (SAR) has the characteristic of obtaining images during all-day,
all-weather and provides object information that is different from visible and
infrared sensors. However, SAR images have more speckle noise and fewer
dimensions. This paper presents a method for denoising, feature extraction and
compares classifications of Optical and SAR images. The image was denoised
using K-Singular Value Decomposition (K-SVD) algorithm. A method to map the
extraordinary goal signatures to be had withinside the SAR or Optical image
using support vector machine (SVM) through offering given the enter facts to
the supervised classifier. Initially, the Gray Level Histogram (GLH) and Gray
Level Co-occurrence Matrix (GLCM) are used for feature extraction. Secondly,
the extracted feature vectors from the first step were combined using
correlation analysis to reduce the dimensionality of the feature spaces.
Thirdly, the Classification of SAR images was done in Sparse Representations
Classification (SRC). The above-mentioned classifications techniques were
developed and performance parameters are accuracy and Kappa Coefficient
calculated using MATLAB 2018a.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01899</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01899</id><submitter>Xinjie Fan</submitter><version version="v1"><date>Tue, 1 Jun 2021 23:58:23 GMT</date><size>6878kb</size><source_type>D</source_type></version><title>Adversarially Adaptive Normalization for Single Domain Generalization</title><authors>Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong, Mingyuan
  Zhou</authors><categories>cs.CV</categories><comments>CVPR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single domain generalization aims to learn a model that performs well on many
unseen domains with only one domain data for training. Existing works focus on
studying the adversarial domain augmentation (ADA) to improve the model's
generalization capability. The impact on domain generalization of the
statistics of normalization layers is still underinvestigated. In this paper,
we propose a generic normalization approach, adaptive standardization and
rescaling normalization (ASR-Norm), to complement the missing part in previous
works. ASR-Norm learns both the standardization and rescaling statistics via
neural networks. This new form of normalization can be viewed as a generic form
of the traditional normalizations. When trained with ADA, the statistics in
ASR-Norm are learned to be adaptive to the data coming from different domains,
and hence improves the model generalization performance across domains,
especially on the target domain with large discrepancy from the source domain.
The experimental results show that ASR-Norm can bring consistent improvement to
the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the
Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the
improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01900</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01900</id><submitter>Luca Mariot</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:43:37 GMT</date><size>84kb</size><source_type>D</source_type></version><title>Salp Swarm Optimization: a Critical Review</title><authors>Mauro Castelli, Luca Manzoni, Luca Mariot, Marco S. Nobile, Andrea
  Tangherloni</authors><categories>cs.NE</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the crowded environment of bio-inspired population-based meta-heuristics,
the Salp Swarm Optimization (SSO) algorithm recently appeared and immediately
gained a lot of momentum. Inspired by the peculiar spatial arrangement of salp
colonies, which are displaced in long chains following a leader, this algorithm
seems to provide interesting optimization performances. However, the original
work was characterized by some conceptual and mathematical flaws, which
influenced all ensuing papers on the subject. In this manuscript, we perform a
critical review of SSO, highlighting all the issues present in the literature
and their negative effects on the optimization process carried out by the
algorithm. We also propose a mathematically correct version of SSO, named
Amended Salp Swarm Optimizer (ASSO) that fixes all the discussed problems.
Finally, we benchmark the performance of ASSO on a set of tailored experiments,
showing it achieves better results than the original SSO.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01901</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01901</id><submitter>Max Smith</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:44:46 GMT</date><size>8079kb</size><source_type>D</source_type></version><title>Iterative Empirical Game Solving via Single Policy Best Response</title><authors>Max Olan Smith, Thomas Anthony, Michael P. Wellman</authors><categories>cs.MA</categories><journal-ref>ICLR 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy-Space Response Oracles (PSRO) is a general algorithmic framework for
learning policies in multiagent systems by interleaving empirical game analysis
with deep reinforcement learning (Deep RL). At each iteration, Deep RL is
invoked to train a best response to a mixture of opponent policies. The
repeated application of Deep RL poses an expensive computational burden as we
look to apply this algorithm to more complex domains. We introduce two
variations of PSRO designed to reduce the amount of simulation required during
Deep RL training. Both algorithms modify how PSRO adds new policies to the
empirical game, based on learned responses to a single opponent policy. The
first, Mixed-Oracles, transfers knowledge from previous iterations of Deep RL,
requiring training only against the opponent's newest policy. The second,
Mixed-Opponents, constructs a pure-strategy opponent by mixing existing
strategy's action-value estimates, instead of their policies. Learning against
a single policy mitigates variance in state outcomes that is induced by an
unobserved distribution of opponents. We empirically demonstrate that these
algorithms substantially reduce the amount of simulation during training
required by PSRO, while producing equivalent or better solutions to the game.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01902</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01902</id><submitter>Henri Gode</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:46:35 GMT</date><size>71kb</size></version><title>Joint Multi-Channel Dereverberation and Noise Reduction Using a Unified
  Convolutional Beamformer With Sparse Priors</title><authors>Henri Gode, Marvin Tammen, Simon Doclo</authors><categories>eess.AS cs.SD</categories><comments>ITG Conference on Speech Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the convolutional weighted power minimization distortionless
response (WPD) beamformer was proposed, which unifies multi-channel weighted
prediction error dereverberation and minimum power distortionless response
beamforming. To optimize the convolutional filter, the desired speech component
is modeled with a time-varying Gaussian model, which promotes the sparsity of
the desired speech component in the short-time Fourier transform domain
compared to the noisy microphone signals. In this paper we generalize the
convolutional WPD beamformer by using an lp-norm cost function, introducing an
adjustable shape parameter which enables to control the sparsity of the desired
speech component. Experiments based on the REVERB challenge dataset show that
the proposed method outperforms the conventional convolutional WPD beamformer
in terms of objective speech quality metrics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01904</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01904</id><submitter>Lorenzo Bertolini</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:53:34 GMT</date><size>7341kb</size><source_type>D</source_type></version><title>Representing Syntax and Composition with Geometric Transformations</title><authors>Lorenzo Bertolini, Julie Weeds, David Weir, Qiwei Peng</authors><categories>cs.CL</categories><comments>to appear in Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploitation of syntactic graphs (SyGs) as a word's context has been
shown to be beneficial for distributional semantic models (DSMs), both at the
level of individual word representations and in deriving phrasal
representations via composition. However, notwithstanding the potential
performance benefit, the syntactically-aware DSMs proposed to date have huge
numbers of parameters (compared to conventional DSMs) and suffer from data
sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic
relations) has been largely limited to linear maps. The knowledge graphs'
literature, on the other hand, has proposed light-weight models employing
different geometric transformations (GTs) to encode edges in a knowledge graph
(KG). Our work explores the possibility of adopting this family of models to
encode SyGs. Furthermore, we investigate which GT better encodes syntactic
relations, so that these representations can be used to enhance phrase-level
composition via syntactic contextualisation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01907</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01907</id><submitter>Jinglun Feng</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:58:49 GMT</date><size>12342kb</size><source_type>D</source_type></version><title>Robotic Inspection and 3D GPR-based Reconstruction for Underground
  Utilities</title><authors>Jinglun Feng, Liang Yang, Jiang Biao, Jizhong Xiao</authors><categories>eess.IV cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:2011.02635</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ground Penetrating Radar (GPR) is an effective non-destructive evaluation
(NDE) device for inspecting and surveying subsurface objects (i.e., rebars,
utility pipes) in complex environments. However, the current practice for GPR
data collection requires a human inspector to move a GPR cart along pre-marked
grid lines and record the GPR data in both X and Y directions for
post-processing by 3D GPR imaging software. It is time-consuming and tedious
work to survey a large area. Furthermore, identifying the subsurface targets
depends on the knowledge of an experienced engineer, who has to make manual and
subjective interpretation that limits the GPR applications, especially in
large-scale scenarios. In addition, the current GPR imaging technology is not
intuitive, and not for normal users to understand, and not friendly to
visualize. To address the above challenges, this paper presents a novel robotic
system to collect GPR data, interpret GPR data, localize the underground
utilities, reconstruct and visualize the underground objects' dense point cloud
model in a user-friendly manner. This system is composed of three modules: 1) a
vision-aided Omni-directional robotic data collection platform, which enables
the GPR antenna to scan the target area freely with an arbitrary trajectory
while using a visual-inertial-based positioning module tags the GPR
measurements with positioning information; 2) a deep neural network (DNN)
migration module to interpret the raw GPR B-scan image into a cross-section of
object model; 3) a DNN-based 3D reconstruction method, i.e., GPRNet, to
generate underground utility model represented as fine 3D point cloud.
Comparative studies on synthetic and field GPR raw data with various
incompleteness and noise are performed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01908</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01908</id><submitter>Yuming Shen</submitter><version version="v1"><date>Thu, 3 Jun 2021 14:59:59 GMT</date><size>2166kb</size><source_type>D</source_type></version><title>You Never Cluster Alone</title><authors>Yuming Shen and Ziyi Shen and Menghan Wang and Jie Qin and Philip H.S.
  Torr and Ling Shao</authors><categories>cs.CV cs.LG</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in self-supervised learning with instance-level contrastive
objectives facilitate unsupervised clustering. However, a standalone datum is
not perceiving the context of the holistic cluster, and may undergo sub-optimal
assignment. In this paper, we extend the mainstream contrastive learning
paradigm to a cluster-level scheme, where all the data subjected to the same
cluster contribute to a unified representation that encodes the context of each
data group. Contrastive learning with this representation then rewards the
assignment of each datum. To implement this vision, we propose twin-contrast
clustering (TCC). We define a set of categorical variables as clustering
assignment confidence, which links the instance-level learning track with the
cluster-level one. On one hand, with the corresponding assignment variables
being the weight, a weighted aggregation along the data points implements the
set representation of a cluster. We further propose heuristic cluster
augmentation equivalents to enable cluster-level contrastive learning. On the
other hand, we derive the evidence lower-bound of the instance-level
contrastive objective with the assignments. By reparametrizing the assignment
variables, TCC is trained end-to-end, requiring no alternating steps. Extensive
experiments show that TCC outperforms the state-of-the-art on challenging
benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01915</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01915</id><submitter>Changhee Han Dr.</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:08:14 GMT</date><size>26577kb</size><source_type>D</source_type></version><title>Pathology-Aware Generative Adversarial Networks for Medical Image
  Augmentation</title><authors>Changhee Han</authors><categories>eess.IV cs.CV cs.LG</categories><comments>Ph.D. Thesis (The University of Tokyo) defended in February, 2020.
  Based on GAN-based synthetic brain MR image generation (ISBI 2018),
  arXiv:1902.09856, arXiv:1903.12564, arXiv:1905.13456, arXiv:1906.04962,
  arXiv:2001.03923</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) can play a key role in Medical Image
Analysis under large-scale annotated datasets. However, preparing such massive
dataset is demanding. In this context, Generative Adversarial Networks (GANs)
can generate realistic but novel samples, and thus effectively cover the real
image distribution. In terms of interpolation, the GAN-based medical image
augmentation is reliable because medical modalities can display the human
body's strong anatomical consistency at fixed position while clearly reflecting
inter-subject variability; thus, we propose to use noise-to-image GANs (e.g.,
random noise samples to diverse pathological images) for (i) medical Data
Augmentation (DA) and (ii) physician training. Regarding the DA, the
GAN-generated images can improve Computer-Aided Diagnosis based on supervised
learning. For the physician training, the GANs can display novel desired
pathological images and help train medical trainees despite
infrastructural/legal constraints. This thesis contains four GAN projects
aiming to present such novel applications' clinical relevance in collaboration
with physicians. Whereas the methods are more generally applicable, this thesis
only explores a few oncological applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01917</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01917</id><submitter>Fabian Bauer-Marquart</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:09:43 GMT</date><size>313kb</size><source_type>D</source_type></version><title>DeepOpt: Scalable Specification-based Falsification of Neural Networks
  using Black-Box Optimization</title><authors>Fabian Bauer-Marquart, Stefan Leue, Christian Schilling</authors><categories>cs.LG cs.LO</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Decisions made by deep neural networks (DNNs) have a tremendous impact on the
dependability of the systems that they are embedded into, which is of
particular concern in the realm of safety-critical systems. In this paper we
consider specification-based falsification of DNNs with the aim to support
debugging and repair. We propose DeepOpt, a falsification technique based on
black-box optimization, which generates counterexamples from a DNN in a
refinement loop. DeepOpt can analyze input-output specifications, which makes
it more general than falsification approaches that only support robustness
specifications. The key idea is to algebraically combine the DNN with the input
and output constraints derived from the specification. We have implemented
DeepOpt and evaluated it on DNNs of varying sizes and architectures.
Experimental comparisons demonstrate DeepOpt's precision and scalability; in
particular, DeepOpt requires very few queries to the DNN.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01920</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01920</id><submitter>Kunal Bhardwaj</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:14:46 GMT</date><size>171kb</size><source_type>D</source_type></version><title>Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement
  Prediction</title><authors>Kunal Bhardwaj</authors><categories>cs.NE cs.AI cs.CV cs.LG</categories><comments>19 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With technological advancements and the exponential growth of data, we have
been unfolding different capabilities of neural networks in different sectors.
In this paper, I have tried to use a specific type of Neural Network known as
Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words,
I have tried to construct and train a convolutional neural network on past
stock prices data and then tried to predict the movement of stock price i.e.
whether the stock price would rise or fall, in the coming time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01921</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01921</id><submitter>James Long</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:15:30 GMT</date><size>1696kb</size></version><title>Sample Selection Bias in Evaluation of Prediction Performance of Causal
  Models</title><authors>James P. Long and Min Jin Ha</authors><categories>stat.ML cs.LG stat.AP</categories><comments>10 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal models are notoriously difficult to validate because they make
untestable assumptions regarding confounding. New scientific experiments offer
the possibility of evaluating causal models using prediction performance.
Prediction performance measures are typically robust to violations in causal
assumptions. However prediction performance does depend on the selection of
training and test sets. In particular biased training sets can lead to
optimistic assessments of model performance. In this work, we revisit the
prediction performance of several recently proposed causal models tested on a
genetic perturbation data set of Kemmeren [Kemmeren et al., 2014]. We find that
sample selection bias is likely a key driver of model performance. We propose
using a less-biased evaluation set for assessing prediction performance on
Kemmeren and compare models on this new set. In this setting, the causal model
tested have similar performance to standard association based estimators such
as Lasso. Finally we compare the performance of causal estimators in simulation
studies which reproduce the Kemmeren structure of genetic knockout experiments
but without any sample selection bias. These results provide an improved
understanding of the performance of several causal models and offer guidance on
how future studies should use Kemmeren.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01925</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01925</id><submitter>Libo Qin</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:22:38 GMT</date><size>7199kb</size><source_type>D</source_type></version><title>GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple
  Intent Detection and Slot Filling</title><authors>Libo Qin, Fuxuan Wei, Tianbao Xie, Xiao Xu, Wanxiang Che, Ting Liu</authors><categories>cs.CL</categories><comments>Accepted at ACL2021 (main conference)</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Multi-intent SLU can handle multiple intents in an utterance, which has
attracted increasing attention. However, the state-of-the-art joint models
heavily rely on autoregressive approaches, resulting in two issues: slow
inference speed and information leakage. In this paper, we explore a
non-autoregressive model for joint multiple intent detection and slot filling,
achieving more fast and accurate. Specifically, we propose a Global-Locally
Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction
layer is proposed to model slot dependency for alleviating uncoordinated slots
problem while a global intent-slot graph interaction layer is introduced to
model the interaction between multiple intents and all slots in the utterance.
Experimental results on two public datasets show that our framework achieves
state-of-the-art performance while being 11.5 times faster.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01926</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01926</id><submitter>Luigi Brugnano</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:22:56 GMT</date><size>48kb</size><source_type>D</source_type></version><title>A new framework for polynomial approximation to differential equations</title><authors>Luigi Brugnano, Gianluca Frasca-Caccia, Felice Iavernaro, Vincenzo
  Vespri</authors><categories>math.NA cs.NA</categories><comments>24 pages, 1 figure</comments><msc-class>65L05, 65L03, 65L06, 65P10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we discuss a framework for the polynomial approximation to the
solution of initial value problems for differential equations. The framework,
initially devised for the approximation of ordinary differential equations, is
further extended to cope with constant delay differential equations. Relevant
classes of Runge-Kutta methods can be derived within this framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01927</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01927</id><submitter>Ao Chen</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:28:17 GMT</date><size>18052kb</size><source_type>D</source_type></version><title>A Comparison for Anti-noise Robustness of Deep Learning Classification
  Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to
  Visual Transformer and Performer</title><authors>Ao Chen, Chen Li, Haoyuan Chen, Hechen Yang, Peng Zhao, Weiming Hu,
  Wanli Liu, Shuojia Zou, and Marcin Grzegorzek</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image classification has achieved unprecedented advance with the the rapid
development of deep learning. However, the classification of tiny object images
is still not well investigated. In this paper, we first briefly review the
development of Convolutional Neural Network and Visual Transformer in deep
learning, and introduce the sources and development of conventional noises and
adversarial attacks. Then we use various models of Convolutional Neural Network
and Visual Transformer to conduct a series of experiments on the image dataset
of tiny objects (sperms and impurities), and compare various evaluation metrics
in the experimental results to obtain a model with stable performance. Finally,
we discuss the problems in the classification of tiny objects and make a
prospect for the classification of tiny objects in the future.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01930</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01930</id><submitter>Omar Saadi</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:30:49 GMT</date><size>151kb</size><source_type>D</source_type></version><title>Tropical linear regression and mean payoff games: or, how to measure the
  distance to equilibria</title><authors>Marianne Akian, St\'ephane Gaubert, Yang Qi and Omar Saadi</authors><categories>math.CO cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a tropical linear regression problem consisting in finding the best
approximation of a set of points by a tropical hyperplane. We establish a
strong duality theorem, showing that the value of this problem coincides with
the maximal radius of a Hilbert's ball included in a tropical polyhedron. We
also show that this regression problem is polynomial-time equivalent to mean
payoff games. We illustrate our results by solving an inverse problem from
auction theory. In this setting, a tropical hyperplane represents the set of
equilibrium prices. Tropical linear regression allows us to quantify the
distance of a market to the set of equilibria, and infer secret preferences of
a decision maker.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01933</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01933</id><submitter>David Gaddy</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:33:23 GMT</date><size>141kb</size><source_type>D</source_type></version><title>An Improved Model for Voicing Silent Speech</title><authors>David Gaddy and Dan Klein</authors><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an improved model for voicing silent speech, where
audio is synthesized from facial electromyography (EMG) signals. To give our
model greater flexibility to learn its own input features, we directly use EMG
signals as input in the place of hand-designed features used by prior work. Our
model uses convolutional layers to extract features from the signals and
Transformer layers to propagate information across longer distances. To provide
better signal for learning, we also introduce an auxiliary task of predicting
phoneme labels in addition to predicting speech audio features. On an open
vocabulary intelligibility evaluation, our model improves the state of the art
for this task by an absolute 25.8%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01939</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01939</id><submitter>Jean Kaddour</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:41:00 GMT</date><size>5940kb</size><source_type>D</source_type></version><title>Graph Intervention Networks for Causal Effect Estimation</title><authors>Jean Kaddour, Qi Liu, Yuchen Zhu, Matt J. Kusner, Ricardo Silva</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the estimation of conditional average treatment effects (CATEs)
when treatments are graph-structured (e.g., molecular graphs of drugs). Given a
weak condition on the effect, we propose a plug-in estimator that decomposes
CATE estimation into separate, simpler optimization problems. Our estimator (a)
isolates the causal estimands (reducing regularization bias), and (b) allows
one to plug in arbitrary models for learning. In experiments with small-world
and molecular graphs, we show that our approach outperforms prior approaches
and is robust to varying selection biases. Our implementation is online.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01940</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01940</id><submitter>Panagiotis Papadopoulos</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:44:31 GMT</date><size>7057kb</size><source_type>D</source_type></version><title>THEMIS: A Decentralized Privacy-Preserving Ad Platform with Reporting
  Integrity</title><authors>Gon\c{c}alo Pestana, I\~nigo Querejeta-Azurmendi, Panagiotis
  Papadopoulos, Benjamin Livshits</authors><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:2007.05556</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Online advertising fuels the (seemingly) free internet. However, although
users can access most of the web services free of charge, they pay a heavy
coston their privacy. They are forced to trust third parties and
intermediaries, who not only collect behavioral data but also absorb great
amounts of ad revenues. Consequently, more and more users opt out from
advertising by resorting to ad blockers, thus costing publishers millions of
dollars in lost ad revenues. Albeit there are various privacy-preserving
advertising proposals (e.g.,Adnostic, Privad, Brave Ads) from both academia and
industry, they all rely on centralized management that users have to blindly
trust without being able to audit, while they also fail to guarantee the
integrity of the per-formance analytics they provide to advertisers.
  In this paper, we design and deploy THEMIS, a novel, decentralized and
privacy-by-design ad platform that requires zero trust by users. THEMIS (i)
provides auditability to its participants, (ii) rewards users for viewing ads,
and (iii) allows advertisers to verify the performance and billing reports of
their ad campaigns. By leveraging smart contracts and zero-knowledge schemes,
we implement a prototype of THEMIS and early performance evaluation results
show that it can scale linearly on a multi sidechain setup while it supports
more than 51M users on a single-sidechain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01941</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01941</id><submitter>Yi Su</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:46:02 GMT</date><size>1005kb</size><source_type>D</source_type></version><title>Optimizing Rankings for Recommendation in Matching Markets</title><authors>Yi Su, Magd Bayoumi, Thorsten Joachims</authors><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the success of recommender systems in e-commerce, there is growing
interest in their use in matching markets (e.g., labor). While this holds
potential for improving market fluidity and fairness, we show in this paper
that naively applying existing recommender systems to matching markets is
sub-optimal. Considering the standard process where candidates apply and then
get evaluated by employers, we present a new recommendation framework to model
this interaction mechanism and propose efficient algorithms for computing
personalized rankings in this setting. We show that the optimal rankings need
to not only account for the potentially divergent preferences of candidates and
employers, but they also need to account for capacity constraints. This makes
conventional ranking systems that merely rank by some local score (e.g.,
one-sided or reciprocal relevance) highly sub-optimal -- not only for an
individual user, but also for societal goals (e.g., low unemployment). To
address this shortcoming, we propose the first method for jointly optimizing
the rankings for all candidates in the market to explicitly maximize social
welfare. In addition to the theoretical derivation, we evaluate the method both
on simulated environments and on data from a real-world
networking-recommendation system that we built and fielded at a large computer
science conference.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01946</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01946</id><submitter>Evgeniya Vorontsova</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:52:21 GMT</date><size>844kb</size><source_type>D</source_type></version><title>Convex optimization</title><authors>Evgeniya Vorontsova, Roland Hildebrand, Alexander Gasnikov, Fedor
  Stonyakin</authors><categories>math.OC cs.NA math.NA</categories><comments>364 pages, in Russian</comments><msc-class>65-01, 90-01, 65K05, 90C30, 90C90</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This textbook is based on lectures given by the authors at MIPT (Moscow), HSE
(Moscow), FEFU (Vladivostok), V.I. Vernadsky KFU (Simferopol), ASU (Republic of
Adygea), and the University of Grenoble-Alpes (Grenoble, France). First of all,
the authors focused on the program of a two-semester course of lectures on
convex optimization, which is given to students of MIPT. The first chapter of
this book contains the materials of the first semester (&quot;Fundamentals of convex
analysis and optimization&quot;), the second and third chapters contain the
materials of the second semester (&quot;Numerical methods of convex optimization&quot;).
  The textbook has a number of features. First, in contrast to the classic
manuals, this book does not provide proofs of all the theorems mentioned. This
allowed, on one side, to describe more themes, but on the other side, made the
presentation less self-sufficient. The second important point is that part of
the material is advanced and is published in the Russian educational
literature, apparently for the first time. Third, the accents that are given do
not always coincide with the generally accepted accents in the textbooks that
are now popular. First of all, we talk about a sufficiently advanced
presentation of conic optimization, including robust optimization, as a vivid
demonstration of the capabilities of modern convex analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01947</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01947</id><submitter>Lirong Xia</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:55:11 GMT</date><size>815kb</size><source_type>D</source_type></version><title>The Smoothed Satisfaction of Voting Axioms</title><authors>Lirong Xia</authors><categories>econ.TH cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the work towards a comprehensive picture of the smoothed
satisfaction of voting axioms, to provide a finer and more realistic foundation
for comparing voting rules. We adopt the smoothed social choice framework,
where an adversary chooses arbitrarily correlated &quot;ground truth&quot; preferences
for the agents, on top of which random noises are added. We focus on
characterizing the smoothed satisfaction of two well-studied voting axioms:
Condorcet criterion and participation. We prove that for any fixed number of
alternatives, when the number of voters $n$ is sufficiently large, the smoothed
satisfaction of the Condorcet criterion under a wide range of voting rules is
$1$, $1-\exp(-\Theta(n))$, $\Theta(n^{-0.5})$, $ \exp(-\Theta(n))$, or being
$\Theta(1)$ and $1-\Theta(1)$ at the same time; and the smoothed satisfaction
of participation is $1-\Theta(n^{-0.5})$. Our results address open questions by
Berg and Lepelley in 1994 for these rules, and also confirm the following
high-level message: the Condorcet criterion is a bigger concern than
participation under realistic models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01950</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01950</id><submitter>Ulme Wennberg</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:56:26 GMT</date><size>5655kb</size><source_type>D</source_type></version><title>The Case for Translation-Invariant Self-Attention in Transformer-Based
  Language Models</title><authors>Ulme Wennberg, Gustav Eje Henter</authors><categories>cs.CL cs.AI cs.LG</categories><comments>11 pages, 8 figures, Accepted to ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mechanisms for encoding positional information are central for
transformer-based language models. In this paper, we analyze the position
embeddings of existing language models, finding strong evidence of translation
invariance, both for the embeddings themselves and for their effect on
self-attention. The degree of translation invariance increases during training
and correlates positively with model performance. Our findings lead us to
propose translation-invariant self-attention (TISA), which accounts for the
relative position between tokens in an interpretable fashion without needing
conventional position embeddings. Our proposal has several theoretical
advantages over existing position-representation approaches. Experiments show
that it improves on regular ALBERT on GLUE tasks, while only adding orders of
magnitude less positional parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01954</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01954</id><submitter>Alexander Korotin</submitter><version version="v1"><date>Thu, 3 Jun 2021 15:59:28 GMT</date><size>5326kb</size><source_type>D</source_type></version><title>Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2
  Benchmark</title><authors>Alexander Korotin, Lingxiao Li, Aude Genevay, Justin Solomon,
  Alexander Filippov, Evgeny Burnaev</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the recent popularity of neural network-based solvers for optimal
transport (OT), there is no standard quantitative way to evaluate their
performance. In this paper, we address this issue for quadratic-cost transport
-- specifically, computation of the Wasserstein-2 distance, a commonly-used
formulation of optimal transport in machine learning. To overcome the challenge
of computing ground truth transport maps between continuous measures needed to
assess these solvers, we use input-convex neural networks (ICNN) to construct
pairs of measures whose ground truth OT maps can be obtained analytically. This
strategy yields pairs of continuous benchmark measures in high-dimensional
spaces such as spaces of images. We thoroughly evaluate existing optimal
transport solvers using these benchmark measures. Even though these solvers
perform well in downstream tasks, many do not faithfully recover optimal
transport maps. To investigate the cause of this discrepancy, we further test
the solvers in a setting of image generation. Our study reveals crucial
limitations of existing solvers and shows that increased OT accuracy does not
necessarily correlate to better results downstream.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01958</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01958</id><submitter>Abhishek Ramdas Nair</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:06:08 GMT</date><size>1272kb</size><source_type>D</source_type></version><title>Multiplierless MP-Kernel Machine For Energy-efficient Edge Devices</title><authors>Abhishek Ramdas Nair, Pallab Kumar Nath, Shantanu Chakrabartty, Chetan
  Singh Thakur</authors><categories>cs.LG cs.AR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a novel framework for designing multiplierless kernel machines
that can be used on resource-constrained platforms like intelligent edge
devices. The framework uses a piecewise linear (PWL) approximation based on a
margin propagation (MP) technique and uses only addition/subtraction, shift,
comparison, and register underflow/overflow operations. We propose a
hardware-friendly MP-based inference and online training algorithm that has
been optimized for a Field Programmable Gate Array (FPGA) platform. Our FPGA
implementation eliminates the need for DSP units and reduces the number of
LUTs. By reusing the same hardware for inference and training, we show that the
platform can overcome classification errors and local minima artifacts that
result from the MP approximation. Using the FPGA platform, we also show that
the proposed multiplierless MP-kernel machine demonstrates superior performance
in terms of power, performance, and area compared to other comparable
implementations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01960</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01960</id><submitter>Gaurav Sahu</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:06:46 GMT</date><size>1555kb</size><source_type>D</source_type></version><title>LyricJam: A system for generating lyrics for live instrumental music</title><authors>Olga Vechtomova, Gaurav Sahu, Dhruv Kumar</authors><categories>cs.SD cs.AI cs.CL cs.LG eess.AS</categories><comments>Accepted to International Conference on Computational Creativity
  (ICCC) 2021 [Oral]</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We describe a real-time system that receives a live audio stream from a jam
session and generates lyric lines that are congruent with the live music being
played. Two novel approaches are proposed to align the learned latent spaces of
audio and text representations that allow the system to generate novel lyric
lines matching live instrumental music. One approach is based on adversarial
alignment of latent representations of audio and lyrics, while the other
approach learns to transfer the topology from the music latent space to the
lyric latent space. A user study with music artists using the system showed
that the system was useful not only in lyric composition, but also encouraged
the artists to improvise and find new musical expressions. Another user study
demonstrated that users preferred the lines generated using the proposed
methods to the lines generated by a baseline model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01963</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01963</id><submitter>Luiz Pereira</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:10:42 GMT</date><size>38kb</size></version><title>A Survey on Optimal Transport for Machine Learning: Theory and
  Applications</title><authors>Luis Caicedo Torres, Luiz Manella Pereira, M. Hadi Amini</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Optimal Transport (OT) theory has seen an increasing amount of attention from
the computer science community due to its potency and relevance in modeling and
machine learning. It introduces means that serve as powerful ways to compare
probability distributions with each other, as well as producing optimal
mappings to minimize cost functions. In this survey, we present a brief
introduction and history, a survey of previous work and propose directions of
future study. We will begin by looking at the history of optimal transport and
introducing the founders of this field. We then give a brief glance into the
algorithms related to OT. Then, we will follow up with a mathematical
formulation and the prerequisites to understand OT. These include Kantorovich
duality, entropic regularization, KL Divergence, and Wassertein barycenters.
Since OT is a computationally expensive problem, we then introduce the
entropy-regularized version of computing optimal mappings, which allowed OT
problems to become applicable in a wide range of machine learning problems. In
fact, the methods generated from OT theory are competitive with the current
state-of-the-art methods. We follow this up by breaking down research papers
that focus on image processing, graph learning, neural architecture search,
document representation, and domain adaptation. We close the paper with a small
section on future research. Of the recommendations presented, three main
problems are fundamental to allow OT to become widely applicable but rely
strongly on its mathematical formulation and thus are hardest to answer. Since
OT is a novel method, there is plenty of space for new research, and with more
and more competitive methods (either on an accuracy level or computational
speed level) being created, the future of applied optimal transport is bright
as it has become pervasive in machine learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01969</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01969</id><submitter>William Overman</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:17:46 GMT</date><size>1057kb</size><source_type>D</source_type></version><title>Global Convergence of Multi-Agent Policy Gradient in Markov Potential
  Games</title><authors>Stefanos Leonardos, Will Overman, Ioannis Panageas, Georgios Piliouras</authors><categories>cs.LG cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Potential games are arguably one of the most important and widely studied
classes of normal form games. They define the archetypal setting of multi-agent
coordination as all agent utilities are perfectly aligned with each other via a
common potential function. Can this intuitive framework be transplanted in the
setting of Markov Games? What are the similarities and differences between
multi-agent coordination with and without state dependence? We present a novel
definition of Markov Potential Games (MPG) that generalizes prior attempts at
capturing complex stateful multi-agent coordination. Counter-intuitively,
insights from normal-form potential games do not carry over as MPGs can consist
of settings where state-games can be zero-sum games. In the opposite direction,
Markov games where every state-game is a potential game are not necessarily
MPGs. Nevertheless, MPGs showcase standard desirable properties such as the
existence of deterministic Nash policies. In our main technical result, we
prove fast convergence of independent policy gradient to Nash policies by
adapting recent gradient dominance property arguments developed for single
agent MDPs to multi-agent learning settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01970</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01970</id><submitter>Xiuming Zhang</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:18:01 GMT</date><size>14348kb</size><source_type>D</source_type></version><title>NeRFactor: Neural Factorization of Shape and Reflectance Under an
  Unknown Illumination</title><authors>Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec,
  William T. Freeman, Jonathan T. Barron</authors><categories>cs.CV cs.GR</categories><comments>Project Page:
  https://people.csail.mit.edu/xiuming/projects/nerfactor/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of recovering the shape and spatially-varying
reflectance of an object from posed multi-view images of the object illuminated
by one unknown lighting condition. This enables the rendering of novel views of
the object under arbitrary environment lighting and editing of the object's
material properties. The key to our approach, which we call Neural Radiance
Factorization (NeRFactor), is to distill the volumetric geometry of a Neural
Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object
into a surface representation and then jointly refine the geometry while
solving for the spatially-varying reflectance and the environment lighting.
Specifically, NeRFactor recovers 3D neural fields of surface normals, light
visibility, albedo, and Bidirectional Reflectance Distribution Functions
(BRDFs) without any supervision, using only a re-rendering loss, simple
smoothness priors, and a data-driven BRDF prior learned from real-world BRDF
measurements. By explicitly modeling light visibility, NeRFactor is able to
separate shadows from albedo and synthesize realistic soft or hard shadows
under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D
models for free-viewpoint relighting in this challenging and underconstrained
capture setup for both synthetic and real scenes. Qualitative and quantitative
experiments show that NeRFactor outperforms classic and deep learning-based
state of the art across various tasks. Our code and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01972</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01972</id><submitter>Ruochen Zhang</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:21:13 GMT</date><size>903kb</size><source_type>D</source_type></version><title>SOCCER: An Information-Sparse Discourse State Tracking Collection in the
  Sports Commentary Domain</title><authors>Ruochen Zhang and Carsten Eickhoff</authors><categories>cs.CL</categories><comments>Accepted at NAACL2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the pursuit of natural language understanding, there has been a long
standing interest in tracking state changes throughout narratives. Impressive
progress has been made in modeling the state of transaction-centric dialogues
and procedural texts. However, this problem has been less intensively studied
in the realm of general discourse where ground truth descriptions of states may
be loosely defined and state changes are less densely distributed over
utterances. This paper proposes to turn to simplified, fully observable systems
that show some of these properties: Sports events. We curated 2,263 soccer
matches including time-stamped natural language commentary accompanied by
discrete events such as a team scoring goals, switching players or being
penalized with cards. We propose a new task formulation where, given paragraphs
of commentary of a game at different timestamps, the system is asked to
recognize the occurrence of in-game events. This domain allows for rich
descriptions of state while avoiding the complexities of many other real-world
settings. As an initial point of performance measurement, we include two
baseline methods from the perspectives of sentence classification with temporal
dependence and current state-of-the-art generative model, respectively, and
demonstrate that even sophisticated existing methods struggle on the state
tracking task when the definition of state broadens or non-event chatter
becomes prevalent.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01974</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01974</id><submitter>Hendrik Kolvenbach</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:33:46 GMT</date><size>22650kb</size><source_type>D</source_type></version><title>Traversing Steep and Granular Martian Analog Slopes With a Dynamic
  Quadrupedal Robot</title><authors>Hendrik Kolvenbach, Philip Arm, Elias Hampp, Alexander Dietsche,
  Valentin Bickel, Benjamin Sun, Christoph Meyer, and Marco Hutter</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Celestial bodies such as the Moon and Mars are mainly covered by loose,
granular soil, a notoriously challenging terrain to traverse with (wheeled)
robotic systems. Here, we present experimental work on traversing steep,
granular slopes with the dynamically walking quadrupedal robot SpaceBok. To
adapt to the challenging environment, we developed passive-adaptive planar feet
and optimized grouser pads to reduce sinkage and increase traction on planar
and inclined granular soil. Single-foot experiments revealed that a large
surface area of 110cm2 per foot reduces sinkage to an acceptable level even on
highly collapsible soil (ES-1). Implementing several 12mm grouser blades
increases traction by 22% to 66% on granular media compared to grouser-less
designs. Together with a terrain-adapting walking controller, we validate - for
the first time - static and dynamic locomotion on Mars analog slopes of up to
25{\deg}(the maximum of the testbed). We evaluated the performance between
point- and planar feet and static and dynamic gaits regarding stability
(safety), velocity, and energy consumption. We show that dynamic gaits are
energetically more efficient than static gaits but are riskier on steep slopes.
Our tests also revealed that planar feet's energy consumption drastically
increases when the slope inclination approaches the soil's angle of internal
friction due to shearing. Point feet are less affected by slippage due to their
excessive sinkage, but in turn, are prone to instabilities and tripping. We
present and discuss safe and energy-efficient global path-planning strategies
for accessing steep topography on Mars based on our findings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01975</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01975</id><submitter>Gurtej Kanwar</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:37:05 GMT</date><size>16328kb</size></version><title>Machine Learning and Variational Algorithms for Lattice Field Theory</title><authors>Gurtej Kanwar</authors><categories>hep-lat cs.LG</categories><comments>PhD Thesis, MIT (June 2021). Based on work appearing in
  arXiv:2101.12668, arXiv:2008.05456, arXiv:2003.06413, arXiv:2003.05914,
  arXiv:1904.12072, arXiv:1806.01832, arXiv:2101.08176, arXiv:2002.02428, and
  arXiv:1811.03944</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In lattice quantum field theory studies, parameters defining the lattice
theory must be tuned toward criticality to access continuum physics. Commonly
used Markov chain Monte Carlo (MCMC) methods suffer from critical slowing down
in this limit, restricting the precision of continuum extrapolations. Further
difficulties arise when measuring correlation functions of operators widely
separated in spacetime: for most correlation functions, an exponentially severe
signal-to-noise problem is encountered as the operators are taken to be widely
separated. This dissertation details two new techniques to address these
issues. First, we define a novel MCMC algorithm based on generative flow-based
models. Such models utilize machine learning methods to describe efficient
approximate samplers for distributions of interest. Independently drawn
flow-based samples are then used as proposals in an asymptotically exact
Metropolis-Hastings Markov chain. We address incorporating symmetries of
interest, including translational and gauge symmetries. We secondly introduce
an approach to &quot;deform&quot; Monte Carlo estimators based on contour deformations
applied to the domain of the path integral. The deformed estimators associated
with an observable give equivalent unbiased measurements of that observable,
but generically have different variances. We define families of deformed
manifolds for lattice gauge theories and introduce methods to efficiently
optimize the choice of manifold (the &quot;observifold&quot;), minimizing the deformed
observable variance. Finally, we demonstrate that flow-based MCMC can mitigate
critical slowing down and observifolds can exponentially reduce variance in
proof-of-principle applications to scalar $\phi^4$ theory and $\mathrm{U}(1)$
and $\mathrm{SU}(N)$ lattice gauge theories.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01977</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01977</id><submitter>Alexandros Nikou PhD</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:45:40 GMT</date><size>1096kb</size><source_type>D</source_type></version><title>Safe RAN control: A Symbolic Reinforcement Learning Approach</title><authors>Alexandros Nikou, Anusha Mujumdar, Marin Orlic, Aneta Vulgarakis
  Feljan</authors><categories>cs.AI</categories><comments>Submitted to IEEE Global Communications Conference (GLOBECOM) 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present a Symbolic Reinforcement Learning (SRL) based
architecture for safety control of Radio Access Network (RAN) applications. In
particular, we provide a purely automated procedure in which a user can specify
high-level logical safety specifications for a given cellular network topology
in order for the latter to execute optimal safe performance which is measured
through certain Key Performance Indicators (KPIs). The network consists of a
set of fixed Base Stations (BS) which are equipped with antennas, which one can
control by adjusting their vertical tilt angle. The aforementioned process is
called Remote Electrical Tilt (RET) optimization. Recent research has focused
on performing this RET optimization by employing Reinforcement Learning (RL)
strategies due to the fact that they have self-learning capabilities to adapt
in uncertain environments. The term safety refers to particular constraints
bounds of the network KPIs in order to guarantee that when the algorithms are
deployed in a live network, the performance is maintained. In our proposed
architecture the safety is ensured through model-checking techniques over
combined discrete system models (automata) that are abstracted through the
learning process. We introduce a user interface (UI) developed to help a user
set intent specifications to the system, and inspect the difference in agent
proposed actions, and those that are allowed and blocked according to the
safety specification.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01978</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01978</id><submitter>Dou Hu</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:47:38 GMT</date><size>1289kb</size><source_type>D</source_type></version><title>DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in
  Conversations</title><authors>Dou Hu, Lingwei Wei, Xiaoyong Huai</authors><categories>cs.CL cs.AI</categories><comments>11 pages, accepted by ACL-IJCNLP 2021 main conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Emotion Recognition in Conversations (ERC) has gained increasing attention
for developing empathetic machines. Recently, many approaches have been devoted
to perceiving conversational context by deep learning models. However, these
approaches are insufficient in understanding the context due to lacking the
ability to extract and integrate emotional clues. In this work, we propose
novel Contextual Reasoning Networks (DialogueCRN) to fully understand the
conversational context from a cognitive perspective. Inspired by the Cognitive
Theory of Emotion, we design multi-turn reasoning modules to extract and
integrate emotional clues. The reasoning module iteratively performs an
intuitive retrieving process and a conscious reasoning process, which imitates
human unique cognitive thinking. Extensive experiments on three public
benchmark datasets demonstrate the effectiveness and superiority of the
proposed model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01979</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01979</id><submitter>Wenhao Li</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:49:03 GMT</date><size>216kb</size><source_type>D</source_type></version><title>CCPM: A Chinese Classical Poetry Matching Dataset</title><authors>Wenhao Li, Fanchao Qi, Maosong Sun, Xiaoyuan Yi, Jiarui Zhang</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Poetry is one of the most important art forms of human languages. Recently
many studies have focused on incorporating some linguistic features of poetry,
such as style and sentiment, into its understanding or generation system.
However, there is no focus on understanding or evaluating the semantics of
poetry. Therefore, we propose a novel task to assess a model's semantic
understanding of poetry by poem matching. Specifically, this task requires the
model to select one line of Chinese classical poetry among four candidates
according to the modern Chinese translation of a line of poetry. To construct
this dataset, we first obtain a set of parallel data of Chinese classical
poetry and modern Chinese translation. Then we retrieve similar lines of poetry
with the lines in a poetry corpus as negative choices. We name the dataset
Chinese Classical Poetry Matching Dataset (CCPM) and release it at
https://github.com/THUNLP-AIPoet/CCPM. We hope this dataset can further enhance
the study on incorporating deep semantics into the understanding and generation
system of Chinese classical poetry. We also preliminarily run two variants of
BERT on this dataset as the baselines for this dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01981</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01981</id><submitter>Boris Oreshkin N</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:56:58 GMT</date><size>1041kb</size><source_type>D</source_type></version><title>ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose</title><authors>Boris N. Oreshkin and Florent Bocquelet and F\'elix H. Harvey and Bay
  Raitt and Dominic Laflamme</authors><categories>cs.CV cs.GR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01982</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01982</id><submitter>Thomas Pinder</submitter><version version="v1"><date>Thu, 3 Jun 2021 16:58:05 GMT</date><size>182kb</size><source_type>D</source_type></version><title>Gaussian Processes on Hypergraphs</title><authors>Thomas Pinder, Kathryn Turnbull, Christopher Nemeth, David Leslie</authors><categories>stat.ML cs.LG</categories><comments>25 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a Matern Gaussian process (GP) on the vertices of a hypergraph.
This enables estimation of regression models of observed or latent values
associated with the vertices, in which the correlation and uncertainty
estimates are informed by the hypergraph structure. We further present a
framework for embedding the vertices of a hypergraph into a latent space using
the hypergraph GP. Finally, we provide a scheme for identifying a small number
of representative inducing vertices that enables scalable inference through
sparse GPs. We demonstrate the utility of our framework on three challenging
real-world problems that concern multi-class classification for the political
party affiliation of legislators on the basis of voting behaviour,
probabilistic matrix factorisation of movie reviews, and embedding a hypergraph
of animals into a low-dimensional latent space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01986</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01986</id><submitter>Hanyuan Hang</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:05:40 GMT</date><size>3453kb</size><source_type>D</source_type></version><title>Gradient Boosted Binary Histogram Ensemble for Large-scale Regression</title><authors>Hanyuan Hang, Tao Huang, Yuchao Cai, Hanfang Yang, Zhouchen Lin</authors><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper, we propose a gradient boosting algorithm for large-scale
regression problems called \textit{Gradient Boosted Binary Histogram Ensemble}
(GBBHE) based on binary histogram partition and ensemble learning. From the
theoretical perspective, by assuming the H\&quot;{o}lder continuity of the target
function, we establish the statistical convergence rate of GBBHE in the space
$C^{0,\alpha}$ and $C^{1,0}$, where a lower bound of the convergence rate for
the base learner demonstrates the advantage of boosting. Moreover, in the space
$C^{1,0}$, we prove that the number of iterations to achieve the fast
convergence rate can be reduced by using ensemble regressor as the base
learner, which improves the computational efficiency. In the experiments,
compared with other state-of-the-art algorithms such as gradient boosted
regression tree (GBRT), Breiman's forest, and kernel-based methods, our GBBHE
algorithm shows promising performance with less running time on large-scale
datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01987</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01987</id><submitter>Donghwan Shin</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:09:11 GMT</date><size>497kb</size><source_type>D</source_type></version><title>PRINS: Scalable Model Inference for Component-based System Logs</title><authors>Donghwan Shin, Domenico Bianculli, Lionel Briand</authors><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1908.02329</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavioral software models play a key role in many software engineering
tasks; unfortunately, these models either are not available during software
development or, if available, quickly become outdated as implementations
evolve. Model inference techniques have been proposed as a viable solution to
extract finite state models from execution logs. However, existing techniques
do not scale well when processing very large logs that can be commonly found in
practice.
  In this paper, we address the scalability problem of inferring the model of a
component-based system from large system logs, without requiring any extra
information. Our model inference technique, called PRINS, follows a divide and
conquer approach. The idea is to first infer a model of each system component
from the corresponding logs; then, the individual component models are merged
together taking into account the flow of events across components, as reflected
in the logs. We evaluated PRINS in terms of scalability and accuracy, using
nine datasets composed of logs extracted from publicly available benchmarks and
a personal computer running desktop business applications. The results show
that PRINS can process large logs much faster than a publicly available and
well-known state-of-the-art tool, without significantly compromising the
accuracy of inferred models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01993</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01993</id><submitter>Adil Khurram</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:12:57 GMT</date><size>2275kb</size><source_type>D</source_type></version><title>Real-time Grid and DER Co-simulation Platform for Validating Large-scale
  DER Control Schemes</title><authors>Adil Khurram, Mahraz Amini, Luis A. Duffaut Espinosa, Paul D. H.
  Hines, Mads Almassalkhi</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed energy resources (DERs) such as responsive loads and energy
storage systems are valuable resources available to grid operators for
balancing supply-demand mismatches via load coordination. However, consumer
acceptance of load coordination schemes depends on ensuring quality of service
(QoS), which embodies device-level constraints. Since each device has its own
internal energy state, the effect of QoS on the fleet can be cast as fleet-wide
energy limits within which the aggregate &quot;state of charge&quot; (SoC) must be
actively maintained. This requires coordination of DERs that is cognizant of
the SoC, responsive to grid conditions, and depends on fast communication
networks. To that effect, this paper presents a novel real-time grid-and-DER
co-simulation platform for validating advanced DER coordination schemes and
characterizing the capability of such a DER fleet. In particular, we present
how the co-simulation platform is suitable for: i) testing real-time
performance of a large fleet of DERs in delivering advanced grid services,
including frequency regulation; ii) online state estimation to characterize the
corresponding SoC of a large fleet of DERs; and iii) incorporating practical
limitations of DERs and communications and analyzing the effects on fleet-wide
performance. To illustrate these benefits of the presented grid-DER
co-simulation platform, we employ the advanced DER coordination scheme called
packetized energy management (PEM), which is a novel device-driven,
asynchronous, and randomizing control paradigm for DERs. A fleet of thousands
of PEM-enabled DERs are then added to a realistic and dynamical model of the
Vermont transmission system to complete validation of the co-simulation
platform.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01994</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01994</id><submitter>Oron Sabag</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:13:41 GMT</date><size>21kb</size></version><title>Feedback Capacity of MIMO Gaussian Channels</title><authors>Oron Sabag, Victoria Kostina, Babak Hassibi</authors><categories>cs.IT math.IT math.OC</categories><comments>This is an extended version (with proofs) of an accepted paper to
  ISIT 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Finding a computable expression for the feedback capacity of additive
channels with colored Gaussian noise is a long standing open problem. In this
paper, we solve this problem in the scenario where the channel has multiple
inputs and multiple outputs (MIMO) and the noise process is generated as the
output of a state-space model (a hidden Markov model). The main result is a
computable characterization of the feedback capacity as a finite-dimensional
convex optimization problem. Our solution subsumes all previous solutions to
the feedback capacity including the auto-regressive moving-average (ARMA) noise
process of first order, even if it is a non-stationary process. The capacity
problem can be viewed as the problem of maximizing the measurements' entropy
rate of a controlled (policy-dependent) state-space subject to a power
constraint. We formulate the finite-block version of this problem as a
\emph{sequential convex optimization problem}, which in turn leads to a
single-letter and computable upper bound. By optimizing over a family of
time-invariant policies that correspond to the channel inputs distribution, a
tight lower bound is realized. We show that one of the optimization constraints
in the capacity characterization boils down to a Riccati equation, revealing an
interesting relation between explicit capacity formulae and Riccati equations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.01998</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.01998</id><submitter>Akbar Siami Namin</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:17:16 GMT</date><size>337kb</size><source_type>D</source_type></version><title>Toward Explainable Users: Using NLP to Enable AI to Understand Users'
  Perceptions of Cyber Attacks</title><authors>Faranak Abri, Luis Felipe Gutierrez, Chaitra T. Kulkarni, Akbar Siami
  Namin, Keith S. Jones</authors><categories>cs.HC cs.AI cs.CR</categories><comments>20 pages, 3 figures, COMPSAC'21</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To understand how end-users conceptualize consequences of cyber security
attacks, we performed a card sorting study, a well-known technique in Cognitive
Sciences, where participants were free to group the given consequences of
chosen cyber attacks into as many categories as they wished using rationales
they see fit. The results of the open card sorting study showed a large amount
of inter-participant variation making the research team wonder how the
consequences of security attacks were comprehended by the participants. As an
exploration of whether it is possible to explain user's mental model and
behavior through Artificial Intelligence (AI) techniques, the research team
compared the card sorting data with the outputs of a number of Natural Language
Processing (NLP) techniques with the goal of understanding how participants
perceived and interpreted the consequences of cyber attacks written in natural
languages. The results of the NLP-based exploration methods revealed an
interesting observation implying that participants had mostly employed checking
individual keywords in each sentence to group cyber attack consequences
together and less considered the semantics behind the description of
consequences of cyber attacks. The results reported in this paper are seemingly
useful and important for cyber attacks comprehension from user's perspectives.
To the best of our knowledge, this paper is the first introducing the use of AI
techniques in explaining and modeling users' behavior and their perceptions
about a context. The novel idea introduced here is about explaining users using
AI.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02000</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02000</id><submitter>S.M. Zafaruddin</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:19:08 GMT</date><size>120kb</size><source_type>D</source_type></version><title>Unified Performance Analysis of Reconfigurable Intelligent Surface
  Empowered Free Space Optical Communications</title><authors>Vinay Kumar Chapala, S. M. Zafaruddin</authors><categories>cs.IT eess.SP math.IT</categories><comments>29 pages, 8 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconfigurable intelligent surface (RIS) is an excellent use case for
line-of-sight (LOS) based technologies such as free-space optical (FSO)
communications. In this paper, we analyze the performance of RIS-empowered FSO
(RISE-FSO) systems by unifying Fisher-Snedecor (F), Gamma-Gamma (GG), and
Malaga (M) distributions for atmospheric turbulence with zero-boresight
pointing errors over deterministic as well as random path-loss in foggy
conditions with heterodyne detection (HD) and intensity modulation/direct
detection (IM/DD) methods. By deriving the probability density function (PDF)
and cumulative distribution function (CDF) of the direct-link (DL) with the
statistical effect of atmospheric turbulence, pointing errors, and random fog,
we develop exact expressions of PDF and CDF of the resultant channel for the
RISE-FSO system. Using the derived statistical results, we present exact
expressions of outage probability, average bit-error-rate (BER), ergodic
capacity, and moments of signal-to-noise ratio (SNR) for both DL-FSO and
RISE-FSO systems. We also develop an asymptotic analysis of the outage
probability and average BER and derive the diversity order of the considered
systems. We validate the analytical expressions using Monte-Carlo simulations
and demonstrate the performance scaling of the FSO system with the number of
RIS elements for various turbulence channels, detection techniques, and weather
conditions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02003</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02003</id><submitter>Kaiwen Jiang</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:21:23 GMT</date><size>597kb</size><source_type>D</source_type></version><title>Individual vs. Joint Perception: a Pragmatic Model of Pointing as
  Communicative Smithian Helping</title><authors>Kaiwen Jiang, Stephanie Stacy, Chuyu Wei, Adelpha Chan, Federico
  Rossano, Yixin Zhu, Tao Gao</authors><categories>cs.AI</categories><comments>7 pages, 3 figures. Accepted to CogSci 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simple gesture of pointing can greatly augment ones ability to comprehend
states of the world based on observations. It triggers additional inferences
relevant to ones task at hand. We model an agents update to its belief of the
world based on individual observations using a partially observable Markov
decision process (POMDP), a mainstream artificial intelligence (AI) model of
how to act rationally according to beliefs formed through observation. On top
of that, we model pointing as a communicative act between agents who have a
mutual understanding that the pointed observation must be relevant and
interpretable. Our model measures relevance by defining a Smithian Value of
Information (SVI) as the utility improvement of the POMDP agent before and
after receiving the pointing. We model that agents calculate SVI by using the
cognitive theory of Smithian helping as a principle of coordinating separate
beliefs for action prediction and action evaluation. We then import SVI into
rational speech act (RSA) as the utility function of an utterance. These lead
us to a pragmatic model of pointing allowing for contextually flexible
interpretations. We demonstrate the power of our Smithian pointing model by
extending the Wumpus world, a classic AI task where a hunter hunts a monster
with only partial observability of the world. We add another agent as a guide
who can only help by marking an observation already perceived by the hunter
with a pointing or not, without providing new observations or offering any
instrumental help. Our results show that this severely limited and overloaded
communication nevertheless significantly improves the hunters performance. The
advantage of pointing is indeed due to a computation of relevance based on
Smithian helping, as it disappears completely when the task is too difficult or
too easy for the guide to help.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02005</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02005</id><submitter>Florian Speelman</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:22:08 GMT</date><size>183kb</size><source_type>D</source_type></version><title>Limits of quantum speed-ups for computational geometry and other
  problems: Fine-grained complexity via quantum walks</title><authors>Harry Buhrman, Bruno Loff, Subhasree Patro, Florian Speelman</authors><categories>quant-ph cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many computational problems are subject to a quantum speed-up: one might find
that a problem having an O(n^3)-time or O(n^2)-time classic algorithm can be
solved by a known O(n^1.5)-time or O(n)-time quantum algorithm. The question
naturally arises: how much quantum speed-up is possible?
  The area of fine-grained complexity allows us to prove optimal lower-bounds
on the complexity of various computational problems, based on the conjectured
hardness of certain natural, well-studied problems. This theory has recently
been extended to the quantum setting, in two independent papers by Buhrman,
Patro, and Speelman (arXiv:1911.05686), and by Aaronson, Chia, Lin, Wang, and
Zhang (arXiv:1911.01973).
  In this paper, we further extend the theory of fine-grained complexity to the
quantum setting. A fundamental conjecture in the classical setting states that
the 3SUM problem cannot be solved by (classical) algorithms in time O(n^{2-a}),
for any a&gt;0. We formulate an analogous conjecture, the Quantum-3SUM-Conjecture,
which states that there exist no sublinear O(n^{1-b})-time quantum algorithms
for the 3SUM problem.
  Based on the Quantum-3SUM-Conjecture, we show new lower-bounds on the time
complexity of quantum algorithms for several computational problems. Most of
our lower-bounds are optimal, in that they match known upper-bounds, and hence
they imply tight limits on the quantum speedup that is possible for these
problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02006</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02006</id><submitter>Khaleel Mershad</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:22:44 GMT</date><size>8410kb</size><source_type>D</source_type></version><title>Cloud-Enabled High-Altitude Platform Systems: Challenges and
  Opportunities</title><authors>Khaleel Mershad, Hayssam Dahrouj, Hadi Sarieddeen, Basem Shihada,
  Tareq Al-Naffouri, and Mohamed-Slim Alouini</authors><categories>cs.NI</categories><comments>18 pages, 4 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Augmenting ground-level communications with flying networks, such as the
high-altitude platform system (HAPS), is among the major innovative initiatives
of the next generation of wireless systems (6G). Given HAPS quasi-static
positioning at the stratosphere, HAPS-to-ground and HAPS-to-air connectivity
frameworks are expected to be prolific in terms of data acquisition and
computing, especially given the mild weather and quasi-constant wind speed
characteristics of the stratospheric layer. This paper explores the
opportunities stemming from the realization of cloud-enabled HAPS in the
context of telecommunications applications and services. The paper first
advocates for the potential physical advantages of deploying HAPS as flying
data-centers, also known as super-macro base stations. The paper then presents
the merits that can be achieved by integrating various cloud services within
the HAPS, and the corresponding cloud-type applications that would utilize the
HAPS for enhancing the quality, range, and types of offered services. The paper
further sheds light on the challenges that need to be addressed for realizing
practical cloud-enabled HAPS, mainly, those related to the high energy,
processing power, quality of service (QoS), and security considerations.
Finally, the paper discusses some open issues on the topic, namely, HAPS
mobility and message routing, HAPS security via blockchain and machine
learning, artificial intelligence-based resource allocation in cloud-enabled
HAPS, and integration with vertical heterogeneous networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02009</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02009</id><submitter>Mario Graff</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:24:31 GMT</date><size>1547kb</size><source_type>D</source_type></version><title>A Case Study of Spanish Text Transformations for Twitter Sentiment
  Analysis</title><authors>Eric S. Tellez, Sabino Miranda-Jim\'enez, Mario Graff, Daniela
  Moctezuma, Oscar S. Siodia, and Elio A. Villase\~nor</authors><categories>cs.CL</categories><doi>10.1016/j.eswa.2017.03.071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis is a text mining task that determines the polarity of a
given text, i.e., its positiveness or negativeness. Recently, it has received a
lot of attention given the interest in opinion mining in micro-blogging
platforms. These new forms of textual expressions present new challenges to
analyze text given the use of slang, orthographic and grammatical errors, among
others. Along with these challenges, a practical sentiment classifier should be
able to handle efficiently large workloads.
  The aim of this research is to identify which text transformations
(lemmatization, stemming, entity removal, among others), tokenizers (e.g.,
words $n$-grams), and tokens weighting schemes impact the most the accuracy of
a classifier (Support Vector Machine) trained on two Spanish corpus. The
methodology used is to exhaustively analyze all the combinations of the text
transformations and their respective parameters to find out which
characteristics the best performing classifiers have in common. Furthermore,
among the different text transformations studied, we introduce a novel approach
based on the combination of word based $n$-grams and character based $q$-grams.
The results show that this novel combination of words and characters produces a
classifier that outperforms the traditional word based combination by $11.17\%$
and $5.62\%$ on the INEGI and TASS'15 dataset, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02011</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02011</id><submitter>Siyu Zhang</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:27:10 GMT</date><size>30kb</size><source_type>D</source_type></version><title>Provably Secure Generative Linguistic Steganography</title><authors>Siyu Zhang, Zhongliang Yang, Jinshuai Yang, Yongfeng Huang</authors><categories>cs.CL cs.CR</categories><comments>Accepted by ACL-IJCNLP 2021: findings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative linguistic steganography mainly utilized language models and
applied steganographic sampling (stegosampling) to generate high-security
steganographic text (stegotext). However, previous methods generally lead to
statistical differences between the conditional probability distributions of
stegotext and natural text, which brings about security risks. In this paper,
to further ensure security, we present a novel provably secure generative
linguistic steganographic method ADG, which recursively embeds secret
information by Adaptive Dynamic Grouping of tokens according to their
probability given by an off-the-shelf language model. We not only prove the
security of ADG mathematically, but also conduct extensive experiments on three
public corpora to further verify its imperceptibility. The experimental results
reveal that the proposed method is able to generate stegotext with nearly
perfect security.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02012</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02012</id><submitter>Akbar Siami Namin</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:32:06 GMT</date><size>2856kb</size><source_type>D</source_type></version><title>Attack Prediction using Hidden Markov Model</title><authors>Shuvalaxmi Dass, Prerit Datta, Akbar Siami Namin</authors><categories>cs.CR cs.AI</categories><comments>20 pages, 4 figures, COMPSAC'21</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It is important to predict any adversarial attacks and their types to enable
effective defense systems. Often it is hard to label such activities as
malicious ones without adequate analytical reasoning. We propose the use of
Hidden Markov Model (HMM) to predict the family of related attacks. Our
proposed model is based on the observations often agglomerated in the form of
log files and from the target or the victim's perspective. We have built an
HMM-based prediction model and implemented our proposed approach using Viterbi
algorithm, which generates a sequence of states corresponding to stages of a
particular attack. As a proof of concept and also to demonstrate the
performance of the model, we have conducted a case study on predicting a family
of attacks called Action Spoofing.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02016</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02016</id><submitter>Somnath Roy</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:35:14 GMT</date><size>16kb</size></version><title>Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for
  End Usability</title><authors>Somnath Roy</authors><categories>cs.CL cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in supervised, semi-supervised and self-supervised deep
learning algorithms have shown significant improvement in the performance of
automatic speech recognition(ASR) systems. The state-of-the-art systems have
achieved a word error rate (WER) less than 5%. However, in the past,
researchers have argued the non-suitability of the WER metric for the
evaluation of ASR systems for downstream tasks such as spoken language
understanding (SLU) and information retrieval. The reason is that the WER works
at the surface level and does not include any syntactic and semantic
knowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate
the ASR transcripts for downstream applications in general. The SWER can be
easily customized for any down-stream task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02017</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02017</id><submitter>Mozhi Zhang</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:36:32 GMT</date><size>209kb</size><source_type>D</source_type></version><title>A Dataset and Baselines for Multilingual Reply Suggestion</title><authors>Mozhi Zhang, Wei Wang, Budhaditya Deb, Guoqing Zheng, Milad Shokouhi,
  Ahmed Hassan Awadallah</authors><categories>cs.CL cs.LG</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reply suggestion models help users process emails and chats faster. Previous
work only studies English reply suggestion. Instead, we present MRS, a
multilingual reply suggestion dataset with ten languages. MRS can be used to
compare two families of models: 1) retrieval models that select the reply from
a fixed set and 2) generation models that produce the reply from scratch.
Therefore, MRS complements existing cross-lingual generalization benchmarks
that focus on classification and sequence labeling tasks. We build a generation
model and a retrieval model as baselines for MRS. The two models have different
strengths in the monolingual setting, and they require different strategies to
generalize across languages. MRS is publicly available at
https://github.com/zhangmozhi/mrs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02018</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02018</id><submitter>Elizaveta Rebrova</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:37:41 GMT</date><size>2251kb</size><source_type>D</source_type></version><title>Nonlinear Matrix Approximation with Radial Basis Function Components</title><authors>Elizaveta Rebrova and Yu-Hang Tang</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and investigate matrix approximation by decomposition into a sum
of radial basis function (RBF) components. An RBF component is a generalization
of the outer product between a pair of vectors, where an RBF function replaces
the scalar multiplication between individual vector elements. Even though the
RBF functions are positive definite, the summation across components is not
restricted to convex combinations and allows us to compute the decomposition
for any real matrix that is not necessarily symmetric or positive definite. We
formulate the problem of seeking such a decomposition as an optimization
problem with a nonlinear and non-convex loss function. Several modern versions
of the gradient descent method, including their scalable stochastic
counterparts, are used to solve this problem. We provide extensive empirical
evidence of the effectiveness of the RBF decomposition and that of the
gradient-based fitting algorithm. While being conceptually motivated by
singular value decomposition (SVD), our proposed nonlinear counterpart
outperforms SVD by drastically reducing the memory required to approximate a
data matrix with the same $L_2$-error for a wide range of matrix types. For
example, it leads to 2 to 10 times memory save for Gaussian noise, graph
adjacency matrices, and kernel matrices. Moreover, this proximity-based
decomposition can offer additional interpretability in applications that
involve, e.g., capturing the inner low-dimensional structure of the data,
retaining graph connectivity structure, and preserving the acutance of images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02019</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02019</id><submitter>Lingjie Liu</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:40:48 GMT</date><size>20689kb</size><source_type>D</source_type></version><title>Neural Actor: Neural Free-view Synthesis of Human Actors with Pose
  Control</title><authors>Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao
  Gu, Christian Theobalt</authors><categories>cs.CV cs.GR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02022</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02022</id><submitter>Micha\&quot;el Ramamonjisoa</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:42:25 GMT</date><size>18446kb</size><source_type>D</source_type></version><title>Single Image Depth Estimation using Wavelet Decomposition</title><authors>Micha\&quot;el Ramamonjisoa and Michael Firman and Jamie Watson and Vincent
  Lepetit and Daniyar Turmukhambetov</authors><categories>cs.CV</categories><comments>CVPR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method for predicting accurate depths from monocular
images with high efficiency. This optimal efficiency is achieved by exploiting
wavelet decomposition, which is integrated in a fully differentiable
encoder-decoder architecture. We demonstrate that we can reconstruct
high-fidelity depth maps by predicting sparse wavelet coefficients. In contrast
with previous works, we show that wavelet coefficients can be learned without
direct supervision on coefficients. Instead we supervise only the final depth
image that is reconstructed through the inverse wavelet transform. We
additionally show that wavelet coefficients can be learned in fully
self-supervised scenarios, without access to ground-truth depth. Finally, we
apply our method to different state-of-the-art monocular depth estimation
models, in each case giving similar or better results compared to the original
model, while requiring less than half the multiply-adds in the decoder network.
Code at https://github.com/nianticlabs/wavelet-monodepth
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02023</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02023</id><submitter>Patrick James Roddy</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:44:09 GMT</date><size>5146kb</size><source_type>D</source_type></version><title>Slepian Scale-Discretised Wavelets on the Sphere</title><authors>Patrick J. Roddy, Jason D. McEwen</authors><categories>cs.IT astro-ph.IM cs.NA math.IT math.NA</categories><comments>10 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work presents the construction of a novel spherical wavelet basis
designed for incomplete spherical datasets, i.e. datasets which are missing in
a particular region of the sphere. The eigenfunctions of the Slepian
spatial-spectral concentration problem (the Slepian functions) are a set of
orthogonal basis functions which exist within a defined region. Slepian
functions allow one to compute a convolution on the incomplete sphere by
leveraging the recently proposed sifting convolution and extending it to any
set of basis functions. Through a tiling of the Slepian harmonic line one may
construct scale-discretised wavelets. An illustration is presented based on an
example region on the sphere defined by the topographic map of the Earth. The
Slepian wavelets and corresponding wavelet coefficients are constructed from
this region, and are used in a straightforward denoising example.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02024</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02024</id><submitter>Thorben Tr\&quot;obst</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:48:40 GMT</date><size>62kb</size><source_type>D</source_type></version><title>Combinatorial Algorithms for Matching Markets via Nash Bargaining:
  One-Sided, Two-Sided and Non-Bipartite</title><authors>Ioannis Panageas, Thorben Tr\&quot;obst, Vijay V. Vazirani</authors><categories>cs.GT econ.GN q-fin.EC</categories><comments>51 pages</comments><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an attempt to deal with the recent realization (Vazirani,
Yannakakis 2021) that the Hylland-Zeckhauser mechanism, which has remained a
classic in economics for one-sided matching markets, is likely to be highly
intractable. HZ uses the power of a pricing mechanism, which has endowed it
with nice game-theoretic properties.
  Hosseini and Vazirani (2021) define a rich collection of
Nash-bargaining-based models for one-sided and two-sided matching markets, in
both Fisher and Arrow-Debreu settings, together with implementations using
available solvers, and very encouraging experimental results. This naturally
raises the question of finding efficient combinatorial algorithms for these
models.
  In this paper, we give efficient combinatorial algorithms based on the
techniques of multiplicative weights update (MWU) and conditional gradient
descent (CGD) for several one-sided and two-sided models defined in HV 2021.
Additionally, we define for the first time a Nash-bargaining-based model for
non-bipartite matching markets and solve it using CGD. Furthermore, in every
case, we study not only the Fisher but also the Arrow-Debreu version; the
latter is also called the exchange version. We give natural applications for
each model studied. These models inherit the game-theoretic and computational
properties of Nash bargaining.
  We also establish a deep connection between HZ and the Nash-bargaining-based
models, thereby confirming that the alternative to HZ proposed in HV 2021 is a
principled one.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02026</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02026</id><submitter>Xiao Mao</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:50:33 GMT</date><size>460kb</size></version><title>Breaking the Cubic Barrier for (Unweighted) Tree Edit Distance</title><authors>Xiao Mao</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The (unweighted) \emph{tree edit distance} problem for $n$ node trees asks to
compute a measure of dissimilarity between two rooted trees with node labels.
The current best algorithm from more than a decade ago runs in $O(n ^ 3)$ time
[Demaine, Mozes, Rossman, and Weimann, ICALP 2007]. The same paper also showed
that $O(n ^ 3)$ is the best possible running time for any algorithm using the
so-called \emph{decomposition strategy}, which underlies almost all the known
algorithms for this problem. These algorithms would also work for the
\emph{weighted} tree edit distance problem, which cannot be solved in truly
sub-cubic time under the APSP conjecture [Bringmann, Gawrychowski, Mozes, and
Weimann, SODA 2018].
  In this paper, we break the cubic barrier by showing an $O(n ^ {2.9546})$
time algorithm for the \emph{unweighted} tree edit distance problem.
  We consider an equivalent maximization problem and use a dynamic programming
scheme involving matrices with many special properties. By using a
decomposition scheme as well as several combinatorial techniques, we reduce
tree edit distance to the max-plus product of bounded-difference matrices,
which can be solved in truly sub-cubic time [Bringmann, Grandoni, Saha, and
Vassilevska Williams, FOCS 2016].
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02029</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02029</id><submitter>Ruohan Zhan</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:54:44 GMT</date><size>1657kb</size><source_type>D</source_type></version><title>Off-Policy Evaluation via Adaptive Weighting with Data from Contextual
  Bandits</title><authors>Ruohan Zhan, Vitor Hadad, David A. Hirshberg, and Susan Athey</authors><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has become increasingly common for data to be collected adaptively, for
example using contextual bandits. Historical data of this type can be used to
evaluate other treatment assignment policies to guide future innovation or
experiments. However, policy evaluation is challenging if the target policy
differs from the one used to collect data, and popular estimators, including
doubly robust (DR) estimators, can be plagued by bias, excessive variance, or
both. In particular, when the pattern of treatment assignment in the collected
data looks little like the pattern generated by the policy to be evaluated, the
importance weights used in DR estimators explode, leading to excessive
variance.
  In this paper, we improve the DR estimator by adaptively weighting
observations to control its variance. We show that a t-statistic based on our
improved estimator is asymptotically normal under certain conditions, allowing
us to form confidence intervals and test hypotheses. Using synthetic data and
public benchmarks, we provide empirical evidence for our estimator's improved
accuracy and inferential properties relative to existing alternatives.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02030</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02030</id><submitter>Stefan Mitsch</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:55:23 GMT</date><size>1144kb</size><source_type>D</source_type></version><title>Formally Verified Next-Generation Airborne Collision Avoidance Games in
  ACAS X</title><authors>Rachel Cleaveland, Stefan Mitsch, Andre Platzer</authors><categories>cs.LO cs.GT cs.SY eess.SY</categories><msc-class>03B70, 68Q60</msc-class><acm-class>F.3.1; C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of aircraft collision avoidance algorithms is a subtle but
important challenge that merits the need for provable safety guarantees.
Obtaining such guarantees is nontrivial given the unpredictability of the
interplay of the intruder aircraft decisions, the ownship pilot reactions, and
the subtlety of the continuous motion dynamics of aircraft. Existing collision
avoidance systems, such as TCAS and the Next-Generation Airborne Collision
Avoidance System ACAS X, have been analyzed assuming severe restrictions on the
intruder's flight maneuvers, limiting their safety guarantees in real-world
scenarios where the intruder may also be changing its course. This work takes a
conceptually significant and practically relevant departure from existing ACAS
X models by generalizing them to hybrid games with first-class representations
of the ownship and intruder decisions coming from two independent players. By
proving the existence of winning strategies for the resulting Adversarial ACAS
X in differential game logic, collision-freedom is established for the rich
encounters of ownship and intruder aircraft with independent decisions along
differential equations for flight paths with evolving vertical/horizontal
velocities. We present three classes of models of increasing complexity:
single-advisory infinite-time models, bounded time models, and infinite time,
multi-advisory models. Within each class of models, we identify symbolic
conditions and prove that there then always is a possible ownship maneuver that
will prevent a collision between the two aircraft.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02034</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02034</id><submitter>Yongming Rao</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:57:41 GMT</date><size>3650kb</size><source_type>D</source_type></version><title>DynamicViT: Efficient Vision Transformers with Dynamic Token
  Sparsification</title><authors>Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, Cho-Jui
  Hsieh</authors><categories>cs.CV cs.AI cs.LG</categories><comments>Project page: https://dynamicvit.ivg-research.xyz/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention is sparse in vision transformers. We observe the final prediction
in vision transformers is only based on a subset of most informative tokens,
which is sufficient for accurate image recognition. Based on this observation,
we propose a dynamic token sparsification framework to prune redundant tokens
progressively and dynamically based on the input. Specifically, we devise a
lightweight prediction module to estimate the importance score of each token
given the current features. The module is added to different layers to prune
redundant tokens hierarchically. To optimize the prediction module in an
end-to-end manner, we propose an attention masking strategy to differentiably
prune a token by blocking its interactions with other tokens. Benefiting from
the nature of self-attention, the unstructured sparse tokens are still hardware
friendly, which makes our framework easy to achieve actual speed-up. By
hierarchically pruning 66% of the input tokens, our method greatly reduces
31%~37% FLOPs and improves the throughput by over 40% while the drop of
accuracy is within 0.5% for various vision transformers. Equipped with the
dynamic token sparsification framework, DynamicViT models can achieve very
competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs
and vision transformers on ImageNet. Code is available at
https://github.com/raoyongming/DynamicViT
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02036</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02036</id><submitter>Rohit Girdhar</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:57:55 GMT</date><size>5134kb</size><source_type>D</source_type></version><title>Anticipative Video Transformer</title><authors>Rohit Girdhar and Kristen Grauman</authors><categories>cs.CV cs.AI cs.LG cs.MM</categories><comments>Ranked #1 on CVPR'21 EPIC-Kitchens Action Anticipation challenge
  leaderboard. Project page: http://facebookresearch.github.io/AVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including
outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02039</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02039</id><submitter>Michael Janner</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:58:51 GMT</date><size>18372kb</size><source_type>D</source_type></version><title>Reinforcement Learning as One Big Sequence Modeling Problem</title><authors>Michael Janner, Qiyang Li, Sergey Levine</authors><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning (RL) is typically concerned with estimating
single-step policies or single-step models, leveraging the Markov property to
factorize the problem in time. However, we can also view RL as a sequence
modeling problem, with the goal being to predict a sequence of actions that
leads to a sequence of high rewards. Viewed in this way, it is tempting to
consider whether powerful, high-capacity sequence prediction models that work
well in other domains, such as natural-language processing, can also provide
simple and effective solutions to the RL problem. To this end, we explore how
RL can be reframed as &quot;one big sequence modeling&quot; problem, using
state-of-the-art Transformer architectures to model distributions over
sequences of states, actions, and rewards. Addressing RL as a sequence modeling
problem significantly simplifies a range of design decisions: we no longer
require separate behavior policy constraints, as is common in prior work on
offline model-free RL, and we no longer require ensembles or other epistemic
uncertainty estimators, as is common in prior work on model-based RL. All of
these roles are filled by the same Transformer sequence model. In our
experiments, we demonstrate the flexibility of this approach across
long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and
offline RL.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2106.02040</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2106.02040</id><submitter>Roberto Calandra</submitter><version version="v1"><date>Thu, 3 Jun 2021 17:59:31 GMT</date><size>3363kb</size><source_type>D</source_type></version><title>Towards Learning to Play Piano with Dexterous Hands and Touch</title><authors>Huazhe Xu, Yuping Luo, Shaoxiong Wang, Trevor Darrell, Roberto
  Calandra</authors><categories>cs.RO cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The virtuoso plays the piano with passion, poetry and extraordinary technical
ability. As Liszt said (a virtuoso)must call up scent and blossom, and breathe
the breath of life. The strongest robots that can play a piano are based on a
combination of specialized robot hands/piano and hardcoded planning algorithms.
In contrast to that, in this paper, we demonstrate how an agent can learn
directly from machine-readable music score to play the piano with dexterous
hands on a simulated piano using reinforcement learning (RL) from scratch. We
demonstrate the RL agents can not only find the correct key position but also
deal with various rhythmic, volume and fingering, requirements. We achieve this
by using a touch-augmented reward and a novel curriculum of tasks. We conclude
by carefully studying the important aspects to enable such learning algorithms
and that can potentially shed light on future research in this direction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0601043</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>cs/0601043</id><submitter>Toni Mancini</submitter><version version="v1"><date>Wed, 11 Jan 2006 14:29:44 GMT</date><size>130kb</size></version><title>Combining Relational Algebra, SQL, Constraint Modelling, and Local
  Search</title><authors>Marco Cadoli and Toni Mancini</authors><categories>cs.AI cs.LO</categories><comments>30 pages, 5 figures</comments><doi>10.1017/S1471068406002857</doi><abstract>  The goal of this paper is to provide a strong integration between constraint
modelling and relational DBMSs. To this end we propose extensions of standard
query languages such as relational algebra and SQL, by adding constraint
modelling capabilities to them. In particular, we propose non-deterministic
extensions of both languages, which are specially suited for combinatorial
problems. Non-determinism is introduced by means of a guessing operator, which
declares a set of relations to have an arbitrary extension. This new operator
results in languages with higher expressive power, able to express all problems
in the complexity class NP. Some syntactical restrictions which make data
complexity polynomial are shown. The effectiveness of both extensions is
demonstrated by means of several examples. The current implementation, written
in Java using local search techniques, is described. To appear in Theory and
Practice of Logic Programming (TPLP)
</abstract></arXivRaw>
</metadata>
</record>
<resumptionToken cursor="2000" completeListSize="2583"></resumptionToken>
</ListRecords>
</OAI-PMH>
