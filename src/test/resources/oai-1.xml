<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2021-06-04T12:58:12Z</responseDate>
<request verb="ListRecords" from="2021-06-01" metadataPrefix="arXivRaw" set="cs">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:0810.4840</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>0810.4840</id><submitter>Or Sattath</submitter><version version="v1"><date>Mon, 27 Oct 2008 18:23:31 GMT</date><size>67kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 08:13:58 GMT</date><size>94kb</size><source_type>D</source_type></version><title>The Pursuit of Uniqueness: Extending Valiant-Vazirani Theorem to the
  Probabilistic and Quantum Settings</title><authors>Dorit Aharonov, Michael Ben-Or, Fernando G.S.L. Brandao, Or Sattath</authors><categories>quant-ph cs.CC</categories><comments>26 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valiant-Vazirani showed in 1985 [VV85] that solving NP with the promise that
&quot;yes&quot; instances have only one witness is powerful enough to solve the entire NP
class (under randomized reductions).
  We are interested in extending this result to the quantum setting. We prove
extensions to the classes Merlin-Arthur MA and Quantum-Classical-Merlin-Arthur
QCMA. Our results have implications for the complexity of approximating the
ground state energy of a quantum local Hamiltonian with a unique ground state
and an inverse polynomial spectral gap. We show that the estimation (to within
polynomial accuracy) of the ground state energy of poly-gapped 1-D local
Hamiltonians is QCMA-hard [AN02], under randomized reductions. This is in stark
contrast to the case of constant gapped 1-D Hamiltonians, which is in NP
[Has07]. Moreover, it shows that unless QCMA can be reduced to NP by randomized
reductions, there is no classical description of the ground state of every
poly-gapped local Hamiltonian that allows efficient calculation of expectation
values.
  Finally, we discuss a few of the obstacles to the establishment of an
analogous result to the class Quantum-Merlin-Arthur (QMA). In particular, we
show that random projections fail to provide a polynomial gap between two
witnesses.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5635</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1204.5635</id><submitter>David Ram\'irez</submitter><version version="v1"><date>Wed, 25 Apr 2012 12:11:40 GMT</date><size>201kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 5 Dec 2012 12:20:17 GMT</date><size>206kb</size><source_type>D</source_type></version><title>Locally Most Powerful Invariant Tests for Correlation and Sphericity of
  Gaussian Vectors</title><authors>D. Ram\'irez, J. V\'ia, I. Santamar\'ia and L. L. Scharf</authors><categories>cs.IT math.IT stat.OT</categories><journal-ref>IEEE Transactions on Information Theory (2013)</journal-ref><doi>10.1109/TIT.2012.2232705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the existence of locally most powerful invariant tests
(LMPIT) for the problem of testing the covariance structure of a set of
Gaussian random vectors. The LMPIT is the optimal test for the case of close
hypotheses, among those satisfying the invariances of the problem, and in
practical scenarios can provide better performance than the typically used
generalized likelihood ratio test (GLRT). The derivation of the LMPIT usually
requires one to find the maximal invariant statistic for the detection problem
and then derive its distribution under both hypotheses, which in general is a
rather involved procedure. As an alternative, Wijsman's theorem provides the
ratio of the maximal invariant densities without even finding an explicit
expression for the maximal invariant. We first consider the problem of testing
whether a set of $N$-dimensional Gaussian random vectors are uncorrelated or
not, and show that the LMPIT is given by the Frobenius norm of the sample
coherence matrix. Second, we study the case in which the vectors under the null
hypothesis are uncorrelated and identically distributed, that is, the
sphericity test for Gaussian vectors, for which we show that the LMPIT is given
by the Frobenius norm of a normalized version of the sample covariance matrix.
Finally, some numerical examples illustrate the performance of the proposed
tests, which provide better results than their GLRT counterparts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2998</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1304.2998</id><submitter>David Ram\'irez</submitter><version version="v1"><date>Wed, 10 Apr 2013 15:34:08 GMT</date><size>3409kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 6 Dec 2013 14:01:49 GMT</date><size>2908kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 10 Jun 2014 07:49:51 GMT</date><size>3458kb</size><source_type>D</source_type></version><title>Detecting Directionality in Random Fields Using the Monogenic Signal</title><authors>Sofia Olhede, David Ram\'irez and Peter J. Schreier</authors><categories>cs.IT cs.CV math.IT</categories><journal-ref>IEEE Transactions on Information Theory (2014)</journal-ref><doi>10.1109/TIT.2014.2342734</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and analyzing directional structures in images is important in many
applications since one-dimensional patterns often correspond to important
features such as object contours or trajectories. Classifying a structure as
directional or non-directional requires a measure to quantify the degree of
directionality and a threshold, which needs to be chosen based on the
statistics of the image. In order to do this, we model the image as a random
field. So far, little research has been performed on analyzing directionality
in random fields. In this paper, we propose a measure to quantify the degree of
directionality based on the random monogenic signal, which enables a unique
decomposition of a 2D signal into local amplitude, local orientation, and local
phase. We investigate the second-order statistical properties of the monogenic
signal for isotropic, anisotropic, and unidirectional random fields. We analyze
our measure of directionality for finite-size sample images, and determine a
threshold to distinguish between unidirectional and non-unidirectional random
fields, which allows the automatic classification of images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2840</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1410.2840</id><submitter>Pengsheng Ji</submitter><version version="v1"><date>Fri, 10 Oct 2014 16:49:31 GMT</date><size>874kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 2 Jul 2015 07:51:07 GMT</date><size>1243kb</size><source_type>D</source_type></version><title>Coauthorship and Citation Networks for Statisticians</title><authors>Pengsheng Ji and Jiashun Jin</authors><categories>stat.AP cs.DL physics.soc-ph stat.ME</categories><msc-class>91C20, 62H30, 62P25</msc-class><journal-ref>Annals of Applied Statistics 2016, 10(4): 1779-1812</journal-ref><doi>10.1214/15-AOAS896</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have collected and cleaned two network data sets: Coauthorship and
Citation networks for statisticians. The data sets are based on all research
papers published in four of the top journals in statistics from $2003$ to the
first half of $2012$. We analyze the data sets from many different
perspectives, focusing on (a) centrality, (b) community structures, and (c)
productivity, patterns and trends.
  For (a), we have identified the most prolific/collaborative/highly cited
authors. We have also identified a handful of &quot;hot&quot; papers, suggesting
&quot;Variable Selection&quot; as one of the &quot;hot&quot; areas.
  For (b), we have identified about $15$ meaningful communities or research
groups, including large-size ones such as &quot;Spatial Statistics&quot;, &quot;Large-Scale
Multiple Testing&quot;, &quot;Variable Selection&quot; as well as small-size ones such as
&quot;Dimensional Reduction&quot;, &quot;Objective Bayes&quot;, &quot;Quantile Regression&quot;, and
&quot;Theoretical Machine Learning&quot;.
  For (c), we find that over the 10-year period, both the average number of
papers per author and the fraction of self citations have been decreasing, but
the proportion of distant citations has been increasing. These suggest that the
statistics community has become increasingly more collaborative, competitive,
and globalized.
  Our findings shed light on research habits, trends, and topological patterns
of statisticians. The data sets provide a fertile ground for future researches
on or related to social networks of statisticians.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.03316</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1601.03316</id><submitter>Atsushi Miyauchi</submitter><version version="v1"><date>Wed, 13 Jan 2016 17:11:16 GMT</date><size>112kb</size><source_type>D</source_type></version><title>Additive Approximation Algorithms for Modularity Maximization</title><authors>Yasushi Kawase, Tomomi Matsui, Atsushi Miyauchi</authors><categories>cs.SI cs.DS physics.soc-ph</categories><comments>23 pages, 4 figures</comments><journal-ref>Journal of Computer and System Sciences, 117 (2021), 182-201</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modularity is a quality function in community detection, which was
introduced by Newman and Girvan (2004). Community detection in graphs is now
often conducted through modularity maximization: given an undirected graph
$G=(V,E)$, we are asked to find a partition $\mathcal{C}$ of $V$ that maximizes
the modularity. Although numerous algorithms have been developed to date, most
of them have no theoretical approximation guarantee. Recently, to overcome this
issue, the design of modularity maximization algorithms with provable
approximation guarantees has attracted significant attention in the computer
science community.
  In this study, we further investigate the approximability of modularity
maximization. More specifically, we propose a polynomial-time
$\left(\cos\left(\frac{3-\sqrt{5}}{4}\pi\right) -
\frac{1+\sqrt{5}}{8}\right)$-additive approximation algorithm for the
modularity maximization problem. Note here that
$\cos\left(\frac{3-\sqrt{5}}{4}\pi\right) - \frac{1+\sqrt{5}}{8} &lt; 0.42084$
holds. This improves the current best additive approximation error of $0.4672$,
which was recently provided by Dinh, Li, and Thai (2015). Interestingly, our
analysis also demonstrates that the proposed algorithm obtains a nearly-optimal
solution for any instance with a very high modularity value. Moreover, we
propose a polynomial-time $0.16598$-additive approximation algorithm for the
maximum modularity cut problem. It should be noted that this is the first
non-trivial approximability result for the problem. Finally, we demonstrate
that our approximation algorithm can be extended to some related problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1612.02990</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1612.02990</id><submitter>Yuk Kuroki</submitter><version version="v1"><date>Fri, 9 Dec 2016 12:04:20 GMT</date><size>163kb</size></version><title>Approximation Algorithm for Cycle-Star Hub Network Design Problems and
  Cycle-Metric Labeling Problems</title><authors>Yuko Kuroki and Tomomi Matsui</authors><categories>cs.DS cs.DM</categories><comments>14 pages, 2 figures; to appear in WALCOM 2017</comments><journal-ref>Journal of Graph Algorithms and Applications (JGAA), vol. 23
  (2019), No. 1, pp. 93-110</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single allocation hub-and-spoke network design problem which
allocates each non-hub node to exactly one of given hub nodes so as to minimize
the total transportation cost. This paper deals with a case in which the hubs
are located in a cycle, which is called a cycle-star hub network design
problem. The problem is essentially equivalent to a cycle-metric labeling
problem. The problem is useful in the design of networks in telecommunications
and airline transportation systems.We propose a $2(1-1/h)$-approximation
algorithm where $h$ denotes the number of hub nodes. Our algorithm solves a
linear relaxation problem and employs a dependent rounding procedure. We
analyze our algorithm by approximating a given cycle-metric matrix by a convex
combination of Monge matrices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1701.01055</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1701.01055</id><submitter>Zhiyong Zhou</submitter><version version="v1"><date>Wed, 4 Jan 2017 15:57:58 GMT</date><size>585kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 7 Apr 2017 08:11:41 GMT</date><size>1000kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 03:13:52 GMT</date><size>867kb</size></version><title>Estimation of block sparsity in compressive sensing</title><authors>Zhiyong Zhou and Jun Yu</authors><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explicitly using the block structure of the unknown signal can achieve better
reconstruction performance in compressive sensing. Theoretically, an unknown
signal with block structure can be accurately recovered from a few number of
under-determined linear measurements provided that it is sufficiently block
sparse. From the practical point of view, a severe concern is that the block
sparse level appears often unknown. In this paper, we introduce a soft measure
of block sparsity
$k_\alpha(\mathbf{x})=\left(\lVert\mathbf{x}\rVert_{2,\alpha}/\lVert\mathbf{x}\rVert_{2,1}\right)^{\frac{\alpha}{1-\alpha}}$
with $\alpha\in[0,\infty]$, and propose an estimation procedure by using
multivariate centered isotropic symmetric $\alpha$-stable random projections.
The limiting distribution of the estimator is established. Simulations are
conducted to illustrate our theoretical results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1704.06879</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1704.06879</id><submitter>Rui Meng</submitter><version version="v1"><date>Sun, 23 Apr 2017 04:34:26 GMT</date><size>2267kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 Sep 2018 21:24:51 GMT</date><size>2267kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 22:34:52 GMT</date><size>2273kb</size><source_type>D</source_type></version><title>Deep Keyphrase Generation</title><authors>Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky,
  Yu Chi</authors><categories>cs.CL</categories><comments>Accepted by ACL2017</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyphrase provides highly-condensed information that can be effectively used
for understanding, organizing and retrieving text content. Though previous
studies have provided many workable solutions for automated keyphrase
extraction, they commonly divided the to-be-summarized content into multiple
text chunks, then ranked and selected the most meaningful ones. These
approaches could neither identify keyphrases that do not appear in the text,
nor capture the real semantic meaning behind the text. We propose a generative
model for keyphrase prediction with an encoder-decoder framework, which can
effectively overcome the above drawbacks. We name it as deep keyphrase
generation since it attempts to capture the deep semantic meaning of the
content with a deep learning method. Empirical analysis on six datasets
demonstrates that our proposed model not only achieves a significant
performance boost on extracting keyphrases that appear in the source text, but
also can generate absent keyphrases based on the semantic meaning of the text.
Code and dataset are available at
https://github.com/memray/OpenNMT-kpg-release.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1709.04673</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1709.04673</id><submitter>Arunselvan Ramaswamy Dr.</submitter><version version="v1"><date>Thu, 14 Sep 2017 09:04:41 GMT</date><size>25kb</size></version><version version="v2"><date>Sun, 10 Dec 2017 08:43:40 GMT</date><size>26kb</size></version><version version="v3"><date>Tue, 4 Sep 2018 08:33:45 GMT</date><size>29kb</size></version><version version="v4"><date>Thu, 24 Oct 2019 09:24:15 GMT</date><size>33kb</size></version><version version="v5"><date>Sun, 30 May 2021 11:34:09 GMT</date><size>41kb</size></version><title>Analyzing Approximate Value Iteration Algorithms</title><authors>Arunselvan Ramaswamy and Shalabh Bhatnagar</authors><categories>cs.SY math.DS stat.ML</categories><msc-class>62L20, 93E35, 37B25, 34A60, 90C39, 37C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the stochastic iterative counterpart of the value
iteration scheme wherein only noisy and possibly biased approximations of the
Bellman operator are available. We call this counterpart as the approximate
value iteration (AVI) scheme. Neural networks are often used as function
approximators, in order to counter Bellman's curse of dimensionality. In this
paper, they are used to approximate the Bellman operator. Since neural networks
are typically trained using sample data, errors and biases may be introduced.
The design of AVI accounts for implementations with biased approximations of
the Bellman operator and sampling errors. We present verifiable sufficient
conditions under which AVI is stable (almost surely bounded) and converges to a
fixed point of the approximate Bellman operator. To ensure the stability of
AVI, we present three different yet related sets of sufficient conditions that
are based on the existence of an appropriate Lyapunov function. These Lyapunov
function based conditions are easily verifiable and new to the literature. The
verifiability is enhanced by the fact that a recipe for the construction of the
necessary Lyapunov function is also provided. We also show that the stability
analysis of AVI can be readily extended to the general case of set-valued
stochastic approximations. Finally, we show that AVI can also be used in more
general circumstances, i.e., for finding fixed points of contractive set-valued
maps.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1711.06134</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1711.06134</id><submitter>Pascal Budner</submitter><version version="v1"><date>Tue, 14 Nov 2017 01:34:12 GMT</date><size>1429kb</size></version><title>&quot;Making you happy makes me happy&quot; -- Measuring Individual Mood with
  Smartwatches</title><authors>Pascal Budner, Joscha Eirich, Peter A. Gloor</authors><categories>cs.HC</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a system to measure individual happiness based on interpreting
body sensors on smartwatches. In our prototype system we use a Pebble
smartwatch to track activity, heartrate, light level, and GPS coordinates, and
extend it with external information such as weather data, humidity, and day of
the week. Training our machine learning-based mood prediction system using
random forests with data manually entered into the smartwatch, we achieve
prediction accuracy of up to 94%. We find that besides body signals, the
weather data exerts a strong influence on mood. In addition our system also
allows us to identify friends who are indicators of our positive or negative
mood.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1802.09205</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1802.09205</id><submitter>Geppino Pucci</submitter><version version="v1"><date>Mon, 26 Feb 2018 09:01:45 GMT</date><size>21kb</size></version><version version="v2"><date>Mon, 23 Apr 2018 15:04:53 GMT</date><size>814kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 10 Jul 2018 13:08:10 GMT</date><size>1230kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 12 Oct 2018 13:34:56 GMT</date><size>2433kb</size><source_type>D</source_type></version><version version="v5"><date>Wed, 16 Jan 2019 07:06:57 GMT</date><size>1485kb</size><source_type>D</source_type></version><version version="v6"><date>Tue, 1 Jun 2021 15:56:13 GMT</date><size>1485kb</size><source_type>D</source_type></version><title>Solving $k$-center Clustering (with Outliers) in MapReduce and
  Streaming, almost as Accurately as Sequentially</title><authors>Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci</authors><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Center-based clustering is a fundamental primitive for data analysis and
becomes very challenging for large datasets. In this paper, we focus on the
popular $k$-center variant which, given a set $S$ of points from some metric
space and a parameter $k&lt;|S|$, requires to identify a subset of $k$ centers in
$S$ minimizing the maximum distance of any point of $S$ from its closest
center. A more general formulation, introduced to deal with noisy datasets,
features a further parameter $z$ and allows up to $z$ points of $S$ (outliers)
to be disregarded when computing the maximum distance from the centers. We
present coreset-based 2-round MapReduce algorithms for the above two
formulations of the problem, and a 1-pass Streaming algorithm for the case with
outliers. For any fixed $\epsilon&gt;0$, the algorithms yield solutions whose
approximation ratios are a mere additive term $\epsilon$ away from those
achievable by the best known polynomial-time sequential algorithms, a result
that substantially improves upon the state of the art. Our algorithms are
rather simple and adapt to the intrinsic complexity of the dataset, captured by
the doubling dimension $D$ of the metric space. Specifically, our analysis
shows that the algorithms become very space-efficient for the important case of
small (constant) $D$. These theoretical results are complemented with a set of
experiments on real-world and synthetic datasets of up to over a billion
points, which show that our algorithms yield better quality solutions over the
state of the art while featuring excellent scalability, and that they also lend
themselves to sequential implementations much faster than existing ones.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1803.00969</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1803.00969</id><submitter>S.M. Zafaruddin</submitter><version version="v1"><date>Fri, 2 Mar 2018 17:43:24 GMT</date><size>697kb</size><source_type>D</source_type></version><title>Energy Efficiency of Opportunistic Device-to-Device Relaying Under
  Lognormal Shadowing</title><authors>S. M. Zafaruddin, Jan Plachy, Zdenek Becvar, Amir Leshem</authors><categories>cs.IT eess.SP math.IT</categories><comments>30 pages, 8 figures</comments><journal-ref>IEEE Systems Journal 2020</journal-ref><doi>10.1109/JSYST.2020.3025106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy consumption is a major limitation of low power and mobile devices.
Efficient transmission protocols are required to minimize an energy consumption
of the mobile devices for ubiquitous connectivity in the next generation
wireless networks. Opportunistic schemes select a single relay using the
criteria of the best channel and achieve a near-optimal diversity performance
in a cooperative wireless system. In this paper, we study the energy efficiency
of the opportunistic schemes for device-to-device communication. In the
opportunistic approach, an energy consumed by devices is minimized by selecting
a single neighboring device as a relay using the criteria of minimum consumed
energy in each transmission in the uplink of a wireless network. We derive
analytical bounds and scaling laws on the expected energy consumption when the
devices experience log-normal shadowing with respect to a base station
considering both the transmission as well as circuit energy consumptions. We
show that the protocol improves the energy efficiency of the network comparing
to the direct transmission even if only a few devices are considered for
relaying. We also demonstrate the effectiveness of the protocol by means of
simulations in realistic scenarios of the wireless network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1803.09435</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1803.09435</id><submitter>Suthee Ruangwises</submitter><version version="v1"><date>Mon, 26 Mar 2018 06:41:07 GMT</date><size>3kb</size></version><version version="v2"><date>Wed, 26 Sep 2018 15:14:44 GMT</date><size>9kb</size></version><version version="v3"><date>Sat, 16 Mar 2019 06:09:25 GMT</date><size>10kb</size></version><version version="v4"><date>Tue, 30 Jul 2019 09:26:23 GMT</date><size>10kb</size></version><version version="v5"><date>Fri, 25 Oct 2019 08:54:06 GMT</date><size>11kb</size></version><version version="v6"><date>Tue, 3 Nov 2020 11:35:03 GMT</date><size>12kb</size></version><title>Unpopularity Factor in the Marriage and Roommates Problems</title><authors>Suthee Ruangwises, Toshiya Itoh</authors><categories>cs.DS</categories><comments>A preliminary version of this paper has appeared at CSR 2019</comments><journal-ref>Theory of Computing Systems, 65(3): 579-592 (2021)</journal-ref><doi>10.1007/s00224-020-09978-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $A$ of $n$ people, with each person having a preference list that
ranks a subset of $A$ as his/her acceptable partners in order of preference, we
consider the Roommates Problem (RP) and the Marriage Problem (MP) of matching
people with their partners. In RP there is no further restriction, while in MP
only people of opposite genders can be acceptable partners. For a pair of
matchings $X$ and $Y$, let $\phi(X,Y)$ denote the number of people who prefer a
person they get matched by $X$ to a person they get matched by $Y$, and define
an unpopularity factor $u(M)$ of a matching $M$ to be the maximum ratio
$\phi(M',M) / \phi(M,M')$ among all other possible matchings $M'$. In this
paper, we develop an algorithm to compute the unpopularity factor of a given
matching in $O(m\sqrt{n}\log^2 n)$ time for RP and in $O(m\sqrt{n}\log n)$ time
for MP, where $m$ is the total length of people's preference lists. We also
generalize the notion of unpopularity factor to a weighted setting where people
are given different voting weights and show that our algorithm can be slightly
modified to support that setting with the same running time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1805.09446</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1805.09446</id><submitter>Richard Zach</submitter><version version="v1"><date>Wed, 23 May 2018 22:12:41 GMT</date><size>78kb</size></version><title>Non-Analytic Tableaux for Chellas's Conditional Logic CK and Lewis's
  Logic of Counterfactuals VC</title><authors>Richard Zach</authors><categories>math.LO cs.LO</categories><msc-class>03B45</msc-class><journal-ref>Australasian Journal of Logic 15 (3):609-628 (2018)</journal-ref><doi>10.26686/ajl.v15i3.4780</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Priest has provided a simple tableau calculus for Chellas's conditional logic
Ck. We provide rules which, when added to Priest's system, result in tableau
calculi for Chellas's CK and Lewis's VC. Completeness of these tableaux,
however, relies on the cut rule.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1805.11137</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1805.11137</id><submitter>C\'onall Kelly</submitter><version version="v1"><date>Mon, 28 May 2018 19:14:42 GMT</date><size>370kb</size></version><version version="v2"><date>Fri, 1 Mar 2019 10:05:35 GMT</date><size>384kb</size></version><version version="v3"><date>Mon, 11 Nov 2019 10:56:26 GMT</date><size>357kb</size></version><version version="v4"><date>Tue, 1 Jun 2021 11:16:41 GMT</date><size>120kb</size></version><title>Adaptive Euler methods for stochastic systems with non-globally
  Lipschitz coefficients</title><authors>C\'onall Kelly and Gabriel Lord</authors><categories>math.NA cs.NA</categories><comments>This is a preprint of an article published in Numerical Algorithms.
  The final authenticated version is available online at:
  https://doi.org/10.1007/s11075-021-01131-8</comments><msc-class>60H15, 60H35, 65C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present strongly convergent explicit and semi-implicit adaptive numerical
schemes for systems of stiff stochastic differential equations (SDEs) where
both the drift and diffusion are non-globally Lipschitz continuous. This
stiffness may originate either from a linear operator in the drift, or from a
perturbation of the nonlinear structures under discretisation, or both. Typical
applications arise from the space discretisation of an SPDE, stochastic
volatility models in finance, or certain ecological models. We prove that a
timetepping strategy that adapts the stepsize based on the drift alone is
sufficient to control growth and to obtain strong convergence with polynomial
order. The order of strong convergence of our scheme is $(1-\varepsilon)/2$,
for $\varepsilon\in(0,1)$, where $\varepsilon$ becomes arbitrarily small as the
number of available finite moments for solutions of the SDE increases.
Numerically, we compare the adaptive semi-implicit method to a fully drift
implicit method, three tamed type methods and a truncated method. Our numerical
results show that the adaptive semi-implicit method is well suited as a general
purpose solver, is more robust than the explicit time stepping methods and more
efficient than the drift implicit method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1806.01344</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1806.01344</id><submitter>Marcos Netto</submitter><version version="v1"><date>Mon, 4 Jun 2018 19:32:33 GMT</date><size>196kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 12 Jun 2018 14:00:37 GMT</date><size>200kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 10 Jul 2018 05:10:20 GMT</date><size>196kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 29 Aug 2018 18:37:59 GMT</date><size>202kb</size><source_type>D</source_type></version><version version="v5"><date>Thu, 20 Sep 2018 01:22:28 GMT</date><size>211kb</size><source_type>D</source_type></version><title>Data-Driven Participation Factors for Nonlinear Systems Based on Koopman
  Mode Decomposition</title><authors>Marcos Netto, Yoshihiko Susuki, Lamine Mili</authors><categories>cs.SY eess.SY</categories><doi>10.1109/LCSYS.2018.2871887</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a novel data-driven technique to compute the
participation factors for nonlinear systems based on the Koopman mode
decomposition. Provided that certain conditions are satisfied, it is shown that
the proposed technique generalizes the original definition of the linear
mode-in-state participation factors. Two numerical examples are provided to
demonstrate the performance of our approach: one relying on a canonical
nonlinear dynamical system, and the other based on the two-area four-machine
power system. The Koopman mode decomposition is capable of coping with a large
class of nonlinearity, thereby making our technique able to deal with
oscillations arising in practice due to nonlinearities while being fast to
compute and compatible with real-time applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1806.06103</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1806.06103</id><submitter>Jeremy Kozdon</submitter><version version="v1"><date>Fri, 15 Jun 2018 19:42:20 GMT</date><size>2318kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:20:11 GMT</date><size>2318kb</size><source_type>D</source_type></version><title>Robust Approaches to Handling Complex Geometries with Galerkin
  Difference Methods</title><authors>Jeremy E. Kozdon, Lucas C. Wilcox, Thomas Hagstrom, Jeffrey W. Banks</authors><categories>math.NA cs.NA</categories><comments>30 pages, 14 figures</comments><journal-ref>Journal of Computational Physics, 392, 483-510 (2019)</journal-ref><doi>10.1016/j.jcp.2019.04.031</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The Galerkin difference (GD) basis is a set of continuous, piecewise
polynomials defined using a finite difference like grid of degrees of freedom.
The one dimensional GD basis functions are naturally extended to multiple
dimensions using the tensor product constructions to quadrilateral elements for
discretizing partial differential equations. Here we propose two approaches to
handling complex geometries using the GD basis within a discontinuous Galerkin
finite element setting: (1) using non-conforming, curvilinear GD elements and
(2) coupling affine GD elements with curvilinear simplicial elements. In both
cases the (semidiscrete) discontinuous Galerkin method is provably energy
stable even when variational crimes are committed and in both cases a
weight-adjusted mass matrix is used, which ensures that only the reference mass
matrix must be inverted. Additionally, we give sufficient conditions on the
treatment of metric terms for the curvilinear, nonconforming GD elements to
ensure that the scheme is both constant preserving and conservative. Numerical
experiments confirm the stability results and demonstrate the accuracy of the
coupled schemes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1806.09888</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1806.09888</id><submitter>Michael Murray</submitter><version version="v1"><date>Tue, 26 Jun 2018 10:21:06 GMT</date><size>16kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 15:13:44 GMT</date><size>206kb</size></version><title>Towards an understanding of CNNs: analysing the recovery of activation
  pathways via Deep Convolutional Sparse Coding</title><authors>Michael Murray, Jared Tanner</authors><categories>cs.LG stat.ML</categories><comments>Long version (8 pages excluding references) of paper accepted at the
  IEEE 2018 Data Science Workshop</comments><msc-class>68T07</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep Convolutional Sparse Coding (D-CSC) is a framework reminiscent of deep
convolutional neural networks (DCNNs), but by omitting the learning of the
dictionaries one can more transparently analyse the role of the activation
function and its ability to recover activation paths through the layers.
Papyan, Romano, and Elad conducted an analysis of such an architecture,
demonstrated the relationship with DCNNs and proved conditions under which the
D-CSC is guaranteed to recover specific activation paths. A technical
innovation of their work highlights that one can view the efficacy of the ReLU
nonlinear activation function of a DCNN through a new variant of the tensor's
sparsity, referred to as stripe-sparsity. Using this they proved that
representations with an activation density proportional to the ambient
dimension of the data are recoverable. We extend their uniform guarantees to a
modified model and prove that with high probability the true activation is
typically possible to recover for a greater density of activations per layer.
Our extension follows from incorporating the prior work on one step
thresholding by Schnass and Vandergheynst.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1806.10436</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1806.10436</id><submitter>Quentin Wargnier</submitter><version version="v1"><date>Wed, 27 Jun 2018 12:23:49 GMT</date><size>4012kb</size></version><version version="v2"><date>Fri, 21 Feb 2020 07:19:11 GMT</date><size>1254kb</size><source_type>D</source_type></version><title>Numerical treatment of the nonconservative product in a multiscale fluid
  model for plasmas in thermal nonequilibrium: application to solar physics</title><authors>Quentin Wargnier (CMAP), Sylvain Faure (LM-Orsay), Benjamin Graille
  (LM-Orsay), Thierry Magin (VKI), Marc Massot (CMAP)</authors><categories>math.NA astro-ph.SR cs.NA math.AP physics.class-ph physics.plasm-ph</categories><proxy>ccsd</proxy><journal-ref>SIAM Journal on Scientific Computing, Society for Industrial and
  Applied Mathematics, 2020, 42 (2), pp.B492-B519</journal-ref><doi>10.1137/18M1194225</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution deals with the modeling of collisional multicomponent
magnetized plasmas in thermal and chemical nonequilibrium aiming at simulating
and predicting magnetic reconnections in the chromosphere of the sun. We focus
on the numerical simulation of a simplified fluid model in order to properly
investigate the influence on shock solutions of a nonconservative product
present in the electron energy equation. Then, we derive jump conditions based
on travelling wave solutions and propose an original numerical treatment in
order to avoid non-physical shocks for the solution, that remains valid in the
case of coarse-resolution simulations. A key element for the numerical scheme
proposed is the presence of diffusion in the electron variables, consistent
with the physically-sound scaling used in the model developed by Graille et al.
following a multiscale Chapman-Enskog expansion method [M3AS, 19 (2009)
527--599]. The numerical strategy is eventually assessed in the framework of a
solar physics test case. The computational method is able to capture the
travelling wave solutions in both the highly- and coarsely-resolved cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1807.01261</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1807.01261</id><submitter>R\'emi Abgrall</submitter><version version="v1"><date>Tue, 3 Jul 2018 16:14:59 GMT</date><size>1089kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:05:25 GMT</date><size>1102kb</size><source_type>D</source_type></version><title>On the Connection between Residual Distribution Schemes and Flux
  Reconstruction</title><authors>Remi Abgrall and Elise le Meledo and Philipp Oeffner</authors><categories>math.NA cs.NA</categories><msc-class>65M06, 65M08, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short paper, we are considering the connection between the
\emph{Residual Distribution Schemes} (RD) and the \emph{Flux Reconstruction}
(FR) approach. We demonstrate that flux reconstruction can be recast into the
RD framework and vice versa. Because of this close connection we are able to
apply known results from RD schemes to FR methods. In this context we propose a
first demonstration of entropy stability for the FR schemes under consideration
and show how to construct entropy stable numerical schemes based on our FR
methods. Simultaneously, we do not restrict the mesh to tensor structures or
triangle elements, but rather allow polygons. The key of our analysis is a
proper choice of the correction functions for which we present an approach
here.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1808.00794</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1808.00794</id><submitter>Rafal Kapelko</submitter><version version="v1"><date>Thu, 2 Aug 2018 13:05:18 GMT</date><size>28kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 14 Aug 2018 21:25:46 GMT</date><size>62kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 26 Feb 2019 21:59:00 GMT</date><size>660kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 3 Mar 2019 00:15:09 GMT</date><size>660kb</size><source_type>D</source_type></version><version version="v5"><date>Sat, 13 Apr 2019 01:31:18 GMT</date><size>660kb</size><source_type>D</source_type></version><version version="v6"><date>Wed, 17 Apr 2019 17:28:24 GMT</date><size>660kb</size><source_type>D</source_type></version><version version="v7"><date>Fri, 1 May 2020 09:22:13 GMT</date><size>663kb</size><source_type>D</source_type></version><version version="v8"><date>Sat, 29 May 2021 19:31:48 GMT</date><size>665kb</size><source_type>D</source_type></version><title>Analysis of the Threshold for Energy Consumption in Displacement of
  Random Sensors</title><authors>Rafal Kapelko</authors><categories>cs.NI cs.DM cs.SY eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental problem of energy-efficient reallocation of mobile random
sensors to provide full coverage without interference is addressed in this
paper. We consider $n$ mobile sensors with identical sensing range placed
randomly on the unit interval and on the unit square. The main contribution is
summarized as follows:
  If the sensors are placed on the unit interval we explain the sharp increase
around the sensing radius equal to $\frac{1}{2n}$ and the interference distance
equal to $\frac{1}{n}$ for the expected minimal $a$-total displacement,
  If the sensors are placed on the unit square we explain the sharp increase
around the square sensing radius equal to $\frac{1}{2 \sqrt{n}}$ and the
interference distance equal to $\frac{1}{\sqrt{n}}$ for the expected minimal
$a$-total displacement.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1808.02362</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1808.02362</id><submitter>Marc Timme</submitter><version version="v1"><date>Tue, 7 Aug 2018 13:43:29 GMT</date><size>680kb</size><source_type>D</source_type></version><title>Adhesion-induced Discontinuous Transitions and Classifying Social
  Networks</title><authors>Nora Molkenthin, Malte Schr\&quot;oder, and Marc Timme</authors><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>6 pages incl. references, accepted at Physical Review Letters</comments><journal-ref>Phys. Rev. Lett. 121, 138301 (2018)</journal-ref><doi>10.1103/PhysRevLett.121.138301</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Transition points mark qualitative changes in the macroscopic properties of
large complex systems. Explosive transitions, exhibiting properties of both
continuous and discontinuous phase transitions, have recently been uncovered in
network growth processes. Real networks not only grow but often also
restructure, yet common network restructuring processes, such as small world
rewiring, do not exhibit phase transitions. Here, we uncover a class of
intrinsically discontinuous transitions emerging in network restructuring
processes controlled by \emph{adhesion} -- the preference of a chosen link to
remain connected to its end node. Deriving a master equation for the temporal
network evolution and working out an analytic solution, we identify genuinely
discontinuous transitions in non-growing networks, separating qualitatively
distinct phases with monotonic and with peaked degree distributions.
Intriguingly, our analysis of heuristic data indicates a separation between the
same two forms of degree distributions distinguishing abstract from
face-to-face social networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06172</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1809.06172</id><submitter>Fabio Calefato</submitter><version version="v1"><date>Fri, 14 Sep 2018 05:17:07 GMT</date><size>1260kb</size></version><version version="v2"><date>Mon, 31 May 2021 08:39:27 GMT</date><size>1260kb</size></version><title>Investigating Crowd Creativity in Online Music Communities</title><authors>Fabio Calefato and Giuseppe Iaffaldano and Filippo Lanubile and
  Federico Maiorano</authors><categories>cs.HC cs.SI</categories><comments>Proceedings of the ACM on Human-Computer Interaction, Vol. 2, No.
  CSCW, Article 27, Publication date: November 2018</comments><doi>10.1145/3274296</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd creativity is typically associated with peer-production communities
focusing on artistic products like animations, video games, and music, but less
frequently to Open Source Software (OSS), despite the fact that also developers
must be creative to come up with new solutions to their technical challenges.
In this paper, we conduct a study to further the understanding of which factors
from prior work in both OSS and art communities are predictive of successful
collaboration - defined as reuse of previous songs - in three different
songwriting communities, namely Songtree, Splice, and ccMixter. The main
findings from this study confirm that the success of collaborations is
associated with high community status of recognizable authors and low degree of
derivativity of songs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07320</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1809.07320</id><submitter>Travis Gagie</submitter><version version="v1"><date>Wed, 19 Sep 2018 15:58:53 GMT</date><size>13kb</size></version><version version="v2"><date>Wed, 14 Nov 2018 15:09:51 GMT</date><size>3kb</size></version><version version="v3"><date>Fri, 1 Feb 2019 11:28:28 GMT</date><size>388kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 10 Feb 2021 17:48:30 GMT</date><size>238kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 1 Jun 2021 17:49:25 GMT</date><size>415kb</size><source_type>D</source_type></version><title>Compressing and Indexing Aligned Readsets</title><authors>Travis Gagie, Garance Gourdel and Giovanni Manzini</authors><categories>cs.DS q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to use one or more assembled or partially assembled
genome as the basis for a compressed full-text index of its readset.
Specifically, we build a labelled tree by taking the assembled genome as a
trunk and grafting onto it the reads that align to it, at the starting
positions of their alignments. Next, we compute the eXtended Burrows-Wheeler
Transform (XBWT) of the resulting labelled tree and build a compressed
full-text index on that. Although this index can occasionally return false
positives, it is usually much more compact than the alternatives. Following the
established practice for datasets with many repetitions, we compare different
full-text indices by looking at the number of runs in the transformed strings.
For a human Chr19 readset our preliminary experiments show that eliminating
separators characters from the EBWT reduces the number of runs by 19\%, from
220 million to 178 million, and using the XBWT reduces it by a further 15\%, to
150 million.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08782</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1810.08782</id><submitter>Abhishek</submitter><version version="v1"><date>Sat, 20 Oct 2018 09:59:31 GMT</date><size>1559kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 27 Oct 2018 08:20:25 GMT</date><size>1559kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 16 Sep 2019 15:59:10 GMT</date><size>2165kb</size><source_type>D</source_type></version><title>Collective Learning From Diverse Datasets for Entity Typing in the Wild</title><authors>Abhishek Abhishek and Amar Prakash Azad and Balaji Ganesan and Ashish
  Anand and Amit Awekar</authors><categories>cs.CL cs.AI</categories><comments>Accepted at EYRE'19 Workshop, CIKM 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity typing (ET) is the problem of assigning labels to given entity
mentions in a sentence. Existing works for ET require knowledge about the
domain and target label set for a given test instance. ET in the absence of
such knowledge is a novel problem that we address as ET in the wild. We
hypothesize that the solution to this problem is to build supervised models
that generalize better on the ET task as a whole, rather than a specific
dataset. In this direction, we propose a Collective Learning Framework (CLF),
which enables learning from diverse datasets in a unified way. The CLF first
creates a unified hierarchical label set (UHLS) and a label mapping by
aggregating label information from all available datasets. Then it builds a
single neural network classifier using UHLS, label mapping, and a partial loss
function. The single classifier predicts the finest possible label across all
available domains even though these labels may not be present in any
domain-specific dataset. We also propose a set of evaluation schemes and
metrics to evaluate the performance of models in this novel problem. Extensive
experimentation on seven diverse real-world datasets demonstrates the efficacy
of our CLF.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00345</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1811.00345</id><submitter>Tomasz Tkocz</submitter><version version="v1"><date>Thu, 1 Nov 2018 12:45:40 GMT</date><size>16kb</size></version><version version="v2"><date>Fri, 5 Apr 2019 01:35:51 GMT</date><size>28kb</size></version><version version="v3"><date>Tue, 5 May 2020 02:40:02 GMT</date><size>30kb</size></version><version version="v4"><date>Tue, 13 Oct 2020 01:37:12 GMT</date><size>32kb</size></version><title>Sharp moment-entropy inequalities and capacity bounds for log-concave
  distributions</title><authors>Mokshay Madiman, Piotr Nayar, Tomasz Tkocz</authors><categories>cs.IT math.IT math.PR</categories><comments>the original paper split into two: this one and arXiv:1904.02314;
  additional results added to this part</comments><msc-class>94A17 (Primary), 60E15 (Secondary)</msc-class><journal-ref>IEEE Trans. Inform. Theory 67 (2021), no. 1, 81-94</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the uniform distribution minimizes entropy among all
one-dimensional symmetric log-concave distributions with fixed variance, as
well as various generalizations of this fact to R\'enyi entropies of orders
less than 1 and with moment constraints involving $p$-th absolute moments with
$p\leq 2$. As consequences, we give new capacity bounds for additive noise
channels with symmetric log-concave noises, as well as for timing channels
involving positive signal and noise where the noise has a decreasing
log-concave density. In particular, we show that the capacity of an additive
noise channel with symmetric, log-concave noise under an average power
constraint is at most 0.254 bits per channel use greater than the capacity of
an additive Gaussian noise channel with the same noise power. Consequences for
reverse entropy power inequalities and connections to the slicing problem in
convex geometry are also discussed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02750</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1811.02750</id><submitter>Bridianne O'Dea</submitter><version version="v1"><date>Wed, 7 Nov 2018 04:11:15 GMT</date><size>849kb</size></version><title>The relationship between linguistic expression and symptoms of
  depression, anxiety, and suicidal thoughts: A longitudinal study of blog
  content</title><authors>B. ODea, T.W. Boonstra, M.E. Larsen, T. Nguyen, S. Venkatesh, H.
  Christensen</authors><categories>cs.CL stat.AP</categories><comments>29 pages, 6 figures</comments><journal-ref>PLoS ONE 16(5): e0251787, 2021</journal-ref><doi>10.1371/journal.pone.0251787</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its popularity and availability, social media data may present a new
way to identify individuals who are experiencing mental illness. By analysing
blog content, this study aimed to investigate the associations between
linguistic features and symptoms of depression, generalised anxiety, and
suicidal ideation. This study utilised a longitudinal study design. Individuals
who blogged were invited to participate in a study in which they completed
fortnightly mental health questionnaires including the PHQ9 and GAD7 for a
period of 36 weeks. Linguistic features were extracted from blog data using the
LIWC tool. Bivariate and multivariate analyses were performed to investigate
the correlations between the linguistic features and mental health scores
between subjects. We then used the multivariate regression model to predict
longitudinal changes in mood within subjects. A total of 153 participants
consented to taking part, with 38 participants completing the required number
of questionnaires and blog posts during the study period. Between-subject
analysis revealed that several linguistic features, including tentativeness and
non-fluencies, were significantly associated with depression and anxiety
symptoms, but not suicidal thoughts. Within-subject analysis showed no robust
correlations between linguistic features and changes in mental health score.
This study provides further support for the relationship between linguistic
features within social media data and symptoms of depression and anxiety. The
lack of robust within-subject correlations indicate that the relationship
observed at the group level may not generalise to individual changes over time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04537</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1811.04537</id><submitter>Pallav Bera</submitter><version version="v1"><date>Mon, 12 Nov 2018 03:22:28 GMT</date><size>578kb</size><source_type>D</source_type></version><title>Identification of Internal Faults in Indirect Symmetrical Phase Shift
  Transformers Using Ensemble Learning</title><authors>Pallav Kumar Bera and Rajesh Kumar and Can Isik</authors><categories>cs.CV</categories><comments>18th IEEE International Symposium on Signal Processing and
  Information Technology (ISSPIT), 2018</comments><journal-ref>IEEE International Symposium on Signal Processing and Information
  Technology (ISSPIT), 2018, pp. 1-6,</journal-ref><doi>10.1109/ISSPIT.2018.8705100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes methods to identify 40 different types of internal faults
in an Indirect Symmetrical Phase Shift Transformer (ISPST). The ISPST was
modeled using Power System Computer Aided Design (PSCAD)/ Electromagnetic
Transients including DC (EMTDC). The internal faults were simulated by varying
the transformer tapping, backward and forward phase shifts, loading, and
percentage of winding faulted. Data for 960 cases of each type of fault was
recorded. A series of features were extracted for a, b, and c phases from time,
frequency, time-frequency, and information theory domains. The importance of
the extracted features was evaluated through univariate tests which helped to
reduce the number of features. The selected features were then used for
training five state-of-the-art machine learning classifiers. Extremely Random
Trees and Random Forest, the ensemble-based learners, achieved the accuracy of
98.76% and 97.54% respectively outperforming Multilayer Perceptron (96.13%),
Logistic Regression (93.54%), and Support Vector Machines (92.60%)
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08661</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1811.08661</id><submitter>Paul Jaeger</submitter><version version="v1"><date>Wed, 21 Nov 2018 10:12:38 GMT</date><size>2321kb</size><source_type>D</source_type></version><title>Retina U-Net: Embarrassingly Simple Exploitation of Segmentation
  Supervision for Medical Object Detection</title><authors>Paul F. Jaeger, Simon A. A. Kohl, Sebastian Bickelhaupt, Fabian
  Isensee, Tristan Anselm Kuder, Heinz-Peter Schlemmer, and Klaus H. Maier-Hein</authors><categories>cs.CV</categories><journal-ref>Neruips ML4H Workshop 2019 PLMR</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of localizing and categorizing objects in medical images often
remains formulated as a semantic segmentation problem. This approach, however,
only indirectly solves the coarse localization task by predicting pixel-level
scores, requiring ad-hoc heuristics when mapping back to object-level scores.
State-of-the-art object detectors on the other hand, allow for individual
object scoring in an end-to-end fashion, while ironically trading in the
ability to exploit the full pixel-wise supervision signal. This can be
particularly disadvantageous in the setting of medical image analysis, where
data sets are notoriously small. In this paper, we propose Retina U-Net, a
simple architecture, which naturally fuses the Retina Net one-stage detector
with the U-Net architecture widely used for semantic segmentation in medical
images. The proposed architecture recaptures discarded supervision signals by
complementing object detection with an auxiliary task in the form of semantic
segmentation without introducing the additional complexity of previously
proposed two-stage detectors. We evaluate the importance of full segmentation
supervision on two medical data sets, provide an in-depth analysis on a series
of toy experiments and show how the corresponding performance gain grows in the
limit of small data sets. Retina U-Net yields strong detection performance only
reached by its more complex two-staged counterparts. Our framework including
all methods implemented for operation on 2D and 3D images is available at
github.com/pfjaeger/medicaldetectiontoolkit.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1812.00263</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1812.00263</id><submitter>Varun Chandrasekaran</submitter><version version="v1"><date>Sat, 1 Dec 2018 21:04:04 GMT</date><size>7502kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 20 Nov 2019 12:02:43 GMT</date><size>9295kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 18:58:02 GMT</date><size>21585kb</size><source_type>D</source_type></version><title>BlackOut and Obfuscator: An Exploration of the Design Space for
  Privacy-Preserving Interventions for Voice Assistants</title><authors>Varun Chandrasekaran, Suman Banerjee, Bilge Mutlu, Kassem Fawaz</authors><categories>cs.HC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pervasive use of smart speakers has raised numerous privacy concerns.
While work to date provides an understanding of user perceptions of these
threats, limited research focuses on how we can mitigate these concerns, either
through redesigning the smart speaker or through dedicated privacy-preserving
interventions. In this paper, we present the design and prototyping of two
privacy-preserving interventions: `Obfuscator' targeted at disabling recording
at the microphones, and `BlackOut' targeted at disabling power to the smart
speaker. We present our findings from a technology probe study involving 24
households that interacted with our prototypes; the primary objective was to
gain a better understanding of the design space for technological interventions
that might address these concerns. Our data and findings reveal complex
trade-offs among utility, privacy, and usability and stresses the importance of
multi-functionality, aesthetics, ease-of-use, and form factor. We discuss the
implications of our findings for the development of subsequent interventions
and the future design of smart speakers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1901.03361</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1901.03361</id><submitter>Marc  Zeitoun</submitter><version version="v1"><date>Thu, 10 Jan 2019 20:02:35 GMT</date><size>49kb</size></version><version version="v2"><date>Sun, 28 Jul 2019 15:03:15 GMT</date><size>49kb</size></version><version version="v3"><date>Tue, 9 Jun 2020 15:57:58 GMT</date><size>47kb</size></version><version version="v4"><date>Mon, 31 May 2021 22:06:45 GMT</date><size>49kb</size></version><title>Separation for dot-depth two</title><authors>Thomas Place and Marc Zeitoun</authors><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dot-depth hierarchy of Brzozowski and Cohen classifies the star-free
languages of finite words. By a theorem of McNaughton and Papert, these are
also the first-order definable languages. The dot-depth rose to prominence
following the work of Thomas, who proved an exact correspondence with the
quantifier alternation hierarchy of first-order logic: each level in the
dot-depth hierarchy consists of all languages that can be defined with a
prescribed number of quantifier blocks. One of the most famous open problems in
automata theory is to settle whether the membership problem is decidable for
each level: is it possible to decide whether an input regular language belongs
to this level?
  Despite a significant research effort, membership by itself has only been
solved for low levels. A recent breakthrough was achieved by replacing
membership with a more general problem: separation. Given two input languages,
one has to decide whether there exists a third language in the investigated
level containing the first language and disjoint from the second. The
motivation is that: (1) while more difficult, separation is more rewarding (2)
it provides a more convenient framework (3) all recent membership algorithms
are reductions to separation for lower levels.
  We present a separation algorithm for dot-depth two. While this is our most
prominent application, our result is more general. We consider a family of
hierarchies that includes the dot-depth: concatenation hierarchies. They are
built via a generic construction process. One first chooses an initial class,
the basis, which is the lowest level in the hierarchy. Further levels are built
by applying generic operations. Our main theorem states that for any
concatenation hierarchy whose basis is finite, separation is decidable for
level one. In the special case of the dot-depth, this can be lifted to level
two using previously known results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1901.04426</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1901.04426</id><submitter>Mat\v{e}j Kone\v{c}n\'y</submitter><version version="v1"><date>Mon, 14 Jan 2019 17:54:20 GMT</date><size>16kb</size></version><version version="v2"><date>Sat, 10 Aug 2019 08:30:12 GMT</date><size>50kb</size><source_type>D</source_type></version><title>Extending partial isometries of antipodal graphs</title><authors>Mat\v{e}j Kone\v{c}n\'y</authors><categories>math.CO cs.DM</categories><comments>Accepted to Discrete Mathematics</comments><msc-class>05C63, 05C75, 05C12, 05D10, 05E18, 20B25, 54E35, 22F50</msc-class><acm-class>G.2.2; F.4.1</acm-class><doi>10.1016/j.disc.2019.111633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove EPPA (extension property for partial automorphisms) for all
antipodal classes from Cherlin's list of metrically homogeneous graphs, thereby
answering a question of Aranda et al. This paper should be seen as the first
application of a new general method for proving EPPA which can bypass the lack
of an automorphism-preserving completion. It is done by combining the recent
strengthening of the Herwig--Lascar theorem by Hubi\v{c}ka, Ne\v{s}et\v{r}il
and the author with the ideas of the proof of EPPA for two-graphs by Evans et
al.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1901.09559</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1901.09559</id><submitter>Lei Liu</submitter><version version="v1"><date>Mon, 28 Jan 2019 09:09:07 GMT</date><size>359kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 2 Apr 2019 11:34:08 GMT</date><size>507kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 11 Sep 2020 17:04:28 GMT</date><size>1869kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 03:54:38 GMT</date><size>2042kb</size><source_type>D</source_type></version><title>Capacity Optimality of AMP in Coded Systems</title><authors>Lei Liu, Chulong Liang, Junjie Ma and Li Ping</authors><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Trans. on Information Theory, 16 pages, 17 figures.
  [A low-complexity capacity optimal AMP is designed for large random linear
  systems with arbitrary input distributions based on matched FEC coding.]</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper studies a large random matrix system (LRMS) model involving an
arbitrary signal distribution and forward error control (FEC) coding. We
establish an area property based on the so-called Turbo approximate message
passing (Turbo-AMP) algorithm. Under the assumption that the state evolution
for AMP is correct for the coded system, the achievable rate of Turbo-AMP is
analyzed. We prove that Turbo-AMP achieves the constraint capacity of the LRMS
with an arbitrary signal distribution provided that a matching condition is
satisfied. As a byproduct, we provide an alternative derivation for the
constraint capacity of an LRMS using a proved property of AMP. We discuss
realization techniques for the matching principle of binary signaling using
irregular low-density parity-check (LDPC) codes and provide related numerical
results. We show that optimized codes demonstrate significantly better
performance over un-matched ones under Turbo-AMP. For quadrature phase shift
keying (QPSK) modulation, bit error rate (BER) performance within 1 dB from the
constrained capacity limit is observed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.10899</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1902.10899</id><submitter>Jiancheng Yang</submitter><version version="v1"><date>Thu, 28 Feb 2019 05:27:40 GMT</date><size>2958kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 12 Apr 2020 16:31:38 GMT</date><size>5069kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 8 Oct 2020 16:13:03 GMT</date><size>9831kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 16:03:23 GMT</date><size>9880kb</size><source_type>D</source_type></version><title>Adversarial Attack and Defense on Point Sets</title><authors>Jiancheng Yang, Qiang Zhang, Rongyao Fang, Bingbing Ni, Jinxian Liu,
  Qi Tian</authors><categories>cs.CV cs.AI cs.CR cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Emergence of the utility of 3D point cloud data in safety-critical vision
tasks (e.g., ADAS) urges researchers to pay more attention to the robustness of
3D representations and deep networks. To this end, we develop an attack and
defense scheme, dedicated to 3D point cloud data, for preventing 3D point
clouds from manipulated as well as pursuing noise-tolerable 3D representation.
A set of novel 3D point cloud attack operations are proposed via pointwise
gradient perturbation and adversarial point attachment / detachment. We then
develop a flexible perturbation-measurement scheme for 3D point cloud data to
detect potential attack data or noisy sensing data. Notably, the proposed
defense methods are even effective to detect the adversarial point clouds
generated by a proof-of-concept attack directly targeting the defense.
Transferability of adversarial attacks between several point cloud networks is
addressed, and we propose an momentum-enhanced pointwise gradient to improve
the attack transferability. We further analyze the transferability from
adversarial point clouds to grid CNNs and the inverse. Extensive experimental
results on common point cloud benchmarks demonstrate the validity of the
proposed 3D attack and defense framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06519</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1903.06519</id><submitter>Renata Rychtarikova</submitter><version version="v1"><date>Thu, 14 Mar 2019 06:40:47 GMT</date><size>8511kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 18:05:39 GMT</date><size>13704kb</size><source_type>D</source_type></version><title>Spectroscopic Approach to Correction and Visualisation of Bright-Field
  Light Transmission Microscopy Biological Data</title><authors>Ganna Platonova, Dalibor Stys, Pavel Soucek, Kirill Lonhus, Jan
  Valenta and Renata Rychtarikova</authors><categories>eess.IV cs.CV</categories><comments>16 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most realistic information about the transparent sample such as a live
cell can be obtained only using bright-field light microscopy. At
high-intensity pulsing LED illumination, we captured a primary
12-bit-per-channel (bpc) response froman observed sample using a bright-field
wide-field microscope equipped with a high-resolution (4872x3248) image sensor.
In order to suppress data distortions originating from the light interactions
with elements in the optical path, poor sensor reproduction (geometrical
defects of the camera sensor and some peculiarities of sensor sensitivity),
this uncompressed 12-bpc data underwent a kind of correction after simultaneous
calibration of all the parts of the experimental arrangement. Moreover, the
final intensities of the corrected images are proportional to the photon fluxes
detected by a camera sensor. It can be visualized in 8-bpc intensity depth
after the Least Information Loss compression [Lect. Notes Bioinform. 9656, 527
(2016)].
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11287</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1903.11287</id><submitter>Mateusz Skomra</submitter><version version="v1"><date>Wed, 27 Mar 2019 08:24:37 GMT</date><size>10kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 17:49:35 GMT</date><size>12kb</size></version><title>Convexly independent subsets of Minkowski sums of convex polygons</title><authors>Mateusz Skomra and St\'ephan Thomass\'e</authors><categories>math.CO cs.CG cs.DM</categories><comments>v1: 9 pages, 3 figures; v2: minor revision, 10 pages, 5 figures</comments><journal-ref>Discrete Mathematics, Volume 344, Issue 8, August 2021, 112472</journal-ref><doi>10.1016/j.disc.2021.112472</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exist convex $n$-gons $P$ and $Q$ such that the largest
convex polygon in the Minkowski sum $P+Q$ has size $\Theta(n\log n)$. This
matches an upper bound of Tiwary.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00257</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1904.00257</id><submitter>Mattia Zanella</submitter><version version="v1"><date>Sat, 30 Mar 2019 17:22:58 GMT</date><size>1113kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 10 Feb 2020 21:45:25 GMT</date><size>1321kb</size><source_type>D</source_type></version><title>Uncertainty damping in kinetic traffic models by driver-assist controls</title><authors>Andrea Tosin, Mattia Zanella</authors><categories>nlin.AO cs.NA math.NA math.OC</categories><msc-class>35Q20, 35Q70, 35Q84, 35Q93, 90B20</msc-class><journal-ref>Math. Control Relat. Fields, 11(3):681-713, 2021</journal-ref><doi>10.3934/mcrf.2021018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a kinetic model of traffic flow with uncertain
binary interactions, which explains the scattering of the fundamental diagram
in terms of the macroscopic variability of aggregate quantities, such as the
mean speed and the flux of the vehicles, produced by the microscopic
uncertainty. Moreover, we design control strategies at the level of the
microscopic interactions among the vehicles, by which we prove that it is
possible to dampen the propagation of such an uncertainty across the scales.
Our analytical and numerical results suggest that the aggregate traffic flow
may be made more ordered, hence predictable, by implementing such control
protocols in driver-assist vehicles. Remarkably, they also provide a precise
relationship between a measure of the macroscopic damping of the uncertainty
and the penetration rate of the driver-assist technology in the traffic stream.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06984</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1904.06984</id><submitter>Itay Safran</submitter><version version="v1"><date>Mon, 15 Apr 2019 12:07:38 GMT</date><size>271kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 26 May 2019 11:22:43 GMT</date><size>271kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 16:35:17 GMT</date><size>271kb</size><source_type>D</source_type></version><title>Depth Separations in Neural Networks: What is Actually Being Separated?</title><authors>Itay Safran, Ronen Eldan, Ohad Shamir</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing depth separation results for constant-depth networks essentially
show that certain radial functions in $\mathbb{R}^d$, which can be easily
approximated with depth $3$ networks, cannot be approximated by depth $2$
networks, even up to constant accuracy, unless their size is exponential in
$d$. However, the functions used to demonstrate this are rapidly oscillating,
with a Lipschitz parameter scaling polynomially with the dimension $d$ (or
equivalently, by scaling the function, the hardness result applies to
$\mathcal{O}(1)$-Lipschitz functions only when the target accuracy $\epsilon$
is at most $\text{poly}(1/d)$). In this paper, we study whether such depth
separations might still hold in the natural setting of
$\mathcal{O}(1)$-Lipschitz radial functions, when $\epsilon$ does not scale
with $d$. Perhaps surprisingly, we show that the answer is negative: In
contrast to the intuition suggested by previous work, it \emph{is} possible to
approximate $\mathcal{O}(1)$-Lipschitz radial functions with depth $2$, size
$\text{poly}(d)$ networks, for every constant $\epsilon$. We complement it by
showing that approximating such functions is also possible with depth $2$, size
$\text{poly}(1/\epsilon)$ networks, for every constant $d$. Finally, we show
that it is not possible to have polynomial dependence in both $d,1/\epsilon$
simultaneously. Overall, our results indicate that in order to show depth
separations for expressing $\mathcal{O}(1)$-Lipschitz functions with constant
accuracy -- if at all possible -- one would need fundamentally different
techniques than existing ones in the literature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07538</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1904.07538</id><submitter>Naoya Fushishita</submitter><version version="v1"><date>Tue, 16 Apr 2019 08:50:44 GMT</date><size>4506kb</size></version><version version="v2"><date>Wed, 17 Apr 2019 02:30:14 GMT</date><size>4506kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 24 Feb 2020 09:39:30 GMT</date><size>8051kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 16:00:22 GMT</date><size>9599kb</size><source_type>D</source_type></version><title>Long-Term Human Video Generation of Multiple Futures Using Poses</title><authors>Naoya Fushishita, Antonio Tejero-de-Pablos, Yusuke Mukuta, Tatsuya
  Harada</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting future human behavior from an input human video is a useful task
for applications such as autonomous driving and robotics. While most previous
works predict a single future, multiple futures with different behavior can
potentially occur. Moreover, if the predicted future is too short (e.g., less
than one second), it may not be fully usable by a human or other systems. In
this paper, we propose a novel method for future human pose prediction capable
of predicting multiple long-term futures. This makes the predictions more
suitable for real applications. Also, from the input video and the predicted
human behavior, we generate future videos. First, from an input human video, we
generate sequences of future human poses (i.e., the image coordinates of their
body-joints) via adversarial learning. Adversarial learning suffers from mode
collapse, which makes it difficult to generate a variety of multiple poses. We
solve this problem by utilizing two additional inputs to the generator to make
the outputs diverse, namely, a latent code (to reflect various behaviors) and
an attraction point (to reflect various trajectories). In addition, we generate
long-term future human poses using a novel approach based on unidimensional
convolutional neural networks. Last, we generate an output video based on the
generated poses for visualization. We evaluate the generated future poses and
videos using three criteria (i.e., realism, diversity and accuracy), and show
that our proposed method outperforms other state-of-the-art works.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10386</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1904.10386</id><submitter>Mario Gleirscher</submitter><version version="v1"><date>Tue, 23 Apr 2019 15:29:00 GMT</date><size>66kb</size></version><title>Risk Structures: Towards Engineering Risk-aware Autonomous Systems</title><authors>Mario Gleirscher</authors><categories>cs.SE cs.AI cs.RO</categories><doi>10.1007/s00165-021-00545-4</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inspired by widely-used techniques of causal modelling in risk, failure, and
accident analysis, this work discusses a compositional framework for risk
modelling. Risk models capture fragments of the space of risky events likely to
occur when operating a machine in a given environment. Moreover, one can build
such models into machines such as autonomous robots, to equip them with the
ability of risk-aware perception, monitoring, decision making, and control.
With the notion of a risk factor as the modelling primitive, the framework
provides several means to construct and shape risk models. Relational and
algebraic properties are investigated and proofs support the validity and
consistency of these properties over the corresponding models. Several examples
throughout the discussion illustrate the applicability of the concepts.
Overall, this work focuses on the qualitative treatment of risk with the
outlook of transferring these results to probabilistic refinements of the
discussed framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.03452</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.03452</id><submitter>Moran Koren</submitter><version version="v1"><date>Thu, 9 May 2019 06:26:14 GMT</date><size>48kb</size></version><title>The Implications of Pricing on Social Learning</title><authors>Itai Arieli, Moran Koren, Rann Smorodinsky</authors><categories>econ.TH cs.GT</categories><doi>10.1145/3328526.3329554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the implications of endogenous pricing for learning and welfare in
the classic herding model . When prices are determined exogenously, it is known
that learning occurs if and only if signals are unbounded. By contrast, we show
that learning can occur when signals are bounded as long as non-conformism
among consumers is scarce. More formally, learning happens if and only if
signals exhibit the vanishing likelihood property introduced bellow. We discuss
the implications of our results for potential market failure in the context of
Schumpeterian growth with uncertainty over the value of innovations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.03846</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.03846</id><submitter>Heiko Gimperlein</submitter><version version="v1"><date>Thu, 9 May 2019 21:01:49 GMT</date><size>596kb</size></version><version version="v2"><date>Tue, 16 Mar 2021 14:32:42 GMT</date><size>699kb</size></version><title>Optimal operator preconditioning for pseudodifferential boundary
  problems</title><authors>Heiko Gimperlein, Jakub Stocek, Carolina Urzua-Torres</authors><categories>math.NA cs.NA math.AP</categories><comments>30 pages, 19 figures, to appear in Numerische Mathematik</comments><journal-ref>Numerische Mathematik 148 (2021), 1-41</journal-ref><doi>10.1007/s00211-021-01193-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an operator preconditioner for general elliptic pseudodifferential
equations in a domain $\Omega$, where $\Omega$ is either in $\mathbb{R}^n$ or
in a Riemannian manifold. For linear systems of equations arising from
low-order Galerkin discretizations, we obtain condition numbers that are
independent of the mesh size and of the choice of bases for test and trial
functions. The basic ingredient is a classical formula by Boggio for the
fractional Laplacian, which is extended analytically. In the special case of
the weakly and hypersingular operators on a line segment or a screen, our
approach gives a unified, independent proof for a series of recent results by
Hiptmair, Jerez-Hanckes, N\'{e}d\'{e}lec and Urz\'{u}a-Torres. We also study
the increasing relevance of the regularity assumptions on the mesh with the
order of the operator. Numerical examples validate our theoretical findings and
illustrate the performance of the proposed preconditioner on quasi-uniform,
graded and adaptively generated meshes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.04629</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.04629</id><submitter>Quanming Yao</submitter><version version="v1"><date>Sun, 12 May 2019 02:30:09 GMT</date><size>324kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 03:19:25 GMT</date><size>2639kb</size><source_type>D</source_type></version><title>Efficient Low-Rank Semidefinite Programming with Robust Loss Functions</title><authors>Quanming Yao and Hangsi Yang and En-Liang Hu and James Kwok</authors><categories>cs.LG stat.ML</categories><comments>Preprint version. Final version is accepted to &quot;IEEE Transactions on
  Pattern Analysis and Machine Intelligence&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-world applications, it is important for machine learning algorithms
to be robust against data outliers or corruptions. In this paper, we focus on
improving the robustness of a large class of learning algorithms that are
formulated as low-rank semi-definite programming (SDP) problems. Traditional
formulations use square loss, which is notorious for being sensitive to
outliers. We propose to replace this with more robust noise models, including
the $\ell_1$-loss and other nonconvex losses. However, the resultant
optimization problem becomes difficult as the objective is no longer convex or
smooth. To alleviate this problem, we design an efficient algorithm based on
majorization-minimization. The crux is on constructing a good optimization
surrogate, and we show that this surrogate can be efficiently obtained by the
alternating direction method of multipliers (ADMM). By properly monitoring
ADMM's convergence, the proposed algorithm is empirically efficient and also
theoretically guaranteed to converge to a critical point. Extensive experiments
are performed on four machine learning applications using both synthetic and
real-world data sets. Results show that the proposed algorithm is not only fast
but also has better performance than the state-of-the-art.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.08671</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.08671</id><submitter>Melih Yesilli</submitter><version version="v1"><date>Tue, 21 May 2019 14:36:52 GMT</date><size>3314kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 27 May 2019 20:15:08 GMT</date><size>3314kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 21:35:27 GMT</date><size>5053kb</size><source_type>D</source_type></version><title>Topological Feature Vectors for Chatter Detection in Turning Processes</title><authors>Melih C. Yesilli, Firas A. Khasawneh, Andreas Otto</authors><categories>eess.SP cs.LG stat.ML</categories><comments>Implementations of parallel computing and Bezier curve approximation
  for persistence diagram computation are added into the manuscript. Abstract
  and results section are updated with respect to the results obtained from
  persistence diagrams computed with Bezier approximation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machining processes are most accurately described using complex dynamical
systems that include nonlinearities, time delays, and stochastic effects. Due
to the nature of these models as well as the practical challenges which include
time-varying parameters, the transition from numerical/analytical modeling of
machining to the analysis of real cutting signals remains challenging. Some
studies have focused on studying the time series of cutting processes using
machine learning algorithms with the goal of identifying and predicting
undesirable vibrations during machining referred to as chatter. These tools
typically decompose the signal using Wavelet Packet Transforms (WPT) or
Ensemble Empirical Mode Decomposition (EEMD). However, these methods require a
significant overhead in identifying the feature vectors before a classifier can
be trained. In this study, we present an alternative approach based on
featurizing the time series of the cutting process using its topological
features. We first embed the time series as a point cloud using Takens
embedding. We then utilize Support Vector Machine, Logistic Regression, Random
Forest and Gradient Boosting classifier combined with feature vectors derived
from persistence diagrams, a tool from persistent homology, to encode chatter's
distinguishing characteristics. We present the results for several choices of
the topological feature vectors, and we compare our results to the WPT and EEMD
methods using experimental turning data. Our results show that in two out of
four cutting configurations the TDA-based features yield accuracies as high as
97%. We also show that combining Bezier curve approximation method and parallel
computing can reduce runtime for persistence diagram computation of a single
time series to less than a second thus making our approach suitable for online
chatter detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.09676</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.09676</id><submitter>Xiaoxi He</submitter><version version="v1"><date>Thu, 23 May 2019 14:23:46 GMT</date><size>313kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:35:47 GMT</date><size>19071kb</size><source_type>D</source_type></version><title>Pruning-Aware Merging for Efficient Multitask Inference</title><authors>Xiaoxi He, Dawei Gao, Zimu Zhou, Yongxin Tong, Lothar Thiele</authors><categories>cs.LG cs.NE</categories><comments>Accepted to KDD'21 as research track paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many mobile applications demand selective execution of multiple correlated
deep learning inference tasks on resource-constrained platforms. Given a set of
deep neural networks, each pre-trained for a single task, it is desired that
executing arbitrary combinations of tasks yields minimal computation cost.
Pruning each network separately yields suboptimal computation cost due to task
relatedness. A promising remedy is to merge the networks into a multitask
network to eliminate redundancy across tasks before network pruning. However,
pruning a multitask network combined by existing network merging schemes cannot
minimise the computation cost of every task combination because they do not
consider such a future pruning. To this end, we theoretically identify the
conditions such that pruning a multitask network minimises the computation of
all task combinations. On this basis, we propose Pruning-Aware Merging (PAM), a
heuristic network merging scheme to construct a multitask network that
approximates these conditions. The merged network is then ready to be further
pruned by existing network pruning methods. Evaluations with different pruning
schemes, datasets, and network architectures show that PAM achieves up to 4.87x
less computation against the baseline without network merging, and up to 2.01x
less computation against the baseline with a state-of-the-art network merging
scheme.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.12629</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.12629</id><submitter>Fabio Paolizzo</submitter><version version="v1"><date>Wed, 29 May 2019 09:33:20 GMT</date><size>932kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 21:55:13 GMT</date><size>441kb</size></version><title>A New Multilabel System for Automatic Music Emotion Recognition</title><authors>Fabio Paolizzo, Natalia Pichierri, Daniele Casali, Daniele Giardino,
  Marco Matta, Giovanni Costantini</authors><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>2 tables. Research supported by the EU through the MUSICAL-MOODS
  project funded by the Marie Sklodowska-Curie Actions Individual Fellowships
  Global Fellowships (MSCA-IF-GF) of the Horizon 2020 Programme
  H2020/2014-2020, REA grant agreement n.659434</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving advancements in automatic recognition of emotions that music can
induce require considering multiplicity and simultaneity of emotions.
Comparison of different machine learning algorithms performing multilabel and
multiclass classification is the core of our work. The study analyzes the
implementation of the Geneva Emotional Music Scale 9 in the Emotify music
dataset and investigates its adoption from a machine-learning perspective. We
approach the scenario of emotions expression/induction through music as a
multilabel and multiclass problem, where multiple emotion labels can be adopted
for the same music track by each annotator (multilabel), and each emotion can
be identified or not in the music (multiclass). The aim is the automatic
recognition of induced emotions through music.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.12698</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.12698</id><submitter>Ronny Luss</submitter><version version="v1"><date>Wed, 29 May 2019 19:48:08 GMT</date><size>3278kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 Feb 2020 15:58:03 GMT</date><size>8546kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 20:30:52 GMT</date><size>29198kb</size><source_type>D</source_type></version><title>Leveraging Latent Features for Local Explanations</title><authors>Ronny Luss, Pin-Yu Chen, Amit Dhurandhar, Prasanna Sattigeri, Yunfeng
  Zhang, Karthikeyan Shanmugam, Chun-Chen Tu</authors><categories>cs.LG stat.ML</categories><comments>Accepted to KDD 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the application of deep neural networks proliferates in numerous areas
such as medical imaging, video surveillance, and self driving cars, the need
for explaining the decisions of these models has become a hot research topic,
both at the global and local level. Locally, most explanation methods have
focused on identifying relevance of features, limiting the types of
explanations possible. In this paper, we investigate a new direction by
leveraging latent features to generate contrastive explanations; predictions
are explained not only by highlighting aspects that are in themselves
sufficient to justify the classification, but also by new aspects which if
added will change the classification. The key contribution of this paper lies
in how we add features to rich data in a formal yet humanly interpretable way
that leads to meaningful results. Our new definition of &quot;addition&quot; uses latent
features to move beyond the limitations of previous explanations and resolve an
open question laid out in Dhurandhar, et. al. (2018), which creates local
contrastive explanations but is limited to simple datasets such as grayscale
images. The strength of our approach in creating intuitive explanations that
are also quantitatively superior to other methods is demonstrated on three
diverse image datasets (skin lesions, faces, and fashion apparel). A user study
with 200 participants further exemplifies the benefits of contrastive
information, which can be viewed as complementary to other state-of-the-art
interpretability methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.13120</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.13120</id><submitter>Tingting Zhao</submitter><version version="v1"><date>Thu, 30 May 2019 15:50:57 GMT</date><size>1415kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 31 May 2019 01:56:29 GMT</date><size>1415kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 3 Jun 2019 14:33:07 GMT</date><size>1415kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 30 May 2021 03:19:17 GMT</date><size>3085kb</size><source_type>D</source_type></version><title>Analysis of high-dimensional Continuous Time Markov Chains using the
  Local Bouncy Particle Sampler</title><authors>Tingting Zhao and Alexandre Bouchard-C\^ot\'e</authors><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling the parameters of high-dimensional Continuous Time Markov Chains
(CTMC) is a challenging problem with important applications in many fields of
applied statistics. In this work a recently proposed type of non-reversible
rejection-free Markov Chain Monte Carlo (MCMC) sampler, the Bouncy Particle
Sampler (BPS), is brought to bear to this problem. BPS has demonstrated its
favorable computational efficiency compared with state-of-the-art MCMC
algorithms, however to date applications to real-data scenario were scarce. An
important aspect of the practical implementation of BPS is the simulation of
event times. Default implementations use conservative thinning bounds. Such
bounds can slow down the algorithm and limit the computational performance. Our
paper develops an algorithm with an exact analytical solution to the random
event times in the context of CTMCs. Our local version of BPS algorithm takes
advantage of the sparse structure in the target factor graph and we also
provide a framework for assessing the computational complexity of local BPS
algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1906.00722</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1906.00722</id><submitter>Michael Moor</submitter><version version="v1"><date>Mon, 3 Jun 2019 11:41:47 GMT</date><size>6083kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 9 Oct 2019 12:00:56 GMT</date><size>6941kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 21 Feb 2020 09:44:54 GMT</date><size>6943kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 15 Oct 2020 12:01:57 GMT</date><size>8284kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 15:02:01 GMT</date><size>8284kb</size><source_type>D</source_type></version><title>Topological Autoencoders</title><authors>Michael Moor and Max Horn and Bastian Rieck and Karsten Borgwardt</authors><categories>cs.LG math.AT stat.ML</categories><comments>Accepted at the International Conference on Machine Learning (ICML)
  2020; camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for preserving topological structures of the
input space in latent representations of autoencoders. Using persistent
homology, a technique from topological data analysis, we calculate topological
signatures of both the input and latent space to derive a topological loss
term. Under weak theoretical assumptions, we construct this loss in a
differentiable manner, such that the encoding learns to retain multi-scale
connectivity information. We show that our approach is theoretically
well-founded and that it exhibits favourable latent representations on a
synthetic manifold as well as on real-world image data sets, while preserving
low reconstruction errors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1906.04285</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1906.04285</id><submitter>Nikola Kovachki</submitter><version version="v1"><date>Mon, 10 Jun 2019 21:36:42 GMT</date><size>556kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 19:32:23 GMT</date><size>930kb</size><source_type>D</source_type></version><title>Continuous Time Analysis of Momentum Methods</title><authors>Nikola B. Kovachki, Andrew M. Stuart</authors><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>40 pages, 7 figures</comments><journal-ref>Journal of Machine Learning Research 21 (2020) 1-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient descent-based optimization methods underpin the parameter training
of neural networks, and hence comprise a significant component in the
impressive test results found in a number of applications. Introducing
stochasticity is key to their success in practical problems, and there is some
understanding of the role of stochastic gradient descent in this context.
Momentum modifications of gradient descent such as Polyak's Heavy Ball method
(HB) and Nesterov's method of accelerated gradients (NAG), are also widely
adopted. In this work our focus is on understanding the role of momentum in the
training of neural networks, concentrating on the common situation in which the
momentum contribution is fixed at each step of the algorithm. To expose the
ideas simply we work in the deterministic setting.
  Our approach is to derive continuous time approximations of the discrete
algorithms; these continuous time approximations provide insights into the
mechanisms at play within the discrete algorithms. We prove three such
approximations. Firstly we show that standard implementations of fixed momentum
methods approximate a time-rescaled gradient descent flow, asymptotically as
the learning rate shrinks to zero; this result does not distinguish momentum
methods from pure gradient descent, in the limit of vanishing learning rate. We
then proceed to prove two results aimed at understanding the observed practical
advantages of fixed momentum methods over gradient descent. We achieve this by
proving approximations to continuous time limits in which the small but fixed
learning rate appears as a parameter. Furthermore in a third result we show
that the momentum methods admit an exponentially attractive invariant manifold
on which the dynamics reduces, approximately, to a gradient flow with respect
to a modified loss function.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1906.04840</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1906.04840</id><submitter>Matthieu Latapy</submitter><version version="v1"><date>Tue, 11 Jun 2019 22:08:38 GMT</date><size>63kb</size></version><title>Weighted, Bipartite, or Directed Stream Graphs for the Modeling of
  Temporal Networks</title><authors>Matthieu Latapy and Cl\'emence Magnien and Tiphaine Viard</authors><categories>cs.SI cs.DS physics.soc-ph</categories><journal-ref>In: Holme P., Saram\&quot;aki J. (eds) Temporal Network Theory.
  Computational Social Sciences. Springer, 2019</journal-ref><doi>10.1007/978-3-030-23495-9_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recently introduced a formalism for the modeling of temporal networks,
that we call stream graphs. It emphasizes the streaming nature of data and
allows rigorous definitions of many important concepts generalizing classical
graphs. This includes in particular size, density, clique, neighborhood,
degree, clustering coefficient, and transitivity. In this contribution, we show
that, like graphs, stream graphs may be extended to cope with bipartite
structures, with node and link weights, or with link directions. We review the
main bipartite, weighted or directed graph concepts proposed in the literature,
we generalize them to the cases of bipartite, weighted, or directed stream
graphs, and we show that obtained concepts are consistent with graph and stream
graph ones. This provides a formal ground for an accurate modeling of the many
temporal networks that have one or several of these features.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1906.06717</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1906.06717</id><submitter>Marko Vasic</submitter><version version="v1"><date>Sun, 16 Jun 2019 15:28:35 GMT</date><size>2318kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 17 Sep 2019 02:51:48 GMT</date><size>2318kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 09:24:03 GMT</date><size>3349kb</size><source_type>D</source_type></version><title>MoET: Mixture of Expert Trees and its Application to Verifiable
  Reinforcement Learning</title><authors>Marko Vasic, Andrija Petrovic, Kaiyuan Wang, Mladen Nikolic, Rishabh
  Singh, Sarfraz Khurshid</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid advancements in deep learning have led to many recent breakthroughs.
While deep learning models achieve superior performance, often statistically
better than humans, their adaption into safety-critical settings, such as
healthcare or self-driving cars is hindered by their inability to provide
safety guarantees or to analyze the inner workings of the model. We present
MoET, a novel model based on Mixture of Experts, consisting of decision tree
experts and a generalized linear model gating function. While decision
boundaries of decision trees (used in an existing verifiable approach), are
axis-perpendicular hyperplanes, MoET supports hyperplanes of arbitrary
orientation as the boundaries. To support non-differentiable decision trees as
experts we formulate a novel training procedure. In addition, we introduce a
hard thresholding version, MoET_h, in which predictions are made solely by a
single expert chosen via the gating function. Thanks to that property, MoET_h
allows each prediction to be easily decomposed into a set of logical rules.
Such rules can be translated into a manageable SMT formula providing rich means
for verification. While MoET is a general use model, we illustrate its power in
the reinforcement learning setting. By training MoET models using an imitation
learning procedure on deep RL agents we outperform the previous
state-of-the-art technique based on decision trees while preserving the
verifiability of the models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1906.09505</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1906.09505</id><submitter>Joaquin Garcia-Alfaro</submitter><version version="v1"><date>Sat, 22 Jun 2019 20:55:59 GMT</date><size>5448kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 27 Jun 2019 12:42:50 GMT</date><size>5448kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 15:24:22 GMT</date><size>4342kb</size><source_type>D</source_type></version><title>Error Tolerant Path Planning for Swarms of Micro Aerial Vehicles with
  Quality Amplification</title><authors>Michel Barbeau, Joaquin Garcia-Alfaro, Evangelos Kranakis, Fillipe
  Santos</authors><categories>cs.RO cs.SY eess.SY</categories><comments>An early version of this paper appeared in the proceedings of IEEE
  GLOBECOM 2019, Waikoloa, Hawaii, Dec 9-14, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an error tolerant path planning algorithm for Micro Aerial Vehicle
(MAV) swarms. We assume navigation without GPS-like techniques. The MAVs find
their path using sensors and cameras, identifying and following a series of
visual landmarks. The visual landmarks lead the MAVs towards their destination.
MAVs are assumed to be unaware of the terrain and locations of the landmarks.
They hold a priori information about landmarks, whose interpretation is prone
to errors. Errors are of two types, recognition or advice. Recognition errors
follow from misinterpretation of sensed data or a priori information, or
confusion of objects, e.g., due to faulty sensors. Advice errors are
consequences of outdated or wrong information about landmarks, e.g., due to
weather conditions. Our path planning algorithm is cooperative. MAVs
communicate and exchange information wirelessly, to minimize the number of
recognition and advice errors. Hence, the quality of the navigation decision
process is amplified. Our solution successfully achieves an adaptive error
tolerant navigation system. Quality amplification is parameterized with respect
to the number of MAVs. We validate our approach with theoretical proofs and
numeric simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1906.10258</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1906.10258</id><submitter>Davide Viviano Mr.</submitter><version version="v1"><date>Mon, 24 Jun 2019 22:42:32 GMT</date><size>2275kb</size><source_type>D</source_type></version><version version="v10"><date>Tue, 30 Mar 2021 14:53:08 GMT</date><size>1044kb</size><source_type>D</source_type></version><version version="v11"><date>Tue, 11 May 2021 03:43:47 GMT</date><size>1014kb</size><source_type>D</source_type></version><version version="v12"><date>Tue, 1 Jun 2021 16:09:04 GMT</date><size>957kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 27 Aug 2019 04:21:07 GMT</date><size>1008kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 28 Aug 2019 16:39:58 GMT</date><size>1008kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 28 Oct 2019 05:37:55 GMT</date><size>1034kb</size><source_type>D</source_type></version><version version="v5"><date>Sun, 10 Nov 2019 17:46:00 GMT</date><size>1039kb</size><source_type>D</source_type></version><version version="v6"><date>Tue, 7 Apr 2020 14:46:03 GMT</date><size>1376kb</size><source_type>D</source_type></version><version version="v7"><date>Fri, 12 Jun 2020 02:23:28 GMT</date><size>1372kb</size><source_type>D</source_type></version><version version="v8"><date>Thu, 17 Dec 2020 02:23:59 GMT</date><size>933kb</size><source_type>D</source_type></version><version version="v9"><date>Thu, 25 Mar 2021 17:35:18 GMT</date><size>1047kb</size><source_type>D</source_type></version><title>Policy Targeting under Network Interference</title><authors>Davide Viviano</authors><categories>econ.EM cs.SI stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the problem of estimating treatment allocation rules
under network interference. I propose a method with several attractive features
for applications: (i) it does not rely on the correct specification of a
particular structural model; (ii) it exploits heterogeneity in treatment
effects for targeting individuals; (iii) it accommodates arbitrary constraints
on the policy function; (iv) it does not necessitate network information of the
target units. I introduce estimation procedures that leverage experimental or
observational data and derive strong guarantees on the utilitarian regret. I
provide a mixed-integer linear program formulation, which can be solved using
off-the-shelf algorithms. I illustrate the advantages of the method for
targeting information on social networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.00865</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.00865</id><submitter>Sebastian Farquhar</submitter><version version="v1"><date>Mon, 1 Jul 2019 15:25:50 GMT</date><size>181kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 15 Oct 2019 09:22:38 GMT</date><size>438kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 7 Apr 2020 10:38:39 GMT</date><size>2044kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 11:12:44 GMT</date><size>2044kb</size><source_type>D</source_type></version><title>Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale
  Bayesian Deep Learning</title><authors>Sebastian Farquhar, Michael Osborne, Yarin Gal</authors><categories>stat.ML cs.LG</categories><journal-ref>AI Stats, PMLR 108:1352-1362, 2020</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Radial Bayesian Neural Networks (BNNs): a variational approximate
posterior for BNNs which scales well to large models while maintaining a
distribution over weight-space with full support. Other scalable Bayesian deep
learning methods, like MC dropout or deep ensembles, have discrete support-they
assign zero probability to almost all of the weight-space. Unlike these
discrete support methods, Radial BNNs' full support makes them suitable for use
as a prior for sequential inference. In addition, they solve the conceptual
challenges with the a priori implausibility of weight distributions with
discrete support. The Radial BNN is motivated by avoiding a sampling problem in
'mean-field' variational inference (MFVI) caused by the so-called 'soap-bubble'
pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are
robust to hyperparameters and can be efficiently applied to a challenging
real-world medical application without needing ad-hoc tweaks and intensive
tuning. In fact, in this setting Radial BNNs out-perform discrete-support
methods like MC dropout. Lastly, by using Radial BNNs as a theoretically
principled, robust alternative to MFVI we make significant strides in a
Bayesian continual learning evaluation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.01495</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.01495</id><submitter>Petr Hlin\v{e}n\'y</submitter><version version="v1"><date>Tue, 2 Jul 2019 16:57:11 GMT</date><size>703kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 3 May 2020 18:58:17 GMT</date><size>107kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 09:48:54 GMT</date><size>49kb</size></version><title>Efficient Isomorphism for $S_d$-graphs and $T$-graphs</title><authors>Deniz A\u{g}ao\u{g}lu, Petr Hlin\v{e}n\'y</authors><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $H$-graph is one representable as the intersection graph of connected
subgraphs of a suitable subdivision of a fixed graph $H$, introduced by
Bir\'{o}, Hujter and Tuza (1992). An $H$-graph is proper if the representing
subgraphs of $H$ can be chosen incomparable by the inclusion. In this paper, we
focus on the isomorphism problem for $S_d$-graphs and $T$-graphs, where $S_d$
is the star with $d$ rays and $T$ is an arbitrary fixed tree.
  Answering an open problem of Chaplick, T\&quot;{o}pfer, Voborn\'{\i}k and Zeman
(2016), we provide an FPT-time algorithm for testing isomorphism and computing
the automorphism group of $S_d$-graphs when parameterized by~$d$, which
involves the classical group-computing machinery by Furst, Hopcroft, and Luks
(1980). We also show that the isomorphism problem of $S_d$-graphs is at least
as hard as the isomorphism problem of posets of bounded width, for which no
efficient combinatorial-only algorithm is known to date. Then we extend our
approach to an XP-time algorithm for isomorphism of $T$-graphs when
parameterized by the size of $T$. Lastly, we contribute a simple FPT-time
combinatorial algorithm for isomorphism testing in the special case of proper
$S_d$- and $T$-graphs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.02170</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.02170</id><submitter>Duligur Ibeling</submitter><version version="v1"><date>Thu, 4 Jul 2019 00:31:20 GMT</date><size>31kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 08:08:14 GMT</date><size>36kb</size></version><title>On Open-Universe Causal Reasoning</title><authors>Duligur Ibeling, Thomas Icard</authors><categories>cs.AI cs.LO</categories><comments>UAI 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend two kinds of causal models, structural equation models and
simulation models, to infinite variable spaces. This enables a semantics for
conditionals founded on a calculus of intervention, and axiomatization of
causal reasoning for rich, expressive generative models -- including those in
which a causal representation exists only implicitly -- in an open-universe
setting. Further, we show that under suitable restrictions the two kinds of
models are equivalent, perhaps surprisingly as their axiomatizations differ
substantially in the general case. We give a series of complete axiomatizations
in which the open-universe nature of the setting is seen to be essential.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.03103</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.03103</id><submitter>Vasisht Duddu</submitter><version version="v1"><date>Sat, 6 Jul 2019 09:39:26 GMT</date><size>141kb</size></version><version version="v2"><date>Tue, 9 Jul 2019 05:37:21 GMT</date><size>143kb</size></version><version version="v3"><date>Sun, 30 May 2021 03:31:58 GMT</date><size>169kb</size><source_type>D</source_type></version><title>Towards Enhancing Fault Tolerance in Neural Networks</title><authors>Vasisht Duddu, D. Vijay Rao, Valentina E. Balas</authors><categories>cs.LG cs.CR cs.DC cs.GT stat.ML</categories><comments>MobiQuitous 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Learning Accelerators are prone to faults which manifest in the form of
errors in Neural Networks. Fault Tolerance in Neural Networks is crucial in
real-time safety critical applications requiring computation for long
durations. Neural Networks with high regularisation exhibit superior fault
tolerance, however, at the cost of classification accuracy. In the view of
difference in functionality, a Neural Network is modelled as two separate
networks, i.e, the Feature Extractor with unsupervised learning objective and
the Classifier with a supervised learning objective. Traditional approaches of
training the entire network using a single supervised learning objective is
insufficient to achieve the objectives of the individual components optimally.
In this work, a novel multi-criteria objective function, combining unsupervised
training of the Feature Extractor followed by supervised tuning with Classifier
Network is proposed. The unsupervised training solves two games simultaneously
in the presence of adversary neural networks with conflicting objectives to the
Feature Extractor. The first game minimises the loss in reconstructing the
input image for indistinguishability given the features from the Extractor, in
the presence of a generative decoder. The second game solves a minimax
constraint optimisation for distributional smoothening of feature space to
match a prior distribution, in the presence of a Discriminator network. The
resultant strongly regularised Feature Extractor is combined with the
Classifier Network for supervised fine-tuning. The proposed Adversarial Fault
Tolerant Neural Network Training is scalable to large networks and is
independent of the architecture. The evaluation on benchmarking datasets:
FashionMNIST and CIFAR10, indicates that the resultant networks have high
accuracy with superior tolerance to stuck at &quot;0&quot; faults compared to widely used
regularisers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.07758</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.07758</id><submitter>Massimo Franceschet</submitter><version version="v1"><date>Tue, 9 Jul 2019 09:49:23 GMT</date><size>73kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 20 Jul 2019 16:30:52 GMT</date><size>204kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 12 Apr 2020 10:26:48 GMT</date><size>84kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 10:34:25 GMT</date><size>155kb</size><source_type>D</source_type></version><title>HITS hits art</title><authors>Massimo Franceschet</authors><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The blockchain art market is partitioned around the roles of artists and
collectors and highly concentrated among few prominent figures. We hence
propose to adapt Kleinberg's authority/hub HITS method to rate artists and
collectors in the art context. This seems a reasonable choice since the
original method deftly defines its scores in terms of a mutual recursive
relationship between authorities/artists - the miners of information/art, and
hubs/collectors - the assemblers of such information/art.
  We evaluated the proposed method on the collector-artist network of SuperRare
gallery, the major crypto art marketplace. We found that the proposed artist
and collector metrics are weakly correlated with other network science metrics
like degree and strength. This hints the possibility of coupling different
measures in order to profile active users of the gallery and suggests
investment strategies with different risk/reward ratios for collectors as well
as marketing strategies with different targets for artists.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.08228</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.08228</id><submitter>Indira Sen</submitter><version version="v1"><date>Thu, 18 Jul 2019 18:18:48 GMT</date><size>1702kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 26 Sep 2019 16:42:26 GMT</date><size>754kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 5 Dec 2019 18:03:44 GMT</date><size>756kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 16:52:11 GMT</date><size>1676kb</size><source_type>D</source_type></version><title>TED-On: A Total Error Framework for Digital Traces of Human Behavior on
  Online Platforms</title><authors>Indira Sen, Fabian Floeck, Katrin Weller, Bernd Weiss, Claudia Wagner</authors><categories>cs.CY cs.HC cs.SI</categories><comments>20 pages, 2 figures, Longer version of paper set to appear in Public
  Opinion Quarterly. Updating terminology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peoples' activities and opinions recorded as digital traces online,
especially on social media and other web-based platforms, offer increasingly
informative pictures of the public. They promise to allow inferences about
populations beyond the users of the platforms on which the traces are recorded,
representing real potential for the Social Sciences and a complement to
survey-based research. But the use of digital traces brings its own
complexities and new error sources to the research enterprise. Recently,
researchers have begun to discuss the errors that can occur when digital traces
are used to learn about humans and social phenomena. This article synthesizes
this discussion and proposes a systematic way to categorize potential errors,
inspired by the Total Survey Error (TSE) Framework developed for survey
methodology. We introduce a conceptual framework to diagnose, understand, and
document errors that may occur in studies based on such digital traces. While
there are clear parallels to the well-known error sources in the TSE framework,
the new &quot;Total Error Framework for Digital Traces of Human Behavior on Online
Platforms&quot; (TED-On) identifies several types of error that are specific to the
use of digital traces. By providing a standard vocabulary to describe these
errors, the proposed framework is intended to advance communication and
research concerning the use of digital traces in scientific social research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.08556</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.08556</id><submitter>Divya Saxena</submitter><version version="v1"><date>Fri, 19 Jul 2019 15:54:54 GMT</date><size>772kb</size></version><version version="v2"><date>Mon, 22 Jul 2019 08:56:50 GMT</date><size>772kb</size></version><version version="v3"><date>Sat, 29 May 2021 10:17:13 GMT</date><size>810kb</size></version><title>D-GAN: Deep Generative Adversarial Nets for Spatio-Temporal Prediction</title><authors>Divya Saxena, Jiannong Cao</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatio-temporal (ST) data for urban applications, such as taxi demand,
traffic flow, regional rainfall is inherently stochastic and unpredictable.
Recently, deep learning based ST prediction models are proposed to learn the ST
characteristics of data. However, it is still very challenging (1) to
adequately learn the complex and non-linear ST relationships; (2) to model the
high variations in the ST data volumes as it is inherently dynamic, changing
over time (i.e., irregular) and highly influenced by many external factors,
such as adverse weather, accidents, traffic control, PoI, etc.; and (3) as
there can be many complicated external factors that can affect the accuracy and
it is impossible to list them explicitly. To handle the aforementioned issues,
in this paper, we propose a novel deep generative adversarial network based
model (named, D-GAN) for more accurate ST prediction by implicitly learning ST
feature representations in an unsupervised manner. D-GAN adopts a GAN-based
structure and jointly learns generation and variational inference of data. More
specifically, D-GAN consists of two major parts: (1) a deep ST feature learning
network to model the ST correlations and semantic variations, and underlying
factors of variations and irregularity in the data through the implicit
distribution modelling; (2) a fusion module to incorporate external factors for
reaching a better inference. To the best our knowledge, no prior work studies
ST prediction problem via deep implicit generative model and in an unsupervised
manner. Extensive experiments performed on two real-world datasets show that
D-GAN achieves more accurate results than traditional as well as deep learning
based ST prediction methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.08559</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.08559</id><submitter>Jonathan Sorenson</submitter><version version="v1"><date>Fri, 19 Jul 2019 16:08:00 GMT</date><size>57kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 30 Aug 2019 16:54:14 GMT</date><size>70kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 17:09:28 GMT</date><size>5kb</size></version><title>An Algorithm and Estimates for the Erd\H{o}s-Selfridge Function (work in
  progress)</title><authors>Brianna Sorenson and Jonathan P Sorenson and Jonathan Webster</authors><categories>math.NT cs.DS</categories><comments>25 pages, 5 figures; See the DOI link for the published version in
  ANTS; This update contains a few new minor results</comments><msc-class>11Y16, 11Y55, 11A63, 11B65, 68Q25</msc-class><doi>10.2140/obs.2020.4.371</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p(n)$ denote the smallest prime divisor of the integer $n$. Define the
function $g(k)$ to be the smallest integer $&gt;k+1$ such that
$p(\binom{g(k)}{k})&gt;k$. So we have $g(2)=6$ and $g(3)=g(4)=7$. In this paper we
present the following new results on the Erd\H{o}s-Selfridge function $g(k)$:
We present a new algorithm to compute the value of $g(k)$, and use it to both
verify previous work and compute new values of $g(k)$, with our current limit
being $$ g(323)= 1\ 69829\ 77104\ 46041\ 21145\ 63251\ 22499. $$ We define a
new function $\hat{g}(k)$, and under the assumption of our Uniform Distribution
Heuristic we show that $$ \log g(k) = \log \hat{g}(k) + O(\log k) $$ with high
&quot;probability&quot;. We also provide computational evidence to support our claim that
$\hat{g}(k)$ estimates $g(k)$ reasonably well in practice. There are several
open conjectures on the behavior of $g(k)$ which we are able to prove for
$\hat{g}(k)$, namely that $$ 0.525\ldots +o(1) \quad \le \quad \frac{\log
\hat{g}(k)}{k/\log k} \quad \le \quad 1+o(1), $$ and that $$
\limsup_{k\rightarrow\infty} \frac{\hat{g}(k+1)}{\hat{g}(k)}=\infty.$$ Let
$G(x,k)$ count the number of integers $n\le x$ such that $p(\binom{n}{k})&gt;k$.
Unconditionally, we prove that for large $x$, $G(x,k)$ is asymptotic to
$x/\hat{g}(k)$. And finally, we show that the running time of our new algorithm
is at most $g(k) \exp[ -c (k\log\log k) /(\log k)^2 (1+o(1))]$ for a constant
$c&gt;0$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.08996</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.08996</id><submitter>Mazharul Islam</submitter><version version="v1"><date>Sun, 21 Jul 2019 16:09:16 GMT</date><size>195kb</size><source_type>D</source_type></version><title>Improving Neural Network Classifier using Gradient-based Floating
  Centroid Method</title><authors>Mazharul Islam, Shuangrong Liu, Lin Wang, Xiaojing Zhang</authors><categories>cs.NE cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Floating centroid method (FCM) offers an efficient way to solve a
fixed-centroid problem for the neural network classifiers. However,
evolutionary computation as its optimization method restrains the FCM to
achieve satisfactory performance for different neural network structures,
because of the high computational complexity and inefficiency. Traditional
gradient-based methods have been extensively adopted to optimize the neural
network classifiers. In this study, a gradient-based floating centroid (GDFC)
method is introduced to address the fixed centroid problem for the neural
network classifiers optimized by gradient-based methods. Furthermore, a new
loss function for optimizing GDFC is introduced. The experimental results
display that GDFC obtains promising classification performance than the
comparison methods on the benchmark datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.10719</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.10719</id><submitter>Akash Abdu Jyothi</submitter><version version="v1"><date>Wed, 24 Jul 2019 20:53:55 GMT</date><size>1781kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 13 Aug 2019 21:49:56 GMT</date><size>5752kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 06:25:20 GMT</date><size>5928kb</size><source_type>D</source_type></version><title>LayoutVAE: Stochastic Scene Layout Generation From a Label Set</title><authors>Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, Greg Mori</authors><categories>cs.CV</categories><comments>20 pages, 24 figures, accepted in ICCV 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there is an increasing interest in scene generation within the
research community. However, models used for generating scene layouts from
textual description largely ignore plausible visual variations within the
structure dictated by the text. We propose LayoutVAE, a variational autoencoder
based framework for generating stochastic scene layouts. LayoutVAE is a
versatile modeling framework that allows for generating full image layouts
given a label set, or per label layouts for an existing image given a new
label. In addition, it is also capable of detecting unusual layouts,
potentially providing a way to evaluate layout generation problem. Extensive
experiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset
verifies the effectiveness of our proposed framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.13101</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.13101</id><submitter>Antonio Fazzi</submitter><version version="v1"><date>Tue, 30 Jul 2019 17:36:14 GMT</date><size>65kb</size></version><version version="v2"><date>Mon, 30 Mar 2020 16:07:30 GMT</date><size>47kb</size></version><version version="v3"><date>Wed, 12 Aug 2020 16:54:27 GMT</date><size>47kb</size></version><version version="v4"><date>Thu, 11 Feb 2021 17:01:59 GMT</date><size>52kb</size><source_type>D</source_type></version><title>Generalized algorithms for the approximate matrix polynomial GCD of
  reducing data uncertainties with application to MIMO system and control</title><authors>A. Fazzi, N. Guglielmi, I. Markovsky</authors><categories>math.NA cs.NA</categories><doi>10.1016/j.cam.2021.113499</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Computation of (approximate) polynomials common factors is an important
problem in several fields of science, like control theory and signal
processing. While the problem has been widely studied for scalar polynomials,
the scientific literature in the framework of matrix polynomials seems to be
limited to the problem of exact greatest common divisors computation. In this
paper, we generalize two algorithms from scalar to matrix polynomials. The
first one is fast and simple. The second one is more accurate but
computationally more expensive. We test the performances of the two algorithms
and observe similar behavior to the one in the scalar case. Finally we describe
an application to multi-input multi-output linear time-invariant dynamical
systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1908.00045</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1908.00045</id><submitter>Itay Safran</submitter><version version="v1"><date>Wed, 31 Jul 2019 18:43:01 GMT</date><size>23kb</size></version><version version="v2"><date>Tue, 28 Jan 2020 13:04:20 GMT</date><size>27kb</size></version><version version="v3"><date>Tue, 9 Jun 2020 08:49:25 GMT</date><size>28kb</size></version><version version="v4"><date>Wed, 2 Jun 2021 16:25:46 GMT</date><size>28kb</size></version><title>How Good is SGD with Random Shuffling?</title><authors>Itay Safran, Ohad Shamir</authors><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of stochastic gradient descent (SGD) on smooth and
strongly-convex finite-sum optimization problems. In contrast to the majority
of existing theoretical works, which assume that individual functions are
sampled with replacement, we focus here on popular but poorly-understood
heuristics, which involve going over random permutations of the individual
functions. This setting has been investigated in several recent works, but the
optimal error rates remain unclear. In this paper, we provide lower bounds on
the expected optimization error with these heuristics (using SGD with any
constant step size), which elucidate their advantages and disadvantages. In
particular, we prove that after $k$ passes over $n$ individual functions, if
the functions are re-shuffled after every pass, the best possible optimization
error for SGD is at least $\Omega\left(1/(nk)^2+1/nk^3\right)$, which partially
corresponds to recently derived upper bounds. Moreover, if the functions are
only shuffled once, then the lower bound increases to $\Omega(1/nk^2)$. Since
there are strictly smaller upper bounds for repeated reshuffling, this proves
an inherent performance gap between SGD with single shuffling and repeated
shuffling. As a more minor contribution, we also provide a non-asymptotic
$\Omega(1/k^2)$ lower bound (independent of $n$) for the incremental gradient
method, when no random shuffling takes place. Finally, we provide an indication
that our lower bounds are tight, by proving matching upper bounds for
univariate quadratic functions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1908.05865</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1908.05865</id><submitter>Minquan Cheng</submitter><version version="v1"><date>Fri, 16 Aug 2019 06:45:35 GMT</date><size>50kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 26 Mar 2020 07:54:08 GMT</date><size>50kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 00:36:10 GMT</date><size>42kb</size><source_type>D</source_type></version><title>A framework of constructing placement delivery arrays for centralized
  coded caching</title><authors>Minquan Cheng, Jinyu Wang, Xi Zhong, Qiang Wang</authors><categories>cs.IT math.CO math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In caching system, it is desirable to design a coded caching scheme with the
transmission load $R$ and subpacketization $F$ as small as possible, in order
to improve efficiency of transmission in the peak traffic times and to decrease
implementation complexity. Yan et al. reformulated the centralized coded
caching scheme as designing a corresponding $F\times K$ array called placement
delivery array (PDA), where $F$ is the subpacketization and $K$ is the number
of users. Motivated by several constructions of PDAs, we introduce a framework
for constructing PDAs, where each row is indexed by a row vector of some matrix
called row index matrix and each column's index is labelled by an element of a
direct product set. Using this framework, a new scheme is obtained, which can
be regarded as a generalization of some previously known schemes. When $K$ is
equal to ${m\choose t}q^t$ for positive integers $m$, $t$ with $t&lt;m$ and $q\geq
2$, we show that the row index matrix must be an orthogonal array if all the
users have the same memory size. Furthermore, the row index matrix must be a
covering array if the coded gain is ${m\choose t}$, which is the maximal coded
gain under our framework. Consequently the lower bounds on the transmission
load and subpacketization of the schemes are derived under our framework.
Finally, using orthogonal arrays as the row index matrix, we obtain two more
explicit classes of schemes which have significantly advantages on the
subpacketization while the transmission load is equal or close to that of the
schemes constructed by Shangguan et al. (IEEE Trans. Inf. Theory, 64,
5755-5766, 2018) for the same number of users and memory size.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1908.06009</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1908.06009</id><submitter>Melina Merkel</submitter><version version="v1"><date>Thu, 15 Aug 2019 15:35:07 GMT</date><size>1382kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 17 Mar 2020 20:19:02 GMT</date><size>1497kb</size><source_type>D</source_type></version><title>Shape Optimization of Rotating Electric Machines using Isogeometric
  Analysis and Harmonic Stator-Rotor Coupling</title><authors>Melina Merkel, Peter Gangl, Sebastian Sch\&quot;ops</authors><categories>cs.CE</categories><journal-ref>IEEE Transactions on Energy Conversion, February 2021</journal-ref><doi>10.1109/TEC.2021.3061271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with shape optimization of electric machines using
isogeometric analysis. Isogeometric analysis is particularly well suited for
shape optimization as it allows to easily modify the geometry without remeshing
the domain. A 6-pole permanent magnet synchronous machine is modeled using a
multipatch isogeometric approach and rotation of the machine is realized by
modeling the stator and rotor domain separately and coupling them at the
interface using harmonic basis functions. Shape optimization is applied to the
model minimizing the total harmonic distortion of the electromotive force as a
goal functional.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1908.06211</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1908.06211</id><submitter>Soham Sinha</submitter><version version="v1"><date>Sat, 17 Aug 2019 00:24:07 GMT</date><size>1866kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:53:06 GMT</date><size>567kb</size><source_type>D</source_type></version><title>PAStime: Progress-aware Scheduling for Time-critical Computing</title><authors>Soham Sinha and Richard West and Ahmad Golchin</authors><categories>cs.OS</categories><comments>24 pages</comments><acm-class>C.3; D.4.7; D.4.1</acm-class><doi>10.4230/LIPIcs.ECRTS.2020.3</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Over-estimation of worst-case execution times (WCETs) of real-time tasks
leads to poor resource utilization. In a mixed-criticality system (MCS), the
over-provisioning of CPU time to accommodate the WCETs of highly critical tasks
may lead to degraded service for less critical tasks. In this paper, we present
PAStime, a novel approach to monitor and adapt the runtime progress of highly
time-critical applications, to allow for improved service to lower criticality
tasks. In PAStime, CPU time is allocated to time-critical tasks according to
the delays they experience as they progress through their control flow graphs.
This ensures that as much time as possible is made available to improve the
Quality-of-Service of less critical tasks, while high-criticality tasks are
compensated after their delays.
  In this paper, we integrate PAStime with Adaptive Mixed-criticality (AMC)
scheduling. The LO-mode budget of a high-criticality task is adjusted according
to the delay observed at execution checkpoints. This is the first
implementation of AMC in the scheduling framework Using LITMUS-RT, which is
extended with our PAStime runtime policy and tested with real-time Linux
applications such as object classification and detection. We observe in our
experimental evaluation that AMC-PAStime significantly improves the utilization
of the low-criticality tasks while guaranteeing service to high-criticality
tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.00257</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.00257</id><submitter>Mitsuru Igami</submitter><version version="v1"><date>Sat, 31 Aug 2019 18:20:25 GMT</date><size>2748kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 4 Aug 2020 17:23:04 GMT</date><size>5660kb</size></version><version version="v3"><date>Fri, 18 Sep 2020 16:13:45 GMT</date><size>5679kb</size></version><version version="v4"><date>Tue, 18 May 2021 15:03:56 GMT</date><size>5680kb</size></version><version version="v5"><date>Thu, 3 Jun 2021 17:21:14 GMT</date><size>5680kb</size></version><title>Mapping Firms' Locations in Technological Space: A Topological Analysis
  of Patent Statistics</title><authors>Emerson G. Escolar, Yasuaki Hiraoka, Mitsuru Igami, Yasin Ozcan</authors><categories>econ.EM cs.DM math.AT math.CO</categories><comments>31 pages (main text), 10 pages (Appendix), 9 tables and 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method to characterize firms' inventive activities via
topological data analysis (TDA) that represents high-dimensional data in a
shape graph. Applying this method to 333 major firms' patents in 1976--2005
reveals substantial heterogeneity: some firms remain undifferentiated; others
develop unique portfolios. Firms with unique trajectories, which we define
graph-theoretically as &quot;flares&quot; in the Mapper graph, perform better. This
association is statistically and economically significant, and continues to
hold after we control for portfolio size and firm survivorship. We further
illustrate Mapper's usefulness for exploratory data analysis in comparison with
existing techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.02061</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.02061</id><submitter>Salem Alqahtani</submitter><version version="v1"><date>Wed, 4 Sep 2019 19:09:38 GMT</date><size>955kb</size><source_type>D</source_type></version><title>Performance Analysis and Comparison of Distributed Machine Learning
  Systems</title><authors>Salem Alqahtani and Murat Demirbas</authors><categories>cs.DC</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning has permeated through many aspects of computing/processing
systems in recent years. While distributed training architectures/frameworks
are adopted for training large deep learning models quickly, there has not been
a systematic study of the communication bottlenecks of these architectures and
their effects on the computation cycle time and scalability. In order to
analyze this problem for synchronous Stochastic Gradient Descent (SGD) training
of deep learning models, we developed a performance model of computation time
and communication latency under three different system architectures: Parameter
Server (PS), peer-to-peer (P2P), and Ring allreduce (RA). To complement and
corroborate our analytical models with quantitative results, we evaluated the
computation and communication performance of these system architectures of the
systems via experiments performed with Tensorflow and Horovod frameworks. We
found that the system architecture has a very significant effect on the
performance of training. RA-based systems achieve scalable performance as they
successfully decouple network usage from the number of workers in the system.
In contrast, 1PS systems suffer from low performance due to network congestion
at the parameter server side. While P2P systems fare better than 1PS systems,
they still suffer from significant network bottleneck. Finally, RA systems also
excel by virtue of overlapping computation time and communication time, which
PS and P2P architectures fail to achieve.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.02707</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.02707</id><submitter>Yuanhao Li</submitter><version version="v1"><date>Fri, 6 Sep 2019 04:18:57 GMT</date><size>264kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 2 Dec 2019 04:37:41 GMT</date><size>583kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 4 Dec 2019 04:34:58 GMT</date><size>418kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 9 Jul 2020 06:26:20 GMT</date><size>1273kb</size><source_type>D</source_type></version><title>Restricted Minimum Error Entropy Criterion for Robust Classification</title><authors>Yuanhao Li, Badong Chen, Natsue Yoshimura, Yasuharu Koike</authors><categories>cs.LG stat.ML</categories><journal-ref>IEEE Transactions on Neural Networks and Learning Systems 2021</journal-ref><doi>10.1109/TNNLS.2021.3082571</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum error entropy (MEE) criterion has been verified as a powerful
approach for non-Gaussian signal processing and robust machine learning.
However, the implementation of MEE on robust classification is rather a vacancy
in the literature. The original MEE only focuses on minimizing the Renyi's
quadratic entropy of the error probability distribution function (PDF), which
could cause failure in noisy classification tasks. To this end, we analyze the
optimal error distribution in the presence of outliers for those classifiers
with continuous errors, and introduce a simple codebook to restrict MEE so that
it drives the error PDF towards the desired case. Half-quadratic based
optimization and convergence analysis of the new learning criterion, called
restricted MEE (RMEE), are provided. Experimental results with logistic
regression and extreme learning machine are presented to verify the desirable
robustness of RMEE.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.03396</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.03396</id><submitter>Tomer Levinboim</submitter><version version="v1"><date>Sun, 8 Sep 2019 06:55:53 GMT</date><size>4648kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:03:27 GMT</date><size>7552kb</size><source_type>D</source_type></version><title>Quality Estimation for Image Captions Based on Large-scale Human
  Evaluations</title><authors>Tomer Levinboim, Ashish V. Thapliyal, Piyush Sharma, Radu Soricut</authors><categories>cs.CL cs.CV</categories><comments>10 pages, 6 figures, 3 tables. Accepted to NAACL2021.
  https://www.aclweb.org/anthology/2021.naacl-main.253/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic image captioning has improved significantly over the last few
years, but the problem is far from being solved, with state of the art models
still often producing low quality captions when used in the wild. In this
paper, we focus on the task of Quality Estimation (QE) for image captions,
which attempts to model the caption quality from a human perspective and
without access to ground-truth references, so that it can be applied at
prediction time to detect low-quality captions produced on previously unseen
images. For this task, we develop a human evaluation process that collects
coarse-grained caption annotations from crowdsourced users, which is then used
to collect a large scale dataset spanning more than 600k caption quality
ratings. We then carefully validate the quality of the collected ratings and
establish baseline models for this new QE task. Finally, we further collect
fine-grained caption quality annotations from trained raters, and use them to
demonstrate that QE models trained over the coarse ratings can effectively
detect and filter out low-quality image captions, thereby improving the user
experience from captioning systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.03448</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.03448</id><submitter>Miao Zhu</submitter><version version="v1"><date>Sun, 8 Sep 2019 12:32:07 GMT</date><size>279kb</size><source_type>D</source_type></version><title>A Generalized Configuration Model with Degree Correlations and Its
  Percolation Analysis</title><authors>Duan-Shin Lee and Cheng-Shang Chang and Miao Zhu and Hung-Chih Li</authors><categories>cs.SI cs.DM</categories><comments>23 pages, 4 figures</comments><journal-ref>Applied Network Science 4, 124 (2019)</journal-ref><doi>10.1016/j.apsusc.2018.12.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a generalization of the classical configuration
model. Like the classical configuration model, the generalized configuration
model allows users to specify an arbitrary degree distribution. In our
generalized configuration model, we partition the stubs in the configuration
model into b blocks of equal sizes and choose a permutation function h for
these blocks. In each block, we randomly designate a number proportional to q
of stubs as type 1 stubs, where q is a parameter in the range [0; 1]. Other
stubs are designated as type 2 stubs. To construct a network, randomly select
an unconnected stub. Suppose that this stub is in block i. If it is a type 1
stub, connect this stub to a randomly selected unconnected type 1 stub in block
h(i). If it is a type 2 stub, connect it to a randomly selected unconnected
type 2 stub. We repeat this process until all stubs are connected. Under an
assumption, we derive a closed form for the joint degree distribution of two
random neighboring vertices in the constructed graph. Based on this joint
degree distribution, we show that the Pearson degree correlation function is
linear in q for any fixed b. By properly choosing h, we show that our
construction algorithm can create assortative networks as well as
disassortative networks. We present a percolation analysis of this model. We
verify our results by extensive computer simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.04032</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.04032</id><submitter>Christian Reul</submitter><version version="v1"><date>Mon, 9 Sep 2019 11:15:40 GMT</date><size>16606kb</size><source_type>D</source_type></version><title>OCR4all -- An Open-Source Tool Providing a (Semi-)Automatic OCR Workflow
  for Historical Printings</title><authors>Christian Reul, Dennis Christ, Alexander Hartelt, Nico Balbach,
  Maximilian Wehner, Uwe Springmann, Christoph Wick, Christine Grundig, Andreas
  B\&quot;uttner, Frank Puppe</authors><categories>cs.CV</categories><comments>submitted to MDPI - Applied Sciences</comments><journal-ref>https://www.mdpi.com/2076-3417/9/22/4853/htm</journal-ref><doi>10.3390/app9224853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Character Recognition (OCR) on historical printings is a challenging
task mainly due to the complexity of the layout and the highly variant
typography. Nevertheless, in the last few years great progress has been made in
the area of historical OCR, resulting in several powerful open-source tools for
preprocessing, layout recognition and segmentation, character recognition and
post-processing. The drawback of these tools often is their limited
applicability by non-technical users like humanist scholars and in particular
the combined use of several tools in a workflow. In this paper we present an
open-source OCR software called OCR4all, which combines state-of-the-art OCR
components and continuous model training into a comprehensive workflow. A
comfortable GUI allows error corrections not only in the final output, but
already in early stages to minimize error propagations. Further on, extensive
configuration capabilities are provided to set the degree of automation of the
workflow and to make adaptations to the carefully selected default parameters
for specific printings, if necessary. Experiments showed that users with
minimal or no experience were able to capture the text of even the earliest
printed books with manageable effort and great quality, achieving excellent
character error rates (CERs) below 0.5%. The fully automated application on
19th century novels showed that OCR4all can considerably outperform the
commercial state-of-the-art tool ABBYY Finereader on moderate layouts if
suitably pretrained mixed OCR models are available. The architecture of OCR4all
allows the easy integration (or substitution) of newly developed tools for its
main components by standardized interfaces like PageXML, thus aiming at
continual higher automation for historical printings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.04261</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.04261</id><submitter>Wei Xie</submitter><version version="v1"><date>Tue, 10 Sep 2019 03:26:01 GMT</date><size>763kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 18 Jul 2020 22:10:27 GMT</date><size>763kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 19 Aug 2020 14:58:08 GMT</date><size>2529kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 2 Jun 2021 16:01:25 GMT</date><size>3310kb</size><source_type>D</source_type></version><title>Interpretable Biomanufacturing Process Risk and Sensitivity Analyses for
  Quality-by-Design and Stability Control</title><authors>Wei Xie and Bo Wang and Cheng Li and Dongming Xie and Jared Auclair</authors><categories>stat.ML cs.LG cs.SY eess.SY</categories><comments>41 pages, 8 figures</comments><journal-ref>Naval Research Logistics, 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While biomanufacturing plays a significant role in supporting the economy and
ensuring public health, it faces critical challenges, including complexity,
high variability, lengthy lead time, and very limited process data, especially
for personalized new cell and gene biotherapeutics. Driven by these challenges,
we propose an interpretable semantic bioprocess probabilistic knowledge graph
and develop a game theory based risk and sensitivity analyses for production
process to facilitate quality-by-design and stability control. Specifically, by
exploring the causal relationships and interactions of critical process
parameters and quality attributes (CPPs/CQAs), we create a Bayesian network
based probabilistic knowledge graph characterizing the complex causal
interdependencies of all factors. Then, we introduce a Shapley value based
sensitivity analysis, which can correctly quantify the variation contribution
from each input factor on the outputs (i.e., productivity, product quality).
Since the bioprocess model coefficients are learned from limited process
observations, we derive the Bayesian posterior distribution to quantify model
uncertainty and further develop the Shapley value based sensitivity analysis to
evaluate the impact of estimation uncertainty from each set of model
coefficients. Therefore, the proposed bioprocess risk and sensitivity analyses
can identify the bottlenecks, guide the reliable process specifications and the
most &quot;informative&quot; data collection, and improve production stability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.06039</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.06039</id><submitter>Neil G. Marchant</submitter><version version="v1"><date>Fri, 13 Sep 2019 05:28:37 GMT</date><size>4261kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 13 Jul 2020 00:58:17 GMT</date><size>4244kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 22 Sep 2020 13:42:27 GMT</date><size>4168kb</size><source_type>D</source_type></version><title>d-blink: Distributed End-to-End Bayesian Entity Resolution</title><authors>Neil G. Marchant, Andee Kaplan, Daniel N. Elazar, Benjamin I. P.
  Rubinstein, Rebecca C. Steorts</authors><categories>stat.CO cs.DB cs.LG stat.ML</categories><comments>32 pages, 6 figures, 5 tables. Includes 22 pages of supplementary
  material. This revision incorporates a case study on the 2010 U.S. Decennial
  Census</comments><msc-class>62F15, 65C40, 68W15</msc-class><doi>10.1080/10618600.2020.1825451</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity resolution (ER; also known as record linkage or de-duplication) is the
process of merging noisy databases, often in the absence of unique identifiers.
A major advancement in ER methodology has been the application of Bayesian
generative models, which provide a natural framework for inferring latent
entities with rigorous quantification of uncertainty. Despite these advantages,
existing models are severely limited in practice, as standard inference
algorithms scale quadratically in the number of records. While scaling can be
managed by fitting the model on separate blocks of the data, such a na\&quot;ive
approach may induce significant error in the posterior. In this paper, we
propose a principled model for scalable Bayesian ER, called &quot;distributed
Bayesian linkage&quot; or d-blink, which jointly performs blocking and ER without
compromising posterior correctness. Our approach relies on several key ideas,
including: (i) an auxiliary variable representation that induces a partition of
the entities and records into blocks; (ii) a method for constructing
well-balanced blocks based on k-d trees; (iii) a distributed
partially-collapsed Gibbs sampler with improved mixing; and (iv) fast
algorithms for performing Gibbs updates. Empirical studies on six data
sets---including a case study on the 2010 Decennial Census---demonstrate the
scalability and effectiveness of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.06397</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.06397</id><submitter>Stefan Sommer</submitter><version version="v1"><date>Fri, 13 Sep 2019 18:27:02 GMT</date><size>447kb</size></version><version version="v2"><date>Mon, 11 May 2020 13:28:51 GMT</date><size>615kb</size></version><version version="v3"><date>Mon, 31 May 2021 19:00:10 GMT</date><size>615kb</size></version><title>Horizontal Flows and Manifold Stochastics in Geometric Deep Learning</title><authors>Stefan Sommer, Alex Bronstein</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two constructions in geometric deep learning for 1) transporting
orientation-dependent convolutional filters over a manifold in a continuous way
and thereby defining a convolution operator that naturally incorporates the
rotational effect of holonomy; and 2) allowing efficient evaluation of manifold
convolution layers by sampling manifold valued random variables that center
around a weighted diffusion mean. Both methods are inspired by stochastics on
manifolds and geometric statistics, and provide examples of how stochastic
methods -- here horizontal frame bundle flows and non-linear bridge sampling
schemes, can be used in geometric deep learning. We outline the theoretical
foundation of the two methods, discuss their relation to Euclidean deep
networks and existing methodology in geometric deep learning, and establish
important properties of the proposed constructions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.10837</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.10837</id><submitter>Ying Chen</submitter><version version="v1"><date>Tue, 24 Sep 2019 12:28:11 GMT</date><size>1533kb</size></version><version version="v2"><date>Sun, 12 Jan 2020 06:28:18 GMT</date><size>1533kb</size></version><version version="v3"><date>Wed, 12 Aug 2020 05:31:43 GMT</date><size>3687kb</size><source_type>D</source_type></version><title>Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust
  Performance</title><authors>Shibo Zhou, Xiaohua LI, Ying Chen, Sanjeev T. Chandrasekaran, Arindam
  Sanyal</authors><categories>cs.CV cs.NE eess.IV</categories><journal-ref>Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21),
  2021: 35(12),11143-11151</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking neural network (SNN) is interesting both theoretically and
practically because of its strong bio-inspiration nature and potentially
outstanding energy efficiency. Unfortunately, its development has fallen far
behind the conventional deep neural network (DNN), mainly because of difficult
training and lack of widely accepted hardware experiment platforms. In this
paper, we show that a deep temporal-coded SNN can be trained easily and
directly over the benchmark datasets CIFAR10 and ImageNet, with testing
accuracy within 1% of the DNN of equivalent size and architecture. Training
becomes similar to DNN thanks to the closed-form solution to the spiking
waveform dynamics. Considering that SNNs should be implemented in practical
neuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2
bits and with weights perturbed by random noise to demonstrate its robustness
in practical applications. In addition, we develop a phase-domain signal
processing circuit schematic to implement our spiking neuron with 90% gain of
energy efficiency over existing work. This paper demonstrates that the
temporal-coded deep SNN is feasible for applications with high performance and
high energy efficient.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.13322</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.13322</id><submitter>Rongrong Wang</submitter><version version="v1"><date>Sun, 29 Sep 2019 17:06:13 GMT</date><size>2781kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 13:59:39 GMT</date><size>2602kb</size><source_type>D</source_type></version><title>Capacity Preserving Mapping for High-dimensional Data Visualization</title><authors>Rongrong Wang, Xiaopeng Zhang</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a rigorous mathematical treatment to the crowding issue in data
visualization when high dimensional data sets are projected down to low
dimensions for visualization. By properly adjusting the capacity of high
dimensional balls, our method makes right enough room to prepare for the
embedding. A key component of the proposed method is an estimation of the
correlation dimension at various scales which reflects the data density
variation. The proposed adjustment to the capacity applies to any distance
(Euclidean, geodesic, diffusion) and can potentially be used in many existing
methods to mitigate the crowding during the dimension reduction. We demonstrate
the effectiveness of the new method using synthetic and real datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.02126</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.02126</id><submitter>Mina Doosti</submitter><version version="v1"><date>Fri, 4 Oct 2019 20:00:36 GMT</date><size>148kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 17 Sep 2020 08:58:08 GMT</date><size>619kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 13 May 2021 14:32:14 GMT</date><size>127kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 2 Jun 2021 10:10:11 GMT</date><size>130kb</size><source_type>D</source_type></version><title>Quantum Physical Unclonable Functions: Possibilities and Impossibilities</title><authors>Myrto Arapinis, Mahshid Delavar, Mina Doosti, and Elham Kashefi</authors><categories>quant-ph cs.CR</categories><comments>32 pages including the appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A Physical Unclonable Function (PUF) is a device with unique behaviour that
is hard to clone hence providing a secure fingerprint. A variety of PUF
structures and PUF-based applications have been explored theoretically as well
as being implemented in practical settings. Recently, the inherent
unclonability of quantum states has been exploited to derive the quantum
analogue of PUF as well as new proposals for the implementation of PUF. We
present the first comprehensive study of quantum Physical Unclonable Functions
(qPUFs) with quantum cryptographic tools. We formally define qPUFs,
encapsulating all requirements of classical PUFs as well as introducing a new
testability feature inherent to the quantum setting only. We use a quantum
game-based framework to define different levels of security for qPUFs: quantum
exponential unforgeability, quantum existential unforgeability and quantum
selective unforgeability. We introduce a new quantum attack technique based on
the universal quantum emulator algorithm of Marvin and Lloyd to prove no qPUF
can provide quantum existential unforgeability. On the other hand, we prove
that a large family of qPUFs (called unitary PUFs) can provide quantum
selective unforgeability which is the desired level of security for most
PUF-based applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.05843</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.05843</id><submitter>Rui Meng</submitter><version version="v1"><date>Sun, 13 Oct 2019 21:53:44 GMT</date><size>1745kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 03:09:40 GMT</date><size>330kb</size><source_type>D</source_type></version><title>Regularized Sparse Gaussian Processes</title><authors>Rui Meng, Herbert Lee, Soper Braden, Priyadip Ray</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes are a flexible Bayesian nonparametric modelling approach
that has been widely applied but poses computational challenges. To address the
poor scaling of exact inference methods, approximation methods based on sparse
Gaussian processes (SGP) are attractive. An issue faced by SGP, especially in
latent variable models, is the inefficient learning of the inducing inputs,
which leads to poor model prediction. We propose a regularization approach by
balancing the reconstruction performance of data and the approximation
performance of the model itself. This regularization improves both inference
and prediction performance. We extend this regularization approach into latent
variable models with SGPs and show that performing variational inference (VI)
on those models is equivalent to performing VI on a related empirical Bayes
model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.08613</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.08613</id><submitter>Ali Girayhan Ozbay</submitter><version version="v1"><date>Fri, 18 Oct 2019 20:29:20 GMT</date><size>8376kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 10:33:36 GMT</date><size>12383kb</size><source_type>D</source_type></version><title>Poisson CNN: Convolutional neural networks for the solution of the
  Poisson equation on a Cartesian mesh</title><authors>Ali Girayhan \&quot;Ozbay, Arash Hamzehloo, Sylvain Laizet, Panagiotis
  Tzirakis, Georgios Rizos, Bj\&quot;orn Schuller</authors><categories>physics.comp-ph cs.LG stat.ML</categories><comments>34 pages, 18 figures. Accepted for publication in Data Centric
  Engineering. Code available at
  https://github.com/aligirayhanozbay/poisson_CNN</comments><msc-class>62M45, 65N99</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Poisson equation is commonly encountered in engineering, for instance in
computational fluid dynamics (CFD) where it is needed to compute corrections to
the pressure field to ensure the incompressibility of the velocity field. In
the present work, we propose a novel fully convolutional neural network (CNN)
architecture to infer the solution of the Poisson equation on a 2D Cartesian
grid with different resolutions given the right hand side term, arbitrary
boundary conditions and grid parameters. It provides unprecedented versatility
for a CNN approach dealing with partial differential equations. The boundary
conditions are handled using a novel approach by decomposing the original
Poisson problem into a homogeneous Poisson problem plus four inhomogeneous
Laplace sub-problems. The model is trained using a novel loss function
approximating the continuous $L^p$ norm between the prediction and the target.
Even when predicting on grids denser than previously encountered, our model
demonstrates encouraging capacity to reproduce the correct solution profile.
The proposed model, which outperforms well-known neural network models, can be
included in a CFD solver to help with solving the Poisson equation. Analytical
test cases indicate that our CNN architecture is capable of predicting the
correct solution of a Poisson problem with mean percentage errors below 10%, an
improvement by comparison to the first step of conventional iterative methods.
Predictions from our model, used as the initial guess to iterative algorithms
like Multigrid, can reduce the RMS error after a single iteration by more than
90% compared to a zero initial guess.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.09227</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.09227</id><submitter>Alisa Kirichenko</submitter><version version="v1"><date>Mon, 21 Oct 2019 09:32:26 GMT</date><size>464kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 12 Jun 2020 09:20:14 GMT</date><size>218kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 21:52:38 GMT</date><size>1071kb</size><source_type>D</source_type></version><title>Safe-Bayesian Generalized Linear Regression</title><authors>Rianne de Heide and Alisa Kirichenko and Nishant Mehta and Peter
  Gr\&quot;unwald</authors><categories>math.ST cs.LG stat.ME stat.TH</categories><comments>Final version. Accepted to AISTATS 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generalized Bayesian inference under misspecification, i.e. when the
model is 'wrong but useful'. Generalized Bayes equips the likelihood with a
learning rate $\eta$. We show that for generalized linear models (GLMs),
$\eta$-generalized Bayes concentrates around the best approximation of the
truth within the model for specific $\eta \neq 1$, even under severely
misspecified noise, as long as the tails of the true distribution are
exponential. We derive MCMC samplers for generalized Bayesian lasso and
logistic regression and give examples of both simulated and real-world data in
which generalized Bayes substantially outperforms standard Bayes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.09713</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.09713</id><submitter>Simon Le Cleac'h</submitter><version version="v1"><date>Tue, 22 Oct 2019 00:44:37 GMT</date><size>3196kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 Feb 2020 22:55:48 GMT</date><size>6974kb</size><source_type>D</source_type></version><title>ALGAMES: A Fast Solver for Constrained Dynamic Games</title><authors>Simon Le Cleac'h, Mac Schwager, Zachary Manchester</authors><categories>cs.RO cs.AI cs.GT</categories><comments>10 pages, 8 figures, submitted to Robotics: Science and Systems
  Conference (RSS) 2020</comments><journal-ref>Proceedings of Robotics: Science and Systems, 2020</journal-ref><doi>10.15607/RSS.2020.XVI.091</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic games are an effective paradigm for dealing with the control of
multiple interacting actors. This paper introduces ALGAMES (Augmented
Lagrangian GAME-theoretic Solver), a solver that handles trajectory
optimization problems with multiple actors and general nonlinear state and
input constraints. Its novelty resides in satisfying the first order optimality
conditions with a quasi-Newton root-finding algorithm and rigorously enforcing
constraints using an augmented Lagrangian formulation. We evaluate our solver
in the context of autonomous driving on scenarios with a strong level of
interactions between the vehicles. We assess the robustness of the solver using
Monte Carlo simulations. It is able to reliably solve complex problems like
ramp merging with three vehicles three times faster than a state-of-the-art
DDP-based approach. A model predictive control (MPC) implementation of the
algorithm demonstrates real-time performance on complex autonomous driving
scenarios with an update frequency higher than 60 Hz.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.09857</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.09857</id><submitter>Nuri Mert Vural</submitter><version version="v1"><date>Tue, 22 Oct 2019 09:30:41 GMT</date><size>70kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 9 Nov 2019 16:40:10 GMT</date><size>516kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 7 Mar 2020 16:12:42 GMT</date><size>517kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 15 Aug 2020 13:43:54 GMT</date><size>158kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 15:39:14 GMT</date><size>1094kb</size><source_type>D</source_type></version><title>An Efficient and Effective Second-Order Training Algorithm for
  LSTM-based Adaptive Learning</title><authors>N. Mert Vural, Salih Erg\&quot;ut and Suleyman S. Kozat</authors><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study adaptive (or online) nonlinear regression with
Long-Short-Term-Memory (LSTM) based networks, i.e., LSTM-based adaptive
learning. In this context, we introduce an efficient Extended Kalman filter
(EKF) based second-order training algorithm. Our algorithm is truly online,
i.e., it does not assume any underlying data generating process and future
information, except that the target sequence is bounded. Through an extensive
set of experiments, we demonstrate significant performance gains achieved by
our algorithm with respect to the state-of-the-art methods. Here, we mainly
show that our algorithm consistently provides 10 to 45\% improvement in the
accuracy compared to the widely-used adaptive methods Adam, RMSprop, and DEKF,
and comparable performance to EKF with a 10 to 15 times reduction in the
run-time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.10251</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.10251</id><submitter>Ali Ayub</submitter><version version="v1"><date>Tue, 22 Oct 2019 22:13:25 GMT</date><size>1057kb</size></version><version version="v2"><date>Tue, 22 Dec 2020 01:12:58 GMT</date><size>844kb</size></version><title>Using Markov Decision Process to Model Deception for Robotic and
  Interactive Game Applications</title><authors>Ali Ayub, Aldo Morales and Amit Banerjee</authors><categories>cs.HC cs.MA cs.RO</categories><comments>Accepted at IEEE International Conference on Consumer Electronics
  (ICCE) 2021</comments><journal-ref>2021 IEEE International Conference on Consumer Electronics (ICCE)</journal-ref><doi>10.1109/ICCE50685.2021.9427633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates deception in the context of motion using a simulated
mobile robot. We analyze some previously designed deceptive strategies on a
mobile robot simulator. We then present a novel approach to adaptively choose
target-oriented deceptive trajectories to deceive humans for multiple
interactions. Additionally, we propose a new metric to evaluate deception on
data collected from the users when interacting with the mobile robot simulator.
We performed a user study to test our proposed adaptive deceptive algorithm,
which shows that our algorithm deceives humans even for multiple interactions
and it is more effective than random choice of deceptive strategies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.10492</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.10492</id><submitter>Haozheng Luo</submitter><version version="v1"><date>Thu, 17 Oct 2019 23:00:22 GMT</date><size>379kb</size></version><version version="v2"><date>Mon, 20 Apr 2020 17:15:23 GMT</date><size>644kb</size></version><title>Question Classification with Deep Contextualized Transformer</title><authors>Haozheng Luo, Ningwei Liu, Charles Feng</authors><categories>cs.CL cs.AI</categories><doi>10.1007/978-3-030-73103-8_32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The latest work for Question and Answer problems is to use the Stanford Parse
Tree. We build on prior work and develop a new method to handle the Question
and Answer problem with the Deep Contextualized Transformer to manage some
aberrant expressions. We also conduct extensive evaluations of the SQuAD and
SwDA dataset and show significant improvement over QA problem classification of
industry needs. We also investigate the impact of different models for the
accuracy and efficiency of the problem answers. It shows that our new method is
more effective for solving QA problems with higher accuracy
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.11370</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.11370</id><submitter>Caterina Strambio-De-Castillia Ph.D.</submitter><version version="v1"><date>Thu, 24 Oct 2019 18:33:17 GMT</date><size>2286kb</size></version><version version="v2"><date>Fri, 10 Jan 2020 21:22:33 GMT</date><size>2216kb</size></version><version version="v3"><date>Thu, 14 May 2020 23:35:45 GMT</date><size>2216kb</size></version><version version="v4"><date>Mon, 26 Apr 2021 22:56:39 GMT</date><size>1489kb</size></version><version version="v5"><date>Thu, 29 Apr 2021 19:37:00 GMT</date><size>1582kb</size></version><version version="v6"><date>Tue, 1 Jun 2021 03:36:07 GMT</date><size>1618kb</size></version><title>A perspective on Microscopy Metadata: data provenance and quality
  control</title><authors>Maximiliaan Huisman, Mathias Hammer, Alex Rigano, Ulrike Boehm, James
  J. Chambers, Nathalie Gaudreault, Alison J. North, Jaime A. Pimentel, Damir
  Sudar, Peter Bajcsy, Claire M. Brown, Alexander D. Corbett, Orestis Faklaris,
  Judith Lacoste, Alex Laude, Glyn Nelson, Roland Nitschke, David Grunwald, and
  Caterina Strambio-De-Castillia</authors><categories>q-bio.QM cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The application of microscopy in biomedical research has come a long way
since Antonie van Leeuwenhoek discovered unicellular organisms. Countless
innovations have positioned light microscopy as a cornerstone of modern biology
and a method of choice for connecting omics datasets to their biological and
clinical correlates. Still, regardless of how convincing published imaging data
looks, it does not always convey meaningful information about the conditions in
which it was acquired, processed, and analyzed. Adequate record-keeping,
reporting, and quality control are therefore essential to ensure experimental
rigor and data fidelity, allow experiments to be reproducibly repeated, and
promote the proper evaluation, interpretation, comparison, and re-use. To this
end, microscopy images should be accompanied by complete descriptions detailing
experimental procedures, biological samples, microscope hardware
specifications, image acquisition parameters, and image analysis procedures, as
well as metrics accounting for instrument performance and calibration. However,
universal, community-accepted Microscopy Metadata standards and reporting
specifications that would result in Findable Accessible Interoperable and
Reproducible (FAIR) microscopy data have not yet been established. To
understand this shortcoming and to propose a way forward, here we provide an
overview of the nature of microscopy metadata and its importance for fostering
data quality, reproducibility, scientific rigor, and sharing value in light
microscopy. The proposal for tiered Microscopy Metadata Specifications that
extend the OME Data Model put forth by the 4D Nucleome Initiative and by
Bioimaging North America [1-3] as well as a suite of three complementary and
interoperable tools are being developed to facilitate the process of image data
documentation and are presented in related manuscripts [4-6].
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.12747</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.12747</id><submitter>Laurent Feuilloley</submitter><version version="v1"><date>Mon, 28 Oct 2019 15:16:15 GMT</date><size>120kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 2 Jan 2020 15:30:14 GMT</date><size>120kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 1 Feb 2021 16:19:39 GMT</date><size>96kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 17:08:12 GMT</date><size>97kb</size><source_type>D</source_type></version><title>Introduction to local certification</title><authors>Laurent Feuilloley</authors><categories>cs.DC cs.DS</categories><comments>Last update: minor editing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed graph algorithm is basically an algorithm where every node of a
graph can look at its neighborhood at some distance in the graph and chose its
output. As distributed environment are subject to faults, an important issue is
to be able to check that the output is correct, or in general that the network
is in proper configuration with respect to some predicate. One would like this
checking to be very local, to avoid using too much resources. Unfortunately
most predicates cannot be checked this way, and that is where certification
comes into play. Local certification (also known as proof-labeling schemes,
locally checkable proofs or distributed verification) consists in assigning
labels to the nodes, that certify that the configuration is correct. There are
several point of view on this topic: it can be seen as a part of
self-stabilizing algorithms, as labeling problem, or as a non-deterministic
distributed decision.
  This paper is an introduction to the domain of local certification, giving an
overview of the history, the techniques and the current research directions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.14379</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.14379</id><submitter>Sergey Rybakov</submitter><version version="v1"><date>Thu, 31 Oct 2019 11:21:46 GMT</date><size>6kb</size></version><title>A family of K3 surfaces and towers of algebraic curves over finite
  fields</title><authors>Sergey Galkin, Sergey Rybakov</authors><categories>math.AG cs.IT math.IT math.NT</categories><journal-ref>Math Notes 106, 1014--1018 (2019)</journal-ref><doi>10.1134/S0001434619110385</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a family of K3 surfaces we implement a variation of a general
construction of towers of algebraic curves over finite fields given in a
previous paper. As a result we get a good tower over $k=\mathbb{F}_{p^2}$, that
is optimal if $p=3$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.14560</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.14560</id><submitter>David Naumann</submitter><version version="v1"><date>Thu, 31 Oct 2019 16:05:54 GMT</date><size>153kb</size></version><version version="v2"><date>Fri, 1 Nov 2019 01:02:26 GMT</date><size>153kb</size></version><version version="v3"><date>Fri, 30 Oct 2020 03:23:54 GMT</date><size>171kb</size></version><version version="v4"><date>Sun, 30 May 2021 04:59:56 GMT</date><size>202kb</size></version><title>A Relational Program Logic with Data Abstraction and Dynamic Framing</title><authors>Anindya Banerjee and Ramana Nagasamudram and David A. Naumann and
  Mohammad Nikouei</authors><categories>cs.LO cs.PL</categories><comments>Submitted for publication. Version 3: Revise exposition with more
  examples; add case study; add author; revise title, add index, minor changes
  throughout. Version 4: Improved relational second order frame rule; revised
  and expanded examples and description of prototype; polish exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a paper published in 1972 Hoare articulated the fundamental notions of
hiding invariants and simulations. Hiding: invariants on encapsulated data
representations need not be mentioned in specifications that comprise the API
of a module. Simulation: correctness of a new data representation and
implementation can be established by proving simulation between the old and new
implementations using a coupling relation defined on the encapsulated state.
These results were formalized semantically and for a simple model of state,
though the paper claimed this could be extended to encompass dynamically
allocated objects. In recent years, progress has been made towards formalizing
the claim, for simulation, though mainly in semantic developments. In this
paper, the ideas are combined with the idea in Hoare's 1969 paper: a logic of
programs. For a language with dynamically allocated shared mutable objects, we
introduce a relational Hoare logic that formalizes encapsulation, hiding of
invariants, and relating two implementations via coupling relations. Relations
and other assertions are expressed in first order logic. Specifications can
express a wide range of relational properties such as conditional equivalence
and noninterference with declassification. The proof rules facilitate reasoning
by means of convenient alignments and are shown sound with respect to a
conventional operational semantics. Applicability to representative examples of
data abstraction is demonstrated using an SMT-based implementation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.14657</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.14657</id><submitter>Andreas Stein</submitter><version version="v1"><date>Thu, 31 Oct 2019 17:50:42 GMT</date><size>585kb</size></version><version version="v2"><date>Fri, 14 Aug 2020 15:35:22 GMT</date><size>598kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 13:52:21 GMT</date><size>196kb</size><source_type>D</source_type></version><title>Stochastic Transport with L\'evy Noise -- Fully Discrete Numerical
  Approximation</title><authors>Andrea Barth, Andreas Stein</authors><categories>math.NA cs.NA math.PR</categories><msc-class>60H15, 60H35, 35R60, 60G51, 60J76, 65M15, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semilinear hyperbolic stochastic partial differential equations have various
applications in the natural and engineering sciences. From a modeling point of
view the Gaussian setting may be too restrictive, since applications in
mathematical finance and phenomena such as porous media or pollution models
indicate an influence of noise of a different nature. In order to capture
temporal discontinuities and allow for heavy-tailed distributions, Hilbert
space-valued L\'evy processes (or L\'evy fields) as driving noise terms are
considered. The numerical discretization of the corresponding SPDE involves
several difficulties: Low spatial and temporal regularity of the solution to
the problem entails slow convergence rates and instabilities for
space/time-discretization schemes. Furthermore, the L\'evy process admits
values in a possibly infinite-dimensional Hilbert space, hence projections onto
a finite-dimensional subspace for each discrete point in time are necessary.
Finally, unbiased sampling from the resulting L\'evy field may not be possible.
We introduce a novel fully discrete approximation scheme that addresses all of
these aspects. Our central contribution is a novel discontinuous
Petrov-Galerkin scheme for the spatial approximation that naturally arises from
the weak formulation of the SPDE. We prove optimal convergence of this approach
and couple it with a suitable time stepping scheme to avoid numerical
oscillations. Moreover, we approximate the driving noise process by truncated
Karhunen-Lo\'eve expansions. The latter essentially yields a sum of scaled and
uncorrelated one-dimensional L\'evy processes, which may be simulated with
controlled bias by Fourier inversion techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.00195</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.00195</id><submitter>Chen Zhao</submitter><version version="v1"><date>Fri, 1 Nov 2019 04:14:19 GMT</date><size>3018kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 27 Feb 2020 02:55:26 GMT</date><size>3169kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 09:42:19 GMT</date><size>3982kb</size><source_type>D</source_type></version><title>Rotation Invariant Point Cloud Classification: Where Local Geometry
  Meets Global Topology</title><authors>Chen Zhao, Jiaqi Yang, Xin Xiong, Angfan Zhu, Zhiguo Cao, Xin Li</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point cloud analysis is a fundamental task in 3D computer vision. Most
previous works have conducted experiments on synthetic datasets with
well-aligned data; while real-world point clouds are often not pre-aligned. How
to achieve rotation invariance remains an open problem in point cloud analysis.
To meet this challenge, we propose a novel approach toward achieving
rotation-invariant (RI) representations by combining local geometry with global
topology. In our local-global-representation (LGR)-Net, we have designed a
two-branch network where one stream encodes local geometric RI features and the
other encodes global topology-preserving RI features. Motivated by the
observation that local geometry and global topology have different yet
complementary RI responses in varying regions, two-branch RI features are fused
by an innovative multi-layer perceptron (MLP) based attention module. To the
best of our knowledge, this work is the first principled approach toward
adaptively combining global and local information under the context of RI point
cloud analysis. Extensive experiments have demonstrated that our LGR-Net
achieves the state-of-the-art performance on various rotation-augmented
versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.01542</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.01542</id><submitter>Florian Heimerl</submitter><version version="v1"><date>Tue, 5 Nov 2019 00:06:41 GMT</date><size>2841kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 00:44:34 GMT</date><size>15149kb</size><source_type>D</source_type></version><title>embComp: Visual Interactive Comparison of Vector Embeddings</title><authors>Florian Heimerl, Christoph Kralj, Torsten M\&quot;oller, Michael Gleicher</authors><categories>cs.HC cs.CL</categories><comments>published in IEEE Transactions on Visualization and Computer Graphics
  (2020)</comments><doi>10.1109/TVCG.2020.3045918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces embComp, a novel approach for comparing two embeddings
that capture the similarity between objects, such as word and document
embeddings. We survey scenarios where comparing these embedding spaces is
useful. From those scenarios, we derive common tasks, introduce visual analysis
methods that support these tasks, and combine them into a comprehensive system.
One of embComp's central features are overview visualizations that are based on
metrics for measuring differences in the local structure around objects.
Summarizing these local metrics over the embeddings provides global overviews
of similarities and differences. Detail views allow comparison of the local
structure around selected objects and relating this local information to the
global views. Integrating and connecting all of these components, embComp
supports a range of analysis workflows that help understand similarities and
differences between embedding spaces. We assess our approach by applying it in
several use cases, including understanding corpora differences via word vector
embeddings, and understanding algorithmic differences in generating embeddings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.03070</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.03070</id><submitter>Mozhi Zhang</submitter><version version="v1"><date>Fri, 8 Nov 2019 06:07:25 GMT</date><size>238kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 8 Apr 2020 17:58:45 GMT</date><size>261kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 7 Oct 2020 17:49:50 GMT</date><size>797kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 17:20:11 GMT</date><size>810kb</size><source_type>D</source_type></version><title>Interactive Refinement of Cross-Lingual Word Embeddings</title><authors>Michelle Yuan, Mozhi Zhang, Benjamin Van Durme, Leah Findlater, Jordan
  Boyd-Graber</authors><categories>cs.CL cs.LG</categories><comments>EMNLP 2020; first two authors contribute equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-lingual word embeddings transfer knowledge between languages: models
trained on high-resource languages can predict in low-resource languages. We
introduce CLIME, an interactive system to quickly refine cross-lingual word
embeddings for a given classification problem. First, CLIME ranks words by
their salience to the downstream task. Then, users mark similarity between
keywords and their nearest neighbors in the embedding space. Finally, CLIME
updates the embeddings using the annotations. We evaluate CLIME on identifying
health-related text in four low-resource languages: Ilocano, Sinhalese,
Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word
semantics and have higher test accuracy than the original embeddings. CLIME
often improves accuracy faster than an active learning baseline and can be
easily combined with active learning to improve results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.03663</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.03663</id><submitter>Dongyeop Kang</submitter><version version="v1"><date>Sat, 9 Nov 2019 10:55:34 GMT</date><size>5843kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 00:41:40 GMT</date><size>11923kb</size><source_type>D</source_type></version><title>Style is NOT a single variable: Case Studies for Cross-Style Language
  Understanding</title><authors>Dongyeop Kang, Eduard Hovy</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every natural text is written in some style. Style is formed by a complex
combination of different stylistic factors, including formality markers,
emotions, metaphors, etc. One cannot form a complete understanding of a text
without considering these factors. The factors combine and co-vary in complex
ways to form styles. Studying the nature of the co-varying combinations sheds
light on stylistic language in general, sometimes called cross-style language
understanding. This paper provides the benchmark corpus (xSLUE) that combines
existing datasets and collects a new one for sentence-level cross-style
language understanding and evaluation. The benchmark contains text in 15
different styles under the proposed four theoretical groupings: figurative,
personal, affective, and interpersonal groups. For valid evaluation, we collect
an additional diagnostic set by annotating all 15 styles on the same text.
Using xSLUE, we propose three interesting cross-style applications in
classification, correlation, and generation. First, our proposed cross-style
classifier trained with multiple styles together helps improve overall
classification performance against individually-trained style classifiers.
Second, our study shows that some styles are highly dependent on each other in
human-written text. Finally, we find that combinations of some contradictive
styles likely generate stylistically less appropriate text. We believe our
benchmark and case studies help explore interesting future directions for
cross-style research. The preprocessed datasets and code are publicly
available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.04436</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.04436</id><submitter>Changxiao Cai</submitter><version version="v1"><date>Mon, 11 Nov 2019 18:21:26 GMT</date><size>212kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 03:32:06 GMT</date><size>352kb</size></version><title>Nonconvex Low-Rank Tensor Completion from Noisy Data</title><authors>Changxiao Cai, Gen Li, H. Vincent Poor, Yuxin Chen</authors><categories>cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH</categories><comments>Accepted to Operations Research</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study a noisy tensor completion problem of broad practical interest,
namely, the reconstruction of a low-rank tensor from highly incomplete and
randomly corrupted observations of its entries. While a variety of prior work
has been dedicated to this problem, prior algorithms either are computationally
too expensive for large-scale applications, or come with sub-optimal
statistical guarantees. Focusing on &quot;incoherent&quot; and well-conditioned tensors
of a constant CP rank, we propose a two-stage nonconvex algorithm -- (vanilla)
gradient descent following a rough initialization -- that achieves the best of
both worlds. Specifically, the proposed nonconvex algorithm faithfully
completes the tensor and retrieves all individual tensor factors within nearly
linear time, while at the same time enjoying near-optimal statistical
guarantees (i.e. minimal sample complexity and optimal estimation accuracy).
The estimation errors are evenly spread out across all entries, thus achieving
optimal $\ell_{\infty}$ statistical accuracy. We have also discussed how to
extend our approach to accommodate asymmetric tensors. The insight conveyed
through our analysis of nonconvex optimization might have implications for
other tensor estimation problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.04788</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.04788</id><submitter>Carlo Tiseo</submitter><version version="v1"><date>Tue, 12 Nov 2019 10:54:20 GMT</date><size>3666kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Dec 2020 11:42:59 GMT</date><size>4803kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 May 2021 18:36:18 GMT</date><size>8526kb</size><source_type>D</source_type></version><title>Fractal Impedance for Passive Controllers</title><authors>Keyhan Kouhkiloui Babarahmati, Carlo Tiseo, Joshua Smith, Hsiu Chin
  Lin, Mustafa Suphi Erden and Michael Mistry</authors><categories>cs.RO</categories><comments>Video Available at https://youtu.be/S06_hqn3NvM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is increasing interest in control frameworks capable of moving robots
from industrial cages to unstructured environments and coexisting with humans.
Despite significant improvement in some specific applications (e.g., medical
robotics), there is still the need for a general control framework that
improves interaction robustness and motion dynamics. Passive controllers show
promising results in this direction; however, they often rely on virtual energy
tanks that can guarantee passivity as long as they do not run out of energy. In
this paper, a fractal attractor is proposed to implement a variable impedance
controller that can retain passivity without relying on energy tanks. The
controller generates a fractal attractor around the desired state using an
asymptotic stable potential field, making the controller robust to
discretization and numerical integration errors. The results prove that it can
accurately track both trajectories and end-effector forces during interaction.
Therefore, these properties make the controller ideal for applications
requiring robust dynamic interaction at the end-effector.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.06819</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.06819</id><submitter>Sebastian Blauth</submitter><version version="v1"><date>Fri, 15 Nov 2019 16:25:41 GMT</date><size>15374kb</size><source_type>AD</source_type></version><version version="v2"><date>Thu, 5 Nov 2020 06:51:41 GMT</date><size>5486kb</size><source_type>D</source_type></version><title>Model Hierarchy for the Shape Optimization of a Microchannel Cooling
  System</title><authors>Sebastian Blauth, Christian Leith\&quot;auser, Ren\'e Pinnau</authors><categories>math.OC cs.NA math.NA</categories><msc-class>49Q10, 65K05, 35Q35, 76D55</msc-class><journal-ref>ZAMM 101 (2021), no. 4</journal-ref><doi>10.1002/zamm.202000166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model a microchannel cooling system and consider the optimization of its
shape by means of shape calculus. A three-dimensional model covering all
relevant physical effects and three reduced models are introduced. The latter
are derived via a homogenization of the geometry in 3D and a transformation of
the three-dimensional models to two dimensions. A shape optimization problem
based on the tracking of heat absorption by the cooler and the uniform
distribution of the flow through the microchannels is formulated and adapted to
all models. We present the corresponding shape derivatives and adjoint systems,
which we derived with a material derivative free adjoint approach. To
demonstrate the feasibility of the reduced models, the optimization problems
are solved numerically with a gradient descent method. A comparison of the
results shows that the reduced models perform similarly to the original one
while using significantly less computational resources.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.12099</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.12099</id><submitter>Matteo Croci</submitter><version version="v1"><date>Wed, 27 Nov 2019 12:03:28 GMT</date><size>641kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 18:31:36 GMT</date><size>211kb</size></version><title>Multilevel quasi Monte Carlo methods for elliptic PDEs with random field
  coefficients via fast white noise sampling</title><authors>M. Croci, M. B. Giles, P. E. Farrell</authors><categories>math.NA cs.NA</categories><msc-class>65C05, 60G60, 65N30, 60H35, 35R60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When solving partial differential equations with random fields as
coefficients the efficient sampling of random field realisations can be
challenging. In this paper we focus on the fast sampling of Gaussian fields
using quasi-random points in a finite element and multilevel quasi Monte Carlo
(MLQMC) setting. Our method uses the SPDE approach of Lindgren et al.~combined
with a new fast algorithm for white noise sampling which is taylored to
(ML)QMC. We express white noise as a wavelet series expansion that we divide in
two parts. The first part is sampled using quasi-random points and contains a
finite number of terms in order of decaying importance to ensure good QMC
convergence. The second part is a correction term which is sampled using
standard pseudo-random numbers. We show how the sampling of both terms can be
performed in linear time and memory complexity in the number of mesh cells via
a supermesh construction, yielding an overall linear cost. Furthermore, our
technique can be used to enforce the MLQMC coupling even in the case of
non-nested mesh hierarchies. We demonstrate the efficacy of our method with
numerical experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.12258</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.12258</id><submitter>Nuri Mert Vural</submitter><version version="v1"><date>Mon, 25 Nov 2019 19:34:31 GMT</date><size>483kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 28 Nov 2019 08:28:41 GMT</date><size>483kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 7 Mar 2020 16:06:22 GMT</date><size>483kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 15:32:54 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Stability of the Decoupled Extended Kalman Filter Learning Algorithm in
  LSTM-Based Online Learning</title><authors>Nuri Mert Vural, Fatih Ilhan and Suleyman S. Kozat</authors><categories>cs.LG eess.SP stat.ML</categories><comments>This paper was an early draft of the presented results. We have
  written and published another paper (arXiv:1911.12258) where we have improved
  on the material in this paper. The published paper covers most of the
  material presented in this paper as well. Therefore, we remove this paper
  from Arxiv and refer the interested readers to arXiv:1911.12258</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the convergence and stability properties of the decoupled
extended Kalman filter learning algorithm (DEKF) within the long-short term
memory network (LSTM) based online learning framework. For this purpose, we
model DEKF as a perturbed extended Kalman filter and derive sufficient
conditions for its stability during LSTM training. We show that if the
perturbations -- introduced due to decoupling -- stay bounded, DEKF learns LSTM
parameters with similar convergence and stability properties of the global
extended Kalman filter learning algorithm. We verify our results with several
numerical simulations and compare DEKF with other LSTM training methods. In our
simulations, we also observe that the well-known hyper-parameter selection
approaches used for DEKF in the literature satisfy our conditions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.01683</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.01683</id><submitter>Alexander Turner</submitter><version version="v1"><date>Tue, 3 Dec 2019 20:45:49 GMT</date><size>845kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 19 Jan 2020 19:25:51 GMT</date><size>43kb</size></version><version version="v3"><date>Mon, 13 Apr 2020 14:56:27 GMT</date><size>47kb</size></version><version version="v4"><date>Tue, 14 Apr 2020 22:13:56 GMT</date><size>47kb</size></version><version version="v5"><date>Fri, 5 Jun 2020 22:41:45 GMT</date><size>40kb</size></version><version version="v6"><date>Wed, 2 Dec 2020 21:40:39 GMT</date><size>54kb</size><source_type>D</source_type></version><version version="v7"><date>Tue, 1 Jun 2021 16:59:04 GMT</date><size>533kb</size><source_type>D</source_type></version><title>Optimal Policies Tend to Seek Power</title><authors>Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, Prasad
  Tadepalli</authors><categories>cs.AI</categories><comments>NeurIPS 2021 submission. 12 pages, 42 pages with references and
  appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some researchers speculate that intelligent reinforcement learning (RL)
agents would be incentivized to seek resources and power in pursuit of their
objectives. Other researchers are skeptical, because human-like power-seeking
instincts need not be present in RL agents. To clarify this debate, we develop
the first formal theory of the statistical tendencies of optimal policies in
reinforcement learning. In the context of Markov decision processes, we prove
that certain environmental symmetries are sufficient for optimal policies to
tend to seek power over the environment. These symmetries exist in many
environments in which the agent can be shut down or destroyed. We prove that
for most prior beliefs one might have about the agent's reward function
(including as a special case the situations where the reward function is
known), one should expect optimal policies to seek power in these environments.
These policies seek power by keeping a range of options available and, when the
discount rate is sufficiently close to 1, by navigating towards larger sets of
potential terminal states.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.01875</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.01875</id><submitter>Yiming He</submitter><version version="v1"><date>Wed, 4 Dec 2019 10:13:06 GMT</date><size>3819kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 11 Mar 2020 08:57:48 GMT</date><size>2874kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 12 Mar 2020 13:28:50 GMT</date><size>2874kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 13:06:47 GMT</date><size>4235kb</size><source_type>D</source_type></version><title>3D Hand Pose Estimation via Regularized Graph Representation Learning</title><authors>Yiming He, Wei Hu</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of 3D hand pose estimation from a monocular
RGB image. While previous methods have shown great success, the structure of
hands has not been fully exploited, which is critical in pose estimation. To
this end, we propose a regularized graph representation learning under a
conditional adversarial learning framework for 3D hand pose estimation, aiming
to capture structural inter-dependencies of hand joints. In particular, we
estimate an initial hand pose from a parametric hand model as a prior of hand
structure, which regularizes the inference of the structural deformation in the
prior pose for accurate graph representation learning via residual graph
convolution. To optimize the hand structure further, we propose two
bone-constrained loss functions, which characterize the morphable structure of
hand poses explicitly. Also, we introduce an adversarial learning framework
conditioned on the input image with a multi-source discriminator, which imposes
the structural constraints onto the distribution of generated 3D hand poses for
anthropomorphically valid hand poses. Extensive experiments demonstrate that
our model sets the new state-of-the-art in 3D hand pose estimation from a
monocular image on five standard benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.04783</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.04783</id><submitter>Xavier Boix</submitter><version version="v1"><date>Tue, 10 Dec 2019 15:53:45 GMT</date><size>7502kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 21 Dec 2019 19:41:16 GMT</date><size>7503kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 1 Jul 2020 16:20:23 GMT</date><size>900kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 17 Sep 2020 02:56:07 GMT</date><size>6088kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 23:42:59 GMT</date><size>12612kb</size><source_type>D</source_type></version><title>Frivolous Units: Wider Networks Are Not Really That Wide</title><authors>Stephen Casper, Xavier Boix, Vanessa D'Amario, Ling Guo, Martin
  Schrimpf, Kasper Vinken, Gabriel Kreiman</authors><categories>cs.LG cs.CV stat.ML</categories><journal-ref>Proceedings of the AAAI Conference on Artificial Intelligence,
  2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A remarkable characteristic of overparameterized deep neural networks (DNNs)
is that their accuracy does not degrade when the network's width is increased.
Recent evidence suggests that developing compressible representations is key
for adjusting the complexity of large networks to the learning task at hand.
However, these compressible representations are poorly understood. A promising
strand of research inspired from biology is understanding representations at
the unit level as it offers a more granular and intuitive interpretation of the
neural mechanisms. In order to better understand what facilitates increases in
width without decreases in accuracy, we ask: Are there mechanisms at the unit
level by which networks control their effective complexity as their width is
increased? If so, how do these depend on the architecture, dataset, and
training parameters? We identify two distinct types of &quot;frivolous&quot; units that
proliferate when the network's width is increased: prunable units which can be
dropped out of the network without significant change to the output and
redundant units whose activities can be expressed as a linear combination of
others. These units imply complexity constraints as the function the network
represents could be expressed by a network without them. We also identify how
the development of these units can be influenced by architecture and a number
of training factors. Together, these results help to explain why the accuracy
of DNNs does not degrade when width is increased and highlight the importance
of frivolous units toward understanding implicit regularization in DNNs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.09787</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.09787</id><submitter>Nirav Vasant Shah Mr.</submitter><version version="v1"><date>Fri, 20 Dec 2019 12:21:12 GMT</date><size>1665kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 25 May 2020 15:42:44 GMT</date><size>2529kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 21 Jun 2020 21:02:34 GMT</date><size>2529kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 25 Jun 2020 10:29:54 GMT</date><size>2529kb</size><source_type>D</source_type></version><title>Discontinuous Galerkin Model Order Reduction of Geometrically
  Parametrized Stokes Equation</title><authors>Nirav Vasant Shah, Martin Hess and Gianluigi Rozza</authors><categories>math.NA cs.NA</categories><comments>9 pages, 11 figures, 9 references, Submitted to European Numerical
  Mathematics and Advanced Applications Conference 2019</comments><doi>10.1007/978-3-030-55874-1_54</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work focuses on the geometric parametrization and the reduced
order modeling of the Stokes equation. We discuss the concept of a parametrized
geometry and its application within a reduced order modeling technique. The
full order model is based on the discontinuous Galerkin method with an interior
penalty formulation. We introduce the broken Sobolev spaces as well as the weak
formulation required for an affine parameter dependency. The operators are
transformed from a fixed domain to a parameter dependent domain using the
affine parameter dependency. The proper orthogonal decomposition is used to
obtain the basis of functions of the reduced order model. By using the Galerkin
projection the linear system is projected onto the reduced space. During this
process, the offline-online decomposition is used to separate parameter
dependent operations from parameter independent operations. Finally this
technique is applied to an obstacle test problem.The numerical outcomes
presented include experimental error analysis, eigenvalue decay and measurement
of online simulation time. Keywords: Discontinuous Galerkin method, Stokes
flow, Geometric parametrization, Proper orthogonal decomposition
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.11108</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.11108</id><submitter>Sagi Marcovich</submitter><version version="v1"><date>Mon, 23 Dec 2019 21:08:03 GMT</date><size>40kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 14:21:24 GMT</date><size>44kb</size></version><title>Reconstruction of Strings from their Substrings Spectrum</title><authors>Sagi Marcovich and Eitan Yaakobi</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies reconstruction of strings based upon their substrings
spectrum. Under this paradigm, it is assumed that all substrings of some fixed
length are received and the goal is to reconstruct the string. While many
existing works assumed that substrings are received error free, we follow in
this paper the noisy setup of this problem that was first studied by Gabrys and
Milenkovic. The goal of this study is twofold. First we study the setup in
which not all substrings in the multispectrum are received, and then we focus
on the case where the read substrings are not error free. In each case we
provide specific code constructions of strings that their reconstruction is
guaranteed even in the presence of failure in either model. We present
efficient encoding and decoding maps and analyze the cardinality of the code
constructions, while studying the cases where the rates of our codes approach
1.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.11347</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.11347</id><submitter>Yunpeng Shi</submitter><version version="v1"><date>Tue, 24 Dec 2019 13:41:00 GMT</date><size>51kb</size></version><version version="v2"><date>Sun, 30 May 2021 01:38:27 GMT</date><size>1584kb</size><source_type>D</source_type></version><title>Robust Group Synchronization via Cycle-Edge Message Passing</title><authors>Gilad Lerman and Yunpeng Shi</authors><categories>stat.ML cs.IT math.IT math.OC math.PR</categories><msc-class>90-08, 60-08, 68W01, 68Q87</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for solving the group synchronization problem,
where we focus on the setting of adversarial or uniform corruption and
sufficiently small noise. Specifically, we apply a novel message passing
procedure that uses cycle consistency information in order to estimate the
corruption levels of group ratios and consequently solve the synchronization
problem in our setting. We first explain why the group cycle consistency
information is essential for effectively solving group synchronization
problems. We then establish exact recovery and linear convergence guarantees
for the proposed message passing procedure under a deterministic setting with
adversarial corruption. These guarantees hold as long as the ratio of corrupted
cycles per edge is bounded by a reasonable constant. We also establish the
stability of the proposed procedure to sub-Gaussian noise. We further establish
exact recovery with high probability under a common uniform corruption model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.00081</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.00081</id><submitter>Patrick Kelley</submitter><version version="v1"><date>Fri, 27 Dec 2019 10:27:05 GMT</date><size>1117kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 17:20:34 GMT</date><size>2350kb</size><source_type>D</source_type></version><title>Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial
  Intelligence in 8 Countries</title><authors>Patrick Gage Kelley, Yongwei Yang, Courtney Heldreth, Christopher
  Moessner, Aaron Sedley, Andreas Kramm, David T. Newman, and Allison Woodruff</authors><categories>cs.CY cs.AI</categories><comments>12 pages, 2 figures, 3 tables. AIES 2021: Proceedings of the AAAI/ACM
  Conference on AI, Ethics, and Society</comments><acm-class>K.4.1; I.2</acm-class><doi>10.1145/3461702.3462605</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the influence and use of artificial intelligence (AI) have grown and its
transformative potential has become more apparent, many questions have been
raised regarding the economic, political, social, and ethical implications of
its use. Public opinion plays an important role in these discussions,
influencing product adoption, commercial development, research funding, and
regulation. In this paper we present results of an in-depth survey of public
opinion of artificial intelligence conducted with 10,005 respondents spanning
eight countries and six continents. We report widespread perception that AI
will have significant impact on society, accompanied by strong support for the
responsible development and use of AI, and also characterize the public's
sentiment towards AI with four key themes (exciting, useful, worrying, and
futuristic) whose prevalence distinguishes response to AI in different
countries.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.01414</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.01414</id><submitter>Debraj Chakraborty</submitter><version version="v1"><date>Mon, 6 Jan 2020 05:52:50 GMT</date><size>828kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 11:05:47 GMT</date><size>2548kb</size><source_type>D</source_type></version><title>A time-optimal feedback control for a particular case of the game of two
  cars</title><authors>Aditya Chaudhari and Debraj Chakraborty</authors><categories>eess.SY cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a time-optimal feedback solution to the game of two cars, for
the case where the pursuer is faster and more agile than the evader, is
presented. The concept of continuous subsets of the reachable set is introduced
to characterize the time-optimal pursuit-evasion game under feedback
strategies. Using these subsets it is shown that, if initially the pursuer is
distant enough from the evader, then the feedback saddle point strategies for
both the pursuer and the evader are coincident with one of the common tangents
from the minimum radius turning circles of the pursuer to the minimum radius
turning circles of the evader. Using geometry, four feasible tangents are
identified and the feedback min-max strategy for the pursuer and the max-min
strategy for the evader are derived by solving a $2 \times 2$ matrix game at
each instant. Insignificant computational effort is involved in evaluating the
pursuer and evader inputs using the proposed feedback control law and hence it
is suitable for real-time implementation. The proposed law is validated further
by comparing the resulting trajectories with those obtained by solving the
differential game using numerical techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.01597</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.01597</id><submitter>Pankaj Mishra</submitter><version version="v1"><date>Thu, 2 Jan 2020 18:11:38 GMT</date><size>3472kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 10 May 2021 16:05:52 GMT</date><size>19409kb</size><source_type>D</source_type></version><title>RBF-FD analysis of 2D time-domain acoustic wave propagation in
  heterogeneous media</title><authors>Jure Mo\v{c}nik - Berljavac, Pankaj K Mishra, Jure Slak, and Gregor
  Kosec</authors><categories>cs.CE cs.NA math.NA physics.geo-ph</categories><comments>To reproduce the numerical tests in this paper, please see the
  project repository
  \url{https://gitlab.com/e62Lab/2019_p_wavepropagation_code}</comments><journal-ref>Computers &amp; Geosciences 153 (2021)</journal-ref><doi>10.1016/j.cageo.2021.104796</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radial Basis Function-generated Finite Differences (RBF-FD) is a popular
variant of local strong-form meshless methods that do not require a predefined
connection between the nodes, making it easier to adapt node-distribution to
the problem under consideration. This paper investigates an RBF-FD solution of
time-domain acoustic wave propagation in the context of seismic modeling in the
Earth's subsurface. Through a number of numerical tests, ranging from
homogeneous to highly-heterogeneous velocity models including non-smooth
irregular topography, we demonstrate that the present approach can be further
generalized to solve large-scale seismic modeling and full waveform inversion
problems in arbitrarily complex models enabling more robust interpretations of
geophysical observations
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.02889</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.02889</id><submitter>Duligur Ibeling</submitter><version version="v1"><date>Thu, 9 Jan 2020 08:52:14 GMT</date><size>34kb</size></version><version version="v2"><date>Sat, 8 Feb 2020 16:03:38 GMT</date><size>25kb</size></version><version version="v3"><date>Thu, 18 Jun 2020 17:40:29 GMT</date><size>31kb</size></version><version version="v4"><date>Wed, 29 Jul 2020 23:55:30 GMT</date><size>32kb</size></version><version version="v5"><date>Wed, 2 Jun 2021 08:14:53 GMT</date><size>35kb</size></version><title>Probabilistic Reasoning across the Causal Hierarchy</title><authors>Duligur Ibeling, Thomas Icard</authors><categories>cs.LO cs.AI</categories><comments>AAAI-20</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formalization of the three-tier causal hierarchy of association,
intervention, and counterfactuals as a series of probabilistic logical
languages. Our languages are of strictly increasing expressivity, the first
capable of expressing quantitative probabilistic reasoning -- including
conditional independence and Bayesian inference -- the second encoding
do-calculus reasoning for causal effects, and the third capturing a fully
expressive do-calculus for arbitrary counterfactual queries. We give a
corresponding series of finitary axiomatizations complete over both structural
causal models and probabilistic programs, and show that satisfiability and
validity for each language are decidable in polynomial space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.03108</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.03108</id><submitter>Song Fang</submitter><version version="v1"><date>Thu, 9 Jan 2020 17:11:40 GMT</date><size>394kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 14 Jan 2020 19:56:28 GMT</date><size>394kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 2 Feb 2021 16:38:29 GMT</date><size>395kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 28 May 2021 15:41:08 GMT</date><size>411kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 15:13:25 GMT</date><size>411kb</size><source_type>D</source_type></version><title>Feedback Capacity and a Variant of the Kalman Filter with ARMA Gaussian
  Noises: Explicit Bounds and Feedback Coding Design</title><authors>Song Fang and Quanyan Zhu</authors><categories>cs.IT cs.LG cs.SY eess.SP eess.SY math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we relate a feedback channel with any finite-order
autoregressive moving-average (ARMA) Gaussian noises to a variant of the Kalman
filter. In light of this, we obtain relatively explicit lower bounds on the
feedback capacity for such colored Gaussian noises, and the bounds are seen to
be consistent with various existing results in the literature. Meanwhile, this
variant of the Kalman filter also leads to explicit recursive coding schemes
with clear structures to achieve the lower bounds. In general, our results
provide an alternative perspective while pointing to potentially tighter bounds
for the feedback capacity problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.03246</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.03246</id><submitter>Jeffrey Ding</submitter><version version="v1"><date>Thu, 9 Jan 2020 22:16:05 GMT</date><size>420kb</size></version><version version="v2"><date>Mon, 31 May 2021 15:16:10 GMT</date><size>585kb</size></version><title>The Logic of Strategic Assets: From Oil to Artificial Intelligence</title><authors>Jeffrey Ding and Allan Dafoe</authors><categories>econ.GN cs.CY q-fin.EC</categories><comments>Added references and corrected typos</comments><doi>10.1080/09636412.2021.1915583</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What resources and technologies are strategic? This question is often the
focus of policy and theoretical debates, where the label &quot;strategic&quot; designates
those assets that warrant the attention of the highest levels of the state. But
these conversations are plagued by analytical confusion, flawed heuristics, and
the rhetorical use of &quot;strategic&quot; to advance particular agendas. We aim to
improve these conversations through conceptual clarification, introducing a
theory based on important rivalrous externalities for which socially optimal
behavior will not be produced alone by markets or individual national security
entities. We distill and theorize the most important three forms of these
externalities, which involve cumulative-, infrastructure-, and
dependency-strategic logics. We then employ these logics to clarify three
important cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology
rivalry in the late 1980s, and contemporary conversations about artificial
intelligence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.03608</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.03608</id><submitter>Samira Pakravan</submitter><version version="v1"><date>Fri, 10 Jan 2020 18:46:50 GMT</date><size>2552kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 6 Apr 2020 06:16:24 GMT</date><size>4632kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 18 Nov 2020 21:47:28 GMT</date><size>5909kb</size><source_type>D</source_type></version><title>Solving inverse-PDE problems with physics-aware neural networks</title><authors>Samira Pakravan, Pouria A. Mistani, Miguel Angel Aragon-Calvo,
  Frederic Gibou</authors><categories>math.NA cs.LG cs.NA physics.comp-ph</categories><comments>39 pages, 17 figures</comments><doi>10.1016/j.jcp.2021.110414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel composite framework to find unknown fields in the context
of inverse problems for partial differential equations (PDEs). We blend the
high expressibility of deep neural networks as universal function estimators
with the accuracy and reliability of existing numerical algorithms for partial
differential equations as custom layers in semantic autoencoders. Our design
brings together techniques of computational mathematics, machine learning and
pattern recognition under one umbrella to incorporate domain-specific knowledge
and physical constraints to discover the underlying hidden fields. The network
is explicitly aware of the governing physics through a hard-coded PDE solver
layer in contrast to most existing methods that incorporate the governing
equations in the loss function or rely on trainable convolutional layers to
discover proper discretizations from data. This subsequently focuses the
computational load to only the discovery of the hidden fields and therefore is
more data efficient. We call this architecture Blended inverse-PDE networks
(hereby dubbed BiPDE networks) and demonstrate its applicability for recovering
the variable diffusion coefficient in Poisson problems in one and two spatial
dimensions, as well as the diffusion coefficient in the time-dependent and
nonlinear Burgers' equation in one dimension. We also show that this approach
is robust to noise.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.03955</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.03955</id><submitter>Masoumeh Soflaei</submitter><version version="v1"><date>Sun, 12 Jan 2020 16:22:24 GMT</date><size>206kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 10 Jul 2020 01:43:20 GMT</date><size>53kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 16:42:00 GMT</date><size>266kb</size><source_type>D</source_type></version><title>Aggregated Learning: A Vector-Quantization Approach to Learning Neural
  Network Classifiers</title><authors>Masoumeh Soflaei, Hongyu Guo, Ali Al-Bashabsheh, Yongyi Mao, Richong
  Zhang</authors><categories>cs.LG stat.ML</categories><comments>Proof of theoretical results are provided</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a neural network classifier. Under the
information bottleneck (IB) principle, we associate with this classification
problem a representation learning problem, which we call &quot;IB learning&quot;. We show
that IB learning is, in fact, equivalent to a special class of the quantization
problem. The classical results in rate-distortion theory then suggest that IB
learning can benefit from a &quot;vector quantization&quot; approach, namely,
simultaneously learning the representations of multiple input objects. Such an
approach assisted with some variational techniques, result in a novel learning
framework, &quot;Aggregated Learning&quot;, for classification with neural network
models. In this framework, several objects are jointly classified by a single
neural network. The effectiveness of this framework is verified through
extensive experiments on standard image recognition and text classification
tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.07417</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.07417</id><submitter>Carlos Fern\'andez-Lor\'ia</submitter><version version="v1"><date>Tue, 21 Jan 2020 09:58:58 GMT</date><size>592kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 5 Feb 2020 13:28:14 GMT</date><size>593kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 9 May 2020 03:30:11 GMT</date><size>593kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 21:52:22 GMT</date><size>629kb</size><source_type>D</source_type></version><title>Explaining Data-Driven Decisions made by AI Systems: The Counterfactual
  Approach</title><authors>Carlos Fern\'andez-Lor\'ia, Foster Provost, Xintian Han</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine counterfactual explanations for explaining the decisions made by
model-based AI systems. The counterfactual approach we consider defines an
explanation as a set of the system's data inputs that causally drives the
decision (i.e., changing the inputs in the set changes the decision) and is
irreducible (i.e., changing any subset of the inputs does not change the
decision). We (1) demonstrate how this framework may be used to provide
explanations for decisions made by general, data-driven AI systems that may
incorporate features with arbitrary data types and multiple predictive models,
and (2) propose a heuristic procedure to find the most useful explanations
depending on the context. We then contrast counterfactual explanations with
methods that explain model predictions by weighting features according to their
importance (e.g., SHAP, LIME) and present two fundamental reasons why we should
carefully consider whether importance-weight explanations are well-suited to
explain system decisions. Specifically, we show that (i) features that have a
large importance weight for a model prediction may not affect the corresponding
decision, and (ii) importance weights are insufficient to communicate whether
and how features influence decisions. We demonstrate this with several concise
examples and three detailed case studies that compare the counterfactual
approach with SHAP to illustrate various conditions under which counterfactual
explanations explain data-driven decisions better than importance weights.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.08780</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.08780</id><submitter>Johann Knechtel</submitter><version version="v1"><date>Thu, 23 Jan 2020 19:38:45 GMT</date><size>2074kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 27 Jan 2020 13:15:27 GMT</date><size>2075kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 29 Jan 2020 13:59:51 GMT</date><size>2057kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 30 Sep 2020 20:27:09 GMT</date><size>2057kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 17:25:50 GMT</date><size>2055kb</size><source_type>D</source_type></version><title>Hardware Security for and beyond CMOS Technology</title><authors>Johann Knechtel</authors><categories>cs.CR cs.ET</categories><comments>[v1] ISPD'20; [v2] extended arXiv version; [v3] some references
  updated, some paragraphs further extended; [v4] ISPD'21, extended arXiv
  version</comments><doi>10.1145/3439706.3446902</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As with most aspects of electronic systems and integrated circuits, hardware
security has traditionally evolved around the dominant CMOS technology.
However, with the rise of various emerging technologies, whose main purpose is
to overcome the fundamental limitations for scaling and power consumption of
CMOS technology, unique opportunities arise also to advance the notion of
hardware security. In this paper, I first provide an overview on hardware
security in general. Next, I review selected emerging technologies, namely (i)
spintronics, (ii) memristors, (iii) carbon nanotubes and related transistors,
(iv) nanowires and related transistors, and (v) 3D and 2.5D integration. I then
discuss their application to advance hardware security and also outline related
challenges.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.09686</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.09686</id><submitter>Felix Wolf</submitter><version version="v1"><date>Mon, 27 Jan 2020 10:54:08 GMT</date><size>1313kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 8 Jun 2020 10:24:27 GMT</date><size>1313kb</size><source_type>D</source_type></version><title>Solving Maxwell's Eigenvalue Problem via Isogeometric Boundary Elements
  and a Contour Integral Method</title><authors>Stefan Kurz, Sebastian Sch\&quot;ops, Gerhard Unger, Felix Wolf</authors><categories>cs.CE cs.NA math.NA</categories><msc-class>34L16, 35P30, 65N38, 65D07</msc-class><journal-ref>Mathematical Methods in the Applied Sciences, May 2021</journal-ref><doi>10.1002/mma.7447</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We solve Maxwell's eigenvalue problem via isogeometric boundary elements and
a contour integral method. We discuss the analytic properties of the
discretisation, outline the implementation, and showcase numerical examples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.11031</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.11031</id><submitter>Jakob Knollm\&quot;uller</submitter><version version="v1"><date>Wed, 29 Jan 2020 19:00:00 GMT</date><size>5390kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 15 Jun 2020 18:00:05 GMT</date><size>5883kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 09:55:29 GMT</date><size>11460kb</size><source_type>D</source_type></version><title>Bayesian Reasoning with Trained Neural Networks</title><authors>Jakob Knollm\&quot;uller and Torsten En{\ss}lin</authors><categories>cs.LG cs.AI stat.ML</categories><journal-ref>Entropy 2021, 23(6), 693</journal-ref><doi>10.3390/e23060693</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We showed how to use trained neural networks to perform Bayesian reasoning in
order to solve tasks outside their initial scope. Deep generative models
provide prior knowledge, and classification/regression networks impose
constraints. The tasks at hand were formulated as Bayesian inference problems,
which we approximately solved through variational or sampling techniques. The
approach built on top of already trained networks, and the addressable
questions grew super-exponentially with the number of available networks. In
its simplest form, the approach yielded conditional generative models. However,
multiple simultaneous constraints constitute elaborate questions. We compared
the approach to specifically trained generators, showed how to solve riddles,
and demonstrated its compatibility with state-of-the-art architectures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.00116</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.00116</id><submitter>Jeremy Kozdon</submitter><version version="v1"><date>Sat, 1 Feb 2020 00:40:48 GMT</date><size>227kb</size></version><version version="v2"><date>Thu, 30 Jul 2020 20:04:30 GMT</date><size>227kb</size></version><version version="v3"><date>Sat, 30 Jan 2021 04:27:41 GMT</date><size>483kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 19:16:47 GMT</date><size>483kb</size><source_type>D</source_type></version><title>Hybridized Summation-By-Parts Finite Difference Methods</title><authors>Jeremy E. Kozdon, Brittany A. Erickson, Lucas C. Wilcox</authors><categories>math.NA cs.NA</categories><comments>26 pages, 6 figures, 3 tables</comments><msc-class>65N06, 65N22, 65N12</msc-class><journal-ref>J Sci Comput 87, 85 (2021)</journal-ref><doi>10.1007/s10915-021-01448-5</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We present a hybridization technique for summation-by-parts finite difference
methods with weak enforcement of interface and boundary conditions for second
order, linear elliptic partial differential equations. The method is based on
techniques from the hybridized discontinuous Galerkin literature where local
and global problems are defined for the volume and trace grid points,
respectively. By using a Schur complement technique the volume points can be
eliminated, which drastically reduces the system size. We derive both the local
and global problems, and show that the linear systems that must be solved are
symmetric positive definite. The theoretical stability results are confirmed
with numerical experiments as is the accuracy of the method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.00253</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.00253</id><submitter>Karthik Abinav Sankararaman</submitter><version version="v1"><date>Sat, 1 Feb 2020 18:50:44 GMT</date><size>40kb</size></version><version version="v2"><date>Wed, 30 Dec 2020 22:45:16 GMT</date><size>121kb</size></version><version version="v3"><date>Mon, 3 May 2021 06:05:07 GMT</date><size>123kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 28 May 2021 16:29:16 GMT</date><size>63kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 17:18:54 GMT</date><size>66kb</size><source_type>D</source_type></version><title>Bandits with Knapsacks beyond the Worst-Case</title><authors>Karthik Abinav Sankararaman and Aleksandrs Slivkins</authors><categories>cs.LG cs.DS stat.ML</categories><comments>The initial version, titled &quot;Advances in Bandits with Knapsacks&quot;, was
  published on arxiv.org in Jan'20. The present version improves both upper and
  lower bounds, deriving Theorem 3.2(ii) and Theorem 4.2. Moreover, it
  simplifies the algorithm and analysis in the main result, and fixes several
  issues in the lower bounds</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bandits with Knapsacks (BwK) is a general model for multi-armed bandits under
supply/budget constraints. While worst-case regret bounds for BwK are
well-understood, we present three results that go beyond the worst-case
perspective. First, we provide upper and lower bounds which amount to a full
characterization for logarithmic, instance-dependent regret rates. Second, we
consider &quot;simple regret&quot; in BwK, which tracks algorithm's performance in a
given round, and prove that it is small in all but a few rounds. Third, we
provide a general &quot;reduction&quot; from BwK to bandits which takes advantage of some
known helpful structure, and apply this reduction to combinatorial
semi-bandits, linear contextual bandits, and multinomial-logit bandits. Our
results build on the BwK algorithm from \citet{AgrawalDevanur-ec14}, providing
new analyses thereof.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.01143</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.01143</id><submitter>Suthee Ruangwises</submitter><version version="v1"><date>Tue, 4 Feb 2020 06:18:47 GMT</date><size>10kb</size></version><version version="v2"><date>Tue, 3 Nov 2020 19:33:07 GMT</date><size>12kb</size></version><title>Physical Zero-Knowledge Proof for Numberlink Puzzle and $k$
  Vertex-Disjoint Paths Problem</title><authors>Suthee Ruangwises, Toshiya Itoh</authors><categories>cs.CR</categories><comments>A preliminary version of this paper has appeared in the proceedings
  of FUN 2021</comments><journal-ref>New Generation Computing, 39(1): 3-17 (2021)</journal-ref><doi>10.1007/s00354-020-00114-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numberlink is a logic puzzle with an objective to connect all pairs of cells
with the same number by non-crossing paths in a rectangular grid. In this
paper, we propose a physical protocol of zero-knowledge proof for Numberlink
using a deck of cards, which allows a prover to convince a verifier that he/she
knows a solution without revealing it. In particular, the protocol shows how to
physically count the number of elements in a list that are equal to a given
secret value without revealing that value, the positions of elements in the
list that are equal to it, or the value of any other element in the list.
Finally, we show that our protocol can be modified to verify a solution of the
well-known $k$ vertex-disjoint paths problem, both the undirected and directed
settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.02512</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.02512</id><submitter>Herman Geuvers</submitter><version version="v1"><date>Thu, 6 Feb 2020 20:58:48 GMT</date><size>36kb</size></version><version version="v2"><date>Thu, 23 Jul 2020 16:23:47 GMT</date><size>47kb</size></version><version version="v3"><date>Fri, 24 Jul 2020 08:02:45 GMT</date><size>47kb</size></version><version version="v4"><date>Thu, 7 Jan 2021 16:46:53 GMT</date><size>48kb</size></version><version version="v5"><date>Tue, 1 Jun 2021 12:40:07 GMT</date><size>41kb</size></version><title>Relating Apartness and Bisimulation</title><authors>Herman Geuvers and Bart Jacobs</authors><categories>cs.LO cs.FL</categories><comments>35 pages</comments><acm-class>D.2.4; D.3.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bisimulation for a coalgebra of a functor on the category of sets can be
described via a coalgebra in the category of relations, of a lifted functor. A
final coalgebra then gives rise to the coinduction principle, which states that
two bisimilar elements are equal. For polynomial functors, this leads to
well-known descriptions. In the present paper we look at the dual notion of
&quot;apartness&quot;. Intuitively, two elements are apart if there is a positive way to
distinguish them. Phrased differently: two elements are apart if and only if
they are not bisimilar. Since apartness is an inductive notion, described by a
least fixed point, we can give a proof system, to derive that two elements are
apart. This proof system has derivation rules and two elements are apart if and
only if there is a finite derivation (using the rules) of this fact.
  We study apartness versus bisimulation in two separate ways. First, for weak
forms of bisimulation on labelled transition systems, where silent (tau) steps
are included, we define an apartness notion that corresponds to weak
bisimulation and another apartness that corresponds to branching bisimulation.
The rules for apartness can be used to show that two states of a labelled
transition system are not branching bismilar. To support the apartness view on
labelled transition systems, we cast a number of well-known properties of
branching bisimulation in terms of branching apartness and prove them. Next, we
also study the more general categorical situation and show that indeed,
apartness is the dual of bisimilarity in a precise categorical sense: apartness
is an initial algebra and gives rise to an induction principle. In this
analogy, we include the powerset functor, which gives a semantics to
non-deterministic choice in process-theory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.02528</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.02528</id><submitter>Zhen Chen</submitter><version version="v1"><date>Thu, 23 Jan 2020 01:50:22 GMT</date><size>3866kb</size></version><title>On generalized residue network for deep learning of unknown dynamical
  systems</title><authors>Zhen Chen and Dongbin Xiu</authors><categories>cs.LG cs.NA math.DS math.NA stat.ML</categories><doi>10.1016/j.jcp.2021.110362</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general numerical approach for learning unknown dynamical
systems using deep neural networks (DNNs). Our method is built upon recent
studies that identified the residue network (ResNet) as an effective neural
network structure. In this paper, we present a generalized ResNet framework and
broadly define residue as the discrepancy between observation data and
prediction made by another model, which can be an existing coarse model or
reduced-order model. In this case, the generalized ResNet serves as a model
correction to the existing model and recovers the unresolved dynamics. When an
existing coarse model is not available, we present numerical strategies for
fast creation of coarse models, to be used in conjunction with the generalized
ResNet. These coarse models are constructed using the same data set and thus do
not require additional resources. The generalized ResNet is capable of learning
the underlying unknown equations and producing predictions with accuracy higher
than the standard ResNet structure. This is demonstrated via several numerical
examples, including long-term prediction of a chaotic system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.02620</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.02620</id><submitter>Jarrad Courts</submitter><version version="v1"><date>Fri, 7 Feb 2020 04:46:14 GMT</date><size>336kb</size></version><version version="v2"><date>Thu, 15 Oct 2020 03:43:29 GMT</date><size>3631kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 05:21:20 GMT</date><size>4532kb</size></version><title>Gaussian Variational State Estimation for Nonlinear State-Space Models</title><authors>Jarrad Courts, Adrian Wills and Thomas B. Sch\&quot;on</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of state estimation, in the context of both
filtering and smoothing, for nonlinear state-space models is considered. Due to
the nonlinear nature of the models, the state estimation problem is generally
intractable as it involves integrals of general nonlinear functions and the
filtered and smoothed state distributions lack closed-form solutions. As such,
it is common to approximate the state estimation problem. In this paper, we
develop an assumed Gaussian solution based on variational inference, which
offers the key advantage of a flexible, but principled, mechanism for
approximating the required distributions. Our main contribution lies in a new
formulation of the state estimation problem as an optimisation problem, which
can then be solved using standard optimisation routines that employ exact
first- and second-order derivatives. The resulting state estimation approach
involves a minimal number of assumptions and applies directly to nonlinear
systems with both Gaussian and non-Gaussian probabilistic models. The
performance of our approach is demonstrated on several examples; a challenging
scalar system, a model of a simple robotic system, and a target tracking
problem using a von Mises-Fisher distribution and outperforms alternative
assumed Gaussian approaches to state estimation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.02930</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.02930</id><submitter>Xiaofeng Cai</submitter><version version="v1"><date>Fri, 7 Feb 2020 18:05:58 GMT</date><size>1007kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 Feb 2020 23:05:20 GMT</date><size>1066kb</size><source_type>D</source_type></version><title>An Eulerian-Lagrangian discontinuous Galerkin method for transport
  problems and its application to nonlinear dynamics</title><authors>Xiaofeng Cai, Jing-Mei Qiu and Yang Yang</authors><categories>math.NA cs.NA physics.comp-ph physics.flu-dyn</categories><doi>10.1016/j.jcp.2021.110392</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new Eulerian-Lagrangian (EL) discontinuous Galerkin (DG) method.
The method is designed as a generalization of the semi-Lagrangian (SL) DG
method for linear advection problems proposed in [J. Sci. Comput. 73: 514-542,
2017], which is formulated based on an adjoint problem and tracing upstream
cells by tracking characteristics curves highly accurately. In the SLDG method,
depending on the velocity field, upstream cells could be of arbitrary shape.
Thus, a more sophisticated approximation to sides of the upstream cells is
required to get high order approximation. For example, quadratic-curved (QC)
quadrilaterals were proposed to approximate upstream cells for a third-order
spatial accuracy in a swirling deformation example. In this paper, for linear
advection problems, we propose a more general formulation, named the ELDG
method. The scheme is formulated based on a {\em modified} adjoint problem for
which the upstream cells are always quadrilaterals, which avoids the need to
use QC quadrilaterals in the SLDG algorithm. The newly proposed ELDG method can
be viewed as a new general framework, in which both the classical Eulerian
Runge-Kutta DG formulation and the SL DG formulation can fit in. Numerical
results on linear transport problems, as well as the nonlinear Vlasov and
incompressible Euler dynamics using the exponential RK time integrators, are
presented to demonstrate the effectiveness of the ELDG method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.03375</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.03375</id><submitter>Jingyu He</submitter><version version="v1"><date>Sun, 9 Feb 2020 14:37:02 GMT</date><size>67kb</size></version><version version="v2"><date>Wed, 9 Dec 2020 03:55:01 GMT</date><size>444kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 10 Mar 2021 16:08:50 GMT</date><size>259kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 14:44:02 GMT</date><size>232kb</size><source_type>D</source_type></version><title>Stochastic tree ensembles for regularized nonlinear regression</title><authors>Jingyu He, P. Richard Hahn</authors><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper develops a novel stochastic tree ensemble method for nonlinear
regression, which we refer to as XBART, short for Accelerated Bayesian Additive
Regression Trees. By combining regularization and stochastic search strategies
from Bayesian modeling with computationally efficient techniques from recursive
partitioning approaches, the new method attains state-of-the-art performance:
in many settings it is both faster and more accurate than the widely-used
XGBoost algorithm. Via careful simulation studies, we demonstrate that our new
approach provides accurate point-wise estimates of the mean function and does
so faster than popular alternatives, such as BART, XGBoost and neural networks
(using Keras). We also prove a number of basic theoretical results about the
new algorithm, including consistency of the single tree version of the model
and stationarity of the Markov chain produced by the ensemble version.
Furthermore, we demonstrate that initializing standard Bayesian additive
regression trees Markov chain Monte Carlo (MCMC) at XBART-fitted trees
considerably improves credible interval coverage and reduces total run-time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.03513</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.03513</id><submitter>Shaojun Ma</submitter><version version="v1"><date>Mon, 10 Feb 2020 03:20:13 GMT</date><size>1632kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 Feb 2020 02:54:51 GMT</date><size>1632kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 5 Feb 2021 02:55:54 GMT</date><size>3521kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 8 Feb 2021 03:32:07 GMT</date><size>3522kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 22 Feb 2021 04:48:44 GMT</date><size>3521kb</size><source_type>D</source_type></version><version version="v6"><date>Sat, 29 May 2021 02:41:01 GMT</date><size>8360kb</size><source_type>D</source_type></version><title>Learning Stochastic Behaviour from Aggregate Data</title><authors>Shaojun Ma, Shu Liu, Hongyuan Zha, Haomin Zhou</authors><categories>cs.LG math.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning nonlinear dynamics from aggregate data is a challenging problem
because the full trajectory of each individual is not available, namely, the
individual observed at one time may not be observed at the next time point, or
the identity of individual is unavailable. This is in sharp contrast to
learning dynamics with full trajectory data, on which the majority of existing
methods are based. We propose a novel method using the weak form of Fokker
Planck Equation (FPE) -- a partial differential equation -- to describe the
density evolution of data in a sampled form, which is then combined with
Wasserstein generative adversarial network (WGAN) in the training process. In
such a sample-based framework we are able to learn the nonlinear dynamics from
aggregate data without explicitly solving the partial differential equation
(PDE) FPE. We demonstrate our approach in the context of a series of synthetic
and real-world data sets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.04242</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.04242</id><submitter>Rui Liu</submitter><version version="v1"><date>Tue, 11 Feb 2020 07:58:48 GMT</date><size>3479kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 14:56:52 GMT</date><size>2525kb</size><source_type>D</source_type></version><title>Human-to-Robot Attention Transfer for Robot Execution Failure Avoidance
  Using Stacked Neural Networks</title><authors>Boyi Song, Yuntao Peng, Ruijiao Luo, Rui Liu</authors><categories>cs.RO cs.AI cs.HC</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to world dynamics and hardware uncertainty, robots inevitably fail in
task executions, leading to undesired or even dangerous executions. To avoid
failures for improved robot performance, it is critical to identify and correct
robot abnormal executions in an early stage. However, limited by reasoning
capability and knowledge level, it is challenging for a robot to self diagnose
and correct their abnormal behaviors. To solve this problem, a novel method is
proposed, human-to-robot attention transfer (H2R-AT) to seek help from a human.
H2R-AT is developed based on a novel stacked neural networks model,
transferring human attention embedded in verbal reminders to robot attention
embedded in robot visual perceiving. With the attention transfer from a human,
a robot understands what and where human concerns are to identify and correct
its abnormal executions. To validate the effectiveness of H2R-AT, two
representative task scenarios, &quot;serve water for a human in a kitchen&quot; and &quot;pick
up a defective gear in a factory&quot; with abnormal robot executions, were designed
in an open-access simulation platform V-REP; $252$ volunteers were recruited to
provide about 12000 verbal reminders to learn and test the attention transfer
model H2R-AT. With an accuracy of $73.68\%$ in transferring attention and
accuracy of $66.86\%$ in avoiding robot execution failures, the effectiveness
of H2R-AT was validated.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.04924</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.04924</id><submitter>Mattias Nilsson</submitter><version version="v1"><date>Wed, 12 Feb 2020 11:26:35 GMT</date><size>261kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 14:05:32 GMT</date><size>417kb</size><source_type>D</source_type></version><title>Synaptic Integration of Spatiotemporal Features with a Dynamic
  Neuromorphic Processor</title><authors>Mattias Nilsson, Foteini Liwicki and Fredrik Sandin</authors><categories>cs.NE cs.CV q-bio.NC</categories><comments>Copyright 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><journal-ref>2020 International Joint Conference on Neural Networks (IJCNN),
  2020, pp. 1-7</journal-ref><doi>10.1109/IJCNN48605.2020.9207210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking neurons can perform spatiotemporal feature detection by nonlinear
synaptic and dendritic integration of presynaptic spike patterns.
Multicompartment models of non-linear dendrites and related neuromorphic
circuit designs enable faithful imitation of such dynamic integration
processes, but these approaches are also associated with a relatively high
computing cost or circuit size. Here, we investigate synaptic integration of
spatiotemporal spike patterns with multiple dynamic synapses on point-neurons
in the DYNAP-SE neuromorphic processor, which offers a complementary
resource-efficient, albeit less flexible, approach to feature detection. We
investigate how previously proposed excitatory--inhibitory pairs of dynamic
synapses can be combined to integrate multiple inputs, and we generalize that
concept to a case in which one inhibitory synapse is combined with multiple
excitatory synapses. We characterize the resulting delayed excitatory
postsynaptic potentials (EPSPs) by measuring and analyzing the membrane
potentials of the neuromorphic neuronal circuits. We find that biologically
relevant EPSP delays, with variability of order 10 milliseconds per neuron, can
be realized in the proposed manner by selecting different synapse combinations,
thanks to device mismatch. Based on these results, we demonstrate that a single
point-neuron with dynamic synapses in the DYNAP-SE can respond selectively to
presynaptic spikes with a particular spatiotemporal structure, which enables,
for instance, visual feature tuning of single neurons.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.05120</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.05120</id><submitter>Jason Jo</submitter><version version="v1"><date>Wed, 12 Feb 2020 17:43:23 GMT</date><size>639kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 11 Jun 2020 20:32:08 GMT</date><size>408kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 11 Dec 2020 18:30:16 GMT</date><size>795kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 2 Jun 2021 20:11:03 GMT</date><size>1884kb</size><source_type>D</source_type></version><title>Parameterizing Branch-and-Bound Search Trees to Learn Branching Policies</title><authors>Giulia Zarpellon, Jason Jo, Andrea Lodi and Yoshua Bengio</authors><categories>cs.LG cs.AI stat.ML</categories><comments>AAAI 2021 camera-ready version with supplementary materials, improved
  readability of figures in main article. Code, data and trained models are
  available at https://github.com/ds4dm/branch-search-trees</comments><journal-ref>Proceedings of the AAAI Conference on Artificial Intelligence
  2021, 35(5), 3931-3939</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Branch and Bound (B&amp;B) is the exact tree search method typically used to
solve Mixed-Integer Linear Programming problems (MILPs). Learning branching
policies for MILP has become an active research area, with most works proposing
to imitate the strong branching rule and specialize it to distinct classes of
problems. We aim instead at learning a policy that generalizes across
heterogeneous MILPs: our main hypothesis is that parameterizing the state of
the B&amp;B search tree can aid this type of generalization. We propose a novel
imitation learning framework, and introduce new input features and
architectures to represent branching. Experiments on MILP benchmark instances
clearly show the advantages of incorporating an explicit parameterization of
the state of the search tree to modulate the branching decisions, in terms of
both higher accuracy and smaller B&amp;B trees. The resulting policies
significantly outperform the current state-of-the-art method for &quot;learning to
branch&quot; by effectively allowing generalization to generic unseen instances.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.06557</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.06557</id><submitter>Gad Zalcberg</submitter><version version="v1"><date>Sun, 16 Feb 2020 11:42:52 GMT</date><size>399kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:10:20 GMT</date><size>477kb</size><source_type>D</source_type></version><title>Fair Principal Component Analysis and Filter Design</title><authors>Gad Zalcberg and Ami Wiesel</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Fair Principal Component Analysis (FPCA) and search for a low
dimensional subspace that spans multiple target vectors in a fair manner. FPCA
is defined as a non-concave maximization of the worst projected target norm
within a given set. The problem arises in filter design in signal processing,
and when incorporating fairness into dimensionality reduction schemes. The
state of the art approach to FPCA is via semidefinite relaxation and involves a
polynomial yet computationally expensive optimization. To allow scalability, we
propose to address FPCA using naive sub-gradient descent. We analyze the
landscape of the underlying optimization in the case of orthogonal targets. We
prove that the landscape is benign and that all local minima are globally
optimal. Interestingly, the SDR approach leads to sub-optimal solutions in this
simple case. Finally, we discuss the equivalence between orthogonal FPCA and
the design of normalized tight frames.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.06716</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.06716</id><submitter>Michael Mahoney</submitter><version version="v1"><date>Mon, 17 Feb 2020 00:01:12 GMT</date><size>961kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 17:21:02 GMT</date><size>3772kb</size><source_type>D</source_type></version><title>Predicting trends in the quality of state-of-the-art neural networks
  without access to training or testing data</title><authors>Charles H. Martin, Tongsu (Serena) Peng, and Michael W. Mahoney</authors><categories>cs.LG physics.data-an stat.ML</categories><comments>35 pages, 8 tables, 17 figures. To appear in Nature Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, one works with neural network models trained by someone
else. For such pretrained models, one may not have access to training data or
test data. Moreover, one may not know details about the model, e.g., the
specifics of the training data, the loss function, the hyperparameter values,
etc. Given one or many pretrained models, it is a challenge to say anything
about the expected performance or quality of the models. Here, we address this
challenge by providing a detailed meta-analysis of hundreds of
publicly-available pretrained models. We examine norm based capacity control
metrics as well as power law based metrics from the recently-developed Theory
of Heavy-Tailed Self Regularization. We find that norm based metrics correlate
well with reported test accuracies for well-trained models, but that they often
cannot distinguish well-trained versus poorly-trained models. We also find that
power law based metrics can do much better -- quantitatively better at
discriminating among series of well-trained models with a given architecture;
and qualitatively better at discriminating well-trained versus poorly-trained
models. These methods can be used to identify when a pretrained neural network
has problems that cannot be detected simply by examining training/test
accuracies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.06979</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.06979</id><submitter>Zixin Wen</submitter><version version="v1"><date>Mon, 17 Feb 2020 14:35:21 GMT</date><size>35kb</size></version><version version="v2"><date>Fri, 21 Feb 2020 00:32:53 GMT</date><size>35kb</size></version><version version="v3"><date>Sun, 30 May 2021 17:23:28 GMT</date><size>35kb</size></version><title>Convergence of End-to-End Training in Deep Unsupervised Contrastive
  Learning</title><authors>Zixin Wen</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised contrastive learning has gained increasing attention in the
latest research and has proven to be a powerful method for learning
representations from unlabeled data. However, little theoretical analysis was
known for this framework. In this paper, we study the optimization of deep
unsupervised contrastive learning. We prove that, by applying end-to-end
training that simultaneously updates two deep over-parameterized neural
networks, one can find an approximate stationary solution for the non-convex
contrastive loss. This result is inherently different from the existing
over-parameterized analysis in the supervised setting because, in contrast to
learning a specific target function, unsupervised contrastive learning tries to
encode the unlabeled data distribution into the neural networks, which
generally has no optimal solution. Our analysis provides theoretical insights
into the practical success of these unsupervised pretraining methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.07309</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.07309</id><submitter>Ross Horne</submitter><version version="v1"><date>Tue, 18 Feb 2020 00:26:19 GMT</date><size>270kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Sep 2020 18:24:05 GMT</date><size>316kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 14 Mar 2021 18:31:43 GMT</date><size>722kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 6 May 2021 10:10:34 GMT</date><size>715kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 1 Jun 2021 14:29:57 GMT</date><size>316kb</size><source_type>D</source_type></version><title>Discovering ePassport Vulnerabilities using Bisimilarity</title><authors>Ross Horne and Sjouke Mauw</authors><categories>cs.CR cs.LO</categories><proxy>Logical Methods In Computer Science</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We uncover privacy vulnerabilities in the ICAO 9303 standard implemented by
ePassports worldwide. These vulnerabilities, confirmed by ICAO, enable an
ePassport holder who recently passed through a checkpoint to be reidentified
without opening their ePassport. This paper explains how bisimilarity was used
to discover these vulnerabilities, which exploit the BAC protocol - the
original ICAO 9303 standard ePassport authentication protocol - and remains
valid for the PACE protocol, which improves on the security of BAC in the
latest ICAO 9303 standards. In order to tackle such bisimilarity problems, we
develop here a chain of methods for the applied $\pi$-calculus including a
symbolic under-approximation of bisimilarity, called open bisimilarity, and a
modal logic, called classical FM, for describing and certifying attacks.
Evidence is provided to argue for a new scheme for specifying such
unlinkability problems that more accurately reflects the capabilities of an
attacker.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.07767</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.07767</id><submitter>Wonjin Yoon</submitter><version version="v1"><date>Tue, 18 Feb 2020 17:59:02 GMT</date><size>1992kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 05:02:43 GMT</date><size>153kb</size><source_type>D</source_type></version><title>Learning by Semantic Similarity Makes Abstractive Summarization Better</title><authors>Wonjin Yoon, Yoon Sun Yeo, Minbyul Jeong, Bong-Jun Yi, Jaewoo Kang</authors><categories>cs.CL</categories><comments>The initial version of the manuscript includes a model design
  (semsim), experimental results, and discussions on the results. We found that
  our model has flaws in its implementation and design. This final version of
  the manuscript is from the rest of the initial paper; we included our
  findings on the benchmark dataset, BART generated results and human
  evaluations, and we excluded our model semsim</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By harnessing pre-trained language models, summarization models had rapid
progress recently. However, the models are mainly assessed by automatic
evaluation metrics such as ROUGE. Although ROUGE is known for having a positive
correlation with human evaluation scores, it has been criticized for its
vulnerability and the gap between actual qualities. In this paper, we compare
the generated summaries from recent LM, BART, and the reference summaries from
a benchmark dataset, CNN/DM, using a crowd-sourced human evaluation metric.
Interestingly, model-generated summaries receive higher scores relative to
reference summaries. Stemming from our experimental results, we first argue the
intrinsic characteristics of the CNN/DM dataset, the progress of pre-trained
language models, and their ability to generalize on the training data. Finally,
we share our insights into the model-generated summaries and presents our
thought on learning methods for abstractive summarization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.08014</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.08014</id><submitter>Xiang Li</submitter><version version="v1"><date>Wed, 19 Feb 2020 05:58:23 GMT</date><size>455kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 5 Apr 2021 12:01:29 GMT</date><size>2263kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 15:17:52 GMT</date><size>1145kb</size><source_type>D</source_type></version><title>Communication-Efficient Distributed SVD via Local Power Iterations</title><authors>Xiang Li, Shusen Wang, Kun Chen, Zhihua Zhang</authors><categories>stat.ML cs.LG math.OC</categories><comments>9 pages, 7 figures, accepted by 2021 ICML</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed computing of the truncated singular value decomposition
problem. We develop an algorithm that we call \texttt{LocalPower} for improving
communication efficiency. Specifically, we uniformly partition the dataset
among $m$ nodes and alternate between multiple (precisely $p$) local power
iterations and one global aggregation. In the aggregation, we propose to weight
each local eigenvector matrix with orthogonal Procrustes transformation (OPT).
As a practical surrogate of OPT, sign-fixing, which uses a diagonal matrix with
$\pm 1$ entries as weights, has better computation complexity and stability in
experiments. We theoretically show that under certain assumptions
\texttt{LocalPower} lowers the required number of communications by a factor of
$p$ to reach a constant accuracy. We also show that the strategy of
periodically decaying $p$ helps obtain high-precision solutions. We conduct
experiments to demonstrate the effectiveness of \texttt{LocalPower}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.08538</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.08538</id><submitter>Yahya Sattar</submitter><version version="v1"><date>Thu, 20 Feb 2020 02:36:44 GMT</date><size>2303kb</size><source_type>D</source_type></version><title>Non-asymptotic and Accurate Learning of Nonlinear Dynamical Systems</title><authors>Yahya Sattar and Samet Oymak</authors><categories>cs.LG cs.SY eess.SY stat.AP stat.ML</categories><journal-ref>arXiv preprint:2002.08538, 2020</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning stabilizable systems governed by
nonlinear state equation $h_{t+1}=\phi(h_t,u_t;\theta)+w_t$. Here $\theta$ is
the unknown system dynamics, $h_t $ is the state, $u_t$ is the input and $w_t$
is the additive noise vector. We study gradient based algorithms to learn the
system dynamics $\theta$ from samples obtained from a single finite trajectory.
If the system is run by a stabilizing input policy, we show that
temporally-dependent samples can be approximated by i.i.d. samples via a
truncation argument by using mixing-time arguments. We then develop new
guarantees for the uniform convergence of the gradients of empirical loss.
Unlike existing work, our bounds are noise sensitive which allows for learning
ground-truth dynamics with high accuracy and small sample complexity. Together,
our results facilitate efficient learning of the general nonlinear system under
stabilizing policy. We specialize our guarantees to entry-wise nonlinear
activations and verify our theory in various numerical experiments
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.08546</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.08546</id><submitter>Jian Liang</submitter><version version="v1"><date>Thu, 20 Feb 2020 03:13:58 GMT</date><size>616kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 29 Apr 2020 17:31:59 GMT</date><size>785kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 26 Jun 2020 17:03:14 GMT</date><size>385kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 6 Aug 2020 09:48:24 GMT</date><size>356kb</size><source_type>D</source_type></version><version version="v5"><date>Fri, 23 Oct 2020 03:22:32 GMT</date><size>379kb</size><source_type>D</source_type></version><version version="v6"><date>Tue, 1 Jun 2021 09:06:00 GMT</date><size>739kb</size><source_type>D</source_type></version><title>Do We Really Need to Access the Source Data? Source Hypothesis Transfer
  for Unsupervised Domain Adaptation</title><authors>Jian Liang, Dapeng Hu, and Jiashi Feng</authors><categories>cs.CV cs.LG</categories><comments>ICML2020. Fix the typos for Digits. Code is available at
  https://github.com/tim-learn/SHOT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned
from a labeled source dataset to solve similar tasks in a new unlabeled domain.
Prior UDA methods typically require to access the source data when learning to
adapt the model, making them risky and inefficient for decentralized private
data. This work tackles a practical setting where only a trained source model
is available and investigates how we can effectively utilize such a model
without source data to solve UDA problems. We propose a simple yet generic
representation learning framework, named \emph{Source HypOthesis Transfer}
(SHOT). SHOT freezes the classifier module (hypothesis) of the source model and
learns the target-specific feature extraction module by exploiting both
information maximization and self-supervised pseudo-labeling to implicitly
align representations from the target domains to the source hypothesis. To
verify its versatility, we evaluate SHOT in a variety of adaptation cases
including closed-set, partial-set, and open-set domain adaptation. Experiments
indicate that SHOT yields state-of-the-art results among multiple domain
adaptation benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.10451</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.10451</id><submitter>Mayank Mittal</submitter><version version="v1"><date>Fri, 21 Feb 2020 16:57:38 GMT</date><size>8478kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 14:37:05 GMT</date><size>11276kb</size><source_type>D</source_type></version><title>Neural Lyapunov Model Predictive Control: Learning Safe Global
  Controllers from Sub-optimal Examples</title><authors>Mayank Mittal, Marco Gallieri, Alessio Quaglino, Seyed Sina Mirrazavi
  Salehian, Jan Koutn\'ik</authors><categories>cs.AI cs.NE cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With a growing interest in data-driven control techniques, Model Predictive
Control (MPC) provides an opportunity to exploit the surplus of data reliably,
particularly while taking safety and stability into account. In many real-world
and industrial applications, it is typical to have an existing control
strategy, for instance, execution from a human operator. The objective of this
work is to improve upon this unknown, safe but suboptimal policy by learning a
new controller that retains safety and stability. Learning how to be safe is
achieved directly from data and from a knowledge of the system constraints. The
proposed algorithm alternatively learns the terminal cost and updates the MPC
parameters according to a stability metric. The terminal cost is constructed as
a Lyapunov function neural network with the aim of recovering or extending the
stable region of the initial demonstrator using a short prediction horizon.
Theorems that characterize the stability and performance of the learned MPC in
the bearing of model uncertainties and sub-optimality due to function
approximation are presented. The efficacy of the proposed algorithm is
demonstrated on non-linear continuous control tasks with soft constraints. The
proposed approach can improve upon the initial demonstrator also in practice
and achieve better stability than popular reinforcement learning baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.10669</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.10669</id><submitter>Eric Neyman</submitter><version version="v1"><date>Tue, 25 Feb 2020 05:06:24 GMT</date><size>155kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 04:43:48 GMT</date><size>314kb</size><source_type>D</source_type></version><title>Binary Scoring Rules that Incentivize Precision</title><authors>Eric Neyman, Georgy Noarov, S. Matthew Weinberg</authors><categories>cs.GT</categories><comments>42 pages, accepted for publication in EC 2021</comments><doi>10.1145/3465456.3467639</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  All proper scoring rules incentivize an expert to predict \emph{accurately}
(report their true estimate), but not all proper scoring rules equally
incentivize \emph{precision}. Rather than treating the expert's belief as
exogenously given, we consider a model where a rational expert can endogenously
refine their belief by repeatedly paying a fixed cost, and is incentivized to
do so by a proper scoring rule.
  Specifically, our expert aims to predict the probability that a biased coin
flipped tomorrow will land heads, and can flip the coin any number of times
today at a cost of $c$ per flip. Our first main result defines an
\emph{incentivization index} for proper scoring rules, and proves that this
index measures the expected error of the expert's estimate (where the number of
flips today is chosen adaptively to maximize the predictor's expected payoff).
Our second main result finds the unique scoring rule which optimizes the
incentivization index over all proper scoring rules.
  We also consider extensions to minimizing the $\ell^{th}$ moment of error,
and again provide an incentivization index and optimal proper scoring rule. In
some cases, the resulting scoring rule is differentiable, but not infinitely
differentiable. In these cases, we further prove that the optimum can be
uniformly approximated by polynomial scoring rules.
  Finally, we compare common scoring rules via our measure, and include
simulations confirming the relevance of our measure even in domains outside
where it provably applies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.10674</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.10674</id><submitter>Elaina Chai</submitter><version version="v1"><date>Tue, 25 Feb 2020 05:25:40 GMT</date><size>700kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 11:22:32 GMT</date><size>9499kb</size><source_type>D</source_type></version><title>Separating the Effects of Batch Normalization on CNN Training Speed and
  Stability Using Classical Adaptive Filter Theory</title><authors>Elaina Chai, Mert Pilanci, Boris Murmann</authors><categories>cs.NE cs.LG eess.SP</categories><comments>Presented at Asilomar Conference on Signals, Systems, and Computers,
  2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Batch Normalization (BatchNorm) is commonly used in Convolutional Neural
Networks (CNNs) to improve training speed and stability. However, there is
still limited consensus on why this technique is effective. This paper uses
concepts from the traditional adaptive filter domain to provide insight into
the dynamics and inner workings of BatchNorm. First, we show that the
convolution weight updates have natural modes whose stability and convergence
speed are tied to the eigenvalues of the input autocorrelation matrices, which
are controlled by BatchNorm through the convolution layers' channel-wise
structure. Furthermore, our experiments demonstrate that the speed and
stability benefits are distinct effects. At low learning rates, it is
BatchNorm's amplification of the smallest eigenvalues that improves convergence
speed, while at high learning rates, it is BatchNorm's suppression of the
largest eigenvalues that ensures stability. Lastly, we prove that in the first
training step, when normalization is needed most, BatchNorm satisfies the same
optimization as Normalized Least Mean Square (NLMS), while it continues to
approximate this condition in subsequent steps. The analyses provided in this
paper lay the groundwork for gaining further insight into the operation of
modern neural network structures using adaptive filter theory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.11205</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.11205</id><submitter>Gideon Amir</submitter><version version="v1"><date>Tue, 25 Feb 2020 22:28:54 GMT</date><size>7kb</size></version><title>The firefighter problem on polynomial and intermediate growth groups</title><authors>Gideon Amir, Rangel Baldasso, Gady Kozma</authors><categories>math.GR cs.DM math.CO</categories><comments>5 pages</comments><msc-class>05C63, 05C57 05C63, 05C57 20F65, 05c63, 05c57, 05C10</msc-class><journal-ref>Discrete Mathematics Volume 343, Issue 11, November 2020, 112077</journal-ref><doi>10.1016/j.disc.2020.112077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that any Cayley graph $G$ with degree $d$ polynomial growth does not
satisfy $\{f(n)\}$-containment for any $f=o(n^{d-2})$. This settles the
asymptotic behaviour of the firefighter problem on such graphs as it was known
that $Cn^{d-2}$ firefighters are enough, answering and strengthening a
conjecture of Develin and Hartke. We also prove that intermediate growth Cayley
graphs do not satisfy polynomial containment, and give explicit lower bounds
depending on the growth rate of the group. These bounds can be further improved
when more geometric information is available, such as for Grigorchuk's group.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.11603</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.11603</id><submitter>Frederik Harder</submitter><version version="v1"><date>Wed, 26 Feb 2020 16:41:41 GMT</date><size>1629kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 10 Mar 2020 22:45:06 GMT</date><size>1636kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 27 Jul 2020 16:32:31 GMT</date><size>3114kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 26 Oct 2020 08:59:13 GMT</date><size>7227kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 1 Jun 2021 14:38:20 GMT</date><size>2773kb</size><source_type>D</source_type></version><title>DP-MERF: Differentially Private Mean Embeddings with Random Features for
  Practical Privacy-Preserving Data Generation</title><authors>Frederik Harder, Kamil Adamczewski, Mijung Park</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a differentially private data generation paradigm using random
feature representations of kernel mean embeddings when comparing the
distribution of true data with that of synthetic data. We exploit the random
feature representations for two important benefits. First, we require a minimal
privacy cost for training deep generative models. This is because unlike
kernel-based distance metrics that require computing the kernel matrix on all
pairs of true and synthetic data points, we can detach the data-dependent term
from the term solely dependent on synthetic data. Hence, we need to perturb the
data-dependent term only once and then use it repeatedly during the generator
training. Second, we can obtain an analytic sensitivity of the kernel mean
embedding as the random features are norm bounded by construction. This removes
the necessity of hyper-parameter search for a clipping norm to handle the
unknown sensitivity of a generator network. We provide several variants of our
algorithm, differentially-private mean embeddings with random features
(DP-MERF) to jointly generate labels and input features for datasets such as
heterogeneous tabular data and image data. Our algorithm achieves drastically
better privacy-utility trade-offs than existing methods when tested on several
datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.11809</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.11809</id><submitter>Joona Karjalainen</submitter><version version="v1"><date>Wed, 26 Feb 2020 21:53:06 GMT</date><size>40kb</size></version><version version="v2"><date>Sun, 24 Jan 2021 18:09:20 GMT</date><size>44kb</size></version><version version="v3"><date>Mon, 31 May 2021 08:50:16 GMT</date><size>213kb</size></version><title>Assortativity and bidegree distributions on Bernoulli random graph
  superpositions</title><authors>Mindaugas Bloznelis, Joona Karjalainen, Lasse Leskel\&quot;a</authors><categories>math.PR cs.SI math.ST stat.TH</categories><comments>32 pages</comments><msc-class>60C05, 60B10, 91D30, 62G35</msc-class><acm-class>G.3; J.4; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A probabilistic generative network model with $n$ nodes and $m$ overlapping
layers is obtained as a superposition of $m$ mutually independent Bernoulli
random graphs of varying size and strength. When $n$ and $m$ are large and of
the same order of magnitude, the model admits a sparse limiting regime with a
tunable power-law degree distribution and nonvanishing clustering coefficient.
In this article we prove an asymptotic formula for the joint degree
distribution of adjacent nodes. This yields a simple analytical formula for the
model assortativity, and opens up ways to analyze rank correlation coefficients
suitable for random graphs with heavy-tailed degree distributions. We also
study the effects of power laws on the asymptotic joint degree distributions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.11985</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.11985</id><submitter>Prakhar Ganesh</submitter><version version="v1"><date>Thu, 27 Feb 2020 09:20:31 GMT</date><size>994kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 02:38:20 GMT</date><size>1433kb</size><source_type>D</source_type></version><title>Compressing Large-Scale Transformer-Based Models: A Case Study on BERT</title><authors>Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan
  Sajjad, Preslav Nakov, Deming Chen, Marianne Winslett</authors><categories>cs.LG stat.ML</categories><comments>To appear in TACL 2021. The arXiv version is a pre-MIT Press
  publication version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pre-trained Transformer-based models have achieved state-of-the-art
performance for various Natural Language Processing (NLP) tasks. However, these
models often have billions of parameters, and, thus, are too resource-hungry
and computation-intensive to suit low-capability devices or applications with
strict latency requirements. One potential remedy for this is model
compression, which has attracted a lot of research attention. Here, we
summarize the research in compressing Transformers, focusing on the especially
popular BERT model. In particular, we survey the state of the art in
compression for BERT, we clarify the current best practices for compressing
large-scale Transformer models, and we provide insights into the workings of
various methods. Our categorization and analysis also shed light on promising
future research directions for achieving lightweight, accurate, and generic NLP
models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.00400</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.00400</id><submitter>Lingfeng Tao</submitter><version version="v1"><date>Sun, 1 Mar 2020 04:41:49 GMT</date><size>653kb</size></version><version version="v2"><date>Mon, 31 May 2021 15:37:54 GMT</date><size>565kb</size></version><title>Learn Task First or Learn Human Partner First: A Hierarchical Task
  Decomposition Method for Human-Robot Cooperation</title><authors>Lingfeng Tao, Michael Bowman, Jiucai Zhang, Xiaoli Zhang</authors><categories>cs.RO cs.AI cs.HC</categories><comments>Submitted to SMC2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying Deep Reinforcement Learning (DRL) to Human-Robot Cooperation (HRC)
in dynamic control problems is promising yet challenging as the robot needs to
learn the dynamics of the controlled system and dynamics of the human partner.
In existing research, the robot powered by DRL adopts coupled observation of
the environment and the human partner to learn both dynamics simultaneously.
However, such a learning strategy is limited in terms of learning efficiency
and team performance. This work proposes a novel task decomposition method with
a hierarchical reward mechanism that enables the robot to learn the
hierarchical dynamic control task separately from learning the human partner's
behavior. The method is validated with a hierarchical control task in a
simulated environment with human subject experiments. Our method also provides
insight into the design of the learning strategy for HRC. The results show that
the robot should learn the task first to achieve higher team performance and
learn the human first to achieve higher learning efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.00865</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.00865</id><submitter>Sakshi Udeshi</submitter><version version="v1"><date>Tue, 25 Feb 2020 04:45:26 GMT</date><size>7194kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 17 Jun 2020 15:15:36 GMT</date><size>4359kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 07:02:14 GMT</date><size>9117kb</size><source_type>D</source_type></version><title>Exposing Backdoors in Robust Machine Learning Models</title><authors>Ezekiel Soremekun, Sakshi Udeshi and Sudipta Chattopadhyay</authors><categories>cs.CV cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The introduction of robust optimisation has pushed the state-of-the-art in
defending against adversarial attacks. However, the behaviour of such
optimisation has not been studied in the light of a fundamentally different
class of attacks called backdoors. In this paper, we demonstrate that
adversarially robust models are susceptible to backdoor attacks. Subsequently,
we observe that backdoors are reflected in the feature representation of such
models. Then, this observation is leveraged to detect backdoor-infected models
via a detection technique called AEGIS. Specifically, AEGIS uses feature
clustering to effectively detect backdoor-infected robust Deep Neural Networks
(DNNs). In our evaluation of several visible and hidden backdoor triggers on
major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS
effectively detects robust DNNs infected with backdoors. AEGIS detects a
backdoor-infected model with 91.6% accuracy, without any false positives.
Furthermore, AEGIS detects the targeted class in the backdoor-infected model
with a reasonably low (11.1%) false positive rate. Our investigation reveals
that salient features of adversarially robust DNNs break the stealthy nature of
backdoor attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.01821</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.01821</id><submitter>Denis Kleyko</submitter><version version="v1"><date>Tue, 3 Mar 2020 22:44:10 GMT</date><size>1837kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 19:28:49 GMT</date><size>1842kb</size><source_type>D</source_type></version><title>HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks
  with Hyperdimensional Computing enabled Embedding of n-gram Statistics</title><authors>Pedro Alonso, Kumar Shridhar, Denis Kleyko, Evgeny Osipov, Marcus
  Liwicki</authors><categories>cs.CL</categories><comments>9 pages, 1 figure, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in Deep Learning have led to a significant performance
increase on several NLP tasks, however, the models become more and more
computationally demanding. Therefore, this paper tackles the domain of
computationally efficient algorithms for NLP tasks. In particular, it
investigates distributed representations of n-gram statistics of texts. The
representations are formed using hyperdimensional computing enabled embedding.
These representations then serve as features, which are used as input to
standard classifiers. We investigate the applicability of the embedding on one
large and three small standard datasets for classification tasks using nine
classifiers. The embedding achieved on par F1 scores while decreasing the time
and memory requirements by several times compared to the conventional n-gram
statistics, e.g., for one of the classifiers on a small dataset, the memory
reduction was 6.18 times; while train and test speed-ups were 4.62 and 3.84
times, respectively. For many classifiers on the large dataset, memory
reduction was ca. 100 times and train and test speed-ups were over 100 times.
Importantly, the usage of distributed representations formed via
hyperdimensional computing allows dissecting strict dependency between the
dimensionality of the representation and n-gram size, thus, opening a room for
tradeoffs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.02422</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.02422</id><submitter>Dongqi Wu</submitter><version version="v1"><date>Thu, 5 Mar 2020 03:57:59 GMT</date><size>1617kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 11 May 2020 15:21:07 GMT</date><size>1064kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 22:08:55 GMT</date><size>1777kb</size><source_type>D</source_type></version><title>Deep Reinforcement Learning-BasedRobust Protection in DER-Rich
  Distribution Grids</title><authors>Dongqi Wu, Dileep Kalathil, Miroslav Begovic, Le Xie</authors><categories>eess.SY cs.SY</categories><comments>Submitted to IEEE Transactions of Smart Grid, under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces the concept of Deep Reinforcement Learning based
architecture for protective relay design in power distribution systems with
many distributed energy resources (DERs). The performance of widely-used
overcurrent protection scheme is hindered by the presence of distributed
generation, power electronic interfaced devices and fault impedance. In this
paper, a reinforcement learning-based approach is proposed to design and
implement protective relays in the distribution grid. The particular algorithm
used is an Long Short-Term Memory (LSTM) enhanced deep neural network that is
highly accurate, communication-free and easy to implement. The proposed relay
design is tested in OpenDSS simulation on the IEEE 34-node test feeder and
demonstrated much more superior performance over traditional overcurrent
protection from the aspect of failure rate, robustness and response speed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.02692</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.02692</id><submitter>Hyeon Cho</submitter><version version="v1"><date>Thu, 5 Mar 2020 15:01:08 GMT</date><size>1373kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 21 May 2021 02:39:54 GMT</date><size>1933kb</size><source_type>D</source_type></version><title>Self-Supervised Visual Learning by Variable Playback Speeds Prediction
  of a Video</title><authors>Hyeon Cho, Taehoon Kim, Hyung Jin Chang, Wonjun Hwang</authors><categories>cs.CV</categories><comments>Accepted by IEEE Access on May 19, 2021</comments><doi>10.1109/ACCESS.2021.3084840</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We propose a self-supervised visual learning method by predicting the
variable playback speeds of a video. Without semantic labels, we learn the
spatio-temporal visual representation of the video by leveraging the variations
in the visual appearance according to different playback speeds under the
assumption of temporal coherence. To learn the spatio-temporal visual
variations in the entire video, we have not only predicted a single playback
speed but also generated clips of various playback speeds and directions with
randomized starting points. Hence the visual representation can be successfully
learned from the meta information (playback speeds and directions) of the
video. We also propose a new layer dependable temporal group normalization
method that can be applied to 3D convolutional networks to improve the
representation learning performance where we divide the temporal features into
several groups and normalize each one using the different corresponding
parameters. We validate the effectiveness of our method by fine-tuning it to
the action recognition and video retrieval tasks on UCF-101 and HMDB-51.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.03601</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.03601</id><submitter>Nuri Mert Vural</submitter><version version="v1"><date>Sat, 7 Mar 2020 16:31:22 GMT</date><size>726kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:30:41 GMT</date><size>0kb</size><source_type>I</source_type></version><title>RNN-based Online Learning: An Efficient First-Order Optimization
  Algorithm with a Convergence Guarantee</title><authors>N. Mert Vural, Selim F. Yilmaz, Fatih Ilhan and Suleyman S. Kozat</authors><categories>cs.LG stat.ML</categories><comments>This paper was an early draft of the presented results. We have
  written and published another paper (arXiv:2005.08948) where we have improved
  the material in this paper. The published paper covers most of the material
  presented in this paper as well. Therefore, we remove this paper from Arxiv
  and kindly refer the interested readers to arXiv:2005.08948</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate online nonlinear regression with continually running recurrent
neural network networks (RNNs), i.e., RNN-based online learning. For RNN-based
online learning, we introduce an efficient first-order training algorithm that
theoretically guarantees to converge to the optimum network parameters. Our
algorithm is truly online such that it does not make any assumption on the
learning environment to guarantee convergence. Through numerical simulations,
we verify our theoretical results and illustrate significant performance
improvements achieved by our algorithm with respect to the state-of-the-art RNN
training methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.03677</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.03677</id><submitter>Michael Bowman</submitter><version version="v1"><date>Sat, 7 Mar 2020 22:49:55 GMT</date><size>2935kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 20:11:31 GMT</date><size>1371kb</size></version><title>An Intent-based Task-aware Shared Control Framework for Intuitive Hands
  Free Telemanipulation</title><authors>Michael Bowman, Jiucai Zhang, and Xiaoli Zhang</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shared control in teleoperation for providing robot assistance to accomplish
object manipulation, called telemanipulation, is a new promising yet
challenging problem. This has unique challenges--on top of teleoperation
challenges in general--due to difficulties of physical discrepancy between
human hands and robot hands as well as the fine motion constraints to
constitute task success. We present an intuitive shared-control strategy where
the focus is on generating robotic grasp poses which are better suited for
human perception of successful teleoperated object manipulation and feeling of
being in control of the robot, rather than developing objective stable grasp
configurations for task success or following the human motion. The former is
achieved by understanding human intent and autonomously taking over control on
that inference. The latter is achieved by considering human inputs as hard
motion constraints which the robot must abide. An arbitration of these two
enables a trade-off for the subsequent robot motion to balance accomplishing
the inferred task and motion constraints imposed by the operator. The
arbitration framework adapts to the level of physical discrepancy between the
human and different robot structures, enabling the assistance to indicate and
appear to intuitively follow the user. To understand how users perceive good
arbitration in object telemanipulation, we have conducted a user study with a
hands-free telemanipulation setup to analyze the effect of factors including
task predictability, perceived following, and user preference. The hands-free
telemanipulation scene is chosen as the validation platform due to its more
urgent need of intuitive robotics assistance for task success.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.03759</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.03759</id><submitter>Elad Plaut</submitter><version version="v1"><date>Sun, 8 Mar 2020 11:03:05 GMT</date><size>8711kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 11 Nov 2020 10:22:35 GMT</date><size>5175kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 05:56:56 GMT</date><size>10391kb</size><source_type>D</source_type></version><title>3D Object Detection from a Single Fisheye Image Without a Single Fisheye
  Training Image</title><authors>Elad Plaut, Erez Ben Yaacov and Bat El Shlomo</authors><categories>cs.CV</categories><comments>9 pages, 7 figures</comments><acm-class>I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing monocular 3D object detection methods have been demonstrated on
rectilinear perspective images and fail in images with alternative projections
such as those acquired by fisheye cameras. Previous works on object detection
in fisheye images have focused on 2D object detection, partly due to the lack
of 3D datasets of such images. In this work, we show how to use existing
monocular 3D object detection models, trained only on rectilinear images, to
detect 3D objects in images from fisheye cameras, without using any fisheye
training data. We outperform the only existing method for monocular 3D object
detection in panoramas on a benchmark of synthetic data, despite the fact that
the existing method trains on the target non-rectilinear projection whereas we
train only on rectilinear images. We also experiment with an internal dataset
of real fisheye images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.03977</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.03977</id><submitter>Nikhil Iyer</submitter><version version="v1"><date>Mon, 9 Mar 2020 09:01:53 GMT</date><size>243kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 27 Oct 2020 15:31:50 GMT</date><size>357kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 28 Oct 2020 05:47:56 GMT</date><size>357kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 29 Oct 2020 06:58:28 GMT</date><size>357kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 1 Jun 2021 05:48:04 GMT</date><size>505kb</size><source_type>D</source_type></version><title>Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate
  Schedule</title><authors>Nikhil Iyer, V Thejas, Nipun Kwatra, Ramachandran Ramjee, Muthian
  Sivathanu</authors><categories>cs.LG stat.ML</categories><comments>34 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Several papers argue that wide minima generalize better than narrow minima.
In this paper, through detailed experiments that not only corroborate the
generalization properties of wide minima, we also provide empirical evidence
for a new hypothesis that the density of wide minima is likely lower than the
density of narrow minima. Further, motivated by this hypothesis, we design a
novel explore-exploit learning rate schedule. On a variety of image and natural
language datasets, compared to their original hand-tuned learning rate
baselines, we show that our explore-exploit schedule can result in either up to
0.84% higher absolute accuracy using the original training budget or up to 57%
reduced training time while achieving the original reported accuracy. For
example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN)
dataset by just modifying the learning rate schedule of a high performing
model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.04696</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.04696</id><submitter>Fernando P\'erez-Garc\'ia</submitter><version version="v1"><date>Mon, 9 Mar 2020 13:36:16 GMT</date><size>2035kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 8 Jan 2021 20:43:32 GMT</date><size>6673kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 12 Jan 2021 09:09:03 GMT</date><size>6673kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 10:05:29 GMT</date><size>6629kb</size><source_type>D</source_type></version><title>TorchIO: a Python library for efficient loading, preprocessing,
  augmentation and patch-based sampling of medical images in deep learning</title><authors>Fernando P\'erez-Garc\'ia, Rachel Sparks and S\'ebastien Ourselin</authors><categories>eess.IV cs.AI cs.CV cs.LG stat.ML</categories><comments>Submitted to Computer Methods and Programs in Biomedicine. 27 pages,
  7 figures. Documentation for TorchIO can be found at http://torchio.rtfd.io/</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.
  We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.
  Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://github.com/fepegar/torchio. The package can be
installed from the Python Package Index running 'pip install torchio'. It
includes a command-line interface which allows users to apply transforms to
image files without using Python. Additionally, we provide a graphical
interface within a TorchIO extension in 3D Slicer to visualize the effects of
transforms.
  TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.04924</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.04924</id><submitter>Saad Qadeer</submitter><version version="v1"><date>Tue, 10 Mar 2020 18:41:22 GMT</date><size>766kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 12 May 2021 02:27:53 GMT</date><size>1314kb</size><source_type>D</source_type></version><title>The Smooth Forcing Extension Method: A High-Order Technique for Solving
  Elliptic Equations on Complex Domains</title><authors>Saad Qadeer and Boyce E. Griffith</authors><categories>math.NA cs.NA</categories><doi>10.1016/j.jcp.2021.110390</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-order numerical methods for solving elliptic equations over arbitrary
domains typically require specialized machinery, such as high-quality
conforming grids for finite elements method, and quadrature rules for boundary
integral methods. These tools make it difficult to apply these techniques to
higher dimensions. In contrast, fixed Cartesian grid methods, such as the
immersed boundary (IB) method, are easy to apply and generalize, but typically
are low-order accurate. In this study, we introduce the Smooth Forcing
Extension (SFE) method, a fixed Cartesian grid technique that builds on the
insights of the IB method, and allows one to obtain arbitrary orders of
accuracy. Our approach relies on a novel Fourier continuation method to compute
extensions of the inhomogeneous terms to any desired regularity. This is
combined with the highly accurate Non-Uniform Fast Fourier Transform for
interpolation operations to yield a fast and robust method. Numerical tests
confirm that the technique performs precisely as expected on one-dimensional
test problems. In higher dimensions, the performance is even better, in some
cases yielding sub-geometric convergence. We also demonstrate how this
technique can be applied to solving parabolic problems and for computing the
eigenvalues of elliptic operators on general domains, in the process
illustrating its stability and amenability to generalization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.07132</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.07132</id><submitter>Aijun Zhang</submitter><version version="v1"><date>Mon, 16 Mar 2020 11:51:38 GMT</date><size>279kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 15:02:15 GMT</date><size>1403kb</size><source_type>D</source_type></version><title>GAMI-Net: An Explainable Neural Network based on Generalized Additive
  Models with Structured Interactions</title><authors>Zebin Yang, Aijun Zhang, Agus Sudjianto</authors><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lack of interpretability is an inevitable problem when using neural
network models in real applications. In this paper, an explainable neural
network based on generalized additive models with structured interactions
(GAMI-Net) is proposed to pursue a good balance between prediction accuracy and
model interpretability. GAMI-Net is a disentangled feedforward network with
multiple additive subnetworks; each subnetwork consists of multiple hidden
layers and is designed for capturing one main effect or one pairwise
interaction. Three interpretability aspects are further considered, including
a) sparsity, to select the most significant effects for parsimonious
representations; b) heredity, a pairwise interaction could only be included
when at least one of its parent main effects exists; and c) marginal clarity,
to make main effects and pairwise interactions mutually distinguishable. An
adaptive training algorithm is developed, where main effects are first trained
and then pairwise interactions are fitted to the residuals. Numerical
experiments on both synthetic functions and real-world datasets show that the
proposed model enjoys superior interpretability and it maintains competitive
prediction accuracy in comparison to the explainable boosting machine and other
classic machine learning models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.09416</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.09416</id><submitter>Nana Liu</submitter><version version="v1"><date>Fri, 20 Mar 2020 17:56:14 GMT</date><size>417kb</size><source_type>D</source_type></version><title>Quantum noise protects quantum classifiers against adversaries</title><authors>Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao and Nana Liu</authors><categories>quant-ph cs.LG</categories><comments>16 pages, 8 figures</comments><journal-ref>Phys. Rev. Research 3, 023153 (2021)</journal-ref><doi>10.1103/PhysRevResearch.3.023153</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise in quantum information processing is often viewed as a disruptive and
difficult-to-avoid feature, especially in near-term quantum technologies.
However, noise has often played beneficial roles, from enhancing weak signals
in stochastic resonance to protecting the privacy of data in differential
privacy. It is then natural to ask, can we harness the power of quantum noise
that is beneficial to quantum computing? An important current direction for
quantum computing is its application to machine learning, such as
classification problems. One outstanding problem in machine learning for
classification is its sensitivity to adversarial examples. These are small,
undetectable perturbations from the original data where the perturbed data is
completely misclassified in otherwise extremely accurate classifiers. They can
also be considered as `worst-case' perturbations by unknown noise sources. We
show that by taking advantage of depolarisation noise in quantum circuits for
classification, a robustness bound against adversaries can be derived where the
robustness improves with increasing noise. This robustness property is
intimately connected with an important security concept called differential
privacy which can be extended to quantum differential privacy. For the
protection of quantum data, this is the first quantum protocol that can be used
against the most general adversaries. Furthermore, we show how the robustness
in the classical case can be sensitive to the details of the classification
model, but in the quantum case the details of classification model are absent,
thus also providing a potential quantum advantage for classical data that is
independent of quantum speedups. This opens the opportunity to explore other
ways in which quantum noise can be used in our favour, as well as identifying
other ways quantum algorithms can be helpful that is independent of quantum
speedups.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.11067</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.11067</id><submitter>Valerio Capraro</submitter><version version="v1"><date>Tue, 24 Mar 2020 18:50:08 GMT</date><size>539kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 16:16:44 GMT</date><size>914kb</size></version><title>Punishing defectors and rewarding cooperators: Do people discriminate
  between genders?</title><authors>H\'el\`ene Barcelo and Valerio Capraro</authors><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>Forthcoming in the Journal of the Economic Science Association</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do people discriminate between men and women when they have the option to
punish defectors or reward cooperators? Here we report on four pre-registered
experiments, that shed some light on this question. Study 1 (N=544) shows that
people do not discriminate between genders when they have the option to punish
(reward) defectors (cooperators) in a one-shot prisoner's dilemma with
third-party punishment/reward. Study 2 (N=253) extends Study 1 to a different
method of punishing/rewarding: participants are asked to rate the behaviour of
a defector/cooperator on a scale of 1 to 5 stars. In this case too, we find
that people do not discriminate between genders. Study 3a (N=331) and Study 3b
(N=310) conceptually replicate Study 2 with a slightly different gender
manipulation. These latter studies show that, in situations where they do not
have specific beliefs about the gender of the defector/cooperator's partner,
neither men nor women discriminate between genders.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.11611</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.11611</id><submitter>Toktam Amanzadeh Oghaz</submitter><version version="v1"><date>Wed, 25 Mar 2020 20:15:54 GMT</date><size>2818kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 17:28:47 GMT</date><size>5638kb</size><source_type>D</source_type></version><title>Deep Agent: Studying the Dynamics of Information Spread and Evolution in
  Social Networks</title><authors>Ivan Garibay, Toktam A. Oghaz, Niloofar Yousefi, Ece C. Mutlu,
  Madeline Schiappa, Steven Scheinert, Georgios C. Anagnostopoulos, Christina
  Bouwens, Stephen M. Fiore, Alexander Mantzaris, John T. Murphy, William Rand,
  Anastasia Salter, Mel Stanfill, Gita Sukthankar, Nisha Baral, Gabriel Fair,
  Chathika Gunaratne, Neda B. Hajiakhoond, Jasser Jasser, Chathura Jayalath,
  Olivia Newton, Samaneh Saadat, Chathurani Senevirathna, Rachel Winter, Xi
  Zhang</authors><categories>cs.SI physics.soc-ph</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explains the design of a social network analysis framework,
developed under DARPA's SocialSim program, with novel architecture that models
human emotional, cognitive and social factors. Our framework is both theory and
data-driven, and utilizes domain expertise. Our simulation effort helps in
understanding how information flows and evolves in social media platforms. We
focused on modeling three information domains: cryptocurrencies, cyber threats,
and software vulnerabilities for the three interrelated social environments:
GitHub, Reddit, and Twitter. We participated in the SocialSim DARPA Challenge
in December 2018, in which our models were subjected to extensive performance
evaluation for accuracy, generalizability, explainability, and experimental
power. This paper reports the main concepts and models, utilized in our social
media modeling effort in developing a multi-resolution simulation at the user,
community, population, and content levels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.12127</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.12127</id><submitter>Matteo Aldeghi</submitter><version version="v1"><date>Thu, 26 Mar 2020 19:52:32 GMT</date><size>8674kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 22:47:31 GMT</date><size>11792kb</size><source_type>D</source_type></version><title>Gryffin: An algorithm for Bayesian optimization of categorical variables
  informed by expert knowledge</title><authors>Florian H\&quot;ase, Matteo Aldeghi, Riley J. Hickman, Lo\&quot;ic M. Roch,
  Al\'an Aspuru-Guzik</authors><categories>stat.ML cs.LG physics.app-ph</categories><comments>19 pages, 6 figures (SI: 16 pages, 14 figures). Expanded background,
  discussion, minor fixes and changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing functional molecules and advanced materials requires complex design
choices: tuning continuous process parameters such as temperatures or flow
rates, while simultaneously selecting catalysts or solvents. To date, the
development of data-driven experiment planning strategies for autonomous
experimentation has largely focused on continuous process parameters despite
the urge to devise efficient strategies for the selection of categorical
variables. Here, we introduce Gryffin, a general purpose optimization framework
for the autonomous selection of categorical variables driven by expert
knowledge. Gryffin augments Bayesian optimization based on kernel density
estimation with smooth approximations to categorical distributions. Leveraging
domain knowledge in the form of physicochemical descriptors, Gryffin can
significantly accelerate the search for promising molecules and materials.
Gryffin can further highlight relevant correlations between the provided
descriptors to inspire physical insights and foster scientific intuition. In
addition to comprehensive benchmarks, we demonstrate the capabilities and
performance of Gryffin on three examples in materials science and chemistry:
(i) the discovery of non-fullerene acceptors for organic solar cells, (ii) the
design of hybrid organic-inorganic perovskites for light harvesting, and (iii)
the identification of ligands and process parameters for Suzuki-Miyaura
reactions. Our results suggest that Gryffin, in its simplest form, is
competitive with state-of-the-art categorical optimization algorithms. However,
when leveraging domain knowledge provided via descriptors, Gryffin outperforms
other approaches while simultaneously refining this domain knowledge to promote
scientific understanding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.13726</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.13726</id><submitter>Sangwon Jung</submitter><version version="v1"><date>Mon, 30 Mar 2020 18:21:04 GMT</date><size>813kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 16 Jun 2020 08:50:49 GMT</date><size>1139kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 22 Dec 2020 07:29:33 GMT</date><size>2588kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 29 May 2021 07:39:32 GMT</date><size>2589kb</size><source_type>D</source_type></version><title>Continual Learning with Node-Importance based Adaptive Group Sparse
  Regularization</title><authors>Sangwon Jung, Hongjoon Ahn, Sungmin Cha and Taesup Moon</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel regularization-based continual learning method, dubbed as
Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group
sparsity-based penalties. Our method selectively employs the two penalties when
learning each node based its the importance, which is adaptively updated after
learning each new task. By utilizing the proximal gradient descent method for
learning, the exact sparsity and freezing of the model is guaranteed, and thus,
the learner can explicitly control the model capacity as the learning
continues. Furthermore, as a critical detail, we re-initialize the weights
associated with unimportant nodes after learning each task in order to prevent
the negative transfer that causes the catastrophic forgetting and facilitate
efficient learning of new tasks. Throughout the extensive experimental results,
we show that our AGS-CL uses much less additional memory space for storing the
regularization parameters, and it significantly outperforms several
state-of-the-art baselines on representative continual learning benchmarks for
both supervised and reinforcement learning tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.00646</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.00646</id><submitter>Dietmar Jannach</submitter><version version="v1"><date>Wed, 1 Apr 2020 18:00:47 GMT</date><size>287kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 06:16:57 GMT</date><size>453kb</size><source_type>D</source_type></version><title>A Survey on Conversational Recommender Systems</title><authors>Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen</authors><categories>cs.HC cs.AI cs.IR</categories><comments>35 pages, 5 figures</comments><journal-ref>ACM Computing Surveys, Volume 54, Issue 5, 2021</journal-ref><doi>10.1145/3453154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems are software applications that help users to find items
of interest in situations of information overload. Current research often
assumes a one-shot interaction paradigm, where the users' preferences are
estimated based on past observed behavior and where the presentation of a
ranked list of suggestions is the main, one-directional form of user
interaction. Conversational recommender systems (CRS) take a different approach
and support a richer set of interactions. These interactions can, for example,
help to improve the preference elicitation process or allow the user to ask
questions about the recommendations and to give feedback. The interest in CRS
has significantly increased in the past few years. This development is mainly
due to the significant progress in the area of natural language processing, the
emergence of new voice-controlled home assistants, and the increased use of
chatbot technology. With this paper, we provide a detailed survey of existing
approaches to conversational recommendation. We categorize these approaches in
various dimensions, e.g., in terms of the supported user intents or the
knowledge they use in the background. Moreover, we discuss technological
approaches, review how CRS are evaluated, and finally identify a number of gaps
that deserve more research in the future.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.01981</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.01981</id><submitter>Ze Yang</submitter><version version="v1"><date>Sat, 4 Apr 2020 17:32:46 GMT</date><size>1873kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 07:43:08 GMT</date><size>1239kb</size><source_type>D</source_type></version><title>Open Domain Dialogue Generation with Latent Images</title><authors>Ze Yang, Wei Wu, Huang Hu, Can Xu, Wei Wang, Zhoujun Li</authors><categories>cs.CL cs.AI cs.CV</categories><comments>AAAI2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider grounding open domain dialogues with images. Existing work
assumes that both an image and a textual context are available, but
image-grounded dialogues by nature are more difficult to obtain than textual
dialogues. Thus, we propose learning a response generation model with both
image-grounded dialogues and textual dialogues by assuming that the visual
scene information at the time of a conversation can be represented by an image,
and trying to recover the latent images of the textual dialogues through
text-to-image generation techniques. The likelihood of the two types of
dialogues is then formulated by a response generator and an image reconstructor
that are learned within a conditional variational auto-encoding framework.
Empirical studies are conducted in both image-grounded conversation and
text-based conversation. In the first scenario, image-grounded dialogues,
especially under a low-resource setting, can be effectively augmented by
textual dialogues with latent images; while in the second scenario, latent
images can enrich the content of responses and at the same time keep them
relevant to contexts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.02023</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.02023</id><submitter>Rishiraj Saha Roy</submitter><version version="v1"><date>Sat, 4 Apr 2020 21:24:52 GMT</date><size>763kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 28 Apr 2020 18:53:21 GMT</date><size>763kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 10:16:23 GMT</date><size>764kb</size><source_type>D</source_type></version><title>Towards Query Logs for Privacy Studies: On Deriving Search Queries from
  Questions</title><authors>Asia J. Biega, Jana Schmidt, Rishiraj Saha Roy</authors><categories>cs.IR</categories><comments>ECIR 2020 Short Paper</comments><doi>10.1007/978-3-030-45442-5_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Translating verbose information needs into crisp search queries is a
phenomenon that is ubiquitous but hardly understood. Insights into this process
could be valuable in several applications, including synthesizing large
privacy-friendly query logs from public Web sources which are readily available
to the academic research community. In this work, we take a step towards
understanding query formulation by tapping into the rich potential of community
question answering (CQA) forums. Specifically, we sample natural language (NL)
questions spanning diverse themes from the Stack Exchange platform, and conduct
a large-scale conversion experiment where crowdworkers submit search queries
they would use when looking for equivalent information. We provide a careful
analysis of this data, accounting for possible sources of bias during
conversion, along with insights into user-specific linguistic patterns and
search behaviors. We release a dataset of 7,000 question-query pairs from this
study to facilitate further research on query understanding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.03853</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.03853</id><submitter>Georgina Hall</submitter><version version="v1"><date>Wed, 8 Apr 2020 07:39:57 GMT</date><size>396kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 08:32:01 GMT</date><size>21266kb</size><source_type>D</source_type></version><title>Shape-Constrained Regression using Sum of Squares Polynomials</title><authors>Mihaela Curmei and Georgina Hall</authors><categories>math.OC cs.CC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of fitting a polynomial function to a set of data
points, each data point consisting of a feature vector and a response variable.
In contrast to standard polynomial regression, we require that the polynomial
regressor satisfy shape constraints, such as monotonicity,
Lipschitz-continuity, or convexity. We show how to use semidefinite programming
to obtain polynomial regressors that have these properties. We then prove that,
under some assumptions on the generation of the data points, the regressor
obtained is a consistent estimator of the underlying shape-constrained
function. We follow up with a thorough empirical comparison of our regressor to
the convex least squares estimator introduced in [Hildreth 1954, Holloway 1979]
and show that our regressor can be very valuable in settings where the number
of data points is large and where new predictions need to be made quickly and
often. We also propose a method that relies on linear and second-order cone
programs to quickly update our regressor when a new batch of data points is
provided. We conclude with two novel applications. The first application aims
to approximate the function that maps a conic program's data to its optimal
value. This enables us to obtain quick estimations of the optimal value without
solving the conic program, which can be useful for real-time decision-making.
We illustrate this on an example in inventory management contract negotiation.
In the second application, we compute optimal transport maps using shape
constraints as regularizers following [Paty 2020], and show, via a color
transfer example, that this is a setting in which our regressor significantly
outperforms other methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.05065</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.05065</id><submitter>Amir Gilad</submitter><version version="v1"><date>Fri, 10 Apr 2020 15:00:29 GMT</date><size>219kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 13 Apr 2020 01:15:29 GMT</date><size>219kb</size><source_type>D</source_type></version><title>On Multiple Semantics for Declarative Database Repairs</title><authors>Amir Gilad, Daniel Deutch, Sudeepa Roy</authors><categories>cs.DB</categories><journal-ref>SIGMOD 2020</journal-ref><doi>10.1145/3318464.3389721</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of database repairs through a rule-based framework that
we refer to as Delta Rules. Delta Rules are highly expressive and allow
specifying complex, cross-relations repair logic associated with Denial
Constraints, Causal Rules, and allowing to capture Database Triggers of
interest. We show that there are no one-size-fits-all semantics for repairs in
this inclusive setting, and we consequently introduce multiple alternative
semantics, presenting the case for using each of them. We then study the
relationships between the semantics in terms of their output and the complexity
of computation. Our results formally establish the tradeoff between the
permissiveness of the semantics and its computational complexity. We
demonstrate the usefulness of the framework in capturing multiple data repair
scenarios for an Academic Search database and the TPC-H databases, showing how
using different semantics affects the repair in terms of size and runtime, and
examining the relationships between the repairs. We also compare our approach
with SQL triggers and a state-of-the-art data repair system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.06003</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.06003</id><submitter>Pallav Bera</submitter><version version="v1"><date>Mon, 13 Apr 2020 15:22:52 GMT</date><size>6764kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 2 Aug 2020 17:38:26 GMT</date><size>8254kb</size><source_type>D</source_type></version><title>Discrimination of Internal Faults and Other Transients in an
  Interconnected System with Power Transformers and Phase Angle Regulators</title><authors>Pallav Kumar Bera, Can Isik, and Vajendra Kumar</authors><categories>eess.SP cs.SY eess.SY</categories><journal-ref>IEEE Systems Journal, 2020</journal-ref><doi>10.1109/JSYST.2020.3009203</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study solves the problem of accurate detection of internal faults and
classification of transients in a 5-bus interconnected system for Phase Angle
Regulators (PAR) and Power Transformers. The analysis prevents mal-operation of
differential relays in case of transients other than faults which include
magnetizing inrush, sympathetic inrush, external faults with CT saturation,
capacitor switching, non-linear load switching, and ferroresonance. A gradient
boosting classifier (GBC) is used to distinguish the internal faults from the
transient disturbances based on 1.5 cycles of 3-phase differential currents
registered by a change detector. After the detection of an internal fault, GBCs
are used to locate the faulty unit (Power Transformer, PAR series, or exciting
unit) and identify the type of fault. In case a transient disturbance is
detected, another GBC classifies them into the six transient disturbances. Five
most relevant frequency and time domain features obtained using Information
Gain are used to train and test the classifiers. The proposed algorithm
distinguishes the internal faults from the other transients with a balanced
accuracy of 99.95%. The faulty transformer unit is located with a balanced
accuracy of 99.5% and the different transient disturbances are identified with
a balanced accuracy of 99.3%. Moreover, the reliability of the scheme is
verified for different rating and connection of the transformers involved, CT
saturation, and noise levels in the signals. These GBC classifiers can work
together with a conventional differential relay and offer a supervisory control
over its operation. PSCAD/EMTDC software is used for simulation of the
transients and to develop the two and three-winding transformer models for
creating the internal faults including inter-turn and inter-winding faults.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.06307</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.06307</id><submitter>Hanjia Lyu</submitter><version version="v1"><date>Tue, 14 Apr 2020 05:27:55 GMT</date><size>2166kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 17 Apr 2020 15:02:50 GMT</date><size>2243kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 24 Apr 2020 13:12:00 GMT</date><size>2243kb</size><source_type>D</source_type></version><title>Sense and Sensibility: Characterizing Social Media Users Regarding the
  Use of Controversial Terms for COVID-19</title><authors>Hanjia Lyu, Long Chen, Yu Wang, Jiebo Luo</authors><categories>cs.SI</categories><journal-ref>IEEE Transactions on Big Data. 2020</journal-ref><doi>10.1109/TBDATA.2020.2996401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the world-wide development of 2019 novel coronavirus, although WHO has
officially announced the disease as COVID-19, one controversial term - &quot;Chinese
Virus&quot; is still being used by a great number of people. In the meantime, global
online media coverage about COVID-19-related racial attacks increases steadily,
most of which are anti-Chinese or anti-Asian. As this pandemic becomes
increasingly severe, more people start to talk about it on social media
platforms such as Twitter. When they refer to COVID-19, there are mainly two
ways: using controversial terms like &quot;Chinese Virus&quot; or &quot;Wuhan Virus&quot;, or using
non-controversial terms like &quot;Coronavirus&quot;. In this study, we attempt to
characterize the Twitter users who use controversial terms and those who use
non-controversial terms. We use the Tweepy API to retrieve 17 million related
tweets and the information of their authors. We find significant differences
between these two groups of Twitter users across their demographics, user-level
features like the number of followers, political following status, as well as
their geo-locations. Moreover, we apply classification models to predict
Twitter users who are more likely to use controversial terms. To our best
knowledge, this is the first large-scale social media-based study to
characterize users with respect to their usage of controversial terms during a
major crisis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.07966</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.07966</id><submitter>Abner Salgado</submitter><version version="v1"><date>Thu, 16 Apr 2020 21:12:03 GMT</date><size>28kb</size></version><version version="v2"><date>Mon, 31 May 2021 22:41:03 GMT</date><size>31kb</size></version><title>On the analysis and approximation of some models of fluids over weighted
  spaces on convex polyhedra</title><authors>Enrique Otarola and Abner Salgado</authors><categories>math.NA cs.NA math.AP</categories><msc-class>35Q35, 35Q30, 35R06, 65N15, 65N30, 76Dxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Stokes problem over convex polyhedral domains on weighted
Sobolev spaces. The weight is assumed to belong to the Muckenhoupt class $A_q$
for $q \in (1,\infty)$. We show that the Stokes problem is well-posed for all
$q$. In addition, we show that the finite element Stokes projection is stable
on weighted spaces. With the aid of these tools, we provide well-posedness and
approximation results to some classes of non-Newtonian fluids.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.09317</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.09317</id><submitter>Federico Paredes-Vall\'es</submitter><version version="v1"><date>Mon, 20 Apr 2020 14:08:28 GMT</date><size>5903kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 08:16:45 GMT</date><size>22806kb</size><source_type>D</source_type></version><title>How Do Neural Networks Estimate Optical Flow? A Neuropsychology-Inspired
  Study</title><authors>D. B. de Jong, F. Paredes-Vall\'es, G. C. H. E. de Croon</authors><categories>cs.CV cs.LG eess.IV</categories><comments>16 pages, 15 figures</comments><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021</journal-ref><doi>10.1109/TPAMI.2021.3083538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  End-to-end trained convolutional neural networks have led to a breakthrough
in optical flow estimation. The most recent advances focus on improving the
optical flow estimation by improving the architecture and setting a new
benchmark on the publicly available MPI-Sintel dataset. Instead, in this
article, we investigate how deep neural networks estimate optical flow. A
better understanding of how these networks function is important for (i)
assessing their generalization capabilities to unseen inputs, and (ii)
suggesting changes to improve their performance. For our investigation, we
focus on FlowNetS, as it is the prototype of an encoder-decoder neural network
for optical flow estimation. Furthermore, we use a filter identification method
that has played a major role in uncovering the motion filters present in animal
brains in neuropsychological research. The method shows that the filters in the
deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not
only do we find translation filters, as demonstrated in animal brains, but
thanks to the easier measurements in artificial neural networks, we even unveil
dilation, rotation, and occlusion filters. Furthermore, we find similarities in
the refinement part of the network and the perceptual filling-in process which
occurs in the mammal primary visual cortex.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.09810</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.09810</id><submitter>Martianus Frederic Ezerman</submitter><version version="v1"><date>Tue, 21 Apr 2020 08:12:08 GMT</date><size>29kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 05:43:43 GMT</date><size>31kb</size></version><title>A Graph Joining Greedy Approach to Binary de Bruijn Sequences</title><authors>Zuling Chang, Martianus Frederic Ezerman, Adamas Aqsa Fahreza, and
  Qiang Wang</authors><categories>cs.IT math.CO math.IT</categories><comments>in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using greedy algorithms to generate de Bruijn sequences is a classical
approach that has produced numerous interesting theoretical results. This paper
investigates an algorithm which we call the Generalized Prefer-Opposite (GPO).
It includes all prior greedy algorithms, with the exception of the Fleury
Algorithm applied on the de Bruijn graph, as specific instances. The GPO
Algorithm can produce any binary periodic sequences with nonlinear complexity
at least two on input a pair of suitable feedback function and initial state.
In particular, a sufficient and necessary condition for the GPO Algorithm to
generate binary de Bruijn sequences is established. This requires the use of
feedback functions with a unique cycle or loop in their respective state
graphs. Moreover, we discuss modifications to the GPO Algorithm to handle more
families of feedback functions whose state graphs have multiple cycles or
loops. These culminate in a graph joining method. Several large classes of
feedback functions are subsequently used to illustrate how the GPO Algorithm
and its modification into the Graph Joining Prefer-Opposite (GJPO) Algorithm
work in practice.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.09914</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.09914</id><submitter>Georg Gottwald A.</submitter><version version="v1"><date>Tue, 21 Apr 2020 11:29:23 GMT</date><size>216kb</size></version><title>Simulation of non-Lipschitz stochastic differential equations driven by
  $\alpha$-stable noise: a method based on deterministic homogenisation</title><authors>Georg A. Gottwald and Ian Melbourne</authors><categories>math.DS cs.NA math.NA</categories><journal-ref>Multiscale Model. Simul. 19 (2021) 665-687</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise an explicit method to integrate $\alpha$-stable stochastic
differential equations (SDEs) with non-Lipschitz coefficients. To mitigate
against numerical instabilities caused by unbounded increments of the L\'evy
noise, we use a deterministic map which has the desired SDE as its homogenised
limit. Moreover, our method naturally overcomes difficulties in expressing the
Marcus integral explicitly. We present an example of an SDE with a natural
boundary showing that our method respects the boundary whereas Euler-Maruyama
discretisation fails to do so. As a by-product we devise an entirely
deterministic method to construct $\alpha$-stable laws.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.11697</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.11697</id><submitter>Jaydip Sen</submitter><version version="v1"><date>Fri, 17 Apr 2020 19:41:22 GMT</date><size>3690kb</size></version><version version="v2"><date>Mon, 31 May 2021 14:46:58 GMT</date><size>4303kb</size></version><title>A Time Series Analysis-Based Stock Price Prediction Using Machine
  Learning and Deep Learning Models</title><authors>Sidra Mehtab and Jaydip Sen</authors><categories>q-fin.ST cs.LG stat.ML</categories><comments>This is the preprint of our paper accepted for publication in the
  Inderscience Journal International Journal of Business Forecasting and
  Marketing Intelligence. The paper consists of 53 pages, 26 Tables, and 46
  Figures</comments><journal-ref>International Journal of Business Forecasting and Marketing
  Intelligence (IJBFMI), Vol 6, No 4, pp. 272 - 335, 2020. Inderscience
  Publishers</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction of future movement of stock prices has always been a challenging
task for the researchers. While the advocates of the efficient market
hypothesis (EMH) believe that it is impossible to design any predictive
framework that can accurately predict the movement of stock prices, there are
seminal work in the literature that have clearly demonstrated that the
seemingly random movement patterns in the time series of a stock price can be
predicted with a high level of accuracy. Design of such predictive models
requires choice of appropriate variables, right transformation methods of the
variables, and tuning of the parameters of the models. In this work, we present
a very robust and accurate framework of stock price prediction that consists of
an agglomeration of statistical, machine learning and deep learning models. We
use the daily stock price data, collected at five minutes interval of time, of
a very well known company that is listed in the National Stock Exchange (NSE)
of India. The granular data is aggregated into three slots in a day, and the
aggregated data is used for building and training the forecasting models. We
contend that the agglomerative approach of model building that uses a
combination of statistical, machine learning, and deep learning approaches, can
very effectively learn from the volatile and random movement patterns in a
stock price data. We build eight classification and eight regression models
based on statistical and machine learning approaches. In addition to these
models, a deep learning regression model using a long-and-short-term memory
(LSTM) network is also built. Extensive results have been presented on the
performance of these models, and the results are critically analyzed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.12019</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.12019</id><submitter>Niladri Chatterji</submitter><version version="v1"><date>Sat, 25 Apr 2020 00:06:18 GMT</date><size>692kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 4 Sep 2020 05:46:13 GMT</date><size>698kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 25 Mar 2021 21:45:53 GMT</date><size>710kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 18:03:02 GMT</date><size>710kb</size><source_type>D</source_type></version><title>Finite-sample Analysis of Interpolating Linear Classifiers in the
  Overparameterized Regime</title><authors>Niladri S. Chatterji, Philip M. Long</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Corrected typographical errors from the previous version of this
  paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove bounds on the population risk of the maximum margin algorithm for
two-class linear classification. For linearly separable training data, the
maximum margin algorithm has been shown in previous work to be equivalent to a
limit of training with logistic loss using gradient descent, as the training
error is driven to zero. We analyze this algorithm applied to random data
including misclassification noise. Our assumptions on the clean data include
the case in which the class-conditional distributions are standard normal
distributions. The misclassification noise may be chosen by an adversary,
subject to a limit on the fraction of corrupted labels. Our bounds show that,
with sufficient over-parameterization, the maximum margin algorithm trained on
noisy data can achieve nearly optimal population risk.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.12764</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.12764</id><submitter>Mattia Samory</submitter><version version="v1"><date>Mon, 27 Apr 2020 13:07:46 GMT</date><size>186kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 10:39:03 GMT</date><size>298kb</size><source_type>D</source_type></version><title>&quot;Call me sexist, but...&quot;: Revisiting Sexism Detection Using
  Psychological Scales and Adversarial Samples</title><authors>Mattia Samory, Indira Sen, Julian Kohne, Fabian Floeck, Claudia Wagner</authors><categories>cs.CY cs.CL cs.SI</categories><comments>Indira Sen and Julian Kohne contributed equally to this work</comments><journal-ref>Proceedings of the 15th International AAAI Conference on Web and
  Social Media (ICWSM), 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research has focused on automated methods to effectively detect sexism
online. Although overt sexism seems easy to spot, its subtle forms and manifold
expressions are not. In this paper, we outline the different dimensions of
sexism by grounding them in their implementation in psychological scales. From
the scales, we derive a codebook for sexism in social media, which we use to
annotate existing and novel datasets, surfacing their limitations in breadth
and validity with respect to the construct of sexism. Next, we leverage the
annotated datasets to generate adversarial examples, and test the reliability
of sexism detection methods. Results indicate that current machine learning
models pick up on a very narrow set of linguistic markers of sexism and do not
generalize well to out-of-domain examples. Yet, including diverse data and
adversarial examples at training time results in models that generalize better
and that are more robust to artifacts of data collection. By providing a
scale-based codebook and insights regarding the shortcomings of the
state-of-the-art, we hope to contribute to the development of better and
broader models for sexism detection, including reflections on theory-driven
approaches to data collection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.13003</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.13003</id><submitter>Tian Shi</submitter><version version="v1"><date>Fri, 24 Apr 2020 20:54:17 GMT</date><size>399kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 18 Sep 2020 21:48:26 GMT</date><size>325kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 1 Jan 2021 04:50:32 GMT</date><size>517kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 03:22:08 GMT</date><size>338kb</size><source_type>D</source_type></version><title>Corpus-level and Concept-based Explanations for Interpretable Document
  Classification</title><authors>Tian Shi, Xuchao Zhang, Ping Wang, Chandan K. Reddy</authors><categories>cs.IR cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using attention weights to identify information that is important for models'
decision-making is a popular approach to interpret attention-based neural
networks. This is commonly realized in practice through the generation of a
heat-map for every single document based on attention weights. However, this
interpretation method is fragile, and easy to find contradictory examples. In
this paper, we propose a corpus-level explanation approach, which aims to
capture causal relationships between keywords and model predictions via
learning the importance of keywords for predicted labels across a training
corpus based on attention weights. Based on this idea, we further propose a
concept-based explanation method that can automatically learn higher-level
concepts and their importance to model prediction tasks. Our concept-based
explanation method is built upon a novel Abstraction-Aggregation Network, which
can automatically cluster important keywords during an end-to-end training
process. We apply these methods to the document classification task and show
that they are powerful in extracting semantically meaningful keywords and
concepts. Our consistency analysis results based on an attention-based Na\&quot;ive
Bayes classifier also demonstrate these keywords and concepts are important for
model predictions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.14064</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.14064</id><submitter>Andrzej Lingas</submitter><version version="v1"><date>Wed, 29 Apr 2020 10:34:54 GMT</date><size>28kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 20 Jul 2020 20:13:54 GMT</date><size>29kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 30 May 2021 20:15:02 GMT</date><size>29kb</size><source_type>D</source_type></version><title>Quantum and approximation algorithms for maximum witnesses of Boolean
  matrix products</title><authors>Miros{\l}aw Kowaluk and Andrzej Lingas</authors><categories>cs.DS</categories><comments>14 pages, 3 figures</comments><msc-class>F.2.2</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding maximum (or minimum) witnesses of the Boolean product
of two Boolean matrices (MW for short) has a number of important applications,
in particular the all-pairs lowest common ancestor (LCA) problem in directed
acyclic graphs (dags). The best known upper time-bound on the MW problem for
n\times n Boolean matrices of the form O(n^{2.575}) has not been substantially
improved since 2006. In order to obtain faster algorithms for this problem, we
study quantum algorithms for MW and approximation algorithms for MW (in the
standard computational model). Some of our quantum algorithms are input or
output sensitive. Our fastest quantum algorithm for the MW problem, and
consequently for the related problems, runs in time
\tilde{O}(n^{2+\lambda/2})=\tilde{O}(n^{2.434}), where \lambda satisfies the
equation \omega(1, \lambda, 1) = 1 + 1.5 \, \lambda and \omega(1, \lambda, 1)
is the exponent of the multiplication of an n \times n^{\lambda}$ matrix by an
n^{\lambda} \times n matrix. Next, we consider a relaxed version of the MW
problem (in the standard model) asking for reporting a witness of bounded rank
(the maximum witness has rank 1) for each non-zero entry of the matrix product.
First, by adapting the fastest known algorithm for maximum witnesses, we obtain
an algorithm for the relaxed problem that reports for each non-zero entry of
the product matrix a witness of rank at most \ell in time
\tilde{O}((n/\ell)n^{\omega(1,\log_n \ell,1)}). Then, by reducing the relaxed
problem to the so called k-witness problem, we provide an algorithm that
reports for each non-zero entry C[i,j] of the product matrix C a witness of
rank O(\lceil W_C(i,j)/k\rceil ), where W_C(i,j) is the number of witnesses for
C[i,j], with high probability. The algorithm runs in
\tilde{O}(n^{\omega}k^{0.4653} +n^2k) time, where \omega=\omega(1,1,1).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.14276</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.14276</id><submitter>Gaurav Mittal Mr</submitter><version version="v1"><date>Wed, 29 Apr 2020 15:46:19 GMT</date><size>23kb</size></version><version version="v2"><date>Mon, 31 May 2021 00:51:23 GMT</date><size>23kb</size></version><title>A novel two-point gradient method for Regularization of inverse problems
  in Banach spaces</title><authors>Gaurav Mittal, Ankik Kumar Giri</authors><categories>math.NA cs.NA</categories><comments>Submitted in Applicable Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel two-point gradient method for solving the
ill-posed problems in Banach spaces and study its convergence analysis. The
method is based on the well known iteratively regularized Landweber iteration
method together with an extrapolation strategy. The general formulation of
iteratively regularized Landweber iteration method in Banach spaces excludes
the use of certain functions such as total variation like penalty functionals,
$L^1$ functions etc. The novel scheme presented in this paper allows to use
such non-smooth penalty terms that can be helpful in practical applications
involving the reconstruction of several important features of solutions such as
piecewise constancy and sparsity. We carefully discuss the choices for
important parameters, such as combination parameters and step sizes involved in
the design of the method. Additionally, we discuss an example to validate our
assumptions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.14740</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.14740</id><submitter>Rawad Bitar</submitter><version version="v1"><date>Thu, 30 Apr 2020 12:58:53 GMT</date><size>105kb</size></version><version version="v2"><date>Wed, 16 Sep 2020 14:33:19 GMT</date><size>127kb</size></version><version version="v3"><date>Thu, 17 Sep 2020 12:08:26 GMT</date><size>107kb</size></version><version version="v4"><date>Thu, 13 May 2021 07:58:18 GMT</date><size>340kb</size></version><version version="v5"><date>Tue, 1 Jun 2021 10:05:13 GMT</date><size>340kb</size></version><title>Criss-Cross Insertion and Deletion Correcting Codes</title><authors>Rawad Bitar, Lorenz Welter, Ilia Smagloy, Antonia Wachter-Zeh and
  Eitan Yaakobi</authors><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory for possible
  publication. Several examples are added to help understand the concepts
  explained in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of constructing codes correcting deletions in
arrays. Under this model, it is assumed that an $n\times n$ array can
experience deletions of rows and columns. These deletion errors are referred to
as $(t_r,t_c)$-criss-cross deletions if $t_r$ rows and $t_c$ columns are
deleted, while a code correcting these deletion patterns is called a
$(t_r,t_c)$-criss-cross deletion correction code. The definitions for
criss-cross insertions are similar.
  It is first shown that when $t_r=t_c$ the problems of correcting criss-cross
deletions and criss-cross insertions are equivalent. The focus of this paper
lies on the case of $(1,1)$-criss-cross deletions. A non-asymptotic upper bound
on the cardinality of $(1,1)$-criss-cross deletion correction codes is shown
which assures that the redundancy is at least $2n-3+2\log n$ bits. A code
construction with an existential encoding and an explicit decoding algorithm is
presented. The redundancy of the construction is at most $2n+4 \log n + 7 +2
\log e$. A construction with explicit encoder and decoder is presented. The
explicit encoder adds an extra $5\log n + 5$ bits of redundancy to the
construction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.00123</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.00123</id><submitter>Dinesh Raghu</submitter><version version="v1"><date>Thu, 30 Apr 2020 22:10:00 GMT</date><size>201kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 04:27:47 GMT</date><size>467kb</size><source_type>D</source_type></version><title>Unsupervised Learning of KB Queries in Task-Oriented Dialogs</title><authors>Dinesh Raghu, Nikhil Gupta, Mausam</authors><categories>cs.LG cs.CL stat.ML</categories><comments>Presented at ACL 2021</comments><journal-ref>Transactions of the Association for Computational Linguistics
  (2021) 9: 374-390</journal-ref><doi>10.1162/tacl_a_00372</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task-oriented dialog (TOD) systems often need to formulate knowledge base
(KB) queries corresponding to the user intent and use the query results to
generate system responses. Existing approaches require dialog datasets to
explicitly annotate these KB queries -- these annotations can be time
consuming, and expensive. In response, we define the novel problems of
predicting the KB query and training the dialog agent, without explicit KB
query annotation. For query prediction, we propose a reinforcement learning
(RL) baseline, which rewards the generation of those queries whose KB results
cover the entities mentioned in subsequent dialog. Further analysis reveals
that correlation among query attributes in KB can significantly confuse memory
augmented policy optimization (MAPO), an existing state of the art RL agent. To
address this, we improve the MAPO baseline with simple but important
modifications suited to our task. To train the full TOD system for our setting,
we propose a pipelined approach: it independently predicts when to make a KB
query (query position predictor), then predicts a KB query at the predicted
position (query predictor), and uses the results of predicted query in
subsequent dialog (next response predictor). Overall, our work proposes first
solutions to our novel problem, and our analysis highlights the research
challenges in training TOD systems without query annotation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.00575</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.00575</id><submitter>Mathieu Mari</submitter><version version="v1"><date>Fri, 1 May 2020 19:10:31 GMT</date><size>3298kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:06:53 GMT</date><size>468kb</size><source_type>D</source_type></version><title>Approximating maximum integral multiflows on bounded genus graphs</title><authors>Chien-chung Huang, Mathieu Mari, Claire Mathieu, Jens Vygen</authors><categories>cs.DS cs.CG cs.DM</categories><msc-class>57M15, 05C38, 05C62, 90C27, 90C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise the first constant-factor approximation algorithm for finding an
integral multi-commodity flow of maximum total value for instances where the
supply graph together with the demand edges can be embedded on an orientable
surface of bounded genus. This extends recent results for planar instances. Our
techniques include an uncrossing algorithm, which is significantly more
difficult than in the planar case, a partition of the cycles in the support of
an LP solution into free homotopy classes, and a new rounding procedure for
freely homotopic non-separating cycles.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.00758</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.00758</id><submitter>P\'eter L\'aszl\'o Juh\'asz</submitter><version version="v1"><date>Sat, 2 May 2020 09:19:31 GMT</date><size>14kb</size></version><version version="v2"><date>Tue, 5 May 2020 20:03:07 GMT</date><size>14kb</size></version><version version="v3"><date>Sat, 28 Nov 2020 19:38:14 GMT</date><size>118kb</size></version><version version="v4"><date>Tue, 1 Dec 2020 15:25:51 GMT</date><size>121kb</size></version><version version="v5"><date>Wed, 17 Mar 2021 10:15:51 GMT</date><size>97kb</size></version><version version="v6"><date>Fri, 30 Apr 2021 07:29:19 GMT</date><size>97kb</size></version><title>Information Propagation in Stochastic Networks</title><authors>Peter Laszlo Juhasz</authors><categories>cs.SI physics.soc-ph</categories><msc-class>90B15 (Primary), 05C80, 60G25, 90C35</msc-class><doi>10.1016/j.physa.2021.126070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a network-based stochastic information propagation model is
developed. The information flow is modeled by a probabilistic differential
equation system. The numerical solution of these equations leads to the
expected number of informed nodes as a function of time and reveals the
relationship between the degrees of the nodes and their reception time. The
validity of the model is justified by Monte Carlo network simulation through
the analysis of information propagation in scale-free and Erdos-Renyi networks.
It has been found that the developed model provides more accurate results
compared to the widely used network-based SI mean-field model, especially in
sparse networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.01317</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.01317</id><submitter>Jicong Fan</submitter><version version="v1"><date>Mon, 4 May 2020 08:32:21 GMT</date><size>3569kb</size></version><version version="v2"><date>Wed, 2 Dec 2020 08:51:54 GMT</date><size>3815kb</size></version><title>Robust Non-Linear Matrix Factorization for Dictionary Learning,
  Denoising, and Clustering</title><authors>Jicong Fan, Chengrun Yang, Madeleine Udell</authors><categories>cs.LG stat.ML</categories><journal-ref>IEEE Transactions on Signal Processing 69, 1755-1770 (2021)</journal-ref><doi>10.1109/TSP.2021.3062988</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low dimensional nonlinear structure abounds in datasets across computer
vision and machine learning. Kernelized matrix factorization techniques have
recently been proposed to learn these nonlinear structures for denoising,
classification, dictionary learning, and missing data imputation, by observing
that the image of the matrix in a sufficiently large feature space is low-rank.
However, these nonlinear methods fail in the presence of sparse noise or
outliers. In this work, we propose a new robust nonlinear factorization method
called Robust Non-Linear Matrix Factorization (RNLMF). RNLMF constructs a
dictionary for the data space by factoring a kernelized feature space; a noisy
matrix can then be decomposed as the sum of a sparse noise matrix and a clean
data matrix that lies in a low dimensional nonlinear manifold. RNLMF is robust
to sparse noise and outliers and scales to matrices with thousands of rows and
columns. Empirically, RNLMF achieves noticeable improvements over baseline
methods in denoising and clustering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.01795</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.01795</id><submitter>Kundan Krishna</submitter><version version="v1"><date>Mon, 4 May 2020 19:10:26 GMT</date><size>436kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 24 Oct 2020 04:09:10 GMT</date><size>7882kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 14:48:09 GMT</date><size>657kb</size><source_type>D</source_type></version><title>Generating SOAP Notes from Doctor-Patient Conversations Using Modular
  Summarization Techniques</title><authors>Kundan Krishna, Sopan Khosla, Jeffrey P. Bigham, Zachary C. Lipton</authors><categories>cs.CL cs.AI cs.LG stat.ML</categories><comments>Published at ACL 2021 Main Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following each patient visit, physicians draft long semi-structured clinical
summaries called SOAP notes. While invaluable to clinicians and researchers,
creating digital SOAP notes is burdensome, contributing to physician burnout.
In this paper, we introduce the first complete pipelines to leverage deep
summarization models to generate these notes based on transcripts of
conversations between physicians and patients. After exploring a spectrum of
methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an
algorithm that (i) extracts important utterances relevant to each summary
section; (ii) clusters together related utterances; and then (iii) generates
one summary sentence per cluster. Cluster2Sent outperforms its purely
abstractive counterpart by 8 ROUGE-1 points, and produces significantly more
factual and coherent sentences as assessed by expert human evaluators. For
reproducibility, we demonstrate similar benefits on the publicly available AMI
dataset. Our results speak to the benefits of structuring summaries into
sections and annotating supporting evidence when constructing summarization
corpora.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.01993</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.01993</id><submitter>Marc Zeller</submitter><version version="v1"><date>Tue, 5 May 2020 08:11:40 GMT</date><size>574kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 10:06:03 GMT</date><size>575kb</size></version><title>Automatic Generation of RAMS Analyses from Model-based Functional
  Descriptions using UML State Machines</title><authors>Christof Kaukewitsch, Henrik Papist, Marc Zeller, Martin Rothfelder</authors><categories>cs.SE cs.SY eess.SY</categories><doi>10.1109/RAMS48030.2020.9153667</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's industrial practice, safety, reliability or availability artifacts
such as fault trees, Markov models or FMEAs are mainly created manually by
experts, often distinctively decoupled from systems engineering activities.
Significant efforts, costs and timely requirements are involved to conduct the
required analyses. In this paper, we describe a novel integrated model-based
approach of systems engineering and dependability analyses. The behavior of
system components is specified by UML state machines determining
intended/correct and undesired/faulty behavior. Based on this information, our
approach automatically generates different dependability analyses in the form
of fault trees. Hence, alternative system layouts can easily be evaluated. The
same applies for simple variations of the logical input-output relations of
logical units such as controllers. We illustrate the feasibility of our
approach with the help of simple examples using a prototypical implementation
of the presented concepts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.03355</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.03355</id><submitter>Xi He</submitter><version version="v1"><date>Thu, 7 May 2020 09:42:36 GMT</date><size>2283kb</size></version><version version="v2"><date>Mon, 11 May 2020 13:09:25 GMT</date><size>2294kb</size></version><version version="v3"><date>Sun, 15 Nov 2020 14:04:37 GMT</date><size>2295kb</size></version><version version="v4"><date>Thu, 3 Jun 2021 15:23:11 GMT</date><size>3318kb</size></version><title>Quantum correlation alignment for unsupervised domain adaptation</title><authors>Xi He</authors><categories>quant-ph cs.LG</categories><comments>11 pages, 9 figures</comments><journal-ref>Phys. Rev. A 102, 032410 (2020)</journal-ref><doi>10.1103/PhysRevA.102.032410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation alignment (CORAL), a representative domain adaptation (DA)
algorithm, decorrelates and aligns a labelled source domain dataset to an
unlabelled target domain dataset to minimize the domain shift such that a
classifier can be applied to predict the target domain labels. In this paper,
we implement the CORAL on quantum devices by two different methods. One method
utilizes quantum basic linear algebra subroutines (QBLAS) to implement the
CORAL with exponential speedup in the number and dimension of the given data
samples. The other method is achieved through a variational hybrid
quantum-classical procedure. In addition, the numerical experiments of the
CORAL with three different types of data sets, namely the synthetic data, the
synthetic-Iris data, the handwritten digit data, are presented to evaluate the
performance of our work. The simulation results prove that the variational
quantum correlation alignment algorithm (VQCORAL) can achieve competitive
performance compared with the classical CORAL.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.03482</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.03482</id><submitter>Ao Liu</submitter><version version="v1"><date>Wed, 6 May 2020 08:15:24 GMT</date><size>851kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 29 Oct 2020 11:14:44 GMT</date><size>885kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 23 Apr 2021 13:44:19 GMT</date><size>4648kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 7 May 2021 08:57:07 GMT</date><size>4879kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 1 Jun 2021 03:17:58 GMT</date><size>4857kb</size><source_type>D</source_type></version><title>AN-GCN: An Anonymous Graph Convolutional Network Defense Against
  Edge-Perturbing Attack</title><authors>Ao Liu, Beibei Li, Tao Li, Pan Zhou</authors><categories>cs.LG cs.CR</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node classification based on graph convolutional networks (GCNs) is
vulnerable to adversarial attacks by maliciously perturbing graph structures,
such as inserting or deleting graph edges. The existing research works do not
seem to be able to unify the formulation of such edge-perturbing attacks, so it
is unable to design a more essential defense scheme. Thus, in this paper,
considering that most researchers find the attack scheme by ergodically
perturbing edge in a diverse and manual way, we unify such edge-perturbing
attacks as an automatic general attack model, named edge-reading attack (ERA).
ERA can find the concealed and high success rate attack scheme by automatically
traverse and perturb edges repeatedly. ERA is also the unified description form
of edge-perturbing attacks in the form of the mathematical formula. Relying on
ERA, we further demonstrate the vulnerability of GCNs, i.e., the edge-reading
permission can easily create opportunities for adversarial attacks. To address
this problem, we propose an anonymous graph convolutional network (AN-GCN),
which allows classifying nodes without reading the edge information of GCNs.
Specifically, we propose the node localization theorem for the first time to
demonstrate how GCN locates nodes during training. Then, AN-GCN is designed to
make the nodes participate in the prediction anonymously, thus withdrawing the
edge-reading permission of the model. Since AN-GCN can predict node categories
without edge information, the administrator can withdraw the read permission of
edge information to all roles (including attackers), so attackers will lose the
basic condition of injecting edge perturbations. Extensive evaluations show
that, our proposed general attack model can accurately manipulate the
classification results of the target nodes, thus maintaining high-level
security in defending against edge-perturbing adversarial attacks on graph
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.03788</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.03788</id><submitter>Xinshao Wang Dr</submitter><version version="v1"><date>Thu, 7 May 2020 22:35:04 GMT</date><size>1516kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 17 May 2020 22:10:17 GMT</date><size>1987kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 8 Jun 2020 13:36:09 GMT</date><size>2470kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 29 Jun 2020 11:04:32 GMT</date><size>2471kb</size><source_type>D</source_type></version><version version="v5"><date>Fri, 9 Oct 2020 12:45:28 GMT</date><size>2723kb</size><source_type>D</source_type></version><version version="v6"><date>Wed, 2 Jun 2021 12:27:53 GMT</date><size>3433kb</size><source_type>D</source_type></version><title>ProSelfLC: Progressive Self Label Correction for Training Robust Deep
  Neural Networks</title><authors>Xinshao Wang, Yang Hua, Elyor Kodirov, David A. Clifton, Neil M.
  Robertson</authors><categories>cs.LG cs.CV stat.ML</categories><comments>ProSelfLC is the first method to trust self knowledge progressively
  and adaptively. ProSelfLC redirects and promotes entropy minimisation, which
  is in marked contrast to recent practices of confidence penalty [42, 33, 6]</comments><journal-ref>CVPR 2021</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  To train robust deep neural networks (DNNs), we systematically study several
target modification approaches, which include output regularisation, self and
non-self label correction (LC). Two key issues are discovered: (1) Self LC is
the most appealing as it exploits its own knowledge and requires no extra
models. However, how to automatically decide the trust degree of a learner as
training goes is not well answered in the literature? (2) Some methods penalise
while the others reward low-entropy predictions, prompting us to ask which one
is better?
  To resolve the first issue, taking two well-accepted propositions--deep
neural networks learn meaningful patterns before fitting noise [3] and minimum
entropy regularisation principle [10]--we propose a novel end-to-end method
named ProSelfLC, which is designed according to learning time and entropy.
Specifically, given a data point, we progressively increase trust in its
predicted label distribution versus its annotated one if a model has been
trained for enough time and the prediction is of low entropy (high confidence).
For the second issue, according to ProSelfLC, we empirically prove that it is
better to redefine a meaningful low-entropy status and optimise the learner
toward it. This serves as a defence of entropy minimisation.
  We demonstrate the effectiveness of ProSelfLC through extensive experiments
in both clean and noisy settings. The source code is available at
https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.
  Keywords: entropy minimisation, maximum entropy, confidence penalty, self
knowledge distillation, label correction, label noise, semi-supervised
learning, output regularisation
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.04493</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.04493</id><submitter>Mihailo Jovanovic</submitter><version version="v1"><date>Sat, 9 May 2020 18:50:18 GMT</date><size>1423kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 23 Feb 2021 17:36:03 GMT</date><size>1211kb</size><source_type>D</source_type></version><title>Well-conditioned ultraspherical and spectral integration methods for
  resolvent analysis of channel flows of Newtonian and viscoelastic fluids</title><authors>Gokul Hariharan, Satish Kumar, Mihailo R. Jovanovi\'c</authors><categories>physics.flu-dyn cs.NA math.AP math.DS math.NA math.OC</categories><comments>To appear in the Journal of Computational Physics</comments><doi>10.1016/j.jcp.2021.110241</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modal and nonmodal analyses of fluid flows provide fundamental insight into
the early stages of transition to turbulence. Eigenvalues of the dynamical
generator govern temporal growth or decay of individual modes, while singular
values of the frequency response operator quantify the amplification of
disturbances for linearly stable flows. In this paper, we develop
well-conditioned ultraspherical and spectral integration methods for frequency
response analysis of channel flows of Newtonian and viscoelastic fluids. Even
if a discretization method is well-conditioned, we demonstrate that
calculations can be erroneous if singular values are computed as the
eigenvalues of a cascade connection of the frequency response operator and its
adjoint. To address this issue, we utilize a feedback interconnection of the
frequency response operator with its adjoint to avoid computation of inverses
and facilitate robust singular value decomposition. Specifically, in contrast
to conventional spectral collocation methods, the proposed method (i) produces
reliable results in channel flows of viscoelastic fluids at high Weissenberg
numbers ($\sim 500$); and (ii) does not require a staggered grid for the
equations in primitive variables.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.05092</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.05092</id><submitter>Christopher Arthurs DPhil</submitter><version version="v1"><date>Sat, 2 May 2020 21:53:39 GMT</date><size>7266kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 12 May 2020 11:24:36 GMT</date><size>7266kb</size><source_type>D</source_type></version><title>Active Training of Physics-Informed Neural Networks to Aggregate and
  Interpolate Parametric Solutions to the Navier-Stokes Equations</title><authors>Christopher J Arthurs and Andrew P King</authors><categories>physics.comp-ph cs.CE cs.LG stat.ML</categories><comments>16 pages, 9 figures; added missing details from author affiliations</comments><doi>10.1016/j.jcp.2021.110364</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to train a neural network which approximates
solutions to the Navier-Stokes equations across a region of parameter space, in
which the parameters define physical properties such as domain shape and
boundary conditions. The contributions of this work are threefold:
  1) To demonstrate that neural networks can be efficient aggregators of whole
families of parameteric solutions to physical problems, trained using data
created with traditional, trusted numerical methods such as finite elements.
Advantages include extremely fast evaluation of pressure and velocity at any
point in physical and parameter space (asymptotically, ~3 $\mu s$ / query), and
data compression (the network requires 99\% less storage space compared to its
own training data).
  2) To demonstrate that the neural networks can accurately interpolate between
finite element solutions in parameter space, allowing them to be instantly
queried for pressure and velocity field solutions to problems for which
traditional simulations have never been performed.
  3) To introduce an active learning algorithm, so that during training, a
finite element solver can automatically be queried to obtain additional
training data in locations where the neural network's predictions are in most
need of improvement, thus autonomously acquiring and efficiently distributing
training data throughout parameter space.
  In addition to the obvious utility of Item 2, above, we demonstrate an
application of the network in rapid parameter sweeping, very precisely
predicting the degree of narrowing in a tube which would result in a 50\%
increase in end-to-end pressure difference at a given flow rate. This
capability could have applications in both medical diagnosis of arterial
disease, and in computer-aided design.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.06571</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.06571</id><submitter>Yubo Wang</submitter><version version="v1"><date>Wed, 13 May 2020 20:17:46 GMT</date><size>1648kb</size></version><version version="v2"><date>Thu, 26 Nov 2020 01:51:34 GMT</date><size>1901kb</size></version><title>An asymptotic-preserving dynamical low-rank method for the multi-scale
  multi-dimensional linear transport equation</title><authors>Lukas Einkemmer, Jingwei Hu, Yubo Wang</authors><categories>math.NA cs.NA</categories><msc-class>82C70, 65M99, 65L04</msc-class><doi>10.1016/j.jcp.2021.110353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a dynamical low-rank method to reduce the computational
complexity for solving the multi-scale multi-dimensional linear transport
equation. The method is based on a macro-micro decomposition of the equation.
The proposed numerical method uses the low rank approximation only for the
micro part of the solution. The time and spatial discretizations are done
properly so that the overall scheme is second order accurate and
asymptotic-preserving (AP); that is, in the diffusive regime, the scheme
becomes a macroscopic solver for the limiting diffusion equation and is
automatically low rank. We demonstrate the accuracy and efficiency of the
proposed low rank method by a number of two-dimensional examples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.07107</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.07107</id><submitter>Alexey Kutalev</submitter><version version="v1"><date>Mon, 27 Apr 2020 11:17:57 GMT</date><size>269kb</size><source_type>D</source_type></version><title>Natural Way to Overcome the Catastrophic Forgetting in Neural Networks</title><authors>Alexey Kutalev</authors><categories>cs.LG stat.ML</categories><comments>9 pages, 3 figures</comments><doi>10.25559/SITITO.16.202002.331-337</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not so long ago, a method was discovered that successfully overcomes the
catastrophic forgetting of neural networks. Although we know about the cases of
using this method to preserve skills when adapting pre-trained networks to
particular tasks, it has not yet obtained widespread distribution. In this
paper, we would like to propose an alternative method of overcoming
catastrophic forgetting based on the total absolute signal passing through each
connection in the network. This method has a simple implementation and seems to
us essentially close to the processes occurring in the brain of animals to
preserve previously learned skills during subsequent learning. We hope that the
ease of implementation of this method will serve its wide application.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.07478</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.07478</id><submitter>Sean Walton</submitter><version version="v1"><date>Fri, 15 May 2020 11:40:53 GMT</date><size>667kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 08:46:49 GMT</date><size>15304kb</size><source_type>D</source_type></version><title>Evaluating Mixed-Initiative Procedural Level Design Tools using a
  Triple-Blind Mixed-Method User Study</title><authors>Sean P. Walton and Alma A. M. Rahat and James Stovold</authors><categories>cs.NE cs.AI cs.HC</categories><comments>Accepted to be Published in: IEEE Transactions on Games</comments><doi>10.1109/TG.2021.3086215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Results from a triple-blind mixed-method user study into the effectiveness of
mixed-initiative tools for the procedural generation of game levels are
presented. A tool which generates levels using interactive evolutionary
optimisation was designed for this study which (a) is focused on supporting the
designer to explore the design space and (b) only requires the designer to
interact with it by designing levels. The tool identifies level design patterns
in an initial hand-designed map and uses that information to drive an
interactive optimisation algorithm. A rigorous user study was designed which
compared the experiences of designers using the mixed-initiative tool to
designers who were given a tool which provided completely random level
suggestions. The designers using the mixed-initiative tool showed an increased
engagement in the level design task, reporting that it was effective in
inspiring new ideas and design directions. This provides significant evidence
that procedural content generation can be used as a powerful tool to support
the human design process.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.08665</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.08665</id><submitter>Shixiang Zhu</submitter><version version="v1"><date>Fri, 15 May 2020 04:22:18 GMT</date><size>4510kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 19:54:08 GMT</date><size>6895kb</size><source_type>D</source_type></version><title>Spatio-Temporal Point Processes with Attention for Traffic Congestion
  Event Modeling</title><authors>Shixiang Zhu, Ruyi Ding, Minghe Zhang, Pascal Van Hentenryck, Yao Xie</authors><categories>cs.LG stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel framework for modeling traffic congestion events over road
networks. Using multi-modal data by combining count data from traffic sensors
with police reports that report traffic incidents, we aim to capture two types
of triggering effect for congestion events. Current traffic congestion at one
location may cause future congestion over the road network, and traffic
incidents may cause spread traffic congestion. To model the non-homogeneous
temporal dependence of the event on the past, we use a novel attention-based
mechanism based on neural networks embedding for point processes. To
incorporate the directional spatial dependence induced by the road network, we
adapt the &quot;tail-up&quot; model from the context of spatial statistics to the traffic
network setting. We demonstrate our approach's superior performance compared to
the state-of-the-art methods for both synthetic and real data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.08948</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.08948</id><submitter>Nuri Mert Vural</submitter><version version="v1"><date>Sat, 16 May 2020 11:41:13 GMT</date><size>1056kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:22:57 GMT</date><size>1055kb</size><source_type>D</source_type></version><title>Achieving Online Regression Performance of LSTMs with Simple RNNs</title><authors>N. Mert Vural, Fatih Ilhan, Selim F. Yilmaz, Salih Erg\&quot;ut and
  Suleyman S. Kozat</authors><categories>cs.LG stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:2003.03601</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) are widely used for online regression due to
their ability to generalize nonlinear temporal dependencies. As an RNN model,
Long-Short-Term-Memory Networks (LSTMs) are commonly preferred in practice, as
these networks are capable of learning long-term dependencies while avoiding
the vanishing gradient problem. However, due to their large number of
parameters, training LSTMs requires considerably longer training time compared
to simple RNNs (SRNNs). In this paper, we achieve the online regression
performance of LSTMs with SRNNs efficiently. To this end, we introduce a
first-order training algorithm with a linear time complexity in the number of
parameters. We show that when SRNNs are trained with our algorithm, they
provide very similar regression performance with the LSTMs in two to three
times shorter training time. We provide strong theoretical analysis to support
our experimental results by providing regret bounds on the convergence rate of
our algorithm. Through an extensive set of experiments, we verify our
theoretical work and demonstrate significant performance improvements of our
algorithm with respect to LSTMs and the other state-of-the-art learning models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.09351</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.09351</id><submitter>Jun Zhang</submitter><version version="v1"><date>Tue, 19 May 2020 10:22:14 GMT</date><size>22kb</size></version><version version="v2"><date>Mon, 31 May 2021 07:46:58 GMT</date><size>37kb</size></version><title>Cores in discrete exchange economies with complex endowments</title><authors>Jun Zhang</authors><categories>econ.TH cs.GT</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The core is a traditional and useful solution concept in economic theory. But
in discrete exchange economies without transfers, when endowments are complex,
the core may be empty. This motivates Balbuzanov and Kotowski (2019) to
interpret endowments as exclusion rights and propose a new concept called
exclusion core. Our contribution is twofold. First, we propose a rectification
of the core to solve its problem under complex endowments. Second, we propose a
refinement of Balbuzanov and Kotowski's exclusion core to improve its
performance. Our two core concepts share a common idea of correcting the
misused altruism of unaffected agents in blocking coalitions. We propose a
mechanism to find allocations in the two cores.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.09890</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.09890</id><submitter>Lars Ailo Bongo</submitter><version version="v1"><date>Wed, 20 May 2020 07:34:50 GMT</date><size>522kb</size></version><title>Interactive exploration of population scale pharmacoepidemiology
  datasets</title><authors>Tengel Ekrem Skar, Einar Holsb{\o}, Kristian Svendsen, Lars Ailo Bongo</authors><categories>q-bio.QM cs.CV</categories><doi>10.1145/3388440.3414862</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population-scale drug prescription data linked with adverse drug reaction
(ADR) data supports the fitting of models large enough to detect drug use and
ADR patterns that are not detectable using traditional methods on smaller
datasets. However, detecting ADR patterns in large datasets requires tools for
scalable data processing, machine learning for data analysis, and interactive
visualization. To our knowledge no existing pharmacoepidemiology tool supports
all three requirements. We have therefore created a tool for interactive
exploration of patterns in prescription datasets with millions of samples. We
use Spark to preprocess the data for machine learning and for analyses using
SQL queries. We have implemented models in Keras and the scikit-learn
framework. The model results are visualized and interpreted using live Python
coding in Jupyter. We apply our tool to explore a 384 million prescription data
set from the Norwegian Prescription Database combined with a 62 million
prescriptions for elders that were hospitalized. We preprocess the data in two
minutes, train models in seconds, and plot the results in milliseconds. Our
results show the power of combining computational power, short computation
times, and ease of use for analysis of population scale pharmacoepidemiology
datasets. The code is open source and available at:
https://github.com/uit-hdl/norpd_prescription_analyses
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.10329</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.10329</id><submitter>Hui-Po Wang</submitter><version version="v1"><date>Wed, 20 May 2020 19:48:04 GMT</date><size>9252kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 13:55:02 GMT</date><size>15658kb</size><source_type>D</source_type></version><title>InfoScrub: Towards Attribute Privacy by Targeted Obfuscation</title><authors>Hui-Po Wang, Tribhuvanesh Orekondy, Mario Fritz</authors><categories>cs.CV cs.AI cs.CR</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personal photos of individuals when shared online, apart from exhibiting a
myriad of memorable details, also reveals a wide range of private information
and potentially entails privacy risks (e.g., online harassment, tracking). To
mitigate such risks, it is crucial to study techniques that allow individuals
to limit the private information leaked in visual data. We tackle this problem
in a novel image obfuscation framework: to maximize entropy on inferences over
targeted privacy attributes, while retaining image fidelity. We approach the
problem based on an encoder-decoder style architecture, with two key novelties:
(a) introducing a discriminator to perform bi-directional translation
simultaneously from multiple unpaired domains; (b) predicting an image
interpolation which maximizes uncertainty over a target set of attributes. We
find our approach generates obfuscated images faithful to the original input
images, and additionally increase uncertainty by 6.2$\times$ (or up to 0.85
bits) over the non-obfuscated counterparts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.10360</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.10360</id><submitter>Gereon Fox</submitter><version version="v1"><date>Wed, 20 May 2020 21:17:43 GMT</date><size>8280kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:00:26 GMT</date><size>12552kb</size><source_type>D</source_type></version><title>VideoForensicsHQ: Detecting High-quality Manipulated Face Videos</title><authors>Gereon Fox, Wentao Liu, Hyeongwoo Kim, Hans-Peter Seidel, Mohamed
  Elgharib, Christian Theobalt</authors><categories>cs.CV</categories><comments>ICME 2021 camera-ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are concerns that new approaches to the synthesis of high quality face
videos may be misused to manipulate videos with malicious intent. The research
community therefore developed methods for the detection of modified footage and
assembled benchmark datasets for this task. In this paper, we examine how the
performance of forgery detectors depends on the presence of artefacts that the
human eye can see. We introduce a new benchmark dataset for face video forgery
detection, of unprecedented quality. It allows us to demonstrate that existing
detection techniques have difficulties detecting fakes that reliably fool the
human eye. We thus introduce a new family of detectors that examine
combinations of spatial and temporal features and outperform existing
approaches both in terms of detection accuracy and generalization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.10619</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.10619</id><submitter>Sindhu Padakandla</submitter><version version="v1"><date>Tue, 19 May 2020 09:42:42 GMT</date><size>165kb</size></version><title>A Survey of Reinforcement Learning Algorithms for Dynamically Varying
  Environments</title><authors>Sindhu Padakandla</authors><categories>cs.LG cs.AI stat.ML</categories><report-no>Volume 54, Issue 6</report-no><journal-ref>ACM Computing Surveys 2021</journal-ref><doi>10.1145/3459991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning (RL) algorithms find applications in inventory
control, recommender systems, vehicular traffic management, cloud computing and
robotics. The real-world complications of many tasks arising in these domains
makes them difficult to solve with the basic assumptions underlying classical
RL algorithms. RL agents in these applications often need to react and adapt to
changing operating conditions. A significant part of research on single-agent
RL techniques focuses on developing algorithms when the underlying assumption
of stationary environment model is relaxed. This paper provides a survey of RL
methods developed for handling dynamically varying environment models. The goal
of methods not limited by the stationarity assumption is to help autonomous
agents adapt to varying operating conditions. This is possible either by
minimizing the rewards lost during learning by RL agent or by finding a
suitable policy for the RL agent which leads to efficient operation of the
underlying system. A representative collection of these algorithms is discussed
in detail in this work along with their categorization and their relative
merits and demerits. Additionally we also review works which are tailored to
application domains. Finally, we discuss future enhancements for this field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.10680</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.10680</id><submitter>Anton G. Artemov</submitter><version version="v1"><date>Wed, 20 May 2020 10:29:22 GMT</date><size>183kb</size></version><version version="v2"><date>Thu, 3 Sep 2020 14:43:23 GMT</date><size>183kb</size></version><version version="v3"><date>Fri, 18 Sep 2020 10:27:35 GMT</date><size>275kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 19 Nov 2020 14:05:47 GMT</date><size>200kb</size><source_type>D</source_type></version><title>Sparse approximate matrix-matrix multiplication for density matrix
  purification with error control</title><authors>Anton G. Artemov, Emanuel H. Rubensson</authors><categories>math.NA cs.NA physics.comp-ph</categories><comments>9 pages, 3 figures</comments><msc-class>68Q25, 65Y05, 65Y20, 65F50</msc-class><acm-class>G.1.0; D.1.3</acm-class><doi>10.1016/j.jcp.2021.110354</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for strict error control in sparse approximate
matrix-matrix multiplication. The method combines an error bound and a
parameter sweep to select an appropriate threshold value. The scheme for error
control and the sparse approximate multiplication are implemented using the
Chunks and Tasks parallel programming model. We demonstrate the performance of
the method in parallel linear scaling electronic structure calculations using
density matrix purification with rigorous error control.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.10711</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.10711</id><submitter>Moran Koren</submitter><version version="v1"><date>Thu, 21 May 2020 15:08:58 GMT</date><size>1055kb</size><source_type>D</source_type></version><title>Sequential Fundraising and Social Insurance</title><authors>Amir Ban and Moran Koren</authors><categories>cs.GT econ.TH</categories><doi>10.1145/3391403.3399479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seed fundraising for ventures often takes place by sequentially approaching
potential contributors, whose decisions are observed by other contributors. The
fundraising succeeds when a target number of investments is reached. When a
single investment suffices, this setting resembles the classic information
cascades model. However, when more than one investment is needed, the solution
is radically different and exhibits surprising complexities. We analyze a
setting where contributors' levels of information are i.i.d. draws from a known
distribution, and find strategies in equilibrium for all. We show that
participants rely on {\em social insurance}, i.e., invest despite having
unfavorable private information, relying on future player strategies to protect
them from loss. {\em Delegation} is an extreme form of social insurance where a
contributor will unconditionally invest, effectively delegating the decision to
future players. In a typical fundraising, early contributors will invest
unconditionally, stopping when the target is &quot;close enough&quot;, thus {\em de
facto} delegating the business of determining fundraising success or failure to
the last contributors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.10930</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.10930</id><submitter>James Melbourne</submitter><version version="v1"><date>Thu, 21 May 2020 22:40:31 GMT</date><size>16kb</size></version><title>Reversals of R\'enyi Entropy Inequalities under Log-Concavity</title><authors>James Melbourne and Tomasz Tkocz</authors><categories>math.PR cs.IT math.IT</categories><journal-ref>IEEE Trans. Inform. Theory 67 (2021), no. 1, 45-51</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a discrete analog of the R\'enyi entropy comparison due to
Bobkov and Madiman. For log-concave variables on the integers, the min entropy
is within log e of the usual Shannon entropy. Additionally we investigate the
entropic Rogers-Shephard inequality studied by Madiman and Kontoyannis, and
establish a sharp R\'enyi version for certain parameters in both the continuous
and discrete cases
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.11516</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.11516</id><submitter>Ivan Puddu</submitter><version version="v1"><date>Sat, 23 May 2020 11:33:34 GMT</date><size>3240kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 10 Jun 2020 12:56:20 GMT</date><size>3240kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 18:40:09 GMT</date><size>1857kb</size><source_type>D</source_type></version><title>Frontal Attack: Leaking Control-Flow in SGX via the CPU Frontend</title><authors>Ivan Puddu, Moritz Schneider, Miro Haller, Srdjan \v{C}apkun</authors><categories>cs.CR</categories><comments>Accepted for publication at the 30th USENIX Security Symposium
  (USENIX Security 21)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new timing side-channel attack on Intel CPU processors. Our
Frontal attack exploits timing differences that arise from how the CPU frontend
fetches and processes instructions while being interrupted. In particular, we
observe that in modern Intel CPUs, some instructions' execution times will
depend on which operations precede and succeed them, and on their virtual
addresses. Unlike previous attacks that could only profile branches if they
contained different code or had known branch targets, the Frontal attack allows
the adversary to distinguish between instruction-wise identical branches. As
the attack requires OS capabilities to set the interrupts, we use it to exploit
SGX enclaves. Our attack further demonstrates that secret-dependent branches
should not be used even alongside defenses to current controlled-channel
attacks. We show that the adversary can use the Frontal attack to extract a
secret from an SGX enclave if that secret was used as a branching condition for
two instruction-wise identical branches. We successfully tested the attack on
all the available Intel CPUs with SGX (until 10th gen) and used it to leak
information from two commonly used cryptographic libraries.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.11524</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.11524</id><submitter>Muhammad E. H. Chowdhury</submitter><version version="v1"><date>Sat, 23 May 2020 12:22:28 GMT</date><size>1709kb</size></version><version version="v2"><date>Mon, 1 Jun 2020 14:34:17 GMT</date><size>1704kb</size></version><version version="v3"><date>Tue, 2 Jun 2020 11:53:04 GMT</date><size>1439kb</size></version><version version="v4"><date>Mon, 8 Jun 2020 10:07:55 GMT</date><size>5258kb</size></version><version version="v5"><date>Thu, 18 Feb 2021 21:34:31 GMT</date><size>1751kb</size></version><version version="v6"><date>Tue, 1 Jun 2021 12:37:22 GMT</date><size>1704kb</size></version><title>Deep Learning for Reliable Classification of COVID-19, MERS, and SARS
  from Chest X-Ray Images</title><authors>Anas Tahir, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Uzair
  Khurshid, Farayi Musharavati, M. T. Islam, Serkan Kiranyaz, Muhammad E. H.
  Chowdhury</authors><categories>eess.IV cs.CV cs.LG</categories><comments>10 Figures, 4 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly
spreading Coronavirus infestation. Severe Acute Respiratory Syndrome (SARS) and
Middle East Respiratory Syndrome (MERS), which outbreak in 2002 and 2011, and
the current COVID-19 pandemic are all from the same family of coronavirus. This
work aims to classify COVID-19, SARS, and MERS chest X-ray (CXR) images using
deep Convolutional Neural Networks (CNNs). A unique database was created,
so-called QU-COVID-family, consisting of 423 COVID-19, 144 MERS, and 134 SARS
CXR images. Besides, a robust COVID-19 recognition system was proposed to
identify lung regions using a CNN segmentation model (U-Net), and then classify
the segmented lung images as COVID-19, MERS, or SARS using a pre-trained CNN
classifier. Furthermore, the Score-CAM visualization method was utilized to
visualize classification output and understand the reasoning behind the
decision of deep CNNs. Several Deep Learning classifiers were trained and
tested; four outperforming algorithms were reported. Original and preprocessed
images were used individually and all together as the input(s) to the networks.
Two recognition schemes were considered: plain CXR classification and segmented
CXR classification. For plain CXRs, it was observed that InceptionV3
outperforms other networks with a 3-channel scheme and achieves sensitivities
of 99.5%, 93.1%, and 97% for classifying COVID-19, MERS, and SARS images,
respectively. In contrast, for segmented CXRs, InceptionV3 outperformed using
the original CXR dataset and achieved sensitivities of 96.94%, 79.68%, and
90.26% for classifying COVID-19, MERS, and SARS images, respectively. All
networks showed high COVID-19 detection sensitivity (&gt;96%) with the segmented
lung images. This indicates the unique radiographic signature of COVID-19 cases
in the eyes of AI, which is often a challenging task for medical doctors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.11791</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.11791</id><submitter>Sourav Das</submitter><version version="v1"><date>Sun, 24 May 2020 16:16:54 GMT</date><size>805kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 26 May 2020 07:30:23 GMT</date><size>1149kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 04:00:46 GMT</date><size>1137kb</size><source_type>D</source_type></version><title>Better Late than Never; Scaling Computation in Blockchains by Delaying
  Execution</title><authors>Sourav Das, Nitin Awathare, Ling Ren, Vinay Joseph Ribeiro, Umesh
  Bellur</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof-of-Work~(PoW) based blockchains typically allocate only a tiny fraction
(e.g., less than 1% for Ethereum) of the average interarrival
time~($\mathbb{I}$) between blocks for validating transactions. A trivial
increase in validation time~($\tau$) introduces the popularly known Verifier's
Dilemma, and as we demonstrate, causes more forking and increases unfairness.
Large $\tau$ also reduces the tolerance for safety against a Byzantine
adversary. Solutions that offload validation to a set of non-chain nodes
(a.k.a. off-chain approaches) suffer from trust issues that are non-trivial to
resolve.
  In this paper, we present Tuxedo, the first on-chain protocol to
theoretically scale $\tau/\mathbb{I} \approx 1$ in PoW blockchains. The key
innovation in Tuxedo is to separate the consensus on the ordering of
transactions from their execution. We achieve this by allowing miners to delay
validation of transactions in a block by up to $\zeta$ blocks, where $\zeta$ is
a system parameter. We perform security analysis of Tuxedo considering all
possible adversarial strategies in a synchronous network with end-to-end delay
$\Delta$ and demonstrate that Tuxedo achieves security equivalent to known
results for longest chain PoW Nakamoto consensus. Additionally, we also suggest
a principled approach for practical choices of parameter $\zeta$ as per the
application requirement. Our prototype implementation of Tuxedo atop Ethereum
demonstrates that it can scale $\tau$ without suffering the harmful effects of
naive scaling in existing blockchains.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.12413</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.12413</id><submitter>Johannes K\&quot;ohler</submitter><version version="v1"><date>Mon, 25 May 2020 21:26:18 GMT</date><size>2217kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 17 Mar 2021 14:33:17 GMT</date><size>2034kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 14:10:40 GMT</date><size>2312kb</size><source_type>D</source_type></version><title>Constrained nonlinear output regulation using model predictive control
  -- extended version</title><authors>Johannes K\&quot;ohler, Matthias A. M\&quot;uller, Frank Allg\&quot;ower</authors><categories>eess.SY cs.SY math.OC</categories><comments>Extended version of accepted paper in Transaction on Automatic
  Control, 2021. Contains the following additional results: Exponential bounds
  on the suboptimality index using an observability condition and an extension
  of the derived theory to the noisy error feedback case</comments><journal-ref>Transaction on Automatic Control, 2021</journal-ref><doi>10.1109/TAC.2021.3081080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model predictive control (MPC) framework to solve the
constrained nonlinear output regulation problem. The main feature of the
proposed framework is that the application does not require the solution to
classical regulator (Francis-Byrnes-Isidori) equations or any other offline
design procedure. In particular, the proposed formulation simply minimizes the
predicted output error, possibly with some input regularization. Instead of
using terminal cost/sets or a positive definite stage cost as is standard in
MPC theory, we build on the theoretical results by Grimm et al. 2005 using a
detectability notion. The proposed formulation is applicable if the constrained
nonlinear regulation problem is (strictly) feasible, the plant is incrementally
stabilizable and incrementally input-output to state stable
(i-IOSS/detectable). We show that for minimum phase systems such a design
ensures exponential stability of the regulator manifold. We also provide a
design procedure in case of unstable zero dynamics using an incremental input
regularization and a nonresonance condition. Inherent robustness properties for
the noisy error/output-feedback case are established under simplifying
assumptions (e.g. no state constraints). The theoretical results are
illustrated with an example involving offset free tracking with noisy error
feedback. The paper also contains novel results for MPC without terminal
constraints with positive semidefinite input/output stage costs that are of
independent interest.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.13746</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.13746</id><submitter>Yinan Wang</submitter><version version="v1"><date>Thu, 28 May 2020 02:35:48 GMT</date><size>2146kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 01:53:10 GMT</date><size>2451kb</size><source_type>D</source_type></version><title>Tensor decomposition to Compress Convolutional Layers in Deep Learning</title><authors>Yinan Wang, Weihong &quot;Grace&quot; Guo, Xiaowei Yue</authors><categories>cs.LG stat.ML</categories><comments>35 pages, IISE Transactions</comments><doi>10.1080/24725854.2021.1894514</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature extraction for tensor data serves as an important step in many tasks
such as anomaly detection, process monitoring, image classification, and
quality control. Although many methods have been proposed for tensor feature
extraction, there are still two challenges that need to be addressed: 1) how to
reduce the computation cost for high dimensional and large volume tensor data;
2) how to interpret the output features and evaluate their significance. {The
most recent methods in deep learning, such as Convolutional Neural Network
(CNN), have shown outstanding performance in analyzing tensor data, but their
wide adoption is still hindered by model complexity and lack of
interpretability. To fill this research gap, we propose to use CP-decomposition
to approximately compress the convolutional layer (CPAC-Conv layer) in deep
learning. The contributions of our work could be summarized into three aspects:
(1) we adapt CP-decomposition to compress convolutional kernels and derive the
expressions of both forward and backward propagations for our proposed
CPAC-Conv layer; (2) compared with the original convolutional layer, the
proposed CPAC-Conv layer can reduce the number of parameters without decaying
prediction performance. It can combine with other layers to build novel deep
Neural Networks; (3) the value of decomposed kernels indicates the significance
of the corresponding feature map, which provides us with insights to guide
feature selection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.14296</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.14296</id><submitter>Hamidreza Abin</submitter><version version="v1"><date>Thu, 28 May 2020 21:05:02 GMT</date><size>606kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 22 Feb 2021 09:43:02 GMT</date><size>1159kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 01:28:30 GMT</date><size>775kb</size><source_type>D</source_type></version><title>An Analytical Model for Molecular Communication over a Non-linear
  Reaction-Diffusion Medium</title><authors>Hamidreza Abin, Amin Gohari, and Masoumeh Nasiri-Kenari</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges in diffusion-based molecular communication is
dealing with the non-linearity of reaction-diffusion chemical equations. While
numerical methods can be used to solve these equations, a change in the input
signals or the parameters of the medium requires one to redo the simulations.
This makes it difficult to design modulation schemes and practically impossible
to prove the optimality of a given transmission strategy. In this paper, we
provide an analytical technique for modeling the non-linearity of chemical
reaction equations based on the perturbation method. The perturbation method
expresses the solution in terms of an infinite power series. An approximate
solution can be found by keeping the leading terms of the power series. The
approximate solution is shown to track the true solution if either the
simulation time interval or the reaction rate is sufficiently small.
Approximate solutions for long time intervals are also discussed. An
illustrative example is given. For this example, it is shown that when the
reaction rate (or the total time interval) is low, instead of using a
continuous release waveform, it is optimal for the transmitters to release
molecules at two time instances.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.14356</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.14356</id><submitter>Aleks Gurfinkel</submitter><version version="v1"><date>Fri, 29 May 2020 00:51:12 GMT</date><size>1284kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 20:56:51 GMT</date><size>3209kb</size><source_type>D</source_type></version><title>Adjustable reach in a network centrality based on current flows</title><authors>Aleks J. Gurfinkel, Per Arne Rikvold</authors><categories>physics.soc-ph cs.SI</categories><journal-ref>Phys. Rev. E 103, 052308 (2021)</journal-ref><doi>10.1103/PhysRevE.103.052308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centrality, which quantifies the &quot;importance&quot; of individual nodes, is among
the most essential concepts in modern network theory. Most prominent centrality
measures can be expressed as an aggregation of influence flows between pairs of
nodes. As there are many ways in which influence can be defined, many different
centrality measures are in use. Parametrized centralities allow further
flexibility and utility by tuning the centrality calculation to the regime most
appropriate for a given network. Here, we identify two categories of centrality
parameters. Reach parameters control the attenuation of influence flows between
distant nodes. Grasp parameters control the centrality's potential to send
influence flows along multiple, often nongeodesic paths. Combining these
categories with Borgatti's centrality types [S. P. Borgatti, Social Networks
27, 55-71 (2005)], we arrive at a novel classification system for parametrized
centralities. Using this classification, we identify the notable absence of any
centrality measures that are radial, reach parametrized, and based on acyclic,
conservative flows of influence. We therefore introduce the ground-current
centrality, which is a measure of precisely this type. Because of its unique
position in the taxonomy, the ground-current centrality has significant
advantages over similar centralities. We demonstrate that, compared to other
conserved-flow centralities, it has a simpler mathematical description.
Compared to other reach centralities, it robustly preserves an intuitive rank
ordering across a wide range of network architectures. We also show that it
produces a consistent distribution of centrality values among the nodes,
neither trivially equally spread (delocalization), nor overly focused on a few
nodes (localization). Other reach centralities exhibit both of these behaviors
on regular networks and hub networks, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.14458</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.14458</id><submitter>Domagoj \'Cevid MMath</submitter><version version="v1"><date>Fri, 29 May 2020 09:05:00 GMT</date><size>6812kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:21:04 GMT</date><size>27646kb</size><source_type>D</source_type></version><title>Distributional Random Forests: Heterogeneity Adjustment and Multivariate
  Distributional Regression</title><authors>Domagoj \'Cevid, Loris Michel, Jeffrey N\&quot;af, Nicolai Meinshausen,
  Peter B\&quot;uhlmann</authors><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Forests (Breiman, 2001) is a successful and widely used regression and
classification algorithm. Part of its appeal and reason for its versatility is
its (implicit) construction of a kernel-type weighting function on training
data, which can also be used for targets other than the original mean
estimation. We propose a novel forest construction for multivariate responses
based on their joint conditional distribution, independent of the estimation
target and the data model. It uses a new splitting criterion based on the MMD
distributional metric, which is suitable for detecting heterogeneity in
multivariate distributions. The induced weights define an estimate of the full
conditional distribution, which in turn can be used for arbitrary and
potentially complicated targets of interest. The method is very versatile and
convenient to use, as we illustrate on a wide range of examples. The code is
available as Python and R packages drf.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.00249</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.00249</id><submitter>Barry Haddow</submitter><version version="v1"><date>Sat, 30 May 2020 12:23:10 GMT</date><size>645kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 22:04:56 GMT</date><size>1279kb</size><source_type>D</source_type></version><title>Dynamic Masking for Improved Stability in Spoken Language Translation</title><authors>Yuekun Yao and Barry Haddow</authors><categories>cs.CL</categories><comments>Presented at AMTA, 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For spoken language translation (SLT) in live scenarios such as conferences,
lectures and meetings, it is desirable to show the translation to the user as
quickly as possible, avoiding an annoying lag between speaker and translated
captions. In other words, we would like low-latency, online SLT. If we assume a
pipeline of automatic speech recognition (ASR) and machine translation (MT)
then a viable approach to online SLT is to pair an online ASR system, with a a
retranslation strategy, where the MT system re-translates every update received
from ASR. However this can result in annoying &quot;flicker&quot; as the MT system
updates its translation. A possible solution is to add a fixed delay, or &quot;mask&quot;
to the the output of the MT system, but a fixed global mask introduces
undesirable latency to the output. We show how this mask can be set
dynamically, improving the latency-flicker trade-off without sacrificing
translation quality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.00262</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.00262</id><submitter>Sosuke Nishikawa</submitter><version version="v1"><date>Sat, 30 May 2020 13:28:03 GMT</date><size>243kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 08:23:32 GMT</date><size>11082kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 07:00:44 GMT</date><size>11082kb</size><source_type>D</source_type></version><title>Data Augmentation with Unsupervised Machine Translation Improves the
  Structural Similarity of Cross-lingual Word Embeddings</title><authors>Sosuke Nishikawa, Ryokan Ri and Yoshimasa Tsuruoka</authors><categories>cs.CL</categories><comments>Accepted to ACL-IJCNLP 2021 SRW</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised cross-lingual word embedding (CLWE) methods learn a linear
transformation matrix that maps two monolingual embedding spaces that are
separately trained with monolingual corpora. This method relies on the
assumption that the two embedding spaces are structurally similar, which does
not necessarily hold true in general. In this paper, we argue that using a
pseudo-parallel corpus generated by an unsupervised machine translation model
facilitates the structural similarity of the two embedding spaces and improves
the quality of CLWEs in the unsupervised mapping method. We show that our
approach outperforms other alternative approaches given the same amount of
data, and, through detailed analysis, we show that data augmentation with the
pseudo data from unsupervised machine translation is especially effective for
mapping-based CLWEs because (1) the pseudo data makes the source and target
corpora (partially) parallel; (2) the pseudo data contains information on the
original language that helps to learn similar embedding spaces between the
source and target languages.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.01236</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.01236</id><submitter>Matteo Pradella</submitter><version version="v1"><date>Mon, 1 Jun 2020 20:14:30 GMT</date><size>271kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 09:41:44 GMT</date><size>277kb</size><source_type>D</source_type></version><title>Aperiodicity, Star-freeness, and First-order Definability of Structured
  Context-Free Languages</title><authors>Dino Mandrioli (1), Matteo Pradella (1 and 2), Stefano Crespi Reghizzi
  (1 and 2) ((1) Politecnico di Milano, (2) CNR-IEIIT)</authors><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classic result in formal language theory is the equivalence among
noncounting, or aperiodic, regular languages, and languages defined through
star-free regular expressions, or first-order logic. Together with first-order
completeness of linear temporal logic these results constitute a theoretical
foundation for model-checking algorithms. Extending these results to structured
subclasses of context-free languages, such as tree-languages did not work as
smoothly: for instance W. Thomas showed that there are star-free tree languages
that are counting. We show, instead, that investigating the same properties
within the family of operator precedence languages leads to equivalences that
perfectly match those on regular languages. The study of this old family of
context-free languages has been recently resumed to enhance not only parsing
(the original motivation of its inventor R. Floyd) but also to exploit their
algebraic and logic properties. We have been able to reproduce the classic
results of regular languages for this much larger class by going back to string
languages rather than tree languages. Since operator precedence languages
strictly include other classes of structured languages such as visibly pushdown
languages, the same results given in this paper hold as trivial corollary for
that family too.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.01414</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.01414</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Tue, 2 Jun 2020 06:42:22 GMT</date><size>35kb</size></version><version version="v2"><date>Thu, 11 Jun 2020 07:10:28 GMT</date><size>35kb</size></version><version version="v3"><date>Wed, 2 Jun 2021 03:07:41 GMT</date><size>35kb</size><source_type>D</source_type></version><title>Enhanced Universal Dependency Parsing with Second-Order Inference and
  Mixture of Training Data</title><authors>Xinyu Wang, Yong Jiang, Kewei Tu</authors><categories>cs.CL cs.LG</categories><comments>IWPT 2020 shared task. After fixing the bug, our proposed parser
  performs better than the team that ranked 1st in the official results</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents the system used in our submission to the \textit{IWPT
2020 Shared Task}. Our system is a graph-based parser with second-order
inference. For the low-resource Tamil corpus, we specially mixed the training
data of Tamil with other languages and significantly improved the performance
of Tamil. Due to our misunderstanding of the submission requirements, we
submitted graphs that are not connected, which makes our system only rank
\textbf{6th} over 10 teams. However, after we fixed this problem, our system is
0.6 ELAS higher than the team that ranked \textbf{1st} in the official results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.03199</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.03199</id><submitter>Chiranjibi Sitaula</submitter><version version="v1"><date>Fri, 5 Jun 2020 01:55:24 GMT</date><size>4328kb</size><source_type>D</source_type></version><title>Scene Image Representation by Foreground, Background and Hybrid Features</title><authors>Chiranjibi Sitaula and Yong Xiang and Sunil Aryal and Xuequan Lu</authors><categories>cs.CV cs.LG</categories><comments>Submitted to Expert Systems with Applications (ESWA), 28 pages and 17
  images</comments><journal-ref>Expert Systems with Applications (ESWA), 2021</journal-ref><doi>10.1016/j.eswa.2021.115285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous methods for representing scene images based on deep learning
primarily consider either the foreground or background information as the
discriminating clues for the classification task. However, scene images also
require additional information (hybrid) to cope with the inter-class similarity
and intra-class variation problems. In this paper, we propose to use hybrid
features in addition to foreground and background features to represent scene
images. We suppose that these three types of information could jointly help to
represent scene image more accurately. To this end, we adopt three VGG-16
architectures pre-trained on ImageNet, Places, and Hybrid (both ImageNet and
Places) datasets for the corresponding extraction of foreground, background and
hybrid information. All these three types of deep features are further
aggregated to achieve our final features for the representation of scene
images. Extensive experiments on two large benchmark scene datasets (MIT-67 and
SUN-397) show that our method produces the state-of-the-art classification
performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.04099</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.04099</id><submitter>Luca Giuzzi DPhil</submitter><version version="v1"><date>Sun, 7 Jun 2020 09:37:03 GMT</date><size>12kb</size></version><version version="v2"><date>Fri, 16 Oct 2020 14:59:40 GMT</date><size>13kb</size></version><version version="v3"><date>Sun, 30 May 2021 12:04:24 GMT</date><size>16kb</size><source_type>D</source_type></version><title>On Hermitian varieties in $\mathrm{PG}(6,q^2)$</title><authors>Angela Aguglia, Luca Giuzzi, Masaaki Homma</authors><categories>math.CO cs.DM</categories><comments>13 pages/revised version</comments><msc-class>51E21, 51E15, 51E20</msc-class><journal-ref>Ars Math. Contemporanea 2021</journal-ref><doi>10.26493/1855-3974.2358.3c9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we characterize the non-singular Hermitian variety ${\mathcal
H}(6,q^2)$ of $\mathrm{PG}(6, q^2)$, $q\neq2$ among the irreducible
hypersurfaces of degree $q+1$ in $\mathrm{PG}(6, q^2)$ not containing solids by
the number of its points and the existence of a solid $S$ meeting it in
$q^4+q^2+1$ points.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05158</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05158</id><submitter>Liangzu Peng</submitter><version version="v1"><date>Tue, 9 Jun 2020 09:52:15 GMT</date><size>199kb</size></version><version version="v2"><date>Wed, 30 Dec 2020 03:27:36 GMT</date><size>27kb</size></version><version version="v3"><date>Tue, 1 Jun 2021 06:36:14 GMT</date><size>78kb</size><source_type>D</source_type></version><title>Homomorphic Sensing of Subspace Arrangements</title><authors>Liangzu Peng and Manolis C. Tsakiris</authors><categories>cs.LG math.AG stat.ML</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homomorphic sensing is a recent algebraic-geometric framework that studies
the unique recovery of points in a linear subspace from their images under a
given collection of linear maps. It has been successful in interpreting such a
recovery in the case of permutations composed by coordinate projections, an
important instance in applications known as unlabeled sensing, which models
data that are out of order and have missing values. In this paper, we provide
tighter and simpler conditions that guarantee the unique recovery for the
single-subspace case, extend the result to the case of a subspace arrangement,
and show that the unique recovery in a single subspace is locally stable under
noise. We specialize our results to several examples of homomorphic sensing
such as real phase retrieval and unlabeled sensing. In so doing, in a unified
way, we obtain conditions that guarantee the unique recovery for those
examples, typically known via diverse techniques in the literature, as well as
novel conditions for sparse and unsigned versions of unlabeled sensing.
Similarly, our noise result also implies that the unique recovery in unlabeled
sensing is locally stable.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05167</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05167</id><submitter>Sara Asgari</submitter><version version="v1"><date>Tue, 9 Jun 2020 10:21:21 GMT</date><size>735kb</size></version><version version="v2"><date>Sun, 14 Jun 2020 12:17:09 GMT</date><size>624kb</size></version><version version="v3"><date>Fri, 3 Jul 2020 12:44:30 GMT</date><size>624kb</size></version><version version="v4"><date>Sat, 30 Jan 2021 16:56:33 GMT</date><size>621kb</size></version><version version="v5"><date>Sun, 21 Feb 2021 20:07:23 GMT</date><size>621kb</size></version><version version="v6"><date>Sun, 30 May 2021 17:25:08 GMT</date><size>621kb</size></version><title>Towards Generating Benchmark Datasets for Worm Infection Studies</title><authors>Sara Asgari and Babak Sadeghiyan</authors><categories>cs.CR cs.NI</categories><doi>10.1109/IST50524.2020.9345845</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Worm origin identification and propagation path reconstruction are among the
essential problems in digital forensics. Until now, several methods have been
proposed for this purpose. However, evaluating these methods is a big challenge
because there are no suitable datasets containing both normal background
traffic and worm traffic to evaluate these methods. In this paper, we
investigate different methods of generating such datasets and suggest a
technique for this purpose. ReaSE is a tool for the creation of realistic
simulation environments. However, it needs some modifications to be suitable
for generating the datasets. So we make required modifications to it. Then, we
generate several datasets for Slammer, Code Red I, Code Red II and modified
versions of these worms in different scenarios using our technique and make
them publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05265</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05265</id><submitter>Shengtian Zhou</submitter><version version="v1"><date>Fri, 5 Jun 2020 20:55:23 GMT</date><size>9161kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 15 Jun 2020 19:56:43 GMT</date><size>9161kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 6 Oct 2020 19:39:45 GMT</date><size>9509kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 9 Oct 2020 00:25:35 GMT</date><size>9509kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 22 Feb 2021 23:46:57 GMT</date><size>11982kb</size><source_type>D</source_type></version><version version="v6"><date>Wed, 2 Jun 2021 20:34:31 GMT</date><size>12258kb</size><source_type>D</source_type></version><title>MISIM: A Neural Code Semantics Similarity System Using the Context-Aware
  Semantics Structure</title><authors>Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Nesime Tatbul,
  Jesmin Jahan Tithi, Niranjan Hasabnis, Paul Petersen, Timothy Mattson, Tim
  Kraska, Pradeep Dubey, Vivek Sarkar, Justin Gottschlich</authors><categories>cs.LG cs.SE stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2003.11118</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code semantics similarity can be used for many tasks such as code
recommendation, automated software defect correction, and clone detection. Yet,
the accuracy of such systems has not yet reached a level of general purpose
reliability. To help address this, we present Machine Inferred Code Similarity
(MISIM), a neural code semantics similarity system consisting of two core
components: (i)MISIM uses a novel context-aware semantics structure, which was
purpose-built to lift semantics from code syntax; (ii)MISIM uses an extensible
neural code similarity scoring algorithm, which can be used for various neural
network architectures with learned parameters. We compare MISIM to four
state-of-the-art systems, including two additional hand-customized models, over
328K programs consisting of over 18 million lines of code. Our experiments show
that MISIM has 8.08% better accuracy (using MAP@R) compared to the next best
performing system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05304</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05304</id><submitter>Georgios Bakirtzis</submitter><version version="v1"><date>Tue, 9 Jun 2020 14:45:01 GMT</date><size>4863kb</size><source_type>D</source_type></version><title>An Ontological Metamodel for Cyber-Physical System Safety, Security, and
  Resilience Coengineering</title><authors>Georgios Bakirtzis and Tim Sherburne and Stephen Adams and Barry M.
  Horowitz and Peter A. Beling and Cody H. Fleming</authors><categories>cs.SE cs.SY eess.SY</categories><doi>10.1007/s10270-021-00892-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System complexity has become ubiquitous in the design, assessment, and
implementation of practical and useful cyber-physical systems. This increased
complexity is impacting the management of models necessary for designing
cyber-physical systems that are able to take into account a number of
``-ilities'', such that they are safe and secure and ultimately resilient to
disruption of service. We propose an ontological metamodel for system design
that augments an already existing industry metamodel to capture the
relationships between various model elements and safety, security, and
resilient considerations. Employing this metamodel leads to more cohesive and
structured modeling efforts with an overall increase in scalability, usability,
and unification of already existing models. In turn, this leads to a
mission-oriented perspective in designing security defenses and resilience
mechanisms to combat undesirable behaviors. We illustrate this metamodel in an
open-source GraphQL implementation, which can interface with a number of
modeling languages. We support our proposed metamodel with a detailed
demonstration using an oil and gas pipeline model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05656</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05656</id><submitter>Jian Liang</submitter><version version="v1"><date>Wed, 10 Jun 2020 05:08:30 GMT</date><size>283kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 9 Oct 2020 06:39:58 GMT</date><size>191kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 05:01:10 GMT</date><size>165kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 06:51:16 GMT</date><size>165kb</size><source_type>D</source_type></version><title>Why Attentions May Not Be Interpretable?</title><authors>Bing Bai, Jian Liang, Guanhua Zhang, Hao Li, Kun Bai, Fei Wang</authors><categories>stat.ML cs.LG</categories><comments>Proceedings of the 27th ACM SIGKDD International Conference on
  Knowledge Discovery &amp; Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention-based methods have played important roles in model interpretations,
where the calculated attention weights are expected to highlight the critical
parts of inputs~(e.g., keywords in sentences). However, recent research found
that attention-as-importance interpretations often do not work as we expected.
For example, learned attention weights sometimes highlight less meaningful
tokens like &quot;[SEP]&quot;, &quot;,&quot;, and &quot;.&quot;, and are frequently uncorrelated with other
feature importance indicators like gradient-based measures. A recent debate
over whether attention is an explanation or not has drawn considerable
interest. In this paper, we demonstrate that one root cause of this phenomenon
is the combinatorial shortcuts, which means that, in addition to the
highlighted parts, the attention weights themselves may carry extra information
that could be utilized by downstream models after attention layers. As a
result, the attention weights are no longer pure importance indicators. We
theoretically analyze combinatorial shortcuts, design one intuitive experiment
to show their existence, and propose two methods to mitigate this issue. We
conduct empirical studies on attention-based interpretation models. The results
show that the proposed methods can effectively improve the interpretability of
attention mechanisms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05681</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05681</id><submitter>Cong Li</submitter><version version="v1"><date>Wed, 10 Jun 2020 06:42:36 GMT</date><size>1899kb</size></version><version version="v2"><date>Fri, 14 Aug 2020 08:31:59 GMT</date><size>3826kb</size></version><version version="v3"><date>Wed, 30 Dec 2020 19:40:30 GMT</date><size>3978kb</size></version><version version="v4"><date>Wed, 2 Jun 2021 14:15:07 GMT</date><size>4379kb</size></version><title>Off-Policy Risk-Sensitive Reinforcement Learning Based Constrained
  Robust Optimal Control</title><authors>Cong Li, Fangzhou Liu, Zhehua Zhou, Martin Buss</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an off-policy risk-sensitive reinforcement learning based
control framework for stabilization of a continuous-time nonlinear system that
subjects to additive disturbances, input saturation, and state constraints. By
introducing pseudo controls and risk-sensitive input and state penalty terms,
the constrained robust stabilization problem of the original system is
converted into an equivalent optimal control problem of an auxiliary system.
Then, aiming at the transformed optimal control problem, we adopt adaptive
dynamic programming (ADP) implemented as a single critic structure to get the
approximate solution to the value function of the Hamilton-Jacobi-Bellman (HJB)
equation, which results in the approximate optimal control policy that is able
to satisfy both input and state constraints under disturbances. By replaying
experience data to the off-policy weight update law of the critic artificial
neural network, the weight convergence is guaranteed. Moreover, to get
experience data to achieve a sufficient excitation required for the weight
convergence, online and offline algorithms are developed to serve as principled
ways to record informative experience data. The equivalence proof demonstrates
that the optimal control strategy of the auxiliary system robustly stabilizes
the original system without violating input and state constraints. The proofs
of system stability and weight convergence are provided. Simulation results
reveal the validity of the proposed control framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05896</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05896</id><submitter>Carl Allen</submitter><version version="v1"><date>Wed, 10 Jun 2020 15:30:54 GMT</date><size>644kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 25 Aug 2020 16:34:19 GMT</date><size>514kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 18 Feb 2021 13:04:04 GMT</date><size>607kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 12:20:00 GMT</date><size>664kb</size><source_type>D</source_type></version><title>A Probabilistic Model for Discriminative and Neuro-Symbolic
  Semi-Supervised Learning</title><authors>Carl Allen, Ivana Bala\v{z}evi\'c, Timothy Hospedales</authors><categories>cs.LG cs.LO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much progress has been made in semi-supervised learning (SSL) by combining
methods that exploit different aspects of the data distribution, e.g.
consistency regularisation relies on properties of $p(x)$, whereas entropy
minimisation pertains to the label distribution $p(y|x)$. Focusing on the
latter, we present a probabilistic model for discriminative SSL, that mirrors
its classical generative counterpart. Under the assumption $y|x$ is
deterministic, the prior over latent variables becomes discrete. We show that
several well-known SSL methods can be interpreted as approximating this prior,
and can be improved upon. We extend the discriminative model to neuro-symbolic
SSL, where label features satisfy logical rules, by showing such rules relate
directly to the above prior, thus justifying a family of methods that link
statistical learning and logical reasoning, and unifying them with regular SSL.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05905</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05905</id><submitter>Weiguo Pian</submitter><version version="v1"><date>Sun, 7 Jun 2020 13:00:19 GMT</date><size>1958kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 9 Dec 2020 08:30:26 GMT</date><size>1958kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 May 2021 19:53:56 GMT</date><size>1960kb</size><source_type>D</source_type></version><title>Spatial-Temporal Dynamic Graph Attention Networks for Ride-hailing
  Demand Prediction</title><authors>Weiguo Pian, Yingbo Wu, Xiangmou Qu, Junpeng Cai, Ziyi Kou</authors><categories>cs.LG cs.AI</categories><comments>11 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:2006.04089</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ride-hailing demand prediction is an essential task in spatial-temporal data
mining. Accurate Ride-hailing demand prediction can help to pre-allocate
resources, improve vehicle utilization and user experiences. Graph
Convolutional Networks (GCN) is commonly used to model the complicated
irregular non-Euclidean spatial correlations. However, existing GCN-based
ride-hailing demand prediction methods only assign the same importance to
different neighbor regions, and maintain a fixed graph structure with static
spatial relationships throughout the timeline when extracting the irregular
non-Euclidean spatial correlations. In this paper, we propose the
Spatial-Temporal Dynamic Graph Attention Network (STDGAT), a novel ride-hailing
demand prediction method. Based on the attention mechanism of GAT, STDGAT
extracts different pair-wise correlations to achieve the adaptive importance
allocation for different neighbor regions. Moreover, in STDGAT, we design a
novel time-specific commuting-based graph attention mode to construct a dynamic
graph structure for capturing the dynamic time-specific spatial relationships
throughout the timeline. Extensive experiments are conducted on a real-world
ride-hailing demand dataset, and the experimental results demonstrate the
significant improvement of our method on three evaluation metrics RMSE, MAPE
and MAE over state-of-the-art baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.06518</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.06518</id><submitter>Minhan Li</submitter><version version="v1"><date>Thu, 11 Jun 2020 15:34:52 GMT</date><size>2696kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 12 Jun 2020 16:07:43 GMT</date><size>2696kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 04:56:40 GMT</date><size>8943kb</size></version><title>Towards Expedited Impedance Tuning of a Robotic Prosthesis for
  Personalized Gait Assistance by Reinforcement Learning Control</title><authors>Minhan Li, Yue Wen, Xiang Gao, Jennie Si, He Helen Huang</authors><categories>eess.SY cs.SY</categories><doi>10.1109/TRO.2021.3078317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalizing medical devices such as lower limb wearable robots is
challenging. While the initial feasibility of automating the process of knee
prosthesis control parameter tuning has been demonstrated in a principled way,
the next critical issue is to improve tuning efficiency and speed it up for the
human user, in clinic settings, while maintaining human safety. We, therefore,
propose a policy iteration with constraint embedded (PICE) method as an
innovative solution to the problem under the framework of reinforcement
learning. Central to PICE is the use of a projected Bellman equation with a
constraint of assuring positive semidefiniteness of performance values during
policy evaluation. Additionally, we developed both online and offline PICE
implementations that provide additional flexibility for the designer to fully
utilize measurement data, either from on-policy or off-policy, to further
improve PICE tuning efficiency. Our human subject testing showed that the PICE
provided effective policies with significantly reduced tuning time. For the
first time, we also experimentally evaluated and demonstrated the robustness of
the deployed policies by applying them to different tasks and users. Putting it
together, our new way of problem solving has been effective as PICE has
demonstrated its potential toward truly automating the process of control
parameter tuning for robotic knee prosthesis users.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.06963</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.06963</id><submitter>Neil G. Marchant</submitter><version version="v1"><date>Fri, 12 Jun 2020 06:17:26 GMT</date><size>336kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 07:33:19 GMT</date><size>687kb</size><source_type>D</source_type></version><title>Needle in a Haystack: Label-Efficient Evaluation under Extreme Class
  Imbalance</title><authors>Neil G. Marchant and Benjamin I. P. Rubinstein</authors><categories>cs.LG cs.IR stat.ML</categories><comments>30 pages, 8 figures, updated to match version accepted for
  publication at KDD'21</comments><acm-class>H.3.4; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Important tasks like record linkage and extreme classification demonstrate
extreme class imbalance, with 1 minority instance to every 1 million or more
majority instances. Obtaining a sufficient sample of all classes, even just to
achieve statistically-significant evaluation, is so challenging that most
current approaches yield poor estimates or incur impractical cost. Where
importance sampling has been levied against this challenge, restrictive
constraints are placed on performance metrics, estimates do not come with
appropriate guarantees, or evaluations cannot adapt to incoming labels. This
paper develops a framework for online evaluation based on adaptive importance
sampling. Given a target performance metric and model for $p(y|x)$, the
framework adapts a distribution over items to label in order to maximize
statistical precision. We establish strong consistency and a central limit
theorem for the resulting performance estimates, and instantiate our framework
with worked examples that leverage Dirichlet-tree models. Experiments
demonstrate an average MSE superior to state-of-the-art on fixed label budgets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.07443</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.07443</id><submitter>Sam Ganzfried</submitter><version version="v1"><date>Fri, 12 Jun 2020 19:53:18 GMT</date><size>90kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 26 Jun 2020 06:33:21 GMT</date><size>90kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 28 Apr 2021 00:34:42 GMT</date><size>90kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 27 May 2021 09:15:14 GMT</date><size>90kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 1 Jun 2021 06:46:50 GMT</date><size>84kb</size><source_type>D</source_type></version><title>Algorithm for Computing Approximate Nash Equilibrium in Continuous Games
  with Application to Continuous Blotto</title><authors>Sam Ganzfried</authors><categories>cs.GT cs.AI cs.MA econ.TH math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful algorithms have been developed for computing Nash equilibrium in a
variety of finite game classes. However, solving continuous games -- in which
the pure strategy space is (potentially uncountably) infinite -- is far more
challenging. Nonetheless, many real-world domains have continuous action
spaces, e.g., where actions refer to an amount of time, money, or other
resource that is naturally modeled as being real-valued as opposed to integral.
We present a new algorithm for {approximating} Nash equilibrium strategies in
continuous games. In addition to two-player zero-sum games, our algorithm also
applies to multiplayer games and games with imperfect information. We
experiment with our algorithm on a continuous imperfect-information Blotto
game, in which two players distribute resources over multiple battlefields.
Blotto games have frequently been used to model national security scenarios and
have also been applied to electoral competition and auction theory. Experiments
show that our algorithm is able to quickly compute close approximations of Nash
equilibrium strategies for this game.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.08855</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.08855</id><submitter>Ye Tian</submitter><version version="v1"><date>Tue, 16 Jun 2020 01:14:38 GMT</date><size>776kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 23 Oct 2020 18:59:19 GMT</date><size>435kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 15:54:44 GMT</date><size>1366kb</size><source_type>D</source_type></version><title>RaSE: Random Subspace Ensemble Classification</title><authors>Ye Tian and Yang Feng</authors><categories>stat.ML cs.LG math.ST stat.CO stat.ME stat.TH</categories><comments>93 pages, 13 figures</comments><journal-ref>Journal of Machine Learning Research 22, no. 45 (2021): 1-93</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a flexible ensemble classification framework, Random Subspace
Ensemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate
many weak learners, where each weak learner is a base classifier trained in a
subspace optimally selected from a collection of random subspaces. To conduct
subspace selection, we propose a new criterion, ratio information criterion
(RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis
includes the risk and Monte-Carlo variance of the RaSE classifier, establishing
the screening consistency and weak consistency of RIC, and providing an upper
bound for the misclassification rate of the RaSE classifier. In addition, we
show that in a high-dimensional framework, the number of random subspaces needs
to be very large to guarantee that a subspace covering signals is selected.
Therefore, we propose an iterative version of the RaSE algorithm and prove that
under some specific conditions, a smaller number of generated random subspaces
are needed to find a desirable subspace through iteration. An array of
simulations under various models and real-data applications demonstrate the
effectiveness and robustness of the RaSE classifier and its iterative version
in terms of low misclassification rate and accurate feature ranking. The RaSE
algorithm is implemented in the R package RaSEn on CRAN.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.08973</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.08973</id><submitter>Andreas Look</submitter><version version="v1"><date>Tue, 16 Jun 2020 08:00:26 GMT</date><size>3731kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 9 Feb 2021 09:44:54 GMT</date><size>5818kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 12 Feb 2021 13:13:04 GMT</date><size>5818kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 2 Jun 2021 12:58:00 GMT</date><size>2750kb</size><source_type>D</source_type></version><title>Deterministic Variational Inference for Neural SDEs</title><authors>Andreas Look, Melih Kandemir, Jan Peters</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Stochastic Differential Equations (NSDEs) model the drift and
diffusion functions of a stochastic process as neural networks. While NSDEs are
known to predict time series accurately, their uncertainty quantification
properties remain unexplored. Currently, there are no approximate inference
methods, which allow flexible models and provide at the same time high quality
uncertainty estimates at a reasonable computational cost. Existing SDE
inference methods either make overly restrictive assumptions, e.g. linearity,
or rely on Monte Carlo integration that requires many samples at prediction
time for reliable uncertainty quantification. However, many real-world safety
critical applications necessitate highly expressive models that can quantify
prediction uncertainty at affordable computational cost. We introduce a
variational inference scheme that approximates the posterior distribution of a
NSDE governing a latent state space by a deterministic chain of operations. We
approximate the intractable data fit term of the evidence lower bound by a
novel bidimensional moment matching algorithm: vertical along the neural net
layers and horizontal along the time direction. Our algorithm achieves
uncertainty calibration scores that can be matched by its sampling-based
counterparts only at significantly higher computation cost, while providing as
accurate forecasts on system dynamics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.09762</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.09762</id><submitter>Lucas Pascal</submitter><version version="v1"><date>Wed, 17 Jun 2020 10:25:41 GMT</date><size>500kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Dec 2020 10:55:28 GMT</date><size>508kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 22 Apr 2021 12:35:19 GMT</date><size>822kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 19 May 2021 09:20:37 GMT</date><size>822kb</size><source_type>D</source_type></version><title>Maximum Roaming Multi-Task Learning</title><authors>Lucas Pascal and Pietro Michiardi and Xavier Bost and Benoit Huet and
  Maria A. Zuluaga</authors><categories>cs.CV cs.LG stat.ML</categories><comments>Accepted at the 35th AAAI Conference on Artificial Intelligence (AAAI
  2021)</comments><journal-ref>Proceedings of the AAAI Conference on Artificial Intelligence:
  35(10), 9331-9341 (2021)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning has gained popularity due to the advantages it provides
with respect to resource usage and performance. Nonetheless, the joint
optimization of parameters with respect to multiple tasks remains an active
research topic. Sub-partitioning the parameters between different tasks has
proven to be an efficient way to relax the optimization constraints over the
shared weights, may the partitions be disjoint or overlapping. However, one
drawback of this approach is that it can weaken the inductive bias generally
set up by the joint task optimization. In this work, we present a novel way to
partition the parameter space without weakening the inductive bias.
Specifically, we propose Maximum Roaming, a method inspired by dropout that
randomly varies the parameter partitioning, while forcing them to visit as many
tasks as possible at a regulated frequency, so that the network fully adapts to
each update. We study the properties of our method through experiments on a
variety of visual multi-task data sets. Experimental results suggest that the
regularization brought by roaming has more impact on performance than usual
partitioning optimization strategies. The overall method is flexible, easily
applicable, provides superior regularization and consistently achieves improved
performances compared to recent multi-task learning formulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.09865</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.09865</id><submitter>Pallav Bera</submitter><version version="v1"><date>Wed, 17 Jun 2020 13:42:58 GMT</date><size>4290kb</size><source_type>D</source_type></version><title>Intelligent Protection &amp; Classification of Transients in Two-Core
  Symmetric Phase Angle Regulating Transformers</title><authors>Pallav Kumar Bera, Can Isik</authors><categories>eess.SP cs.CV</categories><journal-ref>IEEE Access, vol. 9, pp. 72937-72948, 2021</journal-ref><doi>10.1109/ACCESS.2021.3081015</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper investigates the applicability of time and time-frequency features
based classifiers to distinguish internal faults and other transients -
magnetizing inrush, sympathetic inrush, external faults with current
transformer saturation, and overexcitation - for Indirect Symmetrical Phase
Angle Regulating Transformers (ISPAR). Then the faulty transformer unit
(series/exciting) of the ISPAR is located, or else the transient disturbance is
identified. An event detector detects variation in differential currents and
registers one-cycle of 3-phase post transient samples which are used to extract
the time and time-frequency features for training seven classifiers. Three
different sets of features - wavelet coefficients, time-domain features, and
combination of time and wavelet energy - obtained from exhaustive search using
Decision Tree, random forest feature selection, and maximum Relevance Minimum
Redundancy are used. The internal fault is detected with a balanced accuracy of
99.9%, the faulty unit is localized with balanced accuracy of 98.7% and the
no-fault transients are classified with balanced accuracy of 99.5%. The results
show potential for accurate internal fault detection and localization, and
transient identification. The proposed scheme can supervise the operation of
existing microprocessor-based differential relays resulting in higher stability
and dependability. The ISPAR is modeled and the transients are simulated in
PSCAD/EMTDC by varying several parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.10811</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.10811</id><submitter>Ari Benjamin</submitter><version version="v1"><date>Thu, 18 Jun 2020 19:04:47 GMT</date><size>2340kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 17:33:06 GMT</date><size>1850kb</size><source_type>D</source_type></version><title>Learning to infer in recurrent biological networks</title><authors>Ari S. Benjamin and Konrad P. Kording</authors><categories>q-bio.NC cs.NE stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A popular theory of perceptual processing holds that the brain learns both a
generative model of the world and a paired recognition model using variational
Bayesian inference. Most hypotheses of how the brain might learn these models
assume that neurons in a population are conditionally independent given their
common inputs. This simplification is likely not compatible with the type of
local recurrence observed in the brain. Seeking an alternative that is
compatible with complex inter-dependencies yet consistent with known biology,
we argue here that the cortex may learn with an adversarial algorithm. Many
observable symptoms of this approach would resemble known neural phenomena,
including wake/sleep cycles and oscillations that vary in magnitude with
surprise, and we describe how further predictions could be tested. We
illustrate the idea on recurrent neural networks trained to model image and
video datasets. This framework for learning brings variational inference closer
to neuroscience and yields multiple testable hypotheses.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.11122</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.11122</id><submitter>Alessandro Tibo</submitter><version version="v1"><date>Fri, 19 Jun 2020 13:24:20 GMT</date><size>633kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 08:46:35 GMT</date><size>1732kb</size><source_type>D</source_type></version><title>A general framework for defining and optimizing robustness</title><authors>Alessandro Tibo, Manfred Jaeger, Kim G. Larsen</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness of neural networks has recently attracted a great amount of
interest. The many investigations in this area lack a precise common foundation
of robustness concepts. Therefore, in this paper, we propose a rigorous and
flexible framework for defining different types of robustness properties for
classifiers. Our robustness concept is based on postulates that robustness of a
classifier should be considered as a property that is independent of accuracy,
and that it should be defined in purely mathematical terms without reliance on
algorithmic procedures for its measurement. We develop a very general
robustness framework that is applicable to any type of classification model,
and that encompasses relevant robustness concepts for investigations ranging
from safety against adversarial attacks to transferability of models to new
domains. For two prototypical, distinct robustness objectives we then propose
new learning approaches based on neural network co-training strategies for
obtaining image classifiers optimized for these respective objectives.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.11149</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.11149</id><submitter>Muhammad Umer Anwaar</submitter><version version="v1"><date>Fri, 19 Jun 2020 14:21:41 GMT</date><size>1235kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 28 Jun 2020 06:06:12 GMT</date><size>1235kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 21:35:55 GMT</date><size>1234kb</size><source_type>D</source_type></version><title>Compositional Learning of Image-Text Query for Image Retrieval</title><authors>Muhammad Umer Anwaar, Egor Labintcev, Martin Kleinsteuber</authors><categories>cs.CV cs.IR</categories><comments>Published at IEEE WACV 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of retrieving images from a
database based on a multi-modal (image-text) query. Specifically, the query
text prompts some modification in the query image and the task is to retrieve
images with the desired modifications. For instance, a user of an E-Commerce
platform is interested in buying a dress, which should look similar to her
friend's dress, but the dress should be of white color with a ribbon sash. In
this case, we would like the algorithm to retrieve some dresses with desired
modifications in the query dress. We propose an autoencoder based model,
ComposeAE, to learn the composition of image and text query for retrieving
images. We adopt a deep metric learning approach and learn a metric that pushes
composition of source image and text query closer to the target images. We also
propose a rotational symmetry constraint on the optimization problem. Our
approach is able to outperform the state-of-the-art method TIRG \cite{TIRG} on
three benchmark datasets, namely: MIT-States, Fashion200k and Fashion IQ. In
order to ensure fair comparison, we introduce strong baselines by enhancing
TIRG method. To ensure reproducibility of the results, we publish our code
here: \url{https://github.com/ecom-research/ComposeAE}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.11440</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.11440</id><submitter>Josue Ortega Caro</submitter><version version="v1"><date>Fri, 19 Jun 2020 23:50:51 GMT</date><size>4558kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 16 Nov 2020 07:58:09 GMT</date><size>11633kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 06:42:45 GMT</date><size>6224kb</size><source_type>D</source_type></version><title>Local Convolutions Cause an Implicit Bias towards High Frequency
  Adversarial Examples</title><authors>Josue Ortega Caro, Yilong Ju, Ryan Pyle, Sourav Dey, Wieland Brendel,
  Fabio Anselmi, Ankit Patel</authors><categories>stat.ML cs.LG</categories><comments>19 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite great efforts, neural networks are still prone to adversarial
attacks. Recent work has shown that adversarial perturbations typically contain
high-frequency features, but the root cause of this phenomenon remains unknown.
Inspired by the theoretical work in linear full-width convolutional models
(Gunasekar et al, 2018), we hypothesize that the nonlinear local (i.e.
bounded-width) convolutional models used in practice are implicitly biased to
learn high frequency features, and that this is the root cause of high
frequency adversarial examples. To test this hypothesis, we analyzed the impact
of different choices of linear and nonlinear architectures on the implicit bias
of the learned features and the adversarial perturbations, in both spatial and
frequency domains. We find that the high-frequency adversarial perturbations
are critically dependent on the convolution operation in two ways: (i) the
translation invariance of the convolution induces an implicit bias towards
sparsity in the frequency domain; and (ii) the spatially-limited nature of
local convolutions induces an implicit bias towards high frequency features.
The explanation for the latter involves the Fourier Uncertainty Principle: a
spatially-limited (local in the space domain) filter cannot also be
frequency-limited (local in the frequency domain). Furthermore, using larger
convolution kernel sizes or avoiding convolutions altogether (e.g. by using
Visual Transformers architecture) significantly reduces this high frequency
bias, but not the overall susceptibility to attacks. Looking forward, our work
strongly suggests that understanding and controlling the implicit bias of
architectures will be essential for achieving adversarial robustness.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.12897</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.12897</id><submitter>Andreas Emil Feldmann</submitter><version version="v1"><date>Tue, 23 Jun 2020 11:07:39 GMT</date><size>156kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 11 Dec 2020 09:41:43 GMT</date><size>183kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 14:20:57 GMT</date><size>190kb</size><source_type>D</source_type></version><title>Polynomial Time Approximation Schemes for Clustering in Low Highway
  Dimension Graphs</title><authors>Andreas Emil Feldmann, David Saulpic</authors><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study clustering problems such as k-Median, k-Means, and Facility Location
in graphs of low highway dimension, which is a graph parameter modeling
transportation networks. It was previously shown that approximation schemes for
these problems exist, which either run in quasi-polynomial time (assuming
constant highway dimension) [Feldmann et al. SICOMP 2018] or run in FPT time
(parameterized by the number of clusters $k$, the highway dimension, and the
approximation factor) [Becker et al. ESA~2018, Braverman et al. 2020]. In this
paper we show that a polynomial-time approximation scheme (PTAS) exists
(assuming constant highway dimension). We also show that the considered
problems are NP-hard on graphs of highway dimension 1.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.13820</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.13820</id><submitter>JeanBaptiste Bouvier</submitter><version version="v1"><date>Wed, 24 Jun 2020 15:46:28 GMT</date><size>111kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 25 Jun 2020 18:41:25 GMT</date><size>111kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 19:14:03 GMT</date><size>232kb</size><source_type>D</source_type></version><title>Designing Resilient Linear Driftless Systems</title><authors>Jean-Baptiste Bouvier and Melkior Ornik</authors><categories>eess.SY cs.SY</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Critical systems must be designed resilient to all kinds of malfunctions. We
are especially interested by the loss of control authority over actuators. This
malfunction considers actuators producing uncontrolled and possibly undesirable
inputs. We investigate the design of resilient linear systems capable of
reaching their target even after such a malfunction. In contrast with the
settings of robust control and fault-tolerant control, we consider undesirable
but observable inputs of the same magnitude as controls since they are produced
by a faulty actuator of the system. The control inputs can then depend on these
undesirable inputs. Building on our previous work, we focus on designing
resilient systems able to withstand the loss of one or multiple actuators.
Since resilience refers to the existence of a control law driving the state to
the target, we naturally continue with the synthesis of such a control law. We
conclude with a numerical application of our theory on the ADMIRE fighter jet
model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.13912</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.13912</id><submitter>Mathieu Lauri\`ere</submitter><version version="v1"><date>Wed, 24 Jun 2020 17:45:44 GMT</date><size>990kb</size></version><version version="v2"><date>Mon, 29 Mar 2021 17:26:15 GMT</date><size>278kb</size></version><version version="v3"><date>Mon, 31 May 2021 17:08:26 GMT</date><size>1279kb</size><source_type>D</source_type></version><title>Unified Reinforcement Q-Learning for Mean Field Game and Control
  Problems</title><authors>Andrea Angiuli and Jean-Pierre Fouque and Mathieu Lauri\`ere</authors><categories>math.OC cs.LG cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Reinforcement Learning (RL) algorithm to solve infinite horizon
asymptotic Mean Field Game (MFG) and Mean Field Control (MFC) problems. Our
approach can be described as a unified two-timescale Mean Field Q-learning: The
\emph{same} algorithm can learn either the MFG or the MFC solution by simply
tuning the ratio of two learning parameters. The algorithm is in discrete time
and space where the agent not only provides an action to the environment but
also a distribution of the state in order to take into account the mean field
feature of the problem. Importantly, we assume that the agent can not observe
the population's distribution and needs to estimate it in a model-free manner.
The asymptotic MFG and MFC problems are also presented in continuous time and
space, and compared with classical (non-asymptotic or stationary) MFG and MFC
problems. They lead to explicit solutions in the linear-quadratic (LQ) case
that are used as benchmarks for the results of our algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.14947</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.14947</id><submitter>Murat Cubuktepe</submitter><version version="v1"><date>Thu, 25 Jun 2020 00:56:28 GMT</date><size>8495kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 30 Sep 2020 02:41:05 GMT</date><size>17623kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 16:00:35 GMT</date><size>18298kb</size><source_type>D</source_type></version><title>Distributed Policy Synthesis of Multi-Agent Systems With Graph Temporal
  Logic Specifications</title><authors>Murat Cubuktepe, Zhe Xu, Ufuk Topcu</authors><categories>cs.MA cs.LO</categories><comments>Final version of IEEE Transactions on Control of Network Systems.
  arXiv admin note: substantial text overlap with arXiv:2001.09066</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the distributed synthesis of policies for multi-agent systems to
perform \emph{spatial-temporal} tasks. We formalize the synthesis problem as a
\emph{factored} Markov decision process subject to \emph{graph temporal logic}
specifications. The transition function and task of each agent are functions of
the agent itself and its neighboring agents. In this work, we develop another
distributed synthesis method, which improves the scalability and runtime by two
orders of magnitude compared to our prior work. The synthesis method decomposes
the problem into a set of smaller problems, one for each agent by leveraging
the structure in the model, and the specifications. We show that the running
time of the method is linear in the number of agents. The size of the problem
for each agent is exponential only in the number of neighboring agents, which
is typically much smaller than the number of agents. We demonstrate the
applicability of the method in case studies on disease control, urban security,
and search and rescue. The numerical examples show that the method scales to
hundreds of agents with hundreds of states per agent and can also handle
significantly larger state spaces than our prior work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.16811</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.16811</id><submitter>Zheng Ma</submitter><version version="v1"><date>Mon, 29 Jun 2020 16:20:33 GMT</date><size>5682kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 8 Jul 2020 15:03:01 GMT</date><size>5682kb</size><source_type>D</source_type></version><title>Path Integral Based Convolution and Pooling for Graph Neural Networks</title><authors>Zheng Ma, Junyu Xuan, Yu Guang Wang, Ming Li, Pietro Lio</authors><categories>cs.LG cond-mat.dis-nn cs.NI physics.data-an stat.ML</categories><comments>15 pages, 4 figures, 6 tables. arXiv admin note: text overlap with
  arXiv:1904.10996</comments><journal-ref>NeurIPS 2020</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph neural networks (GNNs) extends the functionality of traditional neural
networks to graph-structured data. Similar to CNNs, an optimized design of
graph convolution and pooling is key to success. Borrowing ideas from physics,
we propose a path integral based graph neural networks (PAN) for classification
and regression tasks on graphs. Specifically, we consider a convolution
operation that involves every path linking the message sender and receiver with
learnable weights depending on the path length, which corresponds to the
maximal entropy random walk. It generalizes the graph Laplacian to a new
transition matrix we call maximal entropy transition (MET) matrix derived from
a path integral formalism. Importantly, the diagonal entries of the MET matrix
are directly related to the subgraph centrality, thus providing a natural and
adaptive pooling mechanism. PAN provides a versatile framework that can be
tailored for different graph data with varying sizes and structures. We can
view most existing GNN architectures as special cases of PAN. Experimental
results show that PAN achieves state-of-the-art performance on various graph
classification/regression tasks, including a new benchmark dataset from
statistical mechanics we propose to boost applications of GNN in physical
sciences.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.00302</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.00302</id><submitter>Miguel de Prado</submitter><version version="v1"><date>Wed, 1 Jul 2020 07:54:26 GMT</date><size>3339kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 13 Feb 2021 20:38:02 GMT</date><size>4312kb</size><source_type>D</source_type></version><title>Robustifying the Deployment of tinyML Models for Autonomous
  mini-vehicles</title><authors>Miguel de Prado, Manuele Rusci, Romain Donze, Alessandro Capotondi,
  Serge Monnerat, Luca Benini and, Nuria Pazos</authors><categories>cs.CV cs.LG cs.RO cs.SY eess.SP eess.SY</categories><doi>10.1109/ISCAS51556.2021.9401154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard-size autonomous navigation vehicles have rapidly improved thanks to
the breakthroughs of deep learning. However, scaling autonomous driving to
low-power systems deployed on dynamic environments poses several challenges
that prevent their adoption. To address them, we propose a closed-loop learning
flow for autonomous driving mini-vehicles that includes the target environment
in-the-loop. We leverage a family of compact and high-throughput tinyCNNs to
control the mini-vehicle, which learn in the target environment by imitating a
computer vision algorithm, i.e., the expert. Thus, the tinyCNNs, having only
access to an on-board fast-rate linear camera, gain robustness to lighting
conditions and improve over time. Further, we leverage GAP8, a parallel
ultra-low-power RISC-V SoC, to meet the inference requirements. When running
the family of CNNs, our GAP8's solution outperforms any other implementation on
the STM32L4 and NXP k64f (Cortex-M4), reducing the latency by over 13x and the
energy consummation by 92%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.01276</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.01276</id><submitter>Navid Ghassemi</submitter><version version="v1"><date>Thu, 2 Jul 2020 17:34:02 GMT</date><size>5329kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 26 Jul 2020 17:50:58 GMT</date><size>5329kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 14:18:28 GMT</date><size>5329kb</size><source_type>D</source_type></version><title>Epileptic Seizures Detection Using Deep Learning Techniques: A Review</title><authors>Afshin Shoeibi, Marjane Khodatars, Navid Ghassemi, Mahboobeh Jafari,
  Parisa Moridian, Roohallah Alizadehsani, Maryam Panahiazar, Fahime Khozeimeh,
  Assef Zare, Hossein Hosseini-Nejad, Abbas Khosravi, Amir F. Atiya, Diba
  Aminshahidi, Sadiq Hussain, Modjtaba Rouhani, Saeid Nahavandi, Udyavara
  Rajendra Acharya</authors><categories>cs.LG eess.SP stat.ML</categories><journal-ref>International Journal of Environmental Research and Public Health.
  2021; 18(11):5780</journal-ref><doi>10.3390/ijerph18115780</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A variety of screening approaches have been proposed to diagnose epileptic
seizures, using electroencephalography (EEG) and magnetic resonance imaging
(MRI) modalities. Artificial intelligence encompasses a variety of areas, and
one of its branches is deep learning (DL). Before the rise of DL, conventional
machine learning algorithms involving feature extraction were performed. This
limited their performance to the ability of those handcrafting the features.
However, in DL, the extraction of features and classification are entirely
automated. The advent of these techniques in many areas of medicine, such as in
the diagnosis of epileptic seizures, has made significant advances. In this
study, a comprehensive overview of works focused on automated epileptic seizure
detection using DL techniques and neuroimaging modalities is presented. Various
methods proposed to diagnose epileptic seizures automatically using EEG and MRI
modalities are described. In addition, rehabilitation systems developed for
epileptic seizures using DL have been analyzed, and a summary is provided. The
rehabilitation tools include cloud computing techniques and hardware required
for implementation of DL algorithms. The important challenges in accurate
detection of automated epileptic seizures using DL with EEG and MRI modalities
are discussed. The advantages and limitations in employing DL-based techniques
for epileptic seizures diagnosis are presented. Finally, the most promising DL
models proposed and possible future works on automated epileptic seizure
detection are delineated.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.01309</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.01309</id><submitter>Nik Dennler</submitter><version version="v1"><date>Thu, 2 Jul 2020 18:00:02 GMT</date><size>17279kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 9 Aug 2020 11:14:57 GMT</date><size>9013kb</size><source_type>D</source_type></version><title>Learning-based Defect Recognition for Quasi-Periodic Microscope Images</title><authors>Nik Dennler, Antonio Foncubierta-Rodriguez, Titus Neupert, Marilyne
  Sousa</authors><categories>cond-mat.mtrl-sci cs.CV</categories><comments>11 pages + references and appendix, 5 figures. V2: Added references.
  Corrected typos. Elaborated methodology. In sample figure, replaced grain
  boundary image with more representative image. Results are unchanged</comments><doi>10.1016/j.micron.2021.103069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlling crystalline material defects is crucial, as they affect
properties of the material that may be detrimental or beneficial for the final
performance of a device. Defect analysis on the sub-nanometer scale is enabled
by high-resolution (scanning) transmission electron microscopy [HR(S)TEM],
where the identification of defects is currently carried out based on human
expertise. However, the process is tedious, highly time consuming and, in some
cases, yields ambiguous results. Here we propose a semi-supervised machine
learning method that assists in the detection of lattice defects from atomic
resolution microscope images. It involves a convolutional neural network that
classifies image patches as defective or non-defective, a graph-based heuristic
that chooses one non-defective patch as a model, and finally an automatically
generated convolutional filter bank, which highlights symmetry breaking such as
stacking faults, twin defects and grain boundaries. Additionally, we suggest a
variance filter to segment amorphous regions and beam defects. The algorithm is
tested on III-V/Si crystalline materials and successfully evaluated against
different metrics, showing promising results even for extremely small training
data sets. By combining the data-driven classification generality, robustness
and speed of deep learning with the effectiveness of image filters in
segmenting faulty symmetry arrangements, we provide a valuable open-source tool
to the microscopist community that can streamline future HR(S)TEM analyses of
crystalline materials.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.01416</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.01416</id><submitter>Hugo Carrillo Mr.</submitter><version version="v1"><date>Thu, 2 Jul 2020 22:27:32 GMT</date><size>4101kb</size><source_type>D</source_type></version><title>An order-adaptive compact approximation Taylor method for systems of
  conservation laws</title><authors>H. Carrillo, E. Macca, G. Russo, C. Par\'es, D. Zor\'io</authors><categories>math.NA cs.NA</categories><doi>10.1016/j.jcp.2021.110358</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new family of high-order shock-capturing finite difference
numerical methods for systems of conservation laws. These methods, called
Adaptive Compact Approximation Taylor (ACAT) schemes, use centered $(2p +
1)$-point stencils, where $p$ may take values in $\{1, 2, \dots, P\}$ according
to a new family of smoothness indicators in the stencils. The methods are based
on a combination of a robust first order scheme and the Compact Approximate
Taylor (CAT) methods of order $2p$-order, $p=1,2,\dots, P$ so that they are
first order accurate near discontinuities and have order $2p$ in smooth
regions, where $(2p +1)$ is the size of the biggest stencil in which large
gradients are not detected. CAT methods, introduced in \cite{CP2019}, are an
extension to nonlinear problems of the Lax-Wendroff methods in which the
Cauchy-Kovalesky (CK) procedure is circumvented following the strategy
introduced in \cite{ZBM2017} that allows one to compute time derivatives in a
recursive way using high-order centered differentiation formulas combined with
Taylor expansions in time. The expression of ACAT methods for 1D and 2D systems
of balance laws are given and the performance is tested in a number of test
cases for several linear and nonlinear systems of conservation laws, including
Euler equations for gas dynamics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.03073</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.03073</id><submitter>Jiayi Wang</submitter><version version="v1"><date>Mon, 6 Jul 2020 21:24:25 GMT</date><size>1423kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 11:36:43 GMT</date><size>1423kb</size><source_type>D</source_type></version><title>Generative Model-Based Loss to the Rescue: A Method to Overcome
  Annotation Errors for Depth-Based Hand Pose Estimation</title><authors>Jiayi Wang, Franziska Mueller, Florian Bernard, Christian Theobalt</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to use a model-based generative loss for training hand pose
estimators on depth images based on a volumetric hand model. This additional
loss allows training of a hand pose estimator that accurately infers the entire
set of 21 hand keypoints while only using supervision for 6 easy-to-annotate
keypoints (fingertips and wrist). We show that our partially-supervised method
achieves results that are comparable to those of fully-supervised methods which
enforce articulation consistency. Moreover, for the first time we demonstrate
that such an approach can be used to train on datasets that have erroneous
annotations, i.e. &quot;ground truth&quot; with notable measurement errors, while
obtaining predictions that explain the depth images better than the given
&quot;ground truth&quot;.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.03248</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.03248</id><submitter>Alessandro Bregoli</submitter><version version="v1"><date>Tue, 7 Jul 2020 07:34:09 GMT</date><size>39kb</size></version><version version="v2"><date>Mon, 8 Feb 2021 08:40:00 GMT</date><size>81kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 19:27:44 GMT</date><size>79kb</size><source_type>D</source_type></version><title>A Constraint-Based Algorithm for the Structural Learning of
  Continuous-Time Bayesian Networks</title><authors>Alessandro Bregoli, Marco Scutari, Fabio Stella</authors><categories>cs.AI cs.LG stat.ML</categories><journal-ref>Proceedings of Machine Learning Research (138, PGM 2020), 41-52</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Bayesian networks have been well explored in the literature as
discrete-time models: however, their continuous-time extensions have seen
comparatively little attention. In this paper, we propose the first
constraint-based algorithm for learning the structure of continuous-time
Bayesian networks. We discuss the different statistical tests and the
underlying hypotheses used by our proposal to establish conditional
independence. Furthermore, we analyze and discuss the computational complexity
of the best and worst cases for the proposed algorithm. Finally, we validate
its performance using synthetic data, and we discuss its strengths and
limitations comparing it with the score-based structure learning algorithm from
Nodelman et al. (2003). We find the latter to be more accurate in learning
networks with binary variables, while our constraint-based approach is more
accurate with variables assuming more than two values. Numerical experiments
confirm that score-based and constraint-based algorithms are comparable in
terms of computation time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.04147</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.04147</id><submitter>Vincent Fontaine</submitter><version version="v1"><date>Wed, 8 Jul 2020 14:19:48 GMT</date><size>333kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 05:46:01 GMT</date><size>907kb</size></version><title>Improved error estimates of hybridizable interior penalty methods using
  a variable penalty for highly anisotropic diffusion problems</title><authors>Gregory Etangsale and Marwan Fahs and Vincent Fontaine and Nalitiana
  Rajaonison</authors><categories>math.NA cs.NA</categories><comments>10 pages, 7 figures, 1 table</comments><msc-class>65N12, 65N15, 65N30, 65N38</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive improved a priori error estimates for families of
hybridizable interior penalty discontinuous Galerkin (H-IP) methods using a
variable penalty for second-order elliptic problems. The strategy is to use a
penalization function of the form $\mathcal{O}(1/h^{1+\delta})$, where $h$
denotes the mesh size and $\delta$ is a user-dependent parameter. We then
quantify its direct impact on the convergence analysis, namely, the (strong)
consistency, discrete coercivity, and boundedness (with
$h^{\delta}$-dependency), and we derive updated error estimates for both
discrete energy- and $L^{2}$-norms. All theoretical results are supported by
numerical evidence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.04477</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.04477</id><submitter>Nicholas Kluge Corr\^ea</submitter><version version="v1"><date>Wed, 8 Jul 2020 23:50:28 GMT</date><size>760kb</size></version><version version="v10"><date>Sat, 26 Dec 2020 21:26:07 GMT</date><size>518kb</size></version><version version="v11"><date>Tue, 1 Jun 2021 18:31:11 GMT</date><size>296kb</size></version><version version="v2"><date>Sun, 12 Jul 2020 22:52:10 GMT</date><size>768kb</size></version><version version="v3"><date>Thu, 23 Jul 2020 22:40:36 GMT</date><size>621kb</size></version><version version="v4"><date>Fri, 7 Aug 2020 23:48:38 GMT</date><size>634kb</size></version><version version="v5"><date>Thu, 27 Aug 2020 17:26:32 GMT</date><size>721kb</size></version><version version="v6"><date>Sat, 5 Sep 2020 18:55:27 GMT</date><size>747kb</size></version><version version="v7"><date>Sun, 18 Oct 2020 04:11:19 GMT</date><size>582kb</size></version><version version="v8"><date>Sun, 25 Oct 2020 03:53:29 GMT</date><size>583kb</size></version><version version="v9"><date>Tue, 27 Oct 2020 02:42:33 GMT</date><size>583kb</size></version><title>Good AI for the Present of Humanity Democratizing AI Governance</title><authors>Nicholas Kluge Corr\^ea and Nythamar de Oliveira</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  What do Cyberpunk and AI Ethics have to do with each other? Cyberpunk is a
sub-genre of science fiction that explores the post-human relationships between
human experience and technology. One similarity between AI Ethics and Cyberpunk
literature is that both seek to explore future social and ethical problems that
our technological advances may bring upon society. In recent years, an
increasing number of ethical matters involving AI have been pointed and
debated, and several ethical principles and guides have been suggested as
governance policies for the tech industry. However, would this be the role of
AI Ethics? To serve as a soft and ambiguous version of the law? We would like
to advocate in this article for a more Cyberpunk way of doing AI Ethics, with a
more democratic way of governance. In this study, we will seek to expose some
of the deficits of the underlying power structures of the AI industry, and
suggest that AI governance be subject to public opinion, so that good AI can
become good AI for all.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.06421</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.06421</id><submitter>David Naumann</submitter><version version="v1"><date>Mon, 13 Jul 2020 14:53:22 GMT</date><size>46kb</size></version><version version="v2"><date>Sun, 19 Jul 2020 22:25:19 GMT</date><size>46kb</size></version><version version="v3"><date>Mon, 3 Aug 2020 16:07:12 GMT</date><size>47kb</size></version><version version="v4"><date>Fri, 29 Jan 2021 02:19:59 GMT</date><size>48kb</size></version><version version="v5"><date>Thu, 3 Jun 2021 14:48:32 GMT</date><size>47kb</size></version><title>Thirty-seven years of relational Hoare logic: remarks on its principles
  and history</title><authors>David A. Naumann</authors><categories>cs.LO cs.PL</categories><comments>A version appears in proceedings of ISOLA 2020. Version2: fix typos,
  minor clarifications, add a citation. Version3: copy edits, add citations on
  completeness. Version 4: minor corrections. Version 5: restore missing
  precond in loop rule</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational Hoare logics extend the applicability of modular, deductive
verification to encompass important 2-run properties including dependency
requirements such as confidentiality and program relations such as equivalence
or similarity between program versions. A considerable number of recent works
introduce different relational Hoare logics without yet converging on a core
set of proof rules. This paper looks backwards to little known early work. This
brings to light some principles that clarify and organize the rules as well as
suggesting a new rule and a new notion of completeness.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.06504</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.06504</id><submitter>Pingchuan Ma</submitter><version version="v1"><date>Mon, 13 Jul 2020 16:56:27 GMT</date><size>192kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 12 Feb 2021 15:50:02 GMT</date><size>208kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 09:02:09 GMT</date><size>208kb</size><source_type>D</source_type></version><title>Towards Practical Lipreading with Distilled and Efficient Models</title><authors>Pingchuan Ma, Brais Martinez, Stavros Petridis, Maja Pantic</authors><categories>cs.CV cs.LG</categories><comments>Accepted to ICASSP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lipreading has witnessed a lot of progress due to the resurgence of neural
networks. Recent works have placed emphasis on aspects such as improving
performance by finding the optimal architecture or improving generalization.
However, there is still a significant gap between the current methodologies and
the requirements for an effective deployment of lipreading in practical
scenarios. In this work, we propose a series of innovations that significantly
bridge that gap: first, we raise the state-of-the-art performance by a wide
margin on LRW and LRW-1000 to 88.5% and 46.6%, respectively using
self-distillation. Secondly, we propose a series of architectural changes,
including a novel Depthwise Separable Temporal Convolutional Network (DS-TCN)
head, that slashes the computational cost to a fraction of the (already quite
efficient) original model. Thirdly, we show that knowledge distillation is a
very effective tool for recovering performance of the lightweight models. This
results in a range of models with different accuracy-efficiency trade-offs.
However, our most promising lightweight models are on par with the current
state-of-the-art while showing a reduction of 8.2x and 3.9x in terms of
computational cost and number of parameters, respectively, which we hope will
enable the deployment of lipreading models in practical applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.08790</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.08790</id><submitter>Jiamei Sun</submitter><version version="v1"><date>Fri, 17 Jul 2020 07:28:08 GMT</date><size>1195kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 9 Dec 2020 09:53:24 GMT</date><size>1260kb</size><source_type>D</source_type></version><title>Explanation-Guided Training for Cross-Domain Few-Shot Classification</title><authors>Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing Zhao,
  Ngai-Man Cheung, Alexander Binder</authors><categories>cs.CV cs.LG</categories><journal-ref>Proceedings of the 25th International Conference on Pattern
  Recognition 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-domain few-shot classification task (CD-FSC) combines few-shot
classification with the requirement to generalize across domains represented by
datasets. This setup faces challenges originating from the limited labeled data
in each class and, additionally, from the domain shift between training and
test sets. In this paper, we introduce a novel training approach for existing
FSC models. It leverages on the explanation scores, obtained from existing
explanation methods when applied to the predictions of FSC models, computed for
intermediate feature maps of the models. Firstly, we tailor the layer-wise
relevance propagation (LRP) method to explain the predictions of FSC models.
Secondly, we develop a model-agnostic explanation-guided training strategy that
dynamically finds and emphasizes the features which are important for the
predictions. Our contribution does not target a novel explanation method but
lies in a novel application of explanations for the training phase. We show
that explanation-guided training effectively improves the model generalization.
We observe improved accuracy for three different FSC models: RelationNet, cross
attention network, and a graph neural network-based formulation, on five
few-shot learning datasets: miniImagenet, CUB, Cars, Places, and Plantae. The
source code is available at https://github.com/SunJiamei/few-shot-lrp-guided
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.09075</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.09075</id><submitter>Kuan Cheng</submitter><version version="v1"><date>Fri, 17 Jul 2020 15:56:05 GMT</date><size>40kb</size></version><version version="v2"><date>Mon, 16 Nov 2020 10:07:17 GMT</date><size>41kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 12:54:40 GMT</date><size>41kb</size></version><title>Efficient Linear and Affine Codes for Correcting Insertions/Deletions</title><authors>Kuan Cheng, Venkatesan Guruswami, Bernhard Haeupler, Xin Li</authors><categories>cs.IT cs.DM cs.DS math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies \emph{linear} and \emph{affine} error-correcting codes for
correcting synchronization errors such as insertions and deletions. We call
such codes linear/affine insdel codes.
  Linear codes that can correct even a single deletion are limited to have
information rate at most $1/2$ (achieved by the trivial 2-fold repetition
code). Previously, it was (erroneously) reported that more generally no
non-trivial linear codes correcting $k$ deletions exist, i.e., that the
$(k+1)$-fold repetition codes and its rate of $1/(k+1)$ are basically optimal
for any $k$. We disprove this and show the existence of binary linear codes of
length $n$ and rate just below $1/2$ capable of correcting $\Omega(n)$
insertions and deletions. This identifies rate $1/2$ as a sharp threshold for
recovery from deletions for linear codes, and reopens the quest for a better
understanding of the capabilities of linear codes for correcting
insertions/deletions.
  We prove novel outer bounds and existential inner bounds for the rate vs.
(edit) distance trade-off of linear insdel codes. We complement our existential
results with an efficient synchronization-string-based transformation that
converts any asymptotically-good linear code for Hamming errors into an
asymptotically-good linear code for insdel errors. Lastly, we show that the
$\frac{1}{2}$-rate limitation does not hold for affine codes by giving an
explicit affine code of rate $1-\epsilon$ which can efficiently correct a
constant fraction of insdel errors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.09261</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.09261</id><submitter>Vassilis Digalakis Jr.</submitter><version version="v1"><date>Fri, 17 Jul 2020 22:15:22 GMT</date><size>1060kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 04:38:39 GMT</date><size>1539kb</size><source_type>D</source_type></version><title>Frequency Estimation in Data Streams: Learning the Optimal Hashing
  Scheme</title><authors>Dimitris Bertsimas and Vassilis Digalakis Jr</authors><categories>cs.DS cs.DB cs.LG</categories><comments>Submitted to IEEE Transactions on Knowledge and Data Engineering on
  07/2020. Revised on 05/2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach for the problem of frequency estimation in data
streams that is based on optimization and machine learning. Contrary to
state-of-the-art streaming frequency estimation algorithms, which heavily rely
on random hashing to maintain the frequency distribution of the data steam
using limited storage, the proposed approach exploits an observed stream prefix
to near-optimally hash elements and compress the target frequency distribution.
We develop an exact mixed-integer linear optimization formulation, which
enables us to compute optimal or near-optimal hashing schemes for elements seen
in the observed stream prefix; then, we use machine learning to hash unseen
elements. Further, we develop an efficient block coordinate descent algorithm,
which, as we empirically show, produces high quality solutions, and, in a
special case, we are able to solve the proposed formulation exactly in linear
time using dynamic programming. We empirically evaluate the proposed approach
both on synthetic datasets and on real-world search query data. We show that
the proposed approach outperforms existing approaches by one to two orders of
magnitude in terms of its average (per element) estimation error and by 45-90%
in terms of its expected magnitude of estimation error.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.09532</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.09532</id><submitter>Michael Neely</submitter><version version="v1"><date>Sat, 18 Jul 2020 22:44:13 GMT</date><size>972kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 03:05:25 GMT</date><size>972kb</size><source_type>D</source_type></version><title>Fast Learning for Renewal Optimization in Online Task Scheduling</title><authors>Michael J. Neely</authors><categories>math.OC cs.LG</categories><comments>32 pages, 9 figures. This version 2 fixes some minor typos from my
  submission last year</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers online optimization of a renewal-reward system. A
controller performs a sequence of tasks back-to-back. Each task has a random
vector of parameters, called the task type vector, that affects the task
processing options and also affects the resulting reward and time duration of
the task. The probability distribution for the task type vector is unknown and
the controller must learn to make efficient decisions so that time average
reward converges to optimality. Prior work on such renewal optimization
problems leaves open the question of optimal convergence time. This paper
develops an algorithm with an optimality gap that decays like $O(1/\sqrt{k})$,
where $k$ is the number of tasks processed. The same algorithm is shown to have
faster $O(\log(k)/k)$ performance when the system satisfies a strong concavity
property. The proposed algorithm uses an auxiliary variable that is updated
according to a classic Robbins-Monro iteration. It makes online scheduling
decisions at the start of each renewal frame based on this variable and on the
observed task type. A matching converse is obtained for the strongly concave
case by constructing an example system for which all algorithms have
performance at best $\Omega(\log(k)/k)$. A matching $\Omega(1/\sqrt{k})$
converse is also shown for the general case without strong concavity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.12136</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.12136</id><submitter>Jiaxin Liang</submitter><version version="v1"><date>Thu, 23 Jul 2020 17:07:51 GMT</date><size>248kb</size><source_type>D</source_type></version><title>Is Multichannel Access Useful in Timely Information Update?</title><authors>Jiaxin Liang, Haoyuan Pan, Soung Chang Liew</authors><categories>cs.NI eess.SP</categories><comments>13 pages, 6 figures, submitted to Wireless Communication Letter</comments><doi>10.1109/LWC.2020.3045457</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates information freshness of multichannel access in
information update systems. Age of information (AoI) is a fundamentally
important metric to characterize information freshness, defined as the time
elapsed since the generation of the last successfully received update. When
multiple devices share the same wireless channel to send updates to a common
receiver, an interesting question is whether dividing the whole channel into
several subchannels will lead to better AoI performance. Given the same
frequency band, dividing it into different numbers of subchannels lead to
different transmission times and packet error rates (PER) of short update
packets, thus affecting information freshness. We focus on a multichannel
access system where different devices take turns to transmit with a cyclic
schedule repeated over time. We first derive the average AoI by estimating the
PERs of short packets. Then we examine bounded AoI, for which the instantaneous
AoI is required to be below a threshold a large percentage of the time.
Simulation results indicate that multichannel access can provide low average
AoI and uniform bounded AoI simultaneously across different received powers.
Overall, our investigations provide insights into practical designs of
multichannel access systems with AoI requirements.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.13125</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.13125</id><submitter>Shuonan Wu</submitter><version version="v1"><date>Sun, 26 Jul 2020 13:09:27 GMT</date><size>2345kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 07:39:39 GMT</date><size>1835kb</size></version><title>A parallel-in-time algorithm for high-order BDF methods for diffusion
  and subdiffusion equations</title><authors>Shuonan Wu, Zhi Zhou</authors><categories>math.NA cs.NA</categories><msc-class>65Y05, 65M15, 65M12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a parallel-in-time algorithm for approximately
solving parabolic equations. In particular, we apply the $k$-step backward
differentiation formula, and then develop an iterative solver by using the
waveform relaxation technique. Each resulting iteration represents a
periodic-like system, which could be further solved in parallel by using the
diagonalization technique. The convergence of the waveform relaxation iteration
is theoretically examined by using the generating function method. The approach
we established in this paper extends the existing argument of single-step
methods in Gander and Wu [Numer. Math., 143 (2019), pp. 489--527] to general
BDF methods up to order six. The argument could be further applied to the
time-fractional subdiffusion equation, whose discretization shares common
properties of the standard BDF methods, because of the nonlocality of the
fractional differential operator. Illustrative numerical results are presented
to complement the theoretical analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.13333</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.13333</id><submitter>Qiaosheng Zhang</submitter><version version="v1"><date>Mon, 27 Jul 2020 07:17:50 GMT</date><size>26kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 02:05:35 GMT</date><size>32kb</size></version><title>Covert Identification over Binary-Input Discrete Memoryless Channels</title><authors>Qiaosheng Zhang and Vincent Y. F. Tan</authors><categories>cs.IT cs.CR math.IT</categories><comments>Accepted for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the covert identification problem in which a sender aims
to reliably convey an identification (ID) message to a set of receivers via a
binary-input discrete memoryless channel (BDMC), and simultaneously to
guarantee that the communication is covert with respect to a warden who
monitors the communication via another independent BDMC. We prove a square-root
law for the covert identification problem. This states that an ID message of
size \exp(\exp(\Theta(\sqrt{n}))) can be transmitted over n channel uses. We
then characterize the exact pre-constant in the \Theta(.) notation. This
constant is referred to as the covert identification capacity. We show that it
equals the recently developed covert capacity in the standard covert
communication problem, and somewhat surprisingly, the covert identification
capacity can be achieved without any shared key between the sender and
receivers. The achievability proof relies on a random coding argument with
pulse-position modulation (PPM), coupled with a second stage which performs
code refinements. The converse proof relies on an expurgation argument as well
as results for channel resolvability with stringent input constraints.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.14247</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.14247</id><submitter>Szymon Szott</submitter><version version="v1"><date>Tue, 28 Jul 2020 14:10:33 GMT</date><size>853kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 15 Dec 2020 17:41:02 GMT</date><size>4110kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 24 Mar 2021 10:25:24 GMT</date><size>4158kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 12:44:06 GMT</date><size>4200kb</size><source_type>D</source_type></version><title>Downlink channel access performance of NR-U: Impact of numerology and
  mini-slots on coexistence with Wi-Fi in the 5 GHz band</title><authors>Katarzyna Kosek-Szott and Alice Lo Valvo and Szymon Szott and
  Pierluigi Gallo and Ilenia Tinnirello</authors><categories>cs.NI</categories><comments>20 double-column single-spaced pages, 10 figures, 3 tables, 39
  references, journal submission</comments><msc-class>91A06, 91A10, 91A80</msc-class><acm-class>C.2.0; C.2.5</acm-class><journal-ref>Computer Networks, Volume 195, 4 August 2021</journal-ref><doi>10.1016/j.comnet.2021.108188</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coexistence between cellular systems and Wi-Fi gained the attention of the
research community when LTE License Assisted Access (LAA) entered the
unlicensed band. The recent introduction of NR-U as part of 5G introduces new
coexistence opportunities because it implements scalable numerology (flexible
subcarrier spacing and OFDM symbol lengths), and non-slot based scheduling
(mini-slots), which considerably impact channel access. This paper analyzes the
impact of NR-U settings on its coexistence with Wi-Fi networks and compares it
with LAA operation using simulations and experiments. First, we propose a
downlink channel access simulation model, which addresses the problem of the
dependency and non-uniformity of transmission attempts of different nodes, as a
result of the synchronization mechanism introduced by NR-U. Second, we validate
the accuracy of the proposed model using FPGA-based LAA, NR-U, and Wi-Fi
prototypes with over-the-air transmissions. Additionally, we show that
replacing LAA with NR-U would not only allow to overcome the problem of
bandwidth wastage caused by reservation signals but also, in some cases, to
preserve fairness in channel access for both scheduled and random-access
systems. Finally, we conclude that fair coexistence of the aforementioned
systems in unlicensed bands is not guaranteed in general, and novel mechanisms
are necessary for improving the sharing of resources between scheduled and
contention-based technologies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.14573</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.14573</id><submitter>Yaliang Li</submitter><version version="v1"><date>Wed, 29 Jul 2020 03:33:18 GMT</date><size>135kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 03:00:12 GMT</date><size>5396kb</size><source_type>D</source_type></version><title>FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data</title><authors>Yuexiang Xie, Zhen Wang, Yaliang Li, Bolin Ding, Nezihe Merve G\&quot;urel,
  Ce Zhang, Minlie Huang, Wei Lin, Jingren Zhou</authors><categories>cs.LG stat.ML</categories><comments>Accepted by KDD-21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-order interactive features capture the correlation between different
columns and thus are promising to enhance various learning tasks on ubiquitous
tabular data. To automate the generation of interactive features, existing
works either explicitly traverse the feature space or implicitly express the
interactions via intermediate activations of some designed models. These two
kinds of methods show that there is essentially a trade-off between feature
interpretability and search efficiency. To possess both of their merits, we
propose a novel method named Feature Interaction Via Edge Search (FIVES), which
formulates the task of interactive feature generation as searching for edges on
the defined feature graph. Specifically, we first present our theoretical
evidence that motivates us to search for useful interactive features with
increasing order. Then we instantiate this search strategy by optimizing both a
dedicated graph neural network (GNN) and the adjacency tensor associated with
the defined feature graph. In this way, the proposed FIVES method simplifies
the time-consuming traversal as a typical training course of GNN and enables
explicit feature generation according to the learned adjacency tensor.
Experimental results on both benchmark and real-world datasets show the
advantages of FIVES over several state-of-the-art methods. Moreover, the
interactive features identified by FIVES are deployed on the recommender system
of Taobao, a worldwide leading e-commerce platform. Results of an online A/B
testing further verify the effectiveness of the proposed method FIVES, and we
further provide FIVES as AI utilities for the customers of Alibaba Cloud.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.15107</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.15107</id><submitter>Mo Shan</submitter><version version="v1"><date>Wed, 29 Jul 2020 21:01:37 GMT</date><size>10709kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 13 Apr 2021 15:43:18 GMT</date><size>11046kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 21:22:36 GMT</date><size>13395kb</size><source_type>D</source_type></version><title>OrcVIO: Object residual constrained Visual-Inertial Odometry</title><authors>Mo Shan, Vikas Dhiman, Qiaojun Feng, Jinzhao Li and Nikolay Atanasov</authors><categories>cs.RO cs.CV</categories><comments>Submitted to T-RO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introducing object-level semantic information into simultaneous localization
and mapping (SLAM) system is critical. It not only improves the performance but
also enables tasks specified in terms of meaningful objects. This work presents
OrcVIO, for visual-inertial odometry tightly coupled with tracking and
optimization over structured object models. OrcVIO differentiates through
semantic feature and bounding-box reprojection errors to perform batch
optimization over the pose and shape of objects. The estimated object states
aid in real-time incremental optimization over the IMU-camera states. The
ability of OrcVIO for accurate trajectory estimation and large-scale
object-level mapping is evaluated using real data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.15154</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.15154</id><submitter>Jiajian Liang</submitter><version version="v1"><date>Thu, 30 Jul 2020 00:01:22 GMT</date><size>120kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 21:44:40 GMT</date><size>175kb</size><source_type>D</source_type></version><title>Approximate Ridesharing of Personal Vehicles Problem</title><authors>Qian-Ping Gu, Jiajian Leo Liang, Guochuan Zhang</authors><categories>cs.DS</categories><comments>39 pages, 6 figures</comments><msc-class>68W25</msc-class><acm-class>F.2</acm-class><journal-ref>Theoretical Computer Science, Volume 871, 6 June 2021, Pages 30-50</journal-ref><doi>10.1016/j.tcs.2021.04.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ridesharing problem is that given a set of trips, each trip consists of
an individual, a vehicle of the individual and some requirements, select a
subset of trips and use the vehicles of selected trips to deliver all
individuals to their destinations satisfying the requirements. Requirements of
trips are specified by parameters including source, destination, vehicle
capacity, preferred paths of a driver, detour distance and number of stops a
driver is willing to make, and time constraints. We analyze the relations
between the time complexity and parameters for two optimization problems:
minimizing the number of selected vehicles and minimizing total travel distance
of the vehicles. We consider the following conditions: (1) all trips have the
same source or same destination, (2) no detour is allowed, (3) each participant
has one preferred path, (4) no limit on the number of stops, and (5) all trips
have the same departure and same arrival time. It is known that both
minimization problems are NP-hard if one of Conditions (1), (2) and (3) is not
satisfied. We prove that both problems are NP-hard and further show that it is
NP-hard to approximate both problems within a constant factor if Conditions (4)
or (5) is not satisfied. We give $\frac{K+2}{2}$-approximation algorithms for
minimizing the number of selected vehicles when condition (4) is not satisfied,
where $K$ is the largest capacity of all vehicles.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.15841</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.15841</id><submitter>David Paulius</submitter><version version="v1"><date>Fri, 31 Jul 2020 04:20:31 GMT</date><size>770kb</size><source_type>D</source_type></version><title>Estimating Motion Codes from Demonstration Videos</title><authors>Maxat Alibayev, David Paulius and Yu Sun</authors><categories>cs.RO cs.CV</categories><comments>IROS 2020 Submission -- 6 pages; initial upload (Last updated July
  31st 2020)</comments><doi>10.1109/IROS45743.2020.9341065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A motion taxonomy can encode manipulations as a binary-encoded
representation, which we refer to as motion codes. These motion codes innately
represent a manipulation action in an embedded space that describes the
motion's mechanical features, including contact and trajectory type. The key
advantage of using motion codes for embedding is that motions can be more
appropriately defined with robotic-relevant features, and their distances can
be more reasonably measured using these motion features. In this paper, we
develop a deep learning pipeline to extract motion codes from demonstration
videos in an unsupervised manner so that knowledge from these videos can be
properly represented and used for robots. Our evaluations show that motion
codes can be extracted from demonstrations of action in the EPIC-KITCHENS
dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.16079</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.16079</id><submitter>Silvio Peroni</submitter><version version="v1"><date>Fri, 31 Jul 2020 13:53:29 GMT</date><size>860kb</size></version><version version="v2"><date>Thu, 12 Nov 2020 11:29:29 GMT</date><size>1100kb</size></version><version version="v3"><date>Tue, 9 Mar 2021 15:06:02 GMT</date><size>1196kb</size></version><version version="v4"><date>Sun, 30 May 2021 14:17:17 GMT</date><size>1206kb</size></version><title>Creating RESTful APIs over SPARQL endpoints using RAMOSE</title><authors>Marilena Daquino, Ivan Heibi, Silvio Peroni, David Shotton</authors><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semantic Web technologies are widely used for storing RDF data and making
them available on the Web through SPARQL endpoints, queryable using the SPARQL
query language. While the use of SPARQL endpoints is strongly supported by
Semantic Web experts, it hinders broader use of RDF data by common Web users,
engineers and developers unfamiliar with Semantic Web technologies, who
normally rely on Web RESTful APIs for querying Web-available data and creating
applications over them. To solve this problem, we have developed RAMOSE, a
generic tool developed in Python to create REST APIs over SPARQL endpoints.
Through the creation of source-specific textual configuration files, RAMOSE
enables the querying of SPARQL endpoints via simple Web RESTful API calls that
return either JSON or CSV-formatted data, thus hiding all the intrinsic
complexities of SPARQL and RDF from common Web users. We provide evidence that
the use of RAMOSE to provide REST API access to RDF data within OpenCitations
triplestores is beneficial in terms of the number of queries made by external
users to such RDF data using the RAMOSE API compared with the direct access via
the SPARQL endpoint. Our findings show the importance for suppliers of RDF data
of having an alternative API access service, which enables its use by those
with no (or little) experience in Semantic Web technologies and the SPARQL
query language. RAMOSE can be used both to query any SPARQL endpoint and to
query any other Web API, and thus it represents an easy generic technical
solution for service providers who wish to create an API service to access
Linked Data stored as RDF in a conventional triplestore.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.00335</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.00335</id><submitter>Haoran Su</submitter><version version="v1"><date>Sat, 1 Aug 2020 20:34:16 GMT</date><size>3132kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 17:53:17 GMT</date><size>3303kb</size><source_type>D</source_type></version><title>V2I Connectivity-Based Dynamic Queue-Jump Lane for Emergency Vehicles: A
  Deep Reinforcement Learning Approach</title><authors>Haoran Su, Kejian Shi, Li Jin and Joseph Y.J. Chow</authors><categories>cs.AI cs.LG cs.SY eess.SY</categories><comments>20 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergency vehicle (EMV) service is a key function of cities and is
exceedingly challenging due to urban traffic congestion. A main reason behind
EMV service delay is the lack of communication and cooperation between vehicles
blocking EMVs. In this paper, we study the improvement of EMV service under V2I
connectivity. We consider the establishment of dynamic queue jump lanes (DQJLs)
based on real-time coordination of connected vehicles. We develop a novel
Markov decision process formulation for the DQJL problem, which explicitly
accounts for the uncertainty of drivers' reaction to approaching EMVs. We
propose a deep neural network-based reinforcement learning algorithm that
efficiently computes the optimal coordination instructions. We also validate
our approach on a micro-simulation testbed using Simulation of Urban Mobility
(SUMO). Validation results show that with our proposed methodology, the
centralized control system saves approximately 15\% EMV passing time than the
benchmark system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.01036</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.01036</id><submitter>Lin Chen</submitter><version version="v1"><date>Mon, 3 Aug 2020 17:22:21 GMT</date><size>316kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 12 Aug 2020 16:38:35 GMT</date><size>316kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 22 Oct 2020 17:25:39 GMT</date><size>317kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 23 Nov 2020 18:06:02 GMT</date><size>339kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 9 Feb 2021 00:45:00 GMT</date><size>343kb</size><source_type>D</source_type></version><version version="v6"><date>Tue, 1 Jun 2021 17:03:22 GMT</date><size>334kb</size><source_type>D</source_type></version><title>Multiple Descent: Design Your Own Generalization Curve</title><authors>Lin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi</authors><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>Improved presentation of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the generalization loss of linear regression in variably
parameterized families of models, both under-parameterized and
over-parameterized. We show that the generalization curve can have an arbitrary
number of peaks, and moreover, locations of those peaks can be explicitly
controlled. Our results highlight the fact that both classical U-shaped
generalization curve and the recently observed double descent curve are not
intrinsic properties of the model family. Instead, their emergence is due to
the interaction between the properties of the data and the inductive biases of
learning algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.02047</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.02047</id><submitter>Alexander Mehler</submitter><version version="v1"><date>Wed, 5 Aug 2020 11:11:55 GMT</date><size>36583kb</size><source_type>D</source_type></version><title>Multiple Texts as a Limiting Factor in Online Learning: Quantifying
  (Dis-)similarities of Knowledge Networks across Languages</title><authors>Alexander Mehler and Wahed Hemati and Pascal Welke and Maxim Konca and
  Tolga Uslu</authors><categories>cs.CL</categories><comments>40 pages, 13 figures, 5 tables</comments><msc-class>68T50 (Primary) 68T30, 91F20 (Secondary)</msc-class><acm-class>I.2.7; J.5; K.3.m</acm-class><doi>10.3389/feduc.2020.562670</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We test the hypothesis that the extent to which one obtains information on a
given topic through Wikipedia depends on the language in which it is consulted.
Controlling the size factor, we investigate this hypothesis for a number of 25
subject areas. Since Wikipedia is a central part of the web-based information
landscape, this indicates a language-related, linguistic bias. The article
therefore deals with the question of whether Wikipedia exhibits this kind of
linguistic relativity or not. From the perspective of educational science, the
article develops a computational model of the information landscape from which
multiple texts are drawn as typical input of web-based reading. For this
purpose, it develops a hybrid model of intra- and intertextual similarity of
different parts of the information landscape and tests this model on the
example of 35 languages and corresponding Wikipedias. In this way the article
builds a bridge between reading research, educational science, Wikipedia
research and computational linguistics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.02101</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.02101</id><submitter>Dwarikanath Mahapatra</submitter><version version="v1"><date>Wed, 5 Aug 2020 12:59:15 GMT</date><size>3466kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 26 May 2021 08:57:52 GMT</date><size>3452kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 15:35:06 GMT</date><size>3467kb</size><source_type>D</source_type></version><title>Structure Preserving Stain Normalization of Histopathology Images Using
  Self-Supervised Semantic Guidance</title><authors>Dwarikanath Mahapatra, Behzad Bozorgtabar, Jean-Philippe Thiran, Ling
  Shao</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although generative adversarial network (GAN) based style transfer is state
of the art in histopathology color-stain normalization, they do not explicitly
integrate structural information of tissues. We propose a self-supervised
approach to incorporate semantic guidance into a GAN based stain normalization
framework and preserve detailed structural information. Our method does not
require manual segmentation maps which is a significant advantage over existing
methods. We integrate semantic information at different layers between a
pre-trained semantic network and the stain color normalization network. The
proposed scheme outperforms other color normalization methods leading to better
classification and segmentation performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.03408</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.03408</id><submitter>Bo Wang</submitter><version version="v1"><date>Sat, 8 Aug 2020 00:48:59 GMT</date><size>775kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 04:23:15 GMT</date><size>371kb</size><source_type>D</source_type></version><title>Learning to Detect Bipolar Disorder and Borderline Personality Disorder
  with Language and Speech in Non-Clinical Interviews</title><authors>Bo Wang, Yue Wu, Niall Taylor, Terry Lyons, Maria Liakata, Alejo J
  Nevado-Holgado, Kate E A Saunders</authors><categories>cs.LG cs.CL eess.AS stat.ML</categories><msc-class>60L10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bipolar disorder (BD) and borderline personality disorder (BPD) are both
chronic psychiatric disorders. However, their overlapping symptoms and common
comorbidity make it challenging for the clinicians to distinguish the two
conditions on the basis of a clinical interview. In this work, we first present
a new multi-modal dataset containing interviews involving individuals with BD
or BPD being interviewed about a non-clinical topic . We investigate the
automatic detection of the two conditions, and demonstrate a good linear
classifier that can be learnt using a down-selected set of features from the
different aspects of the interviews and a novel approach of summarising these
features. Finally, we find that different sets of features characterise BD and
BPD, thus providing insights into the difference between the automatic
screening of the two conditions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.03587</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.03587</id><submitter>Laurine B\'en\'eteau</submitter><version version="v1"><date>Sat, 8 Aug 2020 20:26:03 GMT</date><size>7kb</size></version><version version="v2"><date>Mon, 17 Aug 2020 07:25:34 GMT</date><size>7kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 09:39:17 GMT</date><size>8kb</size></version><title>A note on deterministic zombies</title><authors>Valentin Bartier, Laurine B\'en\'eteau, Marthe Bonamy, Hoang La,
  Jonathan Narboni</authors><categories>math.CO cs.DM</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Zombies and Survivor&quot; is a variant of the well-studied game of &quot;Cops and
Robber&quot; where the zombies (cops) can only move closer to the survivor (robber).
We consider the deterministic version of the game where a zombie can choose
their path if multiple options are available. The zombie number, like the cop
number, of a graph is the minimum number of zombies, or cops, required to
capture the survivor. In this short note, we solve a question by Fitzpatrick et
al., proving that the zombie number of the Cartesian product of two graphs is
at most the sum of their zombie numbers. We also give a simple graph family
with cop number $2$ and an arbitrarily large zombie number.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.03820</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.03820</id><submitter>Pengsheng Ji</submitter><version version="v1"><date>Sun, 9 Aug 2020 21:43:32 GMT</date><size>319kb</size><source_type>D</source_type></version><title>Spectral Algorithms for Community Detection in Directed Networks</title><authors>Zhe Wang, Yingbin Liang and Pengsheng Ji</authors><categories>stat.ML cs.LG cs.SI math.ST stat.TH</categories><comments>Journal of Machine Learning Research 2020, to appear</comments><journal-ref>Journal of Machine Learning Research 2020. (153):1-45,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in large social networks is affected by degree
heterogeneity of nodes. The D-SCORE algorithm for directed networks was
introduced to reduce this effect by taking the element-wise ratios of the
singular vectors of the adjacency matrix before clustering. Meaningful results
were obtained for the statistician citation network, but rigorous analysis on
its performance was missing. First, this paper establishes theoretical
guarantee for this algorithm and its variants for the directed degree-corrected
block model (Directed-DCBM). Second, this paper provides significant
improvements for the original D-SCORE algorithms by attaching the nodes outside
of the community cores using the information of the original network instead of
the singular vectors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.03945</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.03945</id><submitter>Leyang Cui</submitter><version version="v1"><date>Mon, 10 Aug 2020 08:12:34 GMT</date><size>2465kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 11:24:36 GMT</date><size>9719kb</size><source_type>D</source_type></version><title>Does BERT Solve Commonsense Task via Commonsense Knowledge?</title><authors>Leyang Cui, Sijie Cheng, Yu Wu, Yue Zhang</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BERT has been used for solving commonsense tasks such as CommonsenseQA. While
prior research has found that BERT does contain commonsense information to some
extent, there has been work showing that pre-trained models can rely on
spurious associations (e.g., data bias) rather than key cues in solving
sentiment classification and other problems. We quantitatively investigate the
presence of structural commonsense cues in BERT when solving commonsense tasks,
and the importance of such cues for the model prediction. Using two different
measures, we find that BERT does use relevant knowledge for solving the task,
and the presence of commonsense knowledge is positively correlated to the model
accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.04262</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.04262</id><submitter>Jeremy Avigad</submitter><version version="v1"><date>Mon, 10 Aug 2020 17:07:09 GMT</date><size>21kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 2 Mar 2021 01:29:12 GMT</date><size>23kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 15:37:40 GMT</date><size>22kb</size><source_type>D</source_type></version><title>Progress on a perimeter surveillance problem</title><authors>Jeremy Avigad and Floris van Doorn</authors><categories>eess.SP cs.SY eess.SY math.OC</categories><msc-class>93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a perimeter surveillance problem introduced by Kingston, Beard,
and Holt in 2008 and studied by Davis, Humphrey, and Kingston in 2019. In this
problem, $n$ drones surveil a finite interval, moving at uniform speed and
exchanging information only when they meet another drone. Kingston et al.
described a particular online algorithm for coordinating their behavior and
asked for an upper bound on how long it can take before the drones are fully
synchronized. They divided the algorithm's behavior into two phases, and
presented upper bounds on the length of each phase based on conjectured
worst-case configurations. Davis et al. presented counterexamples to the
conjecture for phase 1.
  We present sharp upper bounds on phase 2 which show that in this case the
conjectured worst case is correct. We also present new lower bounds on phase 1
and the total time to synchronization, and report partial progress towards
obtaining an upper bound.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.05171</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.05171</id><submitter>Erich Schubert</submitter><version version="v1"><date>Wed, 12 Aug 2020 08:37:50 GMT</date><size>276kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 08:16:45 GMT</date><size>277kb</size><source_type>D</source_type></version><title>Fast and Eager k-Medoids Clustering: O(k) Runtime Improvement of the
  PAM, CLARA, and CLARANS Algorithms</title><authors>Erich Schubert and Peter J. Rousseeuw</authors><categories>cs.LG cs.AI stat.ML</categories><journal-ref>Information Systems, 2021</journal-ref><doi>10.1016/j.is.2021.101804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering non-Euclidean data is difficult, and one of the most used
algorithms besides hierarchical clustering is the popular algorithm
Partitioning Around Medoids (PAM), also simply referred to as k-medoids
clustering. In Euclidean geometry the mean-as used in k-means-is a good
estimator for the cluster center, but this does not exist for arbitrary
dissimilarities. PAM uses the medoid instead, the object with the smallest
dissimilarity to all others in the cluster. This notion of centrality can be
used with any (dis-)similarity, and thus is of high relevance to many domains
and applications. A key issue with PAM is its high run time cost. We propose
modifications to the PAM algorithm that achieve an O(k)-fold speedup in the
second (&quot;SWAP&quot;) phase of the algorithm, but will still find the same results as
the original PAM algorithm. If we relax the choice of swaps performed (while
retaining comparable quality), we can further accelerate the algorithm by
eagerly performing additional swaps in each iteration. With the substantially
faster SWAP, we can now explore faster initialization strategies, because (i)
the classic (&quot;BUILD&quot;) initialization now becomes the bottleneck, and (ii) our
swap is fast enough to compensate for worse starting conditions. We also show
how the CLARA and CLARANS algorithms benefit from the proposed modifications.
While we do not study the parallelization of our approach in this work, it can
easily be combined with earlier approaches to use PAM and CLARA on big data
(some of which use PAM as a subroutine, hence can immediately benefit from
these improvements), where the performance with high k becomes increasingly
important. In experiments on real data with k=100,200, we observed a 458x
respectively 1191x speedup compared to the original PAM SWAP algorithm, making
PAM applicable to larger data sets, and in particular to higher k.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.05758</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.05758</id><submitter>Amrit Singh Bedi</submitter><version version="v1"><date>Thu, 13 Aug 2020 08:56:24 GMT</date><size>828kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 05:32:29 GMT</date><size>3232kb</size><source_type>D</source_type></version><title>Conservative Stochastic Optimization with Expectation Constraints</title><authors>Zeeshan Akhtar, Amrit Singh Bedi, and Ketan Rajawat</authors><categories>math.OC cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers stochastic convex optimization problems where the
objective and constraint functions involve expectations with respect to the
data indices or environmental variables, in addition to deterministic convex
constraints on the domain of the variables. Although the setting is generic and
arises in different machine learning applications, online and efficient
approaches for solving such problems have not been widely studied. Since the
underlying data distribution is unknown a priori, a closed-form solution is
generally not available, and classical deterministic optimization paradigms are
not applicable. State-of-the-art approaches, such as those using the saddle
point framework, can ensure that the optimality gap as well as the constraint
violation decay as $\O\left(T^{-\frac{1}{2}}\right)$ where $T$ is the number of
stochastic gradients. The domain constraints are assumed simple and handled via
projection at every iteration. In this work, we propose a novel conservative
stochastic optimization algorithm (CSOA) that achieves zero constraint
violation and $\O\left(T^{-\frac{1}{2}}\right)$ optimality gap.
  Further, the projection operation (for scenarios when calculating projection
is expensive) in the proposed algorithm can be avoided by considering the
conditional gradient or Frank-Wolfe (FW) variant of the algorithm. The
state-of-the-art stochastic FW variants achieve an optimality gap of
$\O\left(T^{-\frac{1}{3}}\right)$ after $T$ iterations, though these algorithms
have not been applied to problems with functional expectation constraints. In
this work, we propose the FW-CSOA algorithm that is not only projection-free
but also achieves zero constraint violation with
$\O\left(T^{-\frac{1}{4}}\right)$ decay of the optimality gap. The efficacy of
the proposed algorithms is tested on two relevant problems: fair classification
and structured matrix completion.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.05987</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.05987</id><submitter>Alqamah Sayeed</submitter><version version="v1"><date>Thu, 13 Aug 2020 16:02:05 GMT</date><size>1072kb</size></version><title>A Novel CMAQ-CNN Hybrid Model to Forecast Hourly Surface-Ozone
  Concentrations Fourteen Days in Advance</title><authors>Alqamah Sayeed, Yunsoo Choi, Ebrahim Eslami, Jia Jung, Yannic Lops,
  Ahmed Khan Salman</authors><categories>physics.ao-ph cs.LG stat.AP</categories><comments>15 pages, 4 main figures and supplemantary figures and tables</comments><doi>10.1038/s41598-021-90446-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Issues regarding air quality and related health concerns have prompted this
study, which develops an accurate and computationally fast, efficient hybrid
modeling system that combines numerical modeling and machine learning for
forecasting concentrations of surface ozone. Currently available numerical
modeling systems for air quality predictions (e.g., CMAQ, NCEP EMP) can
forecast 24 to 48 hours in advance. In this study, we develop a modeling system
based on a convolutional neural network (CNN) model that is not only fast but
covers a temporal period of two weeks with a resolution as small as a single
hour for 255 stations. The CNN model uses forecasted meteorology from the
Weather Research and Forecasting model (processed by the Meteorology-Chemistry
Interface Processor), forecasted air quality from the Community Multi-scale Air
Quality Model (CMAQ), and previous 24-hour concentrations of various measurable
air quality parameters as inputs and predicts the following 14-day hourly
surface ozone concentrations. The model achieves an average accuracy of 0.91 in
terms of the index of agreement for the first day and 0.78 for the fourteenth
day while the average index of agreement for one day ahead prediction from the
CMAQ is 0.77. Through this study, we intend to amalgamate the best features of
numerical modeling (i.e., fine spatial resolution) and a deep neural network
(i.e., computation speed and accuracy) to achieve more accurate spatio-temporal
predictions of hourly ozone concentrations. Although the primary purpose of
this study is the prediction of hourly ozone concentrations, the system can be
extended to various other pollutants.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.06167</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.06167</id><submitter>Ioannis Avramopoulos</submitter><version version="v1"><date>Fri, 14 Aug 2020 02:25:00 GMT</date><size>20kb</size></version><version version="v2"><date>Wed, 26 May 2021 05:42:13 GMT</date><size>26kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 14:25:33 GMT</date><size>50kb</size><source_type>D</source_type></version><title>Ariadne: An algorithm for computing a maximum clique in polynomial time</title><authors>Ioannis Avramopoulos</authors><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a polynomial-time algorithm for the maximum clique
problem, which implies P = NP. Our algorithm is based on a continuous
game-theoretic representation of this problem and at its heart lies a
discrete-time dynamical system. The rule of our dynamical system depends on a
parameter such that if this parameter is equal to the maximum-clique size, the
iterates of our dynamical system are guaranteed to converge to a maximum
clique.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.07230</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.07230</id><submitter>Ji Guan</submitter><version version="v1"><date>Mon, 17 Aug 2020 11:56:23 GMT</date><size>744kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:59:52 GMT</date><size>2851kb</size><source_type>D</source_type></version><title>Robustness Verification of Quantum Classifiers</title><authors>Ji Guan, Wang Fang, and Mingsheng Ying</authors><categories>quant-ph cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Several important models of machine learning algorithms have been
successfully generalized to the quantum world, with potential speedup to
training classical classifiers and applications to data analytics in quantum
physics that can be implemented on the near future quantum computers. However,
quantum noise is a major obstacle to the practical implementation of quantum
machine learning. In this work, we define a formal framework for the robustness
verification and analysis of quantum machine learning algorithms against
noises. A robust bound is derived and an algorithm is developed to check
whether or not a quantum machine learning algorithm is robust with respect to
quantum training data. In particular, this algorithm can find adversarial
examples during checking. Our approach is implemented on Google's TensorFlow
Quantum and can verify the robustness of quantum machine learning algorithms
with respect to a small disturbance of noises, derived from the surrounding
environment. The effectiveness of our robust bound and algorithm is confirmed
by the experimental results, including quantum bits classification as the
&quot;Hello World&quot; example, quantum phase recognition and cluster excitation
detection from real world intractable physical problems, and the classification
of MNIST from the classical world.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.08198</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.08198</id><submitter>Zhengchun Liu</submitter><version version="v1"><date>Tue, 18 Aug 2020 23:57:07 GMT</date><size>2034kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 20:10:12 GMT</date><size>858kb</size><source_type>D</source_type></version><title>BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning</title><authors>Zhengchun Liu, Hemant Sharma, Jun-Sang Park, Peter Kenesei, Antonino
  Miceli, Jonathan Almer, Rajkumar Kettimuthu, Ian Foster</authors><categories>eess.IV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  X-ray diffraction based microscopy techniques such as High Energy Diffraction
Microscopy rely on knowledge of the position of diffraction peaks with high
precision. These positions are typically computed by fitting the observed
intensities in area detector data to a theoretical peak shape such as
pseudo-Voigt. As experiments become more complex and detector technologies
evolve, the computational cost of such peak detection and shape fitting becomes
the biggest hurdle to the rapid analysis required for real-time feedback during
in-situ experiments. To this end, we propose BraggNN, a deep learning-based
method that can determine peak positions much more rapidly than conventional
pseudo-Voigt peak fitting. When applied to a test dataset, BraggNN gives errors
of less than 0.29 and 0.57 pixels, relative to the conventional method, for 75%
and 95% of the peaks, respectively. When applied to a real experimental
dataset, a 3D reconstruction that used peak positions computed by BraggNN
yields 15% better results on average as compared to a reconstruction obtained
using peak positions determined using conventional 2D pseudo-Voigt fitting.
Recent advances in deep learning method implementations and special-purpose
model inference accelerators allow BraggNN to deliver enormous performance
improvements relative to the conventional method, running, for example, more
than 200 times faster than a conventional method on a consumer-class GPU card
with out-of-the-box software.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.08617</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.08617</id><submitter>Ziheng Duan</submitter><version version="v1"><date>Wed, 19 Aug 2020 18:21:22 GMT</date><size>7426kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 11 Sep 2020 11:30:54 GMT</date><size>15127kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 14:50:27 GMT</date><size>10770kb</size><source_type>D</source_type></version><title>MTHetGNN: A Heterogeneous Graph Embedding Framework for Multivariate
  Time Series Forecasting</title><authors>Yueyang Wang, Ziheng Duan, Yida Huang, Haoyan Xu, Jie Feng, Anni Ren</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate time series forecasting, which analyzes historical time series
to predict future trends, can effectively help decision-making. Complex
relations among variables in MTS, including static, dynamic, predictable, and
latent relations, have made it possible to mining more features of MTS.
Modeling complex relations are not only essential in characterizing latent
dependency as well as modeling temporal dependence, but also brings great
challenges in the MTS forecasting task. However, existing methods mainly focus
on modeling certain relations among MTS variables. In this paper, we propose a
novel end-to-end deep learning model, termed Multivariate Time Series
Forecasting via Heterogeneous Graph Neural Networks (MTHetGNN). To characterize
complex relations among variables, a relation embedding module is designed in
MTHetGNN, where each variable is regarded as a graph node, and each type of
edge represents a specific static or dynamic relationship. Meanwhile, a
temporal embedding module is introduced for time series features extraction,
where involving convolutional neural network (CNN) filters with different
perception scales. Finally, a heterogeneous graph embedding module is adopted
to handle the complex structural information generated by the two modules.
Three benchmark datasets from the real world are used to evaluate the proposed
MTHetGNN. The comprehensive experiments show that MTHetGNN achieves
state-of-the-art results in the MTS forecasting task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.08753</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.08753</id><submitter>Chaochao Chen</submitter><version version="v1"><date>Thu, 20 Aug 2020 03:26:51 GMT</date><size>541kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 08:50:25 GMT</date><size>541kb</size><source_type>D</source_type></version><title>When Homomorphic Encryption Marries Secret Sharing: Secure Large-Scale
  Sparse Logistic Regression and Applications in Risk Control</title><authors>Chaochao Chen, Jun Zhou, Li Wang, Xibin Wu, Wenjing Fang, Jin Tan, Lei
  Wang, Alex X. Liu, Hao Wang, Cheng Hong</authors><categories>cs.CR cs.LG</categories><comments>Accepted by KDD'21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logistic Regression (LR) is the most widely used machine learning model in
industry for its efficiency, robustness, and interpretability. Due to the
problem of data isolation and the requirement of high model performance, many
applications in industry call for building a secure and efficient LR model for
multiple parties. Most existing work uses either Homomorphic Encryption (HE) or
Secret Sharing (SS) to build secure LR. HE based methods can deal with
high-dimensional sparse features, but they incur potential security risks. SS
based methods have provable security, but they have efficiency issue under
high-dimensional sparse features. In this paper, we first present CAESAR, which
combines HE and SS to build secure large-scale sparse logistic regression model
and achieves both efficiency and security. We then present the distributed
implementation of CAESAR for scalability requirement. We have deployed CAESAR
in a risk control task and conducted comprehensive experiments. Our
experimental results show that CAESAR improves the state-of-the-art model by
around 130 times.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.08808</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.08808</id><submitter>Zhou Tianze</submitter><version version="v1"><date>Thu, 20 Aug 2020 07:07:20 GMT</date><size>1678kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 28 Sep 2020 02:34:43 GMT</date><size>1962kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 06:46:08 GMT</date><size>2609kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 09:21:33 GMT</date><size>2609kb</size><source_type>D</source_type></version><title>BGC: Multi-Agent Group Belief with Graph Clustering</title><authors>Tianze Zhou, Fubiao Zhang, Pan Tang, Chenfei Wang</authors><categories>cs.AI cs.LG</categories><comments>7 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances have witnessed that value decomposed-based multi-agent
reinforcement learning methods make an efficient performance in coordination
tasks. Most current methods assume that agents can make communication to assist
decisions, which is impractical in some situations. In this paper, we propose a
semi-communication method to enable agents can exchange information without
communication. Specifically, we introduce a group concept to help agents
learning a belief which is a type of consensus. With this consensus, adjacent
agents tend to accomplish similar sub-tasks to achieve cooperation. We design a
novel agent structure named Belief in Graph Clustering(BGC), composed of an
agent characteristic module, a belief module, and a fusion module. To represent
each agent characteristic, we use an MLP-based characteristic module to
generate agent unique features. Inspired by the neighborhood cognitive
consistency, we propose a group-based module to divide adjacent agents into a
small group and minimize in-group agents' beliefs to accomplish similar
sub-tasks. Finally, we use a hyper-network to merge these features and produce
agent actions. To overcome the agent consistent problem brought by GAT, a split
loss is introduced to distinguish different agents. Results reveal that the
proposed method achieves a significant improvement in the SMAC benchmark.
Because of the group concept, our approach maintains excellent performance with
an increase in the number of agents.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.08930</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.08930</id><submitter>Ziqiang Li</submitter><version version="v1"><date>Wed, 19 Aug 2020 12:52:10 GMT</date><size>445kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 30 Nov 2020 03:15:54 GMT</date><size>1407kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 17 Dec 2020 02:06:58 GMT</date><size>1407kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 29 May 2021 15:52:22 GMT</date><size>1847kb</size><source_type>D</source_type></version><title>Why Adopting Regularization and Normalization For Generative Adversarial
  Networks: A Survey</title><authors>Ziqiang Li, Xintian Wu, Rentuo Tao, Pengfei Xia, Huanhuan Chen, Bin Li</authors><categories>cs.LG cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative Adversarial Networks (GANs) have been widely applied in different
scenarios thanks to the development of deep neural networks. The proposal of
original GAN is based upon the non-parametric assumption of the infinite
capacity of networks. It is still unknown whether GANs can generate realistic
samples without any prior information. Due to the overconfident assumption,
many issues need to be addressed in GANs' training, such as non-convergence,
mode collapses, gradient vanishing, overfitting, discriminator forgetting, and
the sensitivity of hyperparameters. As acknowledged, regularization and
normalization are common methods of introducing prior information that can be
used for stabilizing training and improving discrimination. At present, many
regularization and normalization methods are proposed in GANs. However, as far
as we know, there is no existing survey that has particularly focused on the
systematic purposes and developments of these solutions. In this work, we
perform a comprehensive survey of the regularization and normalization
technologies from different perspectives of GANs training. First, we
systematically and comprehensively describe the different perspectives of GANs
training and thus obtain the different purposes of regularization and
normalization in GANs training. In accordance with the different purposes, we
propose a new taxonomy and summary a large number of existing studies.
Furthermore, we compare the performance of the mainstream methods on different
datasets fairly and investigate the regularization and normalization
technologies that have been frequently employed in SOTA GANs. Finally, we
highlight the possible future studies in this area.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.08996</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.08996</id><submitter>Marcel Wild</submitter><version version="v1"><date>Thu, 20 Aug 2020 14:31:35 GMT</date><size>35kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 16:43:10 GMT</date><size>44kb</size><source_type>D</source_type></version><title>Compression with wildcards: All exact, or all minimal hitting sets</title><authors>Marcel Wild</authors><categories>math.CO cs.DM</categories><comments>30 pages, many Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our main objective is the COMPRESSED enumeration (based on wildcards) of all
minimal hitting sets of general hypergraphs. To the author's best knowledge the
only previous attempt towards compression, due to Toda [T], is based on BDD's
and much different from our techniques. Numerical experiments show that
traditional one-by-one enumeration schemes cannot compete against compressed
enumeration when the degree of compression is high. Our method works
particularly well in these two cases: Either compressing all exact hitting
sets, or all minimum-cardinality hitting sets. It also supports parallelization
and cut-off (i.e. restriction to all minimal hitting sets of cardinality at
most m).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.09551</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.09551</id><submitter>Michael Overton</submitter><version version="v1"><date>Fri, 21 Aug 2020 15:49:13 GMT</date><size>3244kb</size></version><version version="v2"><date>Wed, 3 Feb 2021 20:17:00 GMT</date><size>3007kb</size></version><title>Finding the strongest stable weightless column with a follower load and
  relocatable concentrated masses</title><authors>Oleg N. Kirillov and Michael L. Overton</authors><categories>physics.class-ph cs.NA math.DS math.NA math.OC</categories><journal-ref>The Quarterly Journal of Mechanics and Applied Mathematics, 2021,
  74(2): 223-250</journal-ref><doi>10.1093/qjmam/hbab005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimal placement of concentrated masses along a
massless elastic column that is clamped at one end and loaded by a
nonconservative follower force at the free end. The goal is to find the largest
possible interval such that the variation in the loading parameter within this
interval preserves stability of the structure. The stability constraint is
nonconvex and nonsmooth, making the optimization problem quite challenging. We
give a detailed analytical treatment for the case of two masses, arguing that
the optimal parameter configuration approaches the flutter and divergence
boundaries of the stability region simultaneously. Furthermore, we conjecture
that this property holds for any number of masses, which in turn suggests a
simple formula for the maximal load interval for $n$ masses. This conjecture is
strongly supported by extensive computational results, obtained using the
recently developed open-source software package GRANSO (GRadient-based
Algorithm for Non-Smooth Optimization) to maximize the load interval subject to
an appropriate formulation of the nonsmooth stability constraint. We hope that
our work will provide a foundation for new approaches to classical
long-standing problems of stability optimization for nonconservative elastic
systems arising in civil and mechanical engineering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.10454</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.10454</id><submitter>Sebastiano Verde</submitter><version version="v1"><date>Mon, 24 Aug 2020 13:55:14 GMT</date><size>3885kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 4 Sep 2020 07:55:11 GMT</date><size>3885kb</size><source_type>D</source_type></version><title>FOCAL: A Forgery Localization Framework based on Video Coding
  Self-Consistency</title><authors>Sebastiano Verde, Paolo Bestagini, Simone Milani, Giancarlo Calvagno
  and Stefano Tubaro</authors><categories>cs.CV eess.IV</categories><doi>10.1109/OJSP.2021.3074298</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forgery operations on video contents are nowadays within the reach of anyone,
thanks to the availability of powerful and user-friendly editing software.
Integrity verification and authentication of videos represent a major interest
in both journalism (e.g., fake news debunking) and legal environments dealing
with digital evidence (e.g., a court of law). While several strategies and
different forensics traces have been proposed in recent years, latest solutions
aim at increasing the accuracy by combining multiple detectors and features.
This paper presents a video forgery localization framework that verifies the
self-consistency of coding traces between and within video frames, by fusing
the information derived from a set of independent feature descriptors. The
feature extraction step is carried out by means of an explainable convolutional
neural network architecture, specifically designed to look for and classify
coding artifacts. The overall framework was validated in two typical forgery
scenarios: temporal and spatial splicing. Experimental results show an
improvement to the state-of-the-art on temporal splicing localization and also
promising performance in the newly tackled case of spatial splicing, on both
synthetic and real-world videos.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.11047</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.11047</id><submitter>Sang Hoon Lee</submitter><version version="v1"><date>Tue, 25 Aug 2020 14:18:56 GMT</date><size>1733kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 26 Aug 2020 15:00:53 GMT</date><size>1733kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 23:11:42 GMT</date><size>3366kb</size><source_type>D</source_type></version><title>Uncovering hidden dependency in weighted networks via information
  entropy</title><authors>Mi Jin Lee, Eun Lee, Byunghwee Lee, Hawoong Jeong, Deok-Sun Lee, Sang
  Hoon Lee</authors><categories>physics.soc-ph cs.SI</categories><comments>20 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactions between elements, which are usually represented by networks,
have to delineate potentially unequal relationships in terms of their relative
importance or direction. The intrinsic unequal relationships of such kind,
however, are opaque or hidden in numerous real systems.For instance, when a
node in a network with limited interaction capacity spends its capacity to its
neighboring nodes, the allocation of the total amount of interactions to them
can be vastly diverse. Even if such potentially heterogeneous interactions
epitomized by weighted networks are observable, as a result of the
aforementioned ego-centric allocation of interactions, the relative importance
or dependency between two interacting nodes can only be implicitly accessible.
In this work, we precisely pinpoint such relative dependency by proposing the
framework to discover hidden dependent relations extracted from weighted
networks. For a given weighted network, we provide a systematic criterion to
select the most essential interactions for individual nodes based on the
concept of information entropy. The criterion is symbolized by assigning the
effective number of neighbors or the effective out-degree to each node, and the
resultant directed subnetwork decodes the hidden dependent relations by leaving
only the most essential directed interactions. We apply our methodology to two
time-stamped empirical network data, namely the international trade relations
between nations in the world trade web (WTW) and the network of people in the
historical record of Korea, Annals of the Joseon Dynasty (AJD). Based on the
data analysis, we discover that the properties of mutual dependency encoded in
the two systems are vastly different.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.11448</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.11448</id><submitter>Georgios Kontogeorgiou</submitter><version version="v1"><date>Wed, 26 Aug 2020 09:04:35 GMT</date><size>864kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 09:44:14 GMT</date><size>873kb</size><source_type>D</source_type></version><title>Haystack Hunting Hints and Locker Room Communication</title><authors>Artur Czumaj, George Kontogeorgiou, Mike Paterson</authors><categories>math.CO cs.DS</categories><comments>27 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We want to efficiently find a specific object in a large unstructured set,
which we model by a random $n$-permutation, and we have to do it by revealing
just a single element. Clearly, without any help this task is hopeless and the
best one can do is select the element at random, and achieve the success
probability $\frac{1}{n}$. Can we do better with some small amount of advice
about the permutation, even without knowing the object sought? We show that by
providing advice of just one integer in $\{0,1,...,n-1\}$, one can improve the
success probability considerably, by a $\Theta(\frac{logn}{loglogn})$ factor.
We study this and related problems, and show asymptotically matching upper and
lower bounds for their optimal probability of success.Our analysis relies on a
close relationship of such problems to some intrinsic properties of rendom
permutations related to the rencontres number.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.11552</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.11552</id><submitter>Hampei Sasahara</submitter><version version="v1"><date>Wed, 26 Aug 2020 13:32:09 GMT</date><size>445kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 11 Mar 2021 06:19:16 GMT</date><size>878kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 17:43:40 GMT</date><size>787kb</size><source_type>D</source_type></version><title>Parameterization of All Output-Rectifying Retrofit Controllers</title><authors>Hampei Sasahara, Takayuki Ishizaki, Jun-ichi Imura</authors><categories>eess.SY cs.SY</categories><comments>to be published at IEEE Transactions on Automatic Control</comments><doi>10.1109/TAC.2021.3082513</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study investigates a parameterization of all output-rectifying retrofit
controllers for distributed design of a structured controller. It has been
discovered that all retrofit controllers can be characterized as a constrained
Youla parameterization, which is difficult to solve analytically. For
synthesis, a tractable and insightful class of retrofit controllers, referred
to as output-rectifying retrofit controllers, has been introduced. An
unconstrained parameterization of all output-rectifying retrofit controllers
can be derived under a technical assumption on measurability of particular
signals. The aim of this note is to reveal the structure of all
output-rectifying retrofit controllers in the general output-feedback case. It
is found out that the existing developments can be generalized based on the
notions of state projection and an inverse system. The result leads to the
conclusion that output-rectifying retrofit controllers can readily be designed
even in the general case.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.11700</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.11700</id><submitter>Thomas Lew</submitter><version version="v1"><date>Wed, 26 Aug 2020 17:39:58 GMT</date><size>3094kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 8 Mar 2021 20:04:24 GMT</date><size>11571kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 17:32:12 GMT</date><size>11251kb</size><source_type>D</source_type></version><title>Safe Active Dynamics Learning and Control: A Sequential
  Exploration-Exploitation Framework</title><authors>Thomas Lew, Apoorva Sharma, James Harrison, Andrew Bylard, Marco
  Pavone</authors><categories>cs.RO cs.LG cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safe deployment of autonomous robots in diverse scenarios requires agents
that are capable of efficiently adapting to new environments while satisfying
constraints. In this work, we propose a practical and theoretically-justified
approach to maintaining safety in the presence of dynamics uncertainty. Our
approach leverages Bayesian meta-learning with last-layer adaptation: the
expressiveness of neural-network features trained offline, paired with
efficient last-layer online adaptation, enables the derivation of tight
confidence sets which contract around the true dynamics as the model adapts
online. We exploit these confidence sets to plan trajectories that guarantee
the safety of the system. Our approach handles problems with high dynamics
uncertainty where reaching the goal safely is initially infeasible by first
exploring to gather data and reduce uncertainty, before autonomously exploiting
the acquired information to safely perform the task. Under reasonable
assumptions, we prove that our framework has high-probability guarantees of
satisfying all constraints at all times jointly. This analysis also motivates
two regularizers of last-layer meta-learners that improve online adaptation
capabilities as well as performance by reducing the size of the confidence
sets. We extensively demonstrate our approach in simulation and on hardware.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.13715</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.13715</id><submitter>Hao Sun</submitter><version version="v1"><date>Mon, 31 Aug 2020 16:30:07 GMT</date><size>11223kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Sep 2020 21:45:59 GMT</date><size>11223kb</size><source_type>D</source_type></version><title>Extracting full-field subpixel structural displacements from videos via
  deep learning</title><authors>Lele Luan and Jingwei Zheng and Yongchao Yang and Ming L. Wang and Hao
  Sun</authors><categories>cs.CV cs.LG eess.IV</categories><comments>22 figures; 24 figures</comments><doi>10.1016/j.jsv.2021.116142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a deep learning framework based on convolutional neural
networks (CNNs) that enable real-time extraction of full-field subpixel
structural displacements from videos. In particular, two new CNN architectures
are designed and trained on a dataset generated by the phase-based motion
extraction method from a single lab-recorded high-speed video of a dynamic
structure. As displacement is only reliable in the regions with sufficient
texture contrast, the sparsity of motion field induced by the texture mask is
considered via the network architecture design and loss function definition.
Results show that, with the supervision of full and sparse motion field, the
trained network is capable of identifying the pixels with sufficient texture
contrast as well as their subpixel motions. The performance of the trained
networks is tested on various videos of other structures to extract the
full-field motion (e.g., displacement time histories), which indicates that the
trained networks have generalizability to accurately extract full-field subtle
displacements for pixels with sufficient texture contrast.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.00606</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.00606</id><submitter>Oren Yuval</submitter><version version="v1"><date>Tue, 1 Sep 2020 17:55:51 GMT</date><size>73kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 26 Feb 2021 11:49:52 GMT</date><size>114kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 14:58:19 GMT</date><size>124kb</size><source_type>D</source_type></version><title>Semi-Supervised Empirical Risk Minimization: When can unlabeled data
  improve prediction?</title><authors>Oren Yuval and Saharon Rosset</authors><categories>stat.ML cs.LG stat.ME</categories><comments>36 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general methodology for using unlabeled data to design semi
supervised learning (SSL) variants of the Empirical Risk Minimization (ERM)
learning process. Focusing on generalized linear regression, we provide a
careful treatment of the effectiveness of the SSL to improve prediction
performance. The key ideas are carefully considering the null model as a
competitor, and utilizing the unlabeled data to determine signal-noise
combinations where the SSL outperforms both the ERM learning and the null
model. In the special case of linear regression with Gaussian covariates, we
show that the previously suggested semi-supervised estimator is in fact not
capable of improving on both the supervised estimator and the null model
simultaneously. However, the new estimator presented in this work, can achieve
an improvement of $O(1/n)$ term over both competitors simultaneously. On the
other hand, we show that in other scenarios, such as non-Gaussian covariates,
misspecified linear regression, or generalized linear regression with
non-linear link functions, having unlabeled data can derive substantial
improvement in practice by applying our suggested SSL approach. Moreover, it is
possible to identify the situations where SSL improves prediction, by using the
results we establish throughout this work. This is shown empirically through
extensive simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.01363</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.01363</id><submitter>Yunan Yang</submitter><version version="v1"><date>Wed, 2 Sep 2020 21:55:36 GMT</date><size>3402kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 20 Apr 2021 15:14:06 GMT</date><size>1169kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 10 May 2021 19:20:24 GMT</date><size>1159kb</size><source_type>D</source_type></version><title>Adjoint DSMC for nonlinear Boltzmann equation constrained optimization</title><authors>Russel Caflisch, Denis Silantyev, Yunan Yang</authors><categories>math.NA cs.NA math-ph math.MP math.OC</categories><comments>32 pages. 8 figures</comments><msc-class>76P05, 82C80, 65C05, 65K10, 82B40, 65M32</msc-class><journal-ref>Journal of Computational Physics, 2021, 110404, ISSN 0021-9991.
  (https://www.sciencedirect.com/science/article/pii/S0021999121002990)</journal-ref><doi>10.1016/j.jcp.2021.110404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications for kinetic equations such as optimal design and inverse
problems often involve finding unknown parameters through gradient-based
optimization algorithms. Based on the adjoint-state method, we derive two
different frameworks for approximating the gradient of an objective functional
constrained by the nonlinear Boltzmann equation. While the forward problem can
be solved by the DSMC method, it is difficult to efficiently solve the
high-dimensional continuous adjoint equation obtained by the
&quot;optimize-then-discretize&quot; approach. This challenge motivates us to propose an
adjoint DSMC method following the &quot;discretize-then-optimize&quot; approach for
Boltzmann-constrained optimization. We also analyze the properties of the two
frameworks and their connections. Several numerical examples are presented to
demonstrate their accuracy and efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.02070</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.02070</id><submitter>Wei Zhu</submitter><version version="v1"><date>Fri, 4 Sep 2020 08:46:22 GMT</date><size>3614kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 12:45:31 GMT</date><size>0kb</size><source_type>I</source_type></version><title>AutoTrans: Automating Transformer Design via Reinforced Architecture
  Search</title><authors>Wei Zhu, Xiaoling Wang, Xipeng Qiu, Yuan Ni, Guotong Xie</authors><categories>cs.CL</categories><comments>will add new technical contents</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though the transformer architectures have shown dominance in many natural
language understanding tasks, there are still unsolved issues for the training
of transformer models, especially the need for a principled way of warm-up
which has shown importance for stable training of a transformer, as well as
whether the task at hand prefer to scale the attention product or not. In this
paper, we empirically explore automating the design choices in the transformer
model, i.e., how to set layer-norm, whether to scale, number of layers, number
of heads, activation function, etc, so that one can obtain a transformer
architecture that better suits the tasks at hand. RL is employed to navigate
along search space, and special parameter sharing strategies are designed to
accelerate the search. It is shown that sampling a proportion of training data
per epoch during search help to improve the search quality. Experiments on the
CoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer
model can outperform the standard transformers. In particular, we show that our
learned model can be trained more robustly with large learning rates without
warm-up.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.02551</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.02551</id><submitter>Weidong Mei</submitter><version version="v1"><date>Sat, 5 Sep 2020 15:24:10 GMT</date><size>243kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 16:47:43 GMT</date><size>409kb</size><source_type>D</source_type></version><title>Performance Analysis and User Association Optimization for Wireless
  Network Aided by Multiple Intelligent Reflecting Surfaces</title><authors>Weidong Mei and Rui Zhang</authors><categories>cs.IT math.IT</categories><comments>16 pages, 10 figures. Accepted for publication by IEEE Transactions
  on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent reflecting surface (IRS) is a revolutionizing approach for
achieving low-cost yet spectral and energy efficient wireless communications.
In this paper, we consider a wireless network where multiple base stations
(BSs) serve their respective users with the aid of distributed IRSs in the
downlink communication. Specifically, each IRS assists in the transmission from
its associated BS to user via passive beamforming, while in the meantime, it
also randomly scatters the signals from other co-channel BSs, thus resulting in
additional signal as well as interference paths in the network. As such, a new
IRS-user/BS association problem arises pertaining to optimally balance the
passive beamforming gains from all IRSs among different BS-user communication
links. To address this new problem, we first derive the average
signal-to-interference-plus-noise ratio (SINR) at the receiver of each user in
closed-form, based on which two SINR balancing problems are formulated to
maximize the minimum SINR among all users by optimizing the IRS-user
associations without and with BS transmit power control, respectively. We also
characterize the scaling behavior of user SINRs with the increasing number of
IRS reflecting elements to investigate the different effects of IRS-reflected
signal versus interference power. Moreover, to solve the two SINR balancing
problems that are both non-convex optimization problems, we propose an optimal
solution to the problem without BS power control and low-complexity suboptimal
solutions to both problems by applying the branch-and-bound method and
exploiting new properties of the IRS-user associations, respectively. Numerical
results verify our performance analysis and also demonstrate significant
performance gains of the proposed solutions over benchmark schemes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.03190</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.03190</id><submitter>Zhengchun Liu</submitter><version version="v1"><date>Mon, 7 Sep 2020 15:52:55 GMT</date><size>4160kb</size><source_type>D</source_type></version><title>Design and Evaluation of a Simple Data Interface for Efficient Data
  Transfer Across Diverse Storage</title><authors>Zhengchun Liu, Rajkumar Kettimuthu, Joaquin Chung, Rachana
  Ananthakrishnan, Michael Link, Ian Foster</authors><categories>cs.DC</categories><journal-ref>ACM Transactions on Modeling and Performance Evaluation of
  Computing Systems 2021</journal-ref><doi>10.1145/3452007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern science and engineering computing environments often feature storage
systems of different types, from parallel file systems in high-performance
computing centers to object stores operated by cloud providers. To enable easy,
reliable, secure, and performant data exchange among these different systems,
we propose Connector, a pluggable data access architecture for diverse,
distributed storage. By abstracting low-level storage system details, this
abstraction permits a managed data transfer service (Globus in our case) to
interact with a large and easily extended set of storage systems. Equally
important, it supports third-party transfers: that is, direct data transfers
from source to destination that are initiated by a third-party client but do
not engage that third party in the data path. The abstraction also enables
management of transfers for performance optimization, error handling, and
end-to-end integrity. We present the Connector design, describe implementations
for different storage services, evaluate tradeoffs inherent in managed vs.\
direct transfers, motivate recommended deployment options, and propose a
performance model-based method that allows for easy characterization of
performance in different contexts without exhaustive benchmarking.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.03416</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.03416</id><submitter>Alan Frieze</submitter><version version="v1"><date>Mon, 7 Sep 2020 20:54:41 GMT</date><size>10kb</size><source_type>D</source_type></version><title>Probabilistic analysis of algorithms for cost constrained minimum
  weighted combinatorial objects</title><authors>Alan Frieze and Tomasz Tkocz</authors><categories>cs.DS cs.DM math.CO</categories><comments>8 pages</comments><journal-ref>Oper. Res. Lett. 49 (2021), no. 3, 400-404</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider cost constrained versions of the minimum spanning tree problem
and the assignment problem. We assume edge weights are independent copies of a
continuous random variable $Z$ that satisfies $F(x)=\Pr(Z\leq x)\approx
x^\alpha$ as $x\to0$, where $\alpha\geq 1$. Also, there are $r=O(1)$ budget
constraints with edge costs chosen from the same distribution. We use
Lagrangean duality to construct polynomial time algorithms that produce
asymptotically optimal solutions. For the spanning tree problem, we allow
$r&gt;1$, but for the assignment problem we can only analyse the case $r=1$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.03979</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.03979</id><submitter>Hengrui Luo</submitter><version version="v1"><date>Tue, 8 Sep 2020 20:15:14 GMT</date><size>997kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:51:23 GMT</date><size>1184kb</size><source_type>D</source_type></version><title>A Distance-preserving Matrix Sketch</title><authors>Leland Wilkinson, Hengrui Luo</authors><categories>cs.HC cs.LG stat.ML</categories><comments>46 pages, 11 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visualizing very large matrices involves many formidable problems. Various
popular solutions to these problems involve sampling, clustering, projection,
or feature selection to reduce the size and complexity of the original task. An
important aspect of these methods is how to preserve relative distances between
points in the higher-dimensional space after reducing rows and columns to fit
in a lower dimensional space. This aspect is important because conclusions
based on faulty visual reasoning can be harmful. Judging dissimilar points as
similar or similar points as dissimilar on the basis of a visualization can
lead to false conclusions. To ameliorate this bias and to make visualizations
of very large datasets feasible, we introduce two new algorithms that
respectively select a subset of rows and columns of a rectangular matrix. This
selection is designed to preserve relative distances as closely as possible. We
compare our matrix sketch to more traditional alternatives on a variety of
artificial and real datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.05366</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.05366</id><submitter>Tongjia Zheng</submitter><version version="v1"><date>Thu, 10 Sep 2020 02:33:32 GMT</date><size>1079kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 02:27:16 GMT</date><size>1193kb</size></version><title>Distributed Density Filtering for Large-Scale Systems Using Mean-Filed
  Models</title><authors>Tongjia Zheng and Hai Lin</authors><categories>eess.SY cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:2006.11461</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies distributed (probability) density estimation of large-scale
systems. Such problems are motivated by many density-based distributed control
tasks in which the real-time density of the swarm is used as feedback
information, such as sensor deployment and city traffic scheduling. This work
is built upon our previous work [1] which presented a (centralized) density
filter to estimate the dynamic density of large-scale systems through a novel
integration of mean-field models, kernel density estimation (KDE), and
infinite-dimensional Kalman filters. In this work, we further study how to
decentralize the density filter such that each agent can estimate the global
density only based on its local observation and communication with neighbors.
This is achieved by noting that the global observation constructed by KDE is an
average of the local kernels. Hence, dynamic average consensus algorithms are
used for each agent to track the global observation in a distributed way. We
present a distributed density filter which requires very little information
exchange, and study its stability and optimality using the notion of
input-to-state stability. Simulation results suggest that the distributed
filter is able to converge to the centralized filter and remain close to it.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.06073</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.06073</id><submitter>Amit Saha</submitter><version version="v1"><date>Sun, 13 Sep 2020 19:58:47 GMT</date><size>367kb</size><source_type>D</source_type></version><title>Circuit Design for K-coloring Problem and it's Implementation on
  Near-term Quantum Devices</title><authors>Amit Saha, Debasri Saha, Amlan Chakrabarti</authors><categories>cs.ET</categories><comments>6 pages, 15 figures</comments><journal-ref>2020 IEEE International Symposium on Smart Electronic Systems
  (iSES) (Formerly iNiS)</journal-ref><doi>10.1109/iSES50453.2020.00015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days in Quantum Computing, implementation of quantum algorithm has
created a stir since Noisy Intermediate-Scale Quantum (NISQ) devices are out in
the market. Researchers are mostly interested in solving NP-complete problems
with the help of quantum algorithms for its speed-up. As per the work on
computational complexity by Karp, if any of the NP-complete problem can be
solved then any other NP-complete problem can be reduced to that problem in
polynomial time. In this Paper, K-coloring problem (NP-complete problem) has
been considered to solve using Grover's search. A comparator-based approach has
been used to implement K-coloring problem which enables the reduction of the
qubit cost comparing to the state-of-the-art. An end-to-end automated framework
has been proposed to implement K-coloring problem for any unweighted and
undirected graph on any available Noisy Intermediate-Scale Quantum (NISQ)
devices, which helps in generalizing our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.06211</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.06211</id><submitter>Fangda Gu</submitter><version version="v1"><date>Mon, 14 Sep 2020 06:04:55 GMT</date><size>258kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 4 Jan 2021 19:17:06 GMT</date><size>258kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 07:21:32 GMT</date><size>490kb</size><source_type>D</source_type></version><title>Implicit Graph Neural Networks</title><authors>Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, Laurent El Ghaoui</authors><categories>cs.LG stat.ML</categories><comments>Accepted by NeurIPS 2020 at:
  https://papers.nips.cc/paper/2020/hash/8b5c8441a8ff8e151b191c53c1842a38-Abstract.html</comments><journal-ref>Advances in Neural Information Processing Systems 33 (2020)
  11984-11995</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graph Neural Networks (GNNs) are widely used deep learning models that learn
meaningful representations from graph-structured data. Due to the finite nature
of the underlying recurrent structure, current GNN methods may struggle to
capture long-range dependencies in underlying graphs. To overcome this
difficulty, we propose a graph learning framework, called Implicit Graph Neural
Networks (IGNN), where predictions are based on the solution of a fixed-point
equilibrium equation involving implicitly defined &quot;state&quot; vectors. We use the
Perron-Frobenius theory to derive sufficient conditions that ensure
well-posedness of the framework. Leveraging implicit differentiation, we derive
a tractable projected gradient descent method to train the framework.
Experiments on a comprehensive range of tasks show that IGNNs consistently
capture long-range dependencies and outperform the state-of-the-art GNN models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.06236</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.06236</id><submitter>Bonan Hou</submitter><version version="v1"><date>Mon, 14 Sep 2020 07:42:30 GMT</date><size>663kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 13:59:16 GMT</date><size>3020kb</size><source_type>D</source_type></version><title>Consensus of Multi-agent System via Constrained Invariant Set of a class
  of Unstable System</title><authors>Chong Jin Ong, Bonan Hou</authors><categories>eess.SY cs.SY</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work shows an approach to achieve output consensus among heterogeneous
agents in a multi-agent environment where each agent is subject to input
constraints. The communication among agents is described by a time-varying
directed/undirected graph. The approach is based on the well-known Internal
Model Principle which uses an unstable reference system. One main contribution
of this work is the characterization of the maximal constraint admissible
invariant set (MCAI) for the combined agent-reference system. Typically, MCAI
sets do not exist for unstable system. This work shows that for an important
class of agent-reference system that is unstable, MCAI exists and can be
computed. This MCAI set is used in a Reference Governor approach, combined with
a projected consensus algorithm, to achieve output consensus of all agents
while satisfying constraints of each. Examples are provided to illustrate the
approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.06401</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.06401</id><submitter>Pepa Atanasova</submitter><version version="v1"><date>Thu, 10 Sep 2020 13:54:15 GMT</date><size>1750kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 23 Sep 2020 17:00:26 GMT</date><size>1716kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 14:06:14 GMT</date><size>780kb</size><source_type>D</source_type></version><title>Multi-Hop Fact Checking of Political Claims</title><authors>Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, Isabelle Augenstein</authors><categories>cs.CL cs.AI cs.LG</categories><comments>10 pages, to be published at Proceedings of IJCAI-2021</comments><msc-class>68T07, 68T50</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has proposed multi-hop models and datasets for studying complex
natural language reasoning. One notable task requiring multi-hop reasoning is
fact checking, where a set of connected evidence pieces leads to the final
verdict of a claim. However, existing datasets either do not provide
annotations for gold evidence pages, or the only dataset which does (FEVER)
mostly consists of claims which can be fact-checked with simple reasoning and
is constructed artificially. Here, we study more complex claim verification of
naturally occurring claims with multiple hops over interconnected evidence
chunks. We: 1) construct a small annotated dataset, PolitiHop, of evidence
sentences for claim verification; 2) compare it to existing multi-hop datasets;
and 3) study how to transfer knowledge from more extensive in- and
out-of-domain resources to PolitiHop. We find that the task is complex and
achieve the best performance with an architecture that specifically models
reasoning over evidence pieces in combination with in-domain transfer learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.06628</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.06628</id><submitter>Makrand Khanwale</submitter><version version="v1"><date>Sun, 13 Sep 2020 19:17:00 GMT</date><size>8963kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 3 Oct 2020 17:22:33 GMT</date><size>8964kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 20:49:10 GMT</date><size>9355kb</size><source_type>D</source_type></version><title>A fully-coupled framework for solving Cahn-Hilliard Navier-Stokes
  equations: Second-order, energy-stable numerical methods on adaptive octree
  based meshes</title><authors>Makrand A Khanwale, Kumar Saurabh, Milinda Fernando, Victor M. Calo,
  James A. Rossmanith, Hari Sundar, Baskar Ganapathysubramanian</authors><categories>math.NA cs.NA physics.flu-dyn</categories><comments>48 pages, 18 figures, submitted to Computer Methods in Applied
  Mechanics and Engineering</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a fully-coupled, implicit-in-time framework for solving a
thermodynamically-consistent Cahn-Hilliard Navier-Stokes system that models
two-phase flows. In this work, we extend the block iterative method presented
in Khanwale et al. [{\it Simulating two-phase flows with thermodynamically
consistent energy stable Cahn-Hilliard Navier-Stokes equations on parallel
adaptive octree based meshes}, J. Comput. Phys. (2020)], to a fully-coupled,
provably second-order accurate scheme in time, while maintaining
energy-stability. The new method requires fewer matrix assemblies in each
Newton iteration resulting in faster solution time. The method is based on a
fully-implicit Crank-Nicolson scheme in time and a pressure stabilization for
an equal order Galerkin formulation. That is, we use a conforming continuous
Galerkin (cG) finite element method in space equipped with a residual-based
variational multiscale (RBVMS) procedure to stabilize the pressure. We deploy
this approach on a massively parallel numerical implementation using parallel
octree-based adaptive meshes. We present comprehensive numerical experiments
showing detailed comparisons with results from the literature for canonical
cases, including the single bubble rise, Rayleigh-Taylor instability, and
lid-driven cavity flow problems. We analyze in detail the scaling of our
numerical implementation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.07237</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.07237</id><submitter>Shilin He</submitter><version version="v1"><date>Tue, 15 Sep 2020 17:22:06 GMT</date><size>907kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 04:12:30 GMT</date><size>2020kb</size><source_type>D</source_type></version><title>A Survey on Automated Log Analysis for Reliability Engineering</title><authors>Shilin He, Pinjia He, Zhuangbin Chen, Tianyi Yang, Yuxin Su, Michael
  R. Lyu</authors><categories>cs.SE</categories><comments>accepted by ACM Computing Survey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logs are semi-structured text generated by logging statements in software
source code. In recent decades, software logs have become imperative in the
reliability assurance mechanism of many software systems because they are often
the only data available that record software runtime information. As modern
software is evolving into a large scale, the volume of logs has increased
rapidly. To enable effective and efficient usage of modern software logs in
reliability engineering, a number of studies have been conducted on automated
log analysis. This survey presents a detailed overview of automated log
analysis research, including how to automate and assist the writing of logging
statements, how to compress logs, how to parse logs into structured event
templates, and how to employ logs to detect anomalies, predict failures, and
facilitate diagnosis. Additionally, we survey work that releases open-source
toolkits and datasets. Based on the discussion of the recent advances, we
present several promising future directions toward real-world and
next-generation automated log analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.07360</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.07360</id><submitter>Chidubem Arachie</submitter><version version="v1"><date>Tue, 15 Sep 2020 21:30:53 GMT</date><size>75kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 7 Dec 2020 02:32:49 GMT</date><size>73kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 5 Feb 2021 02:48:45 GMT</date><size>74kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 10 Feb 2021 17:35:46 GMT</date><size>73kb</size><source_type>D</source_type></version><version version="v5"><date>Sat, 29 May 2021 19:51:20 GMT</date><size>65kb</size><source_type>D</source_type></version><title>Constrained Labeling for Weakly Supervised Learning</title><authors>Chidubem Arachie, Bert Huang</authors><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted at UAI 2021</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Curation of large fully supervised datasets has become one of the major
roadblocks for machine learning. Weak supervision provides an alternative to
supervised learning by training with cheap, noisy, and possibly correlated
labeling functions from varying sources. The key challenge in weakly supervised
learning is combining the different weak supervision signals while navigating
misleading correlations in their errors. In this paper, we propose a simple
data-free approach for combining weak supervision signals by defining a
constrained space for the possible labels of the weak signals and training with
a random labeling within this constrained space. Our method is efficient and
stable, converging after a few iterations of gradient descent. We prove
theoretical conditions under which the worst-case error of the randomized label
decreases with the rank of the linear constraints. We show experimentally that
our method outperforms other weak supervision methods on various text- and
image-classification tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.07542</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.07542</id><submitter>Trung Vu</submitter><version version="v1"><date>Wed, 16 Sep 2020 08:28:29 GMT</date><size>36kb</size></version><version version="v2"><date>Sun, 30 May 2021 19:04:35 GMT</date><size>100kb</size></version><title>Perturbation expansions and error bounds for the truncated singular
  value decomposition</title><authors>Trung Vu, Evgenia Chunikhina and Raviv Raich</authors><categories>math.NA cs.NA math.ST stat.TH</categories><comments>Accepted to Linear Algebra and Its Applications</comments><msc-class>15A06 (Primary) 65F06 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Truncated singular value decomposition is a reduced version of the singular
value decomposition in which only a few largest singular values are retained.
This paper presents a novel perturbation analysis for the truncated singular
value decomposition for real matrices. First, we describe perturbation
expansions for the singular value truncation of order $r$. We extend
perturbation results for the singular subspace decomposition to derive the
first-order perturbation expansion of the truncated operator about a matrix
with rank greater than or equal to $r$. Observing that the first-order
expansion can be greatly simplified when the matrix has exact rank $r$, we
further show that the singular value truncation admits a simple second-order
perturbation expansion about a rank-$r$ matrix. Second, we introduce the
first-known error bound on the linear approximation of the truncated singular
value decomposition of a perturbed rank-$r$ matrix. Our bound only depends on
the least singular value of the unperturbed matrix and the norm of the
perturbation matrix. Intriguingly, while the singular subspaces are known to be
extremely sensitive to additive noises, the newly established error bound holds
universally for perturbations with arbitrary magnitude. Finally, we demonstrate
an application of our results to the analysis of the mean squared error
associated with the TSVD-based matrix denoising solution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.07547</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.07547</id><submitter>Ketson Roberto Maximiano dos Santos</submitter><version version="v1"><date>Wed, 16 Sep 2020 08:32:02 GMT</date><size>42713kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 28 Sep 2020 18:55:56 GMT</date><size>26650kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 19:51:41 GMT</date><size>15531kb</size><source_type>D</source_type></version><title>Grassmannian diffusion maps based dimension reduction and classification
  for high-dimensional data</title><authors>K. R. M. dos Santos, D. G. Giovanis, M. D. Shields</authors><categories>cs.LG stat.ML</categories><msc-class>53Z50, 14M15, 60J20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces the Grassmannian Diffusion Maps, a novel nonlinear
dimensionality reduction technique that defines the affinity between points
through their representation as low-dimensional subspaces corresponding to
points on the Grassmann manifold. The method is designed for applications, such
as image recognition and data-based classification of high-dimensional data
that can be compactly represented in a lower dimensional subspace. The GDMaps
is composed of two stages. The first is a pointwise linear dimensionality
reduction wherein each high-dimensional object is mapped onto the Grassmann.
The second stage is a multi-point nonlinear kernel-based dimension reduction
using Diffusion maps to identify the subspace structure of the points on the
Grassmann manifold. To this aim, an appropriate Grassmannian kernel is used to
construct the transition matrix of a random walk on a graph connecting points
on the Grassmann manifold. Spectral analysis of the transition matrix yields
low-dimensional Grassmannian diffusion coordinates embedding the data into a
low-dimensional reproducing kernel Hilbert space. Further, a novel data
classification/recognition technique is developed based on the construction of
an overcomplete dictionary of reduced dimension whose atoms are given by the
Grassmannian diffusion coordinates. Three examples are considered. First, a
&quot;toy&quot; example shows that the GDMaps can identify an appropriate parametrization
of structured points on the unit sphere. The second example demonstrates the
ability of the GDMaps to reveal the intrinsic subspace structure of
high-dimensional random field data. In the last example, a face recognition
problem is solved considering face images subject to varying illumination
conditions, changes in face expressions, and occurrence of occlusions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.07625</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.07625</id><submitter>Hector Garcia de Marina Dr.</submitter><version version="v1"><date>Wed, 16 Sep 2020 12:09:17 GMT</date><size>323kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 11:48:01 GMT</date><size>324kb</size></version><title>Distributed formation maneuver control by manipulating the complex
  Laplacian</title><authors>Hector Garcia de Marina</authors><categories>eess.SY cs.RO cs.SY</categories><comments>Automatica, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel maneuvering technique for the
complex-Laplacian-based formation control. We show how to modify the original
weights that build the Laplacian such that a designed steady-state motion of
the desired shape emerges from the local interactions among the agents. These
collective motions can be exploited to solve problems such as the shaped
consensus (the rendezvous with a particular shape), the enclosing of a target,
or translations with controlled speed and heading to assist mobile robots in
area coverage, escorting, and traveling missions, respectively. The designed
steady-state collective motions correspond to rotations around the centroid,
translations, and scalings of a reference shape. The proposed modification of
the weights relocates one of the Laplacian's zero eigenvalues while preserving
its associated eigenvector that constructs the desired shape. For example, such
relocation on the imaginary or real axis induces rotational and scaling
motions, respectively. We will show how to satisfy a sufficient condition to
guarantee the global convergence to the desired shape and motions. Finally, we
provide simulations and comparisons with other maneuvering techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08008</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08008</id><submitter>Xue Liang</submitter><version version="v1"><date>Thu, 17 Sep 2020 01:42:42 GMT</date><size>23490kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 18 Sep 2020 09:01:20 GMT</date><size>23490kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 31 Jan 2021 11:13:40 GMT</date><size>23490kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 16:13:56 GMT</date><size>7833kb</size><source_type>D</source_type></version><title>The Boundary Element Method of Peridynamics</title><authors>Xue Liang, Linjuan Wang, Jifeng Xu, Jianxiang Wang</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The peridynamic theory brings advantages in dealing with discontinuities,
dynamic loading, and non-locality. The integro-differential formulation of
peridynamics poses challenges to numerical solutions of complicated and
practical problems. Some important issues attract much attention, such as the
computation of infinite domains, the treatment of softening of boundaries due
to an incomplete horizon, and time error accumulation in dynamic processes. In
this work, we develop the \textit{peridynamic boundary element method}
(PD-BEM). The numerical examples demonstrate that the PD-BEM exhibits several
features. First, for non-destructive cases, the PD-BEM can be one to two orders
of magnitude faster than the peridynamic meshless particle method (PD-MPM) that
directly discretizes the computational domains; second, it eliminates the time
accumulation error, and thus conserves the total energy much better than the
PD-MPM; third, it does not exhibit spurious boundary softening phenomena. For
destructive cases where new boundaries emerge during the loading process, we
propose a coupling scheme where the PD-MPM is applied to the cracked region and
the PD-BEM is applied to the un-cracked region such that the time of
computation can be significantly reduced.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08330</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08330</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Thu, 17 Sep 2020 14:28:27 GMT</date><size>56kb</size></version><version version="v2"><date>Sat, 10 Oct 2020 13:55:04 GMT</date><size>56kb</size></version><version version="v3"><date>Wed, 2 Jun 2021 03:09:58 GMT</date><size>56kb</size><source_type>D</source_type></version><title>More Embeddings, Better Sequence Labelers?</title><authors>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
  Huang, Kewei Tu</authors><categories>cs.CL cs.LG</categories><comments>Accepted to Findings of EMNLP 2020. Camera-ready, 16 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent work proposes a family of contextual embeddings that significantly
improves the accuracy of sequence labelers over non-contextual embeddings.
However, there is no definite conclusion on whether we can build better
sequence labelers by combining different kinds of embeddings in various
settings. In this paper, we conduct extensive experiments on 3 tasks over 18
datasets and 8 languages to study the accuracy of sequence labeling with
various embedding concatenations and make three observations: (1) concatenating
more embedding variants leads to better accuracy in rich-resource and
cross-domain settings and some conditions of low-resource settings; (2)
concatenating additional contextual sub-word embeddings with contextual
character embeddings hurts the accuracy in extremely low-resource settings; (3)
based on the conclusion of (1), concatenating additional similar contextual
embeddings cannot lead to further improvements. We hope these conclusions can
help people build stronger sequence labelers in various settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08378</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08378</id><submitter>Timo C. Wunderlich</submitter><version version="v1"><date>Thu, 17 Sep 2020 15:45:00 GMT</date><size>1925kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 21 Sep 2020 15:59:39 GMT</date><size>2528kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 18:00:07 GMT</date><size>2530kb</size><source_type>D</source_type></version><title>Event-Based Backpropagation can compute Exact Gradients for Spiking
  Neural Networks</title><authors>Timo C. Wunderlich, Christian Pehle</authors><categories>q-bio.NC cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking neural networks combine analog computation with event-based
communication using discrete spikes. While the impressive advances of deep
learning are enabled by training non-spiking artificial neural networks using
the backpropagation algorithm, applying this algorithm to spiking networks was
previously hindered by the existence of discrete spike events and
discontinuities. For the first time, this work derives the backpropagation
algorithm for a continuous-time spiking neural network and a general loss
function by applying the adjoint method together with the proper partial
derivative jumps, allowing for backpropagation through discrete spike events
without approximations. This algorithm, EventProp, backpropagates errors at
spike times in order to compute the exact gradient in an event-based,
temporally and spatially sparse fashion. We use gradients computed via
EventProp to train networks on the Yin-Yang and MNIST datasets using either a
spike time or voltage based loss function and report competitive performance.
Our work supports the rigorous study of gradient-based learning algorithms in
spiking neural networks and provides insights toward their implementation in
novel brain-inspired hardware.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08553</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08553</id><submitter>Yuning Mao</submitter><version version="v1"><date>Thu, 17 Sep 2020 23:08:01 GMT</date><size>3272kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 24 Oct 2020 03:23:27 GMT</date><size>1637kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 30 May 2021 22:08:35 GMT</date><size>3278kb</size><source_type>D</source_type></version><title>Generation-Augmented Retrieval for Open-domain Question Answering</title><authors>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao,
  Jiawei Han, Weizhu Chen</authors><categories>cs.CL cs.IR</categories><comments>ACL 2021 Camera-ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Generation-Augmented Retrieval (GAR) for answering open-domain
questions, which augments a query through text generation of heuristically
discovered relevant contexts without external resources as supervision. We
demonstrate that the generated contexts substantially enrich the semantics of
the queries and GAR with sparse representations (BM25) achieves comparable or
better performance than state-of-the-art dense retrieval methods such as DPR.
We show that generating diverse contexts for a query is beneficial as fusing
their results consistently yields better retrieval accuracy. Moreover, as
sparse and dense representations are often complementary, GAR can be easily
combined with DPR to achieve even better performance. GAR achieves
state-of-the-art performance on Natural Questions and TriviaQA datasets under
the extractive QA setup when equipped with an extractive reader, and
consistently outperforms other retrieval methods when the same generative
reader is used.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08633</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08633</id><submitter>Zhichao Geng</submitter><version version="v1"><date>Fri, 18 Sep 2020 05:41:52 GMT</date><size>169kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 03:54:02 GMT</date><size>5461kb</size><source_type>D</source_type></version><title>fastHan: A BERT-based Multi-Task Toolkit for Chinese NLP</title><authors>Zhichao Geng, Hang Yan, Xipeng Qiu, Xuanjing Huang</authors><categories>cs.CL</categories><comments>ACL2021 Demo Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present fastHan, an open-source toolkit for four basic tasks in Chinese
natural language processing: Chinese word segmentation (CWS), Part-of-Speech
(POS) tagging, named entity recognition (NER), and dependency parsing. The
backbone of fastHan is a multi-task model based on a pruned BERT, which uses
the first 8 layers in BERT. We also provide a 4-layer base model compressed
from the 8-layer model. The joint-model is trained and evaluated on 13 corpora
of four tasks, yielding near state-of-the-art (SOTA) performance in dependency
parsing and NER, achieving SOTA performance in CWS and POS. Besides, fastHan's
transferability is also strong, performing much better than popular
segmentation tools on a non-training corpus. To better meet the need of
practical application, we allow users to use their own labeled data to further
fine-tune fastHan. In addition to its small size and excellent performance,
fastHan is user-friendly. Implemented as a python package, fastHan isolates
users from the internal technical details and is convenient to use. The project
is released on Github.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08869</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08869</id><submitter>Wajid Arshad Abbasi</submitter><version version="v1"><date>Wed, 16 Sep 2020 17:12:25 GMT</date><size>605kb</size></version><title>PANDA: Predicting the change in proteins binding affinity upon mutations
  using sequence information</title><authors>Wajid Arshad Abbasi, Syed Ali Abbas, Saiqa Andleeb</authors><categories>q-bio.BM cs.AI cs.LG stat.ML</categories><journal-ref>Journal of Bioinformatics and Computational Biology, 2021</journal-ref><doi>10.1142/S0219720021500153</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurately determining a change in protein binding affinity upon mutations is
important for the discovery and design of novel therapeutics and to assist
mutagenesis studies. Determination of change in binding affinity upon mutations
requires sophisticated, expensive, and time-consuming wet-lab experiments that
can be aided with computational methods. Most of the computational prediction
techniques require protein structures that limit their applicability to protein
complexes with known structures. In this work, we explore the sequence-based
prediction of change in protein binding affinity upon mutation. We have used
protein sequence information instead of protein structures along with machine
learning techniques to accurately predict the change in protein binding
affinity upon mutation. Our proposed sequence-based novel change in protein
binding affinity predictor called PANDA gives better accuracy than existing
methods over the same validation set as well as on an external independent test
dataset. On an external test dataset, our proposed method gives a maximum
Pearson correlation coefficient of 0.52 in comparison to the state-of-the-art
existing protein structure-based method called MutaBind which gives a maximum
Pearson correlation coefficient of 0.59. Our proposed protein sequence-based
method, to predict a change in binding affinity upon mutations, has wide
applicability and comparable performance in comparison to existing protein
structure-based methods. A cloud-based webserver implementation of PANDA and
its python code is available at
https://sites.google.com/view/wajidarshad/software and
https://github.com/wajidarshad/panda.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.09112</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.09112</id><submitter>Tian Shi</submitter><version version="v1"><date>Fri, 18 Sep 2020 22:32:39 GMT</date><size>629kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 03:44:49 GMT</date><size>1231kb</size><source_type>D</source_type></version><title>An Interpretable and Uncertainty Aware Multi-Task Framework for
  Multi-Aspect Sentiment Analysis</title><authors>Tian Shi and Ping Wang and Chandan K. Reddy</authors><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, several online platforms have seen a rapid increase in the
number of review systems that request users to provide aspect-level feedback.
Document-level Multi-aspect Sentiment Classification (DMSC), where the goal is
to predict the ratings/sentiment from a review at an individual aspect level,
has become a challenging and imminent problem. To tackle this challenge, we
propose a deliberate self-attention-based deep neural network model, namely
FEDAR, for the DMSC problem, which can achieve competitive performance while
also being able to interpret the predictions made. FEDAR is equipped with a
highway word embedding layer to transfer knowledge from pre-trained word
embeddings, an RNN encoder layer with output features enriched by pooling and
factorization techniques, and a deliberate self-attention layer. In addition,
we also propose an Attention-driven Keywords Ranking (AKR) method, which can
automatically discover aspect keywords and aspect-level opinion keywords from
the review corpus based on the attention weights. These keywords are
significant for rating predictions by FEDAR. Since crowdsourcing annotation can
be an alternate way to recover missing ratings of reviews, we propose a
LEcture-AuDience (LEAD) strategy to estimate model uncertainty in the context
of multi-task learning, so that valuable human resources can focus on the most
uncertain predictions. Our extensive set of experiments on five different
open-domain DMSC datasets demonstrate the superiority of the proposed FEDAR and
LEAD models. We further introduce two new DMSC datasets in the healthcare
domain and benchmark different baseline models and our models on them.
Attention weights visualization results and visualization of aspect and opinion
keywords demonstrate the interpretability of our model and the effectiveness of
our AKR method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.09235</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.09235</id><submitter>Nils Keunecke</submitter><version version="v1"><date>Sat, 19 Sep 2020 14:06:18 GMT</date><size>14286kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 26 Jan 2021 16:17:28 GMT</date><size>25827kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 May 2021 19:54:03 GMT</date><size>26312kb</size><source_type>D</source_type></version><title>Open-Ended Fine-Grained 3D Object Categorization by Combining Shape and
  Texture Features in Multiple Colorspaces</title><authors>Nils Keunecke and S. Hamidreza Kasaei</authors><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a consequence of an ever-increasing number of service robots, there is a
growing demand for highly accurate real-time 3D object recognition. Considering
the expansion of robot applications in more complex and dynamic environments,it
is evident that it is not possible to pre-program all object categories and
anticipate all exceptions in advance. Therefore, robots should have the
functionality to learn about new object categories in an open-ended fashion
while working in the environment.Towards this goal, we propose a deep transfer
learning approach to generate a scale- and pose-invariant object representation
by considering shape and texture information in multiple colorspaces. The
obtained global object representation is then fed to an instance-based object
category learning and recognition,where a non-expert human user exists in the
learning loop and can interactively guide the process of experience acquisition
by teaching new object categories, or by correcting insufficient or erroneous
categories. In this work, shape information encodes the common patterns of all
categories, while texture information is used to describes the appearance of
each instance in detail.Multiple color space combinations and network
architectures are evaluated to find the most descriptive system. Experimental
results showed that the proposed network architecture out-performed the
selected state-of-the-art approaches in terms of object classification accuracy
and scalability. Furthermore, we performed a real robot experiment in the
context of serve-a-beer scenario to show the real-time performance of the
proposed approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.09780</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.09780</id><submitter>Lucas Teixeira</submitter><version version="v1"><date>Mon, 21 Sep 2020 12:03:54 GMT</date><size>16357kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 30 Jan 2021 11:08:55 GMT</date><size>16068kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 10:09:45 GMT</date><size>18005kb</size><source_type>D</source_type></version><title>Impact of lung segmentation on the diagnosis and explanation of COVID-19
  in chest X-ray images</title><authors>Lucas O. Teixeira, Rodolfo M. Pereira, Diego Bertolini, Luiz S.
  Oliveira, Loris Nanni, George D. C. Cavalcanti, Yandre M. G. Costa</authors><categories>eess.IV cs.CV cs.LG</categories><comments>Submitted to International Journal of Universal Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The COVID-19 pandemic is undoubtedly one of the biggest public health crises
our society has ever faced in recent history. One of the main complications
caused by COVID-19 is pneumonia, which is diagnosed using imaging exams, such
as chest X-ray (CXR) and computed tomography (CT) scan. The CT scan is more
precise than the CXR. However, CXR is suitable in particular situations because
it is cheaper, faster, more widespread, and exposes the patient to less
radiation. This study aims to demonstrate the impact of lung segmentation in
COVID-19 identification using CXR images and evaluate which contents of the
image decisively contribute to its identification. We performed the lung
segmentation using a U-Net CNN architecture, and the classification using three
well-known CNN architectures: VGG, ResNet, and Inception. To estimate the
impact of lung segmentation, we applied some Explainable Artificial
Intelligence (XAI) techniques, specifically LIME and Grad-CAM. To empirically
evaluate our approach, we composed a database with three classes: lung opacity
(pneumonia), COVID-19, and normal. The segmentation achieved a Jaccard distance
of 0.034 and a Dice coefficient of 0.982. The classification using segmented
lung achieved an F1-Score of 0.88 for the multi-class setup and 0.83 for
COVID-19 identification. Further testing and XAI techniques suggest that
segmented CXR images represent a much more realistic and less biased
performance. To the best of our knowledge, no other work tried to estimate the
impact of lung segmentation in COVID-19 identification using comprehensive XAI
techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.10178</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.10178</id><submitter>Gianmarco Mengaldo Dr</submitter><version version="v1"><date>Fri, 18 Sep 2020 09:46:21 GMT</date><size>39034kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 02:50:01 GMT</date><size>9709kb</size><source_type>D</source_type></version><title>Industry-Relevant Implicit Large-Eddy Simulation of a High-Performance
  Road Car via Spectral/hp Element Methods</title><authors>Gianmarco Mengaldo, David Moxey, Michael Turner, Rodrigo C. Moura,
  Ayad Jassim, Mark Taylor, Joaquim Peiro, Spencer J. Sherwin</authors><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a successful deployment of high-fidelity Large-Eddy Simulation
(LES) technologies based on spectral/hp element methods to industrial flow
problems, which are characterized by high Reynolds numbers and complex
geometries. In particular, we describe the numerical methods, software
development and steps that were required to perform the implicit LES of a real
automotive car, namely the Elemental Rp1 model. To the best of the authors'
knowledge, this simulation represents the first fifth-order accurate transient
LES of an entire real car geometry. Moreover, this constitutes a key milestone
towards considerably expanding the computational design envelope currently
allowed in industry, where steady-state modelling remains the standard. To this
end, a number of novel developments had to be made in order to overcome
obstacles in mesh generation and solver technology to achieve this simulation,
which we detail in this paper. The main objective is to present to the
industrial and applied mathematics community, a viable pathway to translate
academic developments into industrial tools, that can substantially advance the
analysis and design capabilities of high-end engineering stakeholders. The
novel developments and results were achieved using the academic-driven
open-source framework Nektar++.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.10580</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.10580</id><submitter>Nannan Li</submitter><version version="v1"><date>Tue, 22 Sep 2020 14:44:27 GMT</date><size>2479kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 08:44:25 GMT</date><size>2481kb</size><source_type>D</source_type></version><title>Heuristic Rank Selection with Progressively Searching Tensor Ring
  Network</title><authors>Nannan Li, Yu Pan, Yaran Chen, Zixiang Ding, Dongbin Zhao, Zenglin Xu</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Tensor Ring Networks (TRNs) have been applied in deep networks,
achieving remarkable successes in compression ratio and accuracy. Although
highly related to the performance of TRNs, rank selection is seldom studied in
previous works and usually set to equal in experiments. Meanwhile, there is not
any heuristic method to choose the rank, and an enumerating way to find
appropriate rank is extremely time-consuming. Interestingly, we discover that
part of the rank elements is sensitive and usually aggregate in a narrow
region, namely an interest region. Therefore, based on the above phenomenon, we
propose a novel progressive genetic algorithm named Progressively Searching
Tensor Ring Network Search (PSTRN), which has the ability to find optimal rank
precisely and efficiently. Through the evolutionary phase and progressive
phase, PSTRN can converge to the interest region quickly and harvest good
performance. Experimental results show that PSTRN can significantly reduce the
complexity of seeking rank, compared with the enumerating method. Furthermore,
our method is validated on public benchmarks like MNIST, CIFAR10/100, UCF11 and
HMDB51, achieving the state-of-the-art performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.11742</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.11742</id><submitter>Francesco Rizzi</submitter><version version="v1"><date>Thu, 24 Sep 2020 15:06:00 GMT</date><size>5998kb</size><source_type>AD</source_type></version><version version="v2"><date>Sun, 14 Feb 2021 16:02:32 GMT</date><size>4507kb</size><source_type>AD</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 11:55:18 GMT</date><size>8721kb</size><source_type>AD</source_type></version><title>A compute-bound formulation of Galerkin model reduction for linear
  time-invariant dynamical systems</title><authors>Francesco Rizzi, Eric J. Parish, Patrick J. Blonigan, John Tencer</authors><categories>physics.comp-ph cs.CE cs.DC cs.MS math.DS</categories><comments>Revised version, 28 pages, 9 figures</comments><report-no>SAND2020-10291</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This work aims to advance computational methods for projection-based reduced
order models (ROMs) of linear time-invariant (LTI) dynamical systems. For such
systems, current practice relies on ROM formulations expressing the state as a
rank-1 tensor (i.e., a vector), leading to computational kernels that are
memory bandwidth bound and, therefore, ill-suited for scalable performance on
modern many-core and hybrid computing nodes. This weakness can be particularly
limiting when tackling many-query studies, where one needs to run a large
number of simulations. This work introduces a reformulation, called rank-2
Galerkin, of the Galerkin ROM for LTI dynamical systems which converts the
nature of the ROM problem from memory bandwidth to compute bound. We present
the details of the formulation and its implementation, and demonstrate its
utility through numerical experiments using, as a test case, the simulation of
elastic seismic shear waves in an axisymmetric domain. We quantify and analyze
performance and scaling results for varying numbers of threads and problem
sizes. Finally, we present an end-to-end demonstration of using the rank-2
Galerkin ROM for a Monte Carlo sampling study. We show that the rank-2 Galerkin
ROM is one order of magnitude more efficient than the rank-1 Galerkin ROM (the
current practice) and about 970X more efficient than the full order model,
while maintaining accuracy in both the mean and statistics of the field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.11920</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.11920</id><submitter>David Guillermo Fajardo Ortiz Dr.</submitter><version version="v1"><date>Thu, 24 Sep 2020 19:29:34 GMT</date><size>1456kb</size></version><version version="v2"><date>Tue, 29 Sep 2020 19:13:03 GMT</date><size>1456kb</size></version><version version="v3"><date>Mon, 19 Oct 2020 21:13:32 GMT</date><size>1420kb</size></version><version version="v4"><date>Tue, 1 Jun 2021 15:25:12 GMT</date><size>1303kb</size></version><title>Funding CRISPR: Understanding the role of government and philanthropic
  institutions in supporting academic research within the CRISPR innovation
  system</title><authors>David Fajardo-Ortiz, Stefan Hornbostel, Maywa Montenegro-de-Wit, Annie
  Shattuck</authors><categories>cs.DL cs.SI</categories><comments>17 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CRISPR/Cas has the potential to revolutionize medicine, agriculture, and
biology. Understanding the trajectory of innovation, how it is influenced and
who pays for it, is an essential research policy question, especially as US
government support for research experiences a relative decline. We use a new
method -- based on funding sources identified in publications' funding
acknowledgements -- to map the networks involved in supporting key stages of
highly influential research, namely basic biological research and technology
development. We present a model of co-funding networks at the two most
prominent institutions for CRISPR/Cas research, the University of California
and the Harvard/MIT/Broad Institute, to illuminate how philanthropic and
charitable organizations have articulated with US government agencies to
co-finance the discovery and development of CRISPR/Cas. We mapped foundational
US government support to both stages of CRISPR/Cas research at both
institutions, while philanthropic organizations have concentrated in co-funding
CRISPR/Cas technology development as opposed to basic biological research. This
is particularly true for the Broad/Harvard/MIT system, where philanthropic
investment clustered around particular technological development themes. These
network models raise fundamental questions about the role of the state and the
influence of philanthropy over the trajectory of transformative technologies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.12143</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.12143</id><submitter>Brian Fitzpatrick</submitter><version version="v1"><date>Fri, 25 Sep 2020 11:51:24 GMT</date><size>2622kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 30 Sep 2020 15:11:20 GMT</date><size>734kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 1 Oct 2020 11:48:18 GMT</date><size>734kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 2 Oct 2020 11:29:59 GMT</date><size>729kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 11 Jan 2021 19:12:53 GMT</date><size>487kb</size><source_type>D</source_type></version><version version="v6"><date>Thu, 3 Jun 2021 10:28:21 GMT</date><size>490kb</size><source_type>D</source_type></version><title>On the Convergence of the Multipole Expansion Method</title><authors>Brian Fitzpatrick, Enzo De Sena, Toon van Waterschoot</authors><categories>math.NA cs.NA</categories><comments>21 pages, 2 figures; Corrected a scaling error that occurred when
  plotting the third columns of Figs 1,2,3, some very minor grammatical edits
  to the intro/conclusion to improve clarity and conciseness, included funding
  info in first page; updated intro with historical info; reformatted several
  sections to reduce no. of pages; changed title, shortened abstract; fixed
  typo in proof of Thm 1.1</comments><msc-class>31A10, 42B10, 65N12, 65N15, 65R20, 70F10, 78M15, 78M16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multipole expansion method (MEM) is a spatial discretization technique
that is widely used in applications that feature scattering of waves from
circular cylinders. Moreover, it also serves as a key component in several
other numerical methods in which scattering computations involving arbitrarily
shaped objects are accelerated by enclosing the objects in artificial
cylinders. A fundamental question is that of how fast the approximation error
of the MEM converges to zero as the truncation number goes to infinity. Despite
the fact that the MEM was introduced in 1913, and has been in widespread usage
as a numerical technique since as far back as 1955, to the best of the authors'
knowledge, a precise characterization of the asymptotic rate of convergence of
the MEM has not been obtained. In this work, we provide a resolution to this
issue. While our focus in this paper is on the Dirichlet scattering problem,
this is merely for convenience and our results actually establish convergence
rates that hold for all MEM formulations irrespective of the specific boundary
conditions or boundary integral equation solution representation chosen.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.12669</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.12669</id><submitter>Rauno Cavallaro</submitter><version version="v1"><date>Sat, 26 Sep 2020 19:17:29 GMT</date><size>13093kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 1 Oct 2020 07:54:49 GMT</date><size>13117kb</size><source_type>D</source_type></version><title>Aerostructural Wing Shape Optimization assisted by Algorithmic
  Differentiation</title><authors>Rocco Bombardieri and Rauno Cavallaro and Ruben Sanchez and Nicolas R.
  Gauger</authors><categories>cs.CE</categories><doi>10.1007/s00158-021-02884-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With more efficient structures, last trends in aeronautics have witnessed an
increased flexibility of wings, calling for adequate design and optimization
approaches. To correctly model the coupled physics, aerostructural optimization
has progressively become more important, being nowadays performed also
considering higher-fidelity discipline methods, i.e., CFD for aerodynamics and
FEM for structures. In this paper a methodology for high-fidelity
gradient-based aerostructural optimization of wings, including aerodynamic and
structural nonlinearities, is presented. The main key feature of the method is
its modularity: each discipline solver, independently employing algorithmic
differentiation for the evaluation of adjoint-based sensitivities, is
interfaced at high-level by means of a wrapper to both solve the aerostructural
primal problem and evaluate exact discrete gradients of the coupled problem.
The implemented capability, ad-hoc created to demonstrate the methodology, and
freely available within the open-source SU2 multiphysics suite, is applied to
perform aerostructural optimization of aeroelastic test cases based on the
ONERA M6 and NASA CRM wings. Single-point optimizations, employing Euler or
RANS flow models, are carried out to find wing optimal outer mold line in terms
of aerodynamic efficiency. Results remark the importance of taking into account
the aerostructural coupling when performing wing shape optimization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.13145</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.13145</id><submitter>Yifei Huang</submitter><version version="v1"><date>Mon, 28 Sep 2020 08:51:42 GMT</date><size>133kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 04:14:08 GMT</date><size>131kb</size><source_type>D</source_type></version><title>Adversarial Robustness of Stabilized NeuralODEs Might be from Obfuscated
  Gradients</title><authors>Yifei Huang, Yaodong Yu, Hongyang Zhang, Yi Ma, Yuan Yao</authors><categories>cs.LG stat.ML</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a provably stable architecture for Neural Ordinary
Differential Equations (ODEs) which achieves non-trivial adversarial robustness
under white-box adversarial attacks even when the network is trained naturally.
For most existing defense methods withstanding strong white-box attacks, to
improve robustness of neural networks, they need to be trained adversarially,
hence have to strike a trade-off between natural accuracy and adversarial
robustness. Inspired by dynamical system theory, we design a stabilized neural
ODE network named SONet whose ODE blocks are skew-symmetric and proved to be
input-output stable. With natural training, SONet can achieve comparable
robustness with the state-of-the-art adversarial defense methods, without
sacrificing natural accuracy. Even replacing only the first layer of a ResNet
by such a ODE block can exhibit further improvement in robustness, e.g., under
PGD-20 ($\ell_\infty=0.031$) attack on CIFAR-10 dataset, it achieves 91.57\%
and natural accuracy and 62.35\% robust accuracy, while a counterpart
architecture of ResNet trained with TRADES achieves natural and robust accuracy
76.29\% and 45.24\%, respectively. To understand possible reasons behind this
surprisingly good result, we further explore the possible mechanism underlying
such an adversarial robustness. We show that the adaptive stepsize numerical
ODE solver, DOPRI5, has a gradient masking effect that fails the PGD attacks
which are sensitive to gradient information of training loss; on the other
hand, it cannot fool the CW attack of robust gradients and the SPSA attack that
is gradient-free. This provides a new explanation that the adversarial
robustness of ODE-based networks mainly comes from the obfuscated gradients in
numerical ODE solvers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.13284</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.13284</id><submitter>Hongjin Qian</submitter><version version="v1"><date>Mon, 28 Sep 2020 12:49:07 GMT</date><size>7149kb</size></version><version version="v2"><date>Thu, 15 Apr 2021 02:51:35 GMT</date><size>1056kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 05:53:44 GMT</date><size>1300kb</size><source_type>D</source_type></version><title>Pchatbot: A Large-Scale Dataset for Personalized Chatbot</title><authors>Hongjin Qian, Xiaohe Li, Hanxun Zhong, Yu Guo, Yueyuan Ma, Yutao Zhu,
  Zhanliang Liu, Zhicheng Dou, Ji-Rong Wen</authors><categories>cs.CL cs.AI</categories><comments>Camera-ready version, SIGIR 2021 (Resource Track), the dataset and
  codes are available at https://github.com/qhjqhj00/Pchatbot</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language dialogue systems raise great attention recently. As many
dialogue models are data-driven, high-quality datasets are essential to these
systems. In this paper, we introduce Pchatbot, a large-scale dialogue dataset
that contains two subsets collected from Weibo and Judicial forums
respectively. To adapt the raw dataset to dialogue systems, we elaborately
normalize the raw dataset via processes such as anonymization, deduplication,
segmentation, and filtering. The scale of Pchatbot is significantly larger than
existing Chinese datasets, which might benefit the data-driven models. Besides,
current dialogue datasets for personalized chatbot usually contain several
persona sentences or attributes. Different from existing datasets, Pchatbot
provides anonymized user IDs and timestamps for both posts and responses. This
enables the development of personalized dialogue models that directly learn
implicit user personality from the user's dialogue history. Our preliminary
experimental study benchmarks several state-of-the-art dialogue models to
provide a comparison for future work. The dataset can be publicly accessed at
Github.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.13845</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.13845</id><submitter>Xi Victoria Lin</submitter><version version="v1"><date>Tue, 29 Sep 2020 08:17:58 GMT</date><size>12910kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 01:30:29 GMT</date><size>13036kb</size><source_type>D</source_type></version><title>GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing</title><authors>Tao Yu and Chien-Sheng Wu and Xi Victoria Lin and Bailin Wang and Yi
  Chern Tan and Xinyi Yang and Dragomir Radev and Richard Socher and Caiming
  Xiong</authors><categories>cs.CL cs.AI</categories><comments>16 pages; Accepted to ICLR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present GraPPa, an effective pre-training approach for table semantic
parsing that learns a compositional inductive bias in the joint representations
of textual and tabular data. We construct synthetic question-SQL pairs over
high-quality tables via a synchronous context-free grammar (SCFG) induced from
existing text-to-SQL datasets. We pre-train our model on the synthetic data
using a novel text-schema linking objective that predicts the syntactic role of
a table field in the SQL for each question-SQL pair. To maintain the model's
ability to represent real-world data, we also include masked language modeling
(MLM) over several existing table-and-language datasets to regularize the
pre-training process. On four popular fully supervised and weakly supervised
table semantic parsing benchmarks, GraPPa significantly outperforms
RoBERTa-large as the feature representation layers and establishes new
state-of-the-art results on all of them.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.14180</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.14180</id><submitter>Max Smith</submitter><version version="v1"><date>Tue, 29 Sep 2020 17:48:10 GMT</date><size>2258kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 14:36:51 GMT</date><size>3326kb</size><source_type>D</source_type></version><title>Learning to Play against Any Mixture of Opponents</title><authors>Max Olan Smith, Thomas Anthony, Yongzhao Wang, Michael P. Wellman</authors><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitively, experience playing against one mixture of opponents in a given
domain should be relevant for a different mixture in the same domain. We
propose a transfer learning method, Q-Mixing, that starts by learning Q-values
against each pure-strategy opponent. Then a Q-value for any distribution of
opponent strategies is approximated by appropriately averaging the separately
learned Q-values. From these components, we construct policies against all
opponent mixtures without any further training. We empirically validate
Q-Mixing in two environments: a simple grid-world soccer environment, and a
complicated cyber-security game. We find that Q-Mixing is able to successfully
transfer knowledge across any mixture of opponents. We next consider the use of
observations during play to update the believed distribution of opponents. We
introduce an opponent classifier -- trained in parallel to Q-learning, using
the same data -- and use the classifier results to refine the mixing of
Q-values. We find that Q-Mixing augmented with the opponent classifier function
performs comparably, and with lower variance, than training directly against a
mixed-strategy opponent.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.00174</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.00174</id><submitter>Hongyuan Diao</submitter><version version="v1"><date>Thu, 1 Oct 2020 02:10:36 GMT</date><size>19050kb</size></version><version version="v2"><date>Sun, 30 May 2021 01:53:31 GMT</date><size>19122kb</size></version><title>Information Propagation Model in Hybrid Networks</title><authors>Fuzhong Nian and Hongyuan Diao</authors><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is in practice impossible to describe the topology of a real network or
its message propagation process using a single dynamic model. To address this
issue, we constructed a new hybrid network model based on scale-free (SF),
small-world (SW) features that functions as closely as possible to a real
network. And the hybrid propagation model is constructed with
susceptible-infected-susceptible (SIS), susceptible-infected-recovered (SIR)
and susceptible-infected-recovered-susceptible (SIRS) model mixed in arbitrary
proportions. The model applies the concepts of blockbuster effect and implicit
node edges to reflect explosive spread as a significant characteristic of
information propagation. A theoretical analysis and derivation of the new model
in which hybrid networks were simulated revealed that the network degree
distribution closely follows a power law. Using an improved similarity function
to define the degree of closeness to real network cases, the proposed model was
shown to be valid and very close to a real network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.00406</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.00406</id><submitter>Aaron Defazio</submitter><version version="v1"><date>Thu, 1 Oct 2020 13:46:32 GMT</date><size>225kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 3 Nov 2020 22:05:04 GMT</date><size>225kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 23 Feb 2021 19:49:57 GMT</date><size>218kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 17:53:38 GMT</date><size>287kb</size><source_type>D</source_type></version><title>Momentum via Primal Averaging: Theoretical Insights and Learning Rate
  Schedules for Non-Convex Optimization</title><authors>Aaron Defazio</authors><categories>cs.LG math.OC stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Momentum methods are now used pervasively within the machine learning
community for training non-convex models such as deep neural networks.
Empirically, they out perform traditional stochastic gradient descent (SGD)
approaches. In this work we develop a Lyapunov analysis of SGD with momentum
(SGD+M), by utilizing a equivalent rewriting of the method known as the
stochastic primal averaging (SPA) form. This analysis is much tighter than
previous theory in the non-convex case, and due to this we are able to give
precise insights into when SGD+M may out-perform SGD, and what hyper-parameter
schedules will work and why.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.00538</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.00538</id><submitter>Yingkai Ouyang</submitter><version version="v1"><date>Thu, 1 Oct 2020 16:39:21 GMT</date><size>500kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 13:05:28 GMT</date><size>423kb</size><source_type>D</source_type></version><title>Avoiding coherent errors with rotated concatenated stabilizer codes</title><authors>Yingkai Ouyang</authors><categories>quant-ph cs.IT math.IT</categories><comments>8 pages, 5 figures, two columns</comments><doi>10.1038/s41534-021-00429-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coherent errors, which arise from collective couplings, are a dominant form
of noise in many realistic quantum systems, and are more damaging than oft
considered stochastic errors. Here, we propose integrating stabilizer codes
with constant-excitation codes by code concatenation. Namely, by concatenating
an $[[n,k,d]]$ stabilizer outer code with dual-rail inner codes, we obtain a
$[[2n,k,d]]$ constant-excitation code immune from coherent phase errors and
also equivalent to a Pauli-rotated stabilizer code. When the stabilizer outer
code is fault-tolerant, the constant-excitation code has a positive
fault-tolerant threshold against stochastic errors. Setting the outer code as a
four-qubit amplitude damping code yields an eight-qubit constant-excitation
code that corrects a single amplitude damping error, and we analyze this code's
potential as a quantum memory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.00656</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.00656</id><submitter>Rui Meng</submitter><version version="v1"><date>Thu, 1 Oct 2020 19:33:27 GMT</date><size>1260kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 22:34:58 GMT</date><size>508kb</size><source_type>D</source_type></version><title>Predicting User Engagement Status for Online Evaluation of Intelligent
  Assistants</title><authors>Rui Meng, Zhen Yue, Alyssa Glass</authors><categories>cs.CL cs.HC</categories><comments>Paper has been accepted by ECIR 2021 (43rd edition of the annual
  European Conference on Information Retrieval)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation of intelligent assistants in large-scale and online settings
remains an open challenge. User behavior-based online evaluation metrics have
demonstrated great effectiveness for monitoring large-scale web search and
recommender systems. Therefore, we consider predicting user engagement status
as the very first and critical step to online evaluation for intelligent
assistants. In this work, we first proposed a novel framework for classifying
user engagement status into four categories -- fulfillment, continuation,
reformulation and abandonment. We then demonstrated how to design simple but
indicative metrics based on the framework to quantify user engagement levels.
We also aim for automating user engagement prediction with machine learning
methods. We compare various models and features for predicting engagement
status using four real-world datasets. We conducted detailed analyses on
features and failure cases to discuss the performance of current models as well
as challenges.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01175</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01175</id><submitter>Lun Wang</submitter><version version="v1"><date>Fri, 2 Oct 2020 19:37:02 GMT</date><size>105kb</size></version><version version="v2"><date>Sun, 30 May 2021 20:24:12 GMT</date><size>858kb</size><source_type>D</source_type></version><title>Towards Bidirectional Protection in Federated Learning</title><authors>Lun Wang, Qi Pang, Shuai Wang and Dawn Song</authors><categories>cs.DC cs.AI cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Prior efforts in enhancing federated learning (FL) security fall into two
categories. At one end of the spectrum, some work uses secure aggregation
techniques to hide the individual client's updates and only reveal the
aggregated global update to a malicious server that strives to infer the
clients' privacy from their updates. At the other end of the spectrum, some
work uses Byzantine-robust FL protocols to suppress the influence of malicious
clients' updates. We present a federated learning protocol F2ED-LEARNING,
which, for the first time, offers bidirectional defense to simultaneously
combat against the malicious centralized server and Byzantine malicious
clients. To defend against Byzantine malicious clients, F2ED-LEARNING provides
dimension-free estimation error by employing and calibrating a well-studied
robust mean estimator FilterL2. F2ED-LEARNING also leverages secure aggregation
to protect clients from a malicious server. One key challenge of F2ED-LEARNING
is to address the incompatibility between FilterL2 and secure aggregation
schemes. Concretely, FilterL2 has to check the individual updates from clients
whereas secure aggregation hides those updates from the malicious server. To
this end, we propose a practical and highly effective solution to split the
clients into shards, where F2ED-LEARNING securely aggregates each shard's
update and launches FilterL2 on updates from different shards. The evaluation
shows that F2ED-LEARNING consistently achieves optimal or close-to-optimal
performance and outperforms five secure FL protocols under five popular
attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01282</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01282</id><submitter>Chao Tan</submitter><version version="v1"><date>Sat, 3 Oct 2020 05:34:00 GMT</date><size>470kb</size></version><version version="v2"><date>Sun, 30 May 2021 07:19:32 GMT</date><size>470kb</size></version><title>TCLNet: Learning to Locate Typhoon Center Using Deep Neural Network</title><authors>Chao Tan</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of typhoon center location plays an important role in typhoon
intensity analysis and typhoon path prediction. Conventional typhoon center
location algorithms mostly rely on digital image processing and mathematical
morphology operation, which achieve limited performance. In this paper, we
proposed an efficient fully convolutional end-to-end deep neural network named
TCLNet to automatically locate the typhoon center position. We design the
network structure carefully so that our TCLNet can achieve remarkable
performance base on its lightweight architecture. In addition, we also present
a brand new large-scale typhoon center location dataset (TCLD) so that the
TCLNet can be trained in a supervised manner. Furthermore, we propose to use a
novel TCL+ piecewise loss function to further improve the performance of
TCLNet. Extensive experimental results and comparison demonstrate the
performance of our model, and our TCLNet achieve a 14.4% increase in accuracy
on the basis of a 92.7% reduction in parameters compared with SOTA deep
learning based typhoon center location methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01283</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01283</id><submitter>Chao Tan</submitter><version version="v1"><date>Sat, 3 Oct 2020 05:40:36 GMT</date><size>780kb</size></version><version version="v2"><date>Sun, 30 May 2021 07:21:57 GMT</date><size>780kb</size></version><title>Generating the Cloud Motion Winds Field from Satellite Cloud Imagery
  Using Deep Learning Approach</title><authors>Chao Tan</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud motion winds (CMW) are routinely derived by tracking features in
sequential geostationary satellite infrared cloud imagery. In this paper, we
explore the cloud motion winds algorithm based on data-driven deep learning
approach, and different from conventional hand-craft feature tracking and
correlation matching algorithms, we use deep learning model to automatically
learn the motion feature representations and directly output the field of cloud
motion winds. In addition, we propose a novel large-scale cloud motion winds
dataset (CMWD) for training deep learning models. We also try to use a single
cloud imagery to predict the cloud motion winds field in a fixed region, which
is impossible to achieve using traditional algorithms. The experimental results
demonstrate that our algorithm can predict the cloud motion winds field
efficiently, and even with a single cloud imagery as input.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01291</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01291</id><submitter>Chao Tan</submitter><version version="v1"><date>Sat, 3 Oct 2020 06:55:26 GMT</date><size>590kb</size></version><version version="v2"><date>Sun, 30 May 2021 08:07:03 GMT</date><size>4000kb</size><source_type>D</source_type></version><title>Unsupervised Shadow Removal Using Target Consistency Generative
  Adversarial Network</title><authors>Chao Tan, Xin Feng</authors><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised shadow removal aims to learn a non-linear function to map the
original image from shadow domain to non-shadow domain in the absence of paired
shadow and non-shadow data. In this paper, we develop a simple yet efficient
target-consistency generative adversarial network (TC-GAN) for the shadow
removal task in the unsupervised manner. Compared with the bidirectional
mapping in cycle-consistency GAN based methods for shadow removal, TC-GAN tries
to learn a one-sided mapping to cast shadow images into shadow-free ones. With
the proposed target-consistency constraint, the correlations between shadow
images and the output shadow-free image are strictly confined. Extensive
comparison experiments results show that TC-GAN outperforms the
state-of-the-art unsupervised shadow removal methods by 14.9% in terms of FID
and 31.5% in terms of KID. It is rather remarkable that TC-GAN achieves
comparable performance with supervised shadow removal methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01590</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01590</id><submitter>Laurence Aitchison</submitter><version version="v1"><date>Sun, 4 Oct 2020 14:31:18 GMT</date><size>495kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 12:23:26 GMT</date><size>473kb</size><source_type>D</source_type></version><title>Deep kernel processes</title><authors>Laurence Aitchison, Adam X. Yang, Sebastian W. Ober</authors><categories>stat.ML cs.LG</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define deep kernel processes in which positive definite Gram matrices are
progressively transformed by nonlinear kernel functions and by sampling from
(inverse) Wishart distributions. Remarkably, we find that deep Gaussian
processes (DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite
BNNs with bottlenecks can all be written as deep kernel processes. For DGPs the
equivalence arises because the Gram matrix formed by the inner product of
features is Wishart distributed, and as we show, standard isotropic kernels can
be written entirely in terms of this Gram matrix -- we do not need knowledge of
the underlying features. We define a tractable deep kernel process, the deep
inverse Wishart process, and give a doubly-stochastic inducing-point
variational inference scheme that operates on the Gram matrices, not on the
features, as in DGPs. We show that the deep inverse Wishart process gives
superior performance to DGPs and infinite BNNs on standard fully-connected
baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01684</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01684</id><submitter>Ayed Alrashdi</submitter><version version="v1"><date>Sun, 4 Oct 2020 21:10:49 GMT</date><size>169kb</size></version><version version="v2"><date>Sat, 10 Oct 2020 18:49:26 GMT</date><size>171kb</size></version><version version="v3"><date>Sat, 17 Oct 2020 21:25:00 GMT</date><size>171kb</size></version><version version="v4"><date>Fri, 25 Dec 2020 20:41:35 GMT</date><size>171kb</size></version><version version="v5"><date>Thu, 27 May 2021 20:50:00 GMT</date><size>174kb</size></version><version version="v6"><date>Wed, 2 Jun 2021 08:41:02 GMT</date><size>172kb</size></version><title>Large System Analysis of Box-Relaxation in Correlated Massive MIMO
  Channels Under Imperfect CSI</title><authors>Ayed M. Alrashdi</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the mean square error (MSE) and the bit error rate
(BER) performance of the box-relaxation decoder in massive
multiple-input-multiple-output (MIMO) systems under the assumptions of
imperfect channel state information (CSI) and receive-side channel correlation.
Our analysis assumes that the number of transmit and receive antennas ($n$,and
$m$) grow simultaneously large while their ratio remains fixed. For simplicity
of the analysis, we consider binary phase shift keying (BPSK) modulated
signals. The asymptotic approximations of the MSE and BER enable us to derive
the optimal power allocation scheme under MSE/BER minimization. Numerical
simulations suggest that the asymptotic approximations are accurate even for
small $n$ and $m$. They also show the important role of the box constraint in
mitigating the so called double descent phenomenon.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01736</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01736</id><submitter>Jingfeng Zhang</submitter><version version="v1"><date>Mon, 5 Oct 2020 01:33:11 GMT</date><size>25206kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 02:49:55 GMT</date><size>34170kb</size><source_type>D</source_type></version><title>Geometry-aware Instance-reweighted Adversarial Training</title><authors>Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and
  Mohan Kankanhalli</authors><categories>cs.LG cs.AI</categories><comments>ICLR 2021, Oral, Code
  &lt;https://github.com/zjfheart/Geometry-aware-Instance-reweighted-Adversarial-Training&gt;</comments><journal-ref>International Conference on Learning Representations (ICLR 2021)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In adversarial machine learning, there was a common belief that robustness
and accuracy hurt each other. The belief was challenged by recent studies where
we can maintain the robustness and improve the accuracy. However, the other
direction, whether we can keep the accuracy while improving the robustness, is
conceptually and practically more interesting, since robust accuracy should be
lower than standard accuracy for any model. In this paper, we show this
direction is also promising. Firstly, we find even over-parameterized deep
networks may still have insufficient model capacity, because adversarial
training has an overwhelming smoothing effect. Secondly, given limited model
capacity, we argue adversarial data should have unequal importance:
geometrically speaking, a natural data point closer to/farther from the class
boundary is less/more robust, and the corresponding adversarial data point
should be assigned with larger/smaller weight. Finally, to implement the idea,
we propose geometry-aware instance-reweighted adversarial training, where the
weights are based on how difficult it is to attack a natural data point.
Experiments show that our proposal boosts the robustness of standard
adversarial training; combining two directions, we improve both robustness and
accuracy of standard adversarial training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01845</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01845</id><submitter>Francisco Ruiz</submitter><version version="v1"><date>Mon, 5 Oct 2020 08:11:55 GMT</date><size>264kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 15:37:29 GMT</date><size>274kb</size><source_type>D</source_type></version><title>Unbiased Gradient Estimation for Variational Auto-Encoders using Coupled
  Markov Chains</title><authors>Francisco J. R. Ruiz, Michalis K. Titsias, Taylan Cemgil, Arnaud
  Doucet</authors><categories>cs.LG stat.ML</categories><journal-ref>Conference on Uncertainty in Artificial Intelligence (UAI, 2021)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variational auto-encoder (VAE) is a deep latent variable model that has
two neural networks in an autoencoder-like architecture; one of them
parameterizes the model's likelihood. Fitting its parameters via maximum
likelihood (ML) is challenging since the computation of the marginal likelihood
involves an intractable integral over the latent space; thus the VAE is trained
instead by maximizing a variational lower bound. Here, we develop a ML training
scheme for VAEs by introducing unbiased estimators of the log-likelihood
gradient. We obtain the estimators by augmenting the latent space with a set of
importance samples, similarly to the importance weighted auto-encoder (IWAE),
and then constructing a Markov chain Monte Carlo coupling procedure on this
augmented space. We provide the conditions under which the estimators can be
computed in finite time and with finite variance. We show experimentally that
VAEs fitted with unbiased estimators exhibit better predictive performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.02172</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.02172</id><submitter>Tiago Pimentel</submitter><version version="v1"><date>Mon, 5 Oct 2020 17:19:10 GMT</date><size>1087kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 16 Nov 2020 23:23:38 GMT</date><size>1088kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 15:13:18 GMT</date><size>1088kb</size><source_type>D</source_type></version><title>Speakers Fill Lexical Semantic Gaps with Context</title><authors>Tiago Pimentel, Rowan Hall Maudslay, Dami\'an Blasi, Ryan Cotterell</authors><categories>cs.CL</categories><comments>Camera ready version of EMNLP 2020 publication. Code is available in
  https://github.com/tpimentelms/lexical-ambiguity-in-context</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexical ambiguity is widespread in language, allowing for the reuse of
economical word forms and therefore making language more efficient. If
ambiguous words cannot be disambiguated from context, however, this gain in
efficiency might make language less clear -- resulting in frequent
miscommunication. For a language to be clear and efficiently encoded, we posit
that the lexical ambiguity of a word type should correlate with how much
information context provides about it, on average. To investigate whether this
is the case, we operationalise the lexical ambiguity of a word as the entropy
of meanings it can take, and provide two ways to estimate this -- one which
requires human annotation (using WordNet), and one which does not (using BERT),
making it readily applicable to a large number of languages. We validate these
measures by showing that, on six high-resource languages, there are significant
Pearson correlations between our BERT-based estimate of ambiguity and the
number of synonyms a word has in WordNet (e.g. $\rho = 0.40$ in English). We
then test our main hypothesis -- that a word's lexical ambiguity should
negatively correlate with its contextual uncertainty -- and find significant
correlations on all 18 typologically diverse languages we analyse. This
suggests that, in the presence of ambiguity, speakers compensate by making
contexts more informative.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.02188</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.02188</id><submitter>James Houghton</submitter><version version="v1"><date>Mon, 5 Oct 2020 17:45:22 GMT</date><size>2474kb</size></version><version version="v2"><date>Thu, 21 Jan 2021 16:54:48 GMT</date><size>1699kb</size></version><version version="v3"><date>Tue, 1 Jun 2021 14:35:12 GMT</date><size>5197kb</size><source_type>D</source_type></version><title>Interdependent Diffusion: The social contagion of interacting beliefs</title><authors>James P. Houghton</authors><categories>cs.SI physics.soc-ph</categories><comments>Body and Supplement Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social contagion is the process in which people adopt a belief, idea, or
practice from a neighbor and pass it along to someone else. For over 100 years,
scholars of social contagion have almost exclusively made the same implicit
assumption: that only one belief, idea, or practice spreads through the
population at a time. It is a default assumption that we don't bother to state,
let alone justify. The assumption is so ingrained that our literature doesn't
even have a word for &quot;whatever is to be diffused&quot;, because we have never needed
to discuss more than one of them. But this assumption is obviously false.
Millions of beliefs, ideas, and practices (let's call them &quot;diffusants&quot;) spread
through social contagion every day. To assume that diffusants spread one at a
time - or more generously, that they spread independently of one another - is
to assume that interactions between diffusants have no influence on adoption
patterns. This could be true, or it could be wildly off the mark. We've never
stopped to find out. This paper makes a direct comparison between the spread of
independent and interdependent beliefs using simulations, observational data,
and a 2400-subject laboratory experiment. I find that in assuming independence
between diffusants, scholars have overlooked social processes that
fundamentally change the outcomes of social contagion. Interdependence between
beliefs generates polarization, irrespective of social network structure,
homophily, demographics, politics, or any other commonly cited cause. It also
coordinates structures of beliefs that can have both internal justification and
social support without any grounding in external truth.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.02709</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.02709</id><submitter>Agustinus Kristiadi</submitter><version version="v1"><date>Tue, 6 Oct 2020 13:32:18 GMT</date><size>3557kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 16:22:21 GMT</date><size>3567kb</size><source_type>D</source_type></version><title>An Infinite-Feature Extension for Bayesian ReLU Nets That Fixes Their
  Asymptotic Overconfidence</title><authors>Agustinus Kristiadi, Matthias Hein, Philipp Hennig</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian treatment can mitigate overconfidence in ReLU nets around the
training data. But far away from them, ReLU Bayesian neural networks (BNNs) can
still underestimate uncertainty and thus be asymptotically overconfident. This
issue arises since the output variance of a BNN with finitely many features is
quadratic in the distance from the data region. Meanwhile, Bayesian linear
models with ReLU features converge, in the infinite-width limit, to a
particular Gaussian process (GP) with a variance that grows cubically so that
no asymptotic overconfidence can occur. While this may seem of mostly
theoretical interest, in this work, we show that it can be used concretely to
the benefit of BNNs. We extend finite ReLU BNNs with infinite ReLU features via
the GP and show that the resulting model is asymptotically maximally uncertain
far away from the data while the BNNs' predictive power is unaffected near the
data. Although the resulting model approximates a full GP posterior, thanks to
its structure, it can be applied post-hoc to any pre-trained ReLU BNN at a low
cost.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.02716</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.02716</id><submitter>Lu\'is Cruz</submitter><version version="v1"><date>Sat, 3 Oct 2020 19:25:01 GMT</date><size>5052kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:55:40 GMT</date><size>4960kb</size><source_type>D</source_type></version><title>AI Lifecycle Models Need To Be Revised. An Exploratory Study in Fintech</title><authors>Mark Haakman, Lu\'is Cruz, Hennie Huijgens, Arie van Deursen</authors><categories>cs.SE</categories><comments>Accepted in Empirical Software Engineering in April, 2021</comments><msc-class>68T01</msc-class><acm-class>I.2.0; D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tech-leading organizations are embracing the forthcoming artificial
intelligence revolution. Intelligent systems are replacing and cooperating with
traditional software components. Thus, the same development processes and
standards in software engineering ought to be complied in artificial
intelligence systems. This study aims to understand the processes by which
artificial intelligence-based systems are developed and how state-of-the-art
lifecycle models fit the current needs of the industry. We conducted an
exploratory case study at ING, a global bank with a strong European base. We
interviewed 17 people with different roles and from different departments
within the organization. We have found that the following stages have been
overlooked by previous lifecycle models: data collection, feasibility study,
documentation, model monitoring, and model risk assessment. Our work shows that
the real challenges of applying Machine Learning go much beyond sophisticated
learning algorithms - more focus is needed on the entire lifecycle. In
particular, regardless of the existing development tools for Machine Learning,
we observe that they are still not meeting the particularities of this field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.02823</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.02823</id><submitter>Geoff Hamilton</submitter><version version="v1"><date>Tue, 6 Oct 2020 15:38:00 GMT</date><size>65kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 14:51:26 GMT</date><size>140kb</size></version><title>Tight Polynomial Bounds for Loop Programs in Polynomial Space</title><authors>A. M. Ben-Amram and G. W. Hamilton</authors><categories>cs.LO</categories><comments>50 pages</comments><msc-class>F.2.0</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem: given a program, find tight asymptotic
bounds on the values of some variables at the end of the computation (or at any
given program point) in terms of its input values. We focus on the case of
polynomially-bounded variables, and on a weak programming language for which we
have recently shown that tight bounds for polynomially-bounded variables are
computable. These bounds are sets of multivariate polynomials. While their
computability has been settled, the complexity of this program-analysis problem
remained open. In this paper, we show the problem to be PSPACE-complete. The
main contribution is a new, space-efficient analysis algorithm. This algorithm
is obtained in a few steps. First, we develop an algorithm for univariate
bounds, a sub-problem which is already PSPACE-hard. Then, a decision procedure
for multivariate bounds is achieved by reducing this problem to the univariate
case; this reduction is orthogonal to the solution of the univariate problem
and uses observations on the geometry of a set of vectors that represent
multivariate bounds. Finally, we transform the univariate-bound algorithm to
produce multivariate bounds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.03438</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.03438</id><submitter>Sajjad Ghobadi</submitter><version version="v1"><date>Wed, 7 Oct 2020 14:28:40 GMT</date><size>450kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 2 Mar 2021 15:25:47 GMT</date><size>319kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 12:48:44 GMT</date><size>319kb</size><source_type>D</source_type></version><title>Fairness in Influence Maximization through Randomization</title><authors>Ruben Becker, Gianlorenzo D'Angelo, Sajjad Ghobadi, Hugo Gilbert</authors><categories>cs.SI cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The influence maximization paradigm has been used by researchers in various
fields in order to study how information spreads in social networks. While
previously the attention was mostly on efficiency, more recently fairness
issues have been taken into account in this scope. In this paper, we propose to
use randomization as a mean for achieving fairness. Similar to previous works
like Fish et al. (WWW '19) and Tsang et al. (IJCAI '19), we study the maximin
criterion for (group) fairness. In contrast to their work however, we model the
problem in such a way that, when choosing the seed sets, probabilistic
strategies are possible rather than only deterministic ones. We introduce two
different variants of this probabilistic problem, one that entails
probabilistic strategies over nodes (node-based problem) and a second one that
entails probabilistic strategies over sets of nodes (set-based problem). While
the original deterministic problem involving the maximin criterion has been
shown to be inapproximable, interestingly, we show that both probabilistic
variants permit approximation algorithms that achieve a constant multiplicative
factor of 1-1/e plus an additive arbitrarily small error that is due to the
simulation of the information spread. For an experimental study, we provide
implementations of multiplicative-weight routines for both problems and compare
the achieved fairness values to existing methods. Maybe non-surprisingly, we
show that the ex-ante values of the computed probabilistic strategies are
significantly larger than the (ex-post) fairness values of previous methods.
This indicates that studying fairness via randomization is a worthwhile path to
follow. Interestingly and maybe more surprisingly, we observe that even the
ex-post fairness values computed by our routines, dominate over the fairness
achieved by previous methods on most of the instances tested.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.03536</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.03536</id><submitter>Salvatore Di Girolamo</submitter><version version="v1"><date>Wed, 7 Oct 2020 17:32:57 GMT</date><size>1181kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 9 Oct 2020 10:28:40 GMT</date><size>1181kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 09:31:51 GMT</date><size>1044kb</size><source_type>D</source_type></version><title>PsPIN: A high-performance low-power architecture for flexible in-network
  compute</title><authors>Salvatore Di Girolamo, Andreas Kurth, Alexandru Calotoiu, Thomas Benz,
  Timo Schneider, Jakub Ber\'anek, Luca Benini, Torsten Hoefler</authors><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of offloading data and control tasks to the network is becoming
increasingly important, especially if we consider the faster growth of network
speed when compared to CPU frequencies. In-network compute alleviates the host
CPU load by running tasks directly in the network, enabling additional
computation/communication overlap and potentially improving overall application
performance. However, sustaining bandwidths provided by next-generation
networks, e.g., 400 Gbit/s, can become a challenge. sPIN is a programming model
for in-NIC compute, where users specify handler functions that are executed on
the NIC, for each incoming packet belonging to a given message or flow. It
enables a CUDA-like acceleration, where the NIC is equipped with lightweight
processing elements that process network packets in parallel. We investigate
the architectural specialties that a sPIN NIC should provide to enable
high-performance, low-power, and flexible packet processing. We introduce
PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V
architecture and designed according to the identified architectural
specialties. We investigate the performance of PsPIN with cycle-accurate
simulations, showing that it can process packets at 400 Gbit/s for several use
cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a
total area of 18.5 mm 2 (22 nm FDSOI).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.04434</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.04434</id><submitter>Tielin Zhang</submitter><version version="v1"><date>Fri, 9 Oct 2020 08:42:13 GMT</date><size>1703kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 12 Nov 2020 06:06:27 GMT</date><size>1703kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 13:50:56 GMT</date><size>12879kb</size><source_type>D</source_type></version><title>Tuning Convolutional Spiking Neural Network with Biologically-plausible
  Reward Propagation</title><authors>Tielin Zhang and Shuncheng Jia and Xiang Cheng and Bo Xu</authors><categories>cs.NE cs.AI</categories><comments>Final Version. Accepted by IEEE Transactions on Neural Networks and
  Learning Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking Neural Networks (SNNs) contain more biologically realistic structures
and biologically-inspired learning principles than those in standard Artificial
Neural Networks (ANNs). SNNs are considered the third generation of ANNs,
powerful on the robust computation with a low computational cost. The neurons
in SNNs are non-differential, containing decayed historical states and
generating event-based spikes after their states reaching the firing threshold.
These dynamic characteristics of SNNs make it difficult to be directly trained
with the standard backpropagation (BP), which is also considered not
biologically plausible. In this paper, a Biologically-plausible Reward
Propagation (BRP) algorithm is proposed and applied to the SNN architecture
with both spiking-convolution (with both 1D and 2D convolutional kernels) and
full-connection layers. Unlike the standard BP that propagates error signals
from post to presynaptic neurons layer by layer, the BRP propagates target
labels instead of errors directly from the output layer to all pre-hidden
layers. This effort is more consistent with the top-down reward-guiding
learning in cortical columns of the neocortex. Synaptic modifications with only
local gradient differences are induced with pseudo-BP that might also be
replaced with the Spike-Timing Dependent Plasticity (STDP). The performance of
the proposed BRP-SNN is further verified on the spatial (including MNIST and
Cifar-10) and temporal (including TIDigits and DvsGesture) tasks, where the SNN
using BRP has reached a similar accuracy compared to other state-of-the-art
BP-based SNNs and saved 50% more computational cost than ANNs. We think the
introduction of biologically plausible learning rules to the training procedure
of biologically realistic SNNs will give us more hints and inspirations toward
a better understanding of the biological system's intelligent nature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.04525</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.04525</id><submitter>Zhizheng Zhang</submitter><version version="v1"><date>Fri, 9 Oct 2020 12:26:27 GMT</date><size>378kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 13:48:34 GMT</date><size>94kb</size><source_type>D</source_type></version><title>Uncertainty-Aware Few-Shot Image Classification</title><authors>Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Shih-Fu Chang</authors><categories>cs.CV</categories><comments>Accepted by IJCAI2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Few-shot image classification learns to recognize new categories from limited
labelled data. Metric learning based approaches have been widely investigated,
where a query sample is classified by finding the nearest prototype from the
support set based on their feature similarities. A neural network has different
uncertainties on its calculated similarities of different pairs. Understanding
and modeling the uncertainty on the similarity could promote the exploitation
of limited samples in few-shot optimization. In this work, we propose
Uncertainty-Aware Few-Shot framework for image classification by modeling
uncertainty of the similarities of query-support pairs and performing
uncertainty-aware optimization. Particularly, we exploit such uncertainty by
converting observed similarities to probabilistic representations and
incorporate them to the loss for more effective optimization. In order to
jointly consider the similarities between a query and the prototypes in a
support set, a graph-based model is utilized to estimate the uncertainty of the
pairs. Extensive experiments show our proposed method brings significant
improvements on top of a strong baseline and achieves the state-of-the-art
performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.04643</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.04643</id><submitter>Hendrik Molter</submitter><version version="v1"><date>Fri, 9 Oct 2020 15:34:39 GMT</date><size>20kb</size></version><version version="v2"><date>Mon, 31 May 2021 11:38:05 GMT</date><size>24kb</size></version><title>Equitable Scheduling on a Single Machine</title><authors>Klaus Heeger, Danny Hermelin, George B. Mertzios, Hendrik Molter, Rolf
  Niedermeier, Dvir Shabtay</authors><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a natural but seemingly yet unstudied generalization of the
problem of scheduling jobs on a single machine so as to minimize the number of
tardy jobs. Our generalization lies in simultaneously considering several
instances of the problem at once. In particular, we have $n$ clients over a
period of $m$ days, where each client has a single job with its own processing
time and deadline per day. Our goal is to provide a schedule for each of the
$m$ days, so that each client is guaranteed to have their job meet its deadline
in at least $k \le m$ days. This corresponds to an equitable schedule where
each client is guaranteed a minimal level of service throughout the period of
$m$ days. We provide a thorough analysis of the computational complexity of
three main variants of this problem, identifying both efficient algorithms and
worst-case intractability results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.04690</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.04690</id><submitter>Shaifali Parashar</submitter><version version="v1"><date>Fri, 9 Oct 2020 17:25:00 GMT</date><size>12153kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:23:42 GMT</date><size>25972kb</size><source_type>D</source_type></version><title>Robust Isometric Non-Rigid Structure-from-Motion</title><authors>Shaifali Parashar, Adrien Bartoli and Daniel Pizarro</authors><categories>cs.CV</categories><comments>Accepted in TPAMI 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object
from the correspondences established between monocular 2D images. Current NRSfM
methods lack statistical robustness, which is the ability to cope with
correspondence errors.This prevents one to use automatically established
correspondences, which are prone to errors, thereby strongly limiting the scope
of NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by
exploiting isometry. Step 1 computes the optical flow from correspondences,
step 2 reconstructs each 3D point's normal vector using multiple reference
images and integrates them to form surfaces with the best reference and step 3
rejects the 3D points that break isometry in their local neighborhood.
Importantly, each step is designed to discard or flag erroneous
correspondences. Our contributions include the robustification of optical flow
by warp estimation, new fast analytic solutions to local normal reconstruction
and their robustification, and a new scale-independent measure of 3D local
isometric coherence. Experimental results show that our robust NRSfM method
consistently outperforms existing methods on both synthetic and real datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.05003</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.05003</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Sat, 10 Oct 2020 13:49:46 GMT</date><size>38kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 03:06:52 GMT</date><size>38kb</size><source_type>D</source_type></version><title>Second-Order Neural Dependency Parsing with Message Passing and
  End-to-End Training</title><authors>Xinyu Wang, Kewei Tu</authors><categories>cs.CL cs.LG</categories><comments>Accepted to AACL 2020. 7 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose second-order graph-based neural dependency parsing
using message passing and end-to-end neural networks. We empirically show that
our approaches match the accuracy of very recent state-of-the-art second-order
graph-based neural dependency parsers and have significantly faster speed in
both training and testing. We also empirically show the advantage of
second-order parsing over first-order parsing and observe that the usefulness
of the head-selection structured constraint vanishes when using BERT embedding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.05006</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.05006</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Sat, 10 Oct 2020 14:03:20 GMT</date><size>248kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Dec 2020 06:30:35 GMT</date><size>250kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 11:15:40 GMT</date><size>156kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 13:23:25 GMT</date><size>157kb</size><source_type>D</source_type></version><title>Automated Concatenation of Embeddings for Structured Prediction</title><authors>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
  Huang, Kewei Tu</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to Proceedings of ACL-IJCNLP 2021. 17 pages</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Pretrained contextualized embeddings are powerful word representations for
structured prediction tasks. Recent work found that better word representations
can be obtained by concatenating different types of embeddings. However, the
selection of embeddings to form the best concatenated representation usually
varies depending on the task and the collection of candidate embeddings, and
the ever-increasing number of embedding types makes it a more difficult
problem. In this paper, we propose Automated Concatenation of Embeddings (ACE)
to automate the process of finding better concatenations of embeddings for
structured prediction tasks, based on a formulation inspired by recent progress
on neural architecture search. Specifically, a controller alternately samples a
concatenation of embeddings, according to its current belief of the
effectiveness of individual embedding types in consideration for a task, and
updates the belief based on a reward. We follow strategies in reinforcement
learning to optimize the parameters of the controller and compute the reward
based on the accuracy of a task model, which is fed with the sampled
concatenation as input and trained on a task dataset. Empirical results on 6
tasks and 21 datasets show that our approach outperforms strong baselines and
achieves state-of-the-art performance with fine-tuned embeddings in all the
evaluations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.05010</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.05010</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Sat, 10 Oct 2020 14:19:25 GMT</date><size>244kb</size></version><version version="v2"><date>Tue, 18 May 2021 12:07:25 GMT</date><size>60kb</size></version><version version="v3"><date>Tue, 1 Jun 2021 13:31:22 GMT</date><size>62kb</size></version><version version="v4"><date>Wed, 2 Jun 2021 02:31:19 GMT</date><size>62kb</size><source_type>D</source_type></version><title>Structural Knowledge Distillation: Tractably Distilling Information for
  Structured Predictor</title><authors>Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia, Nguyen Bach, Tao Wang,
  Zhongqiang Huang, Fei Huang, Kewei Tu</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to Proceedings of ACL-IJCNLP 2021. 15 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Knowledge distillation is a critical technique to transfer knowledge between
models, typically from a large model (the teacher) to a more fine-grained one
(the student). The objective function of knowledge distillation is typically
the cross-entropy between the teacher and the student's output distributions.
However, for structured prediction problems, the output space is exponential in
size; therefore, the cross-entropy objective becomes intractable to compute and
optimize directly. In this paper, we derive a factorized form of the knowledge
distillation objective for structured prediction, which is tractable for many
typical choices of the teacher and student models. In particular, we show the
tractability and empirical effectiveness of structural knowledge distillation
between sequence labeling and dependency parsing models under four different
scenarios: 1) the teacher and student share the same factorization form of the
output structure scoring function; 2) the student factorization produces more
fine-grained substructures than the teacher factorization; 3) the teacher
factorization produces more fine-grained substructures than the student
factorization; 4) the factorization forms from the teacher and the student are
incompatible.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.05991</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.05991</id><submitter>Kalyana Babu Nakshatrala</submitter><version version="v1"><date>Mon, 12 Oct 2020 19:37:52 GMT</date><size>2900kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 21:20:54 GMT</date><size>2778kb</size><source_type>D</source_type></version><title>On optimal designs using topology optimization for flow through porous
  media applications</title><authors>T. Phatak, and K. B. Nakshatrala</authors><categories>math.NA cs.NA</categories><doi>10.1007/s11242-021-01616-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topology optimization (TopOpt) is a mathematical-driven design procedure to
realize optimal material architectures. This procedure is often used to
automate the design of devices involving flow through porous media, such as
micro-fluidic devices. TopOpt offers material layouts that control the flow of
fluids through porous materials, providing desired functionalities. Many prior
studies in this application area have used Darcy equations for primal analysis
and the minimum power theorem (MPT) to drive the optimization problem. But both
these choices (Darcy equations and MPT) are restrictive and not valid for
general working conditions of modern devices. Being simple and linear, Darcy
equations are often used to model flow of fluids through porous media. However,
two inherent assumptions of the Darcy model are: the viscosity of a fluid is a
constant, and inertial effects are negligible. There is irrefutable
experimental evidence that viscosity of a fluid, especially organic liquids,
depends on the pressure. Given the typical small pore-sizes, inertial effects
are dominant in micro-fluidic devices. Next, MPT is not a general principle and
is not valid for (nonlinear) models that relax the assumptions of the Darcy
model. This paper aims to overcome the mentioned deficiencies by presenting a
general strategy for using TopOpt. First, we will consider nonlinear models
that take into account the pressure-dependent viscosity and inertial effects,
and study the effect of these nonlinearities on the optimal material layouts
under TopOpt. Second, we will explore the rate of mechanical dissipation, valid
even for nonlinear models, as an alternative for the objective function. Third,
we will present analytical solutions of optimal designs for canonical problems;
these solutions not only possess research and pedagogical values but also
facilitate verification of computer implementations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.06002</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.06002</id><submitter>Andrea Loreggia</submitter><version version="v1"><date>Mon, 12 Oct 2020 20:10:05 GMT</date><size>33kb</size></version><version version="v2"><date>Tue, 15 Dec 2020 21:12:08 GMT</date><size>204kb</size></version><title>Thinking Fast and Slow in AI</title><authors>Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jon Lenchner,
  Nick Linck, Andrea Loreggia, Keerthiram Murugesan, Nicholas Mattei, Francesca
  Rossi, Biplav Srivastava</authors><categories>cs.AI</categories><journal-ref>Proceedings of the AAAI Conference on Artificial Intelligence
  2021, 35(17), 15042-15046</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a research direction to advance AI which draws
inspiration from cognitive theories of human decision making. The premise is
that if we gain insights about the causes of some human capabilities that are
still lacking in AI (for instance, adaptability, generalizability, common
sense, and causal reasoning), we may obtain similar capabilities in an AI
system by embedding these causal components. We hope that the high-level
description of our vision included in this paper, as well as the several
research questions that we propose to consider, can stimulate the AI research
community to define, try and evaluate new methodologies, frameworks, and
evaluation metrics, in the spirit of achieving a better understanding of both
human and machine intelligence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.06100</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.06100</id><submitter>Sarah Ostadabbas</submitter><version version="v1"><date>Tue, 13 Oct 2020 01:10:14 GMT</date><size>10177kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Dec 2020 01:29:16 GMT</date><size>28924kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Dec 2020 01:43:49 GMT</date><size>28924kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 30 May 2021 01:45:50 GMT</date><size>15347kb</size><source_type>D</source_type></version><title>Invariant Representation Learning for Infant Pose Estimation with Small
  Data</title><authors>Xiaofei Huang, Nihang Fu, Shuangjun Liu, Sarah Ostadabbas</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Infant motion analysis is a topic with critical importance in early childhood
development studies. However, while the applications of human pose estimation
have become more and more broad, models trained on large-scale adult pose
datasets are barely successful in estimating infant poses due to the
significant differences in their body ratio and the versatility of their poses.
Moreover, the privacy and security considerations hinder the availability of
adequate infant pose data required for training of a robust model from scratch.
To address this problem, this paper presents (1) building and publicly
releasing a hybrid synthetic and real infant pose (SyRIP) dataset with small
yet diverse real infant images as well as generated synthetic infant poses and
(2) a multi-stage invariant representation learning strategy that could
transfer the knowledge from the adjacent domains of adult poses and synthetic
infant images into our fine-tuned domain-adapted infant pose (FiDIP) estimation
model. In our ablation study, with identical network structure, models trained
on SyRIP dataset show noticeable improvement over the ones trained on the only
other public infant pose datasets. Integrated with pose estimation backbone
networks with varying complexity, FiDIP performs consistently better than the
fine-tuned versions of those models. One of our best infant pose estimation
performers on the state-of-the-art DarkPose model shows mean average precision
(mAP) of 93.6.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.07326</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.07326</id><submitter>Andrea L\'opez-Incera</submitter><version version="v1"><date>Wed, 14 Oct 2020 18:00:50 GMT</date><size>914kb</size><source_type>D</source_type></version><title>Collective defense of honeybee colonies: experimental results and
  theoretical modeling</title><authors>Andrea L\'opez-Incera, Morgane Nouvian, Katja Ried, Thomas M\&quot;uller
  and Hans J. Briegel</authors><categories>q-bio.PE cs.AI cs.LG</categories><journal-ref>BMC Biol 19, 106 (2021)</journal-ref><doi>10.1186/s12915-021-01028-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social insect colonies routinely face large vertebrate predators, against
which they need to mount a collective defense. To do so, honeybees use an alarm
pheromone that recruits nearby bees into mass stinging of the perceived threat.
This alarm pheromone is carried directly on the stinger, hence its
concentration builds up during the course of the attack. Here, we investigate
how individual bees react to different alarm pheromone concentrations, and how
this evolved response-pattern leads to better coordination at the group level.
We first present an individual dose-response curve to the alarm pheromone,
obtained experimentally. Second, we apply Projective Simulation to model each
bee as an artificial learning agent that relies on the pheromone concentration
to decide whether to sting or not. If the emergent collective performance
benefits the colony, the individual reactions that led to it are enhanced via
reinforcement learning, thus emulating natural selection. Predators are modeled
in a realistic way so that the effect of factors such as their resistance,
their killing rate or their frequency of attacks can be studied. We are able to
reproduce the experimentally measured response-pattern of real bees, and to
identify the main selection pressures that shaped it. Finally, we apply the
model to a case study: by tuning the parameters to represent the environmental
conditions of European or African bees, we can predict the difference in
aggressiveness observed between these two subspecies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.07799</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.07799</id><submitter>Alina Ene</submitter><version version="v1"><date>Thu, 15 Oct 2020 14:44:26 GMT</date><size>19kb</size></version><version version="v2"><date>Mon, 31 May 2021 15:59:31 GMT</date><size>8948kb</size><source_type>D</source_type></version><title>Adaptive and Universal Algorithms for Variational Inequalities with
  Optimal Convergence</title><authors>Alina Ene, Huy L. Nguyen</authors><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop new adaptive algorithms for variational inequalities with monotone
operators, which capture many problems of interest, notably convex optimization
and convex-concave saddle point problems. Our algorithms automatically adapt to
unknown problem parameters such as the smoothness and the norm of the operator,
and the variance of the stochastic evaluation oracle. We show that our
algorithms are universal and simultaneously achieve the optimal convergence
rates in the non-smooth, smooth, and stochastic settings. The convergence
guarantees of our algorithms improve over existing adaptive methods by a
$\Omega(\sqrt{\ln T})$ factor, matching the optimal non-adaptive algorithms.
Additionally, prior works require that the optimization domain is bounded. In
this work, we remove this restriction and give algorithms for unbounded domains
that are adaptive and universal. Our general proof techniques can be used for
many variants of the algorithm using one or two operator evaluations per
iteration. The classical methods based on the ExtraGradient/MirrorProx
algorithm require two operator evaluations per iteration, which is the dominant
factor in the running time in many settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.08264</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.08264</id><submitter>Laurent B\'etermin</submitter><version version="v1"><date>Fri, 16 Oct 2020 09:36:12 GMT</date><size>547kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 11:31:18 GMT</date><size>1784kb</size><source_type>D</source_type></version><title>Theta functions and optimal lattices for a grid cells model</title><authors>Laurent B\'etermin (University of Vienna)</authors><categories>math.OC cs.IT math.IT math.PR</categories><comments>22 pages. 11 Figures. Version accepted by SIAM Journal of Applied
  Mathematics (SIAP)</comments><msc-class>49N20, 62P10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Certain types of neurons, called &quot;grid cells&quot;, have been shown to fire on a
triangular grid when an animal is navigating on a two-dimensional environment,
whereas recent studies suggest that the face-centred-cubic (FCC) lattice is the
good candidate for the same phenomenon in three dimensions. The goal of this
paper is to give new evidences of these phenomena by considering a infinite set
of independent neurons (a module) with Poisson statistics and periodic spread
out Gaussian tuning curves. This question of the existence of an optimal grid
is transformed into a maximization problem among all possible unit density
lattices for a Fisher Information which measures the accuracy of grid-cells
representations in $\mathbb{R}^d$. This Fisher Information has translated
lattice theta functions as building blocks. We first derive asymptotic and
numerical results showing the (non-)maximality of the triangular lattice with
respect to the Gaussian parameter and the size of the firing field. In a
particular case where the size of the firing fields and the lattice spacing
match with experiments, we have numerically checked that it is possible to find
a value for the Gaussian parameter above which the triangular lattice is always
optimal. In the case of a radially symmetric distribution of firing locations,
we also characterize all the lattices that are critical points for the Fisher
Information at fixed scales belonging to an open interval (we call these
lattices &quot;volume stationary&quot;). It allows us to compare the Fisher Information
of a finite number of lattices in dimension 2 and 3 and to give another
evidences of the optimality of the triangular and FCC lattices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.08762</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.08762</id><submitter>Fan Mo</submitter><version version="v1"><date>Sat, 17 Oct 2020 10:49:14 GMT</date><size>2038kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 11 Jan 2021 11:49:14 GMT</date><size>2040kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 20 Jan 2021 20:35:58 GMT</date><size>2055kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 29 May 2021 11:10:58 GMT</date><size>620kb</size><source_type>D</source_type></version><title>Layer-wise Characterization of Latent Information Leakage in Federated
  Learning</title><authors>Fan Mo, Anastasia Borovykh, Mohammad Malekzadeh, Hamed Haddadi,
  Soteris Demetriou</authors><categories>cs.CR cs.AI</categories><comments>9 pages, at ICLR workshop (Distributed and Private Machine Learning)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training deep neural networks via federated learning allows clients to share,
instead of the original data, only the model trained on their data. Prior work
has demonstrated that in practice a client's private information, unrelated to
the main learning task, can be discovered from the model's gradients, which
compromises the promised privacy protection. However, there is still no formal
approach for quantifying the leakage of private information via the shared
updated model or gradients. In this work, we analyze property inference attacks
and define two metrics based on (i) an adaptation of the empirical
$\mathcal{V}$-information, and (ii) a sensitivity analysis using Jacobian
matrices allowing us to measure changes in the gradients with respect to latent
information. We show the applicability of our proposed metrics in localizing
private latent information in a layer-wise manner and in two settings where (i)
we have or (ii) we do not have knowledge of the attackers' capabilities. We
evaluate the proposed metrics for quantifying information leakage on three
real-world datasets using three benchmark models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09055</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09055</id><submitter>Paritosh Ramanan</submitter><version version="v1"><date>Sun, 18 Oct 2020 18:19:02 GMT</date><size>13553kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 08:51:26 GMT</date><size>520kb</size><source_type>D</source_type></version><title>Large-Scale Maintenance and Unit Commitment: A Decentralized Subgradient
  Approach</title><authors>Paritosh Ramanan, Murat Yildirim, Nagi Gebraeel, Edmond Chow</authors><categories>cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unit Commitment (UC) is a fundamental problem in power system operations.
When coupled with generation maintenance, the joint optimization problem poses
significant computational challenges due to coupling constraints linking
maintenance and UC decisions. Obviously, these challenges grow with the size of
the network. With the introduction of sensors for monitoring generator health
and condition-based maintenance(CBM), these challenges have been magnified.
ADMM-based decentralized methods have shown promise in solving large-scale UC
problems, especially in vertically integrated power systems. However, in their
current form, these methods fail to deliver similar computational performance
and scalability when considering the joint UC and CBM problem.
  This paper provides a novel decentralized optimization framework for solving
large-scale, joint UC and CBM problems. Our approach relies on the novel use of
the subgradient method to temporally decouple various subproblems of the
ADMM-based formulation of the joint problem along the maintenance horizon. By
effectively utilizing multithreading, our decentralized subgradient approach
delivers superior computational performance and eliminates the need to move
sensor data thereby alleviating privacy and security concerns. Using
experiments on large scale test cases, we show that our framework can provide a
speedup of upto 50x as compared to various state of the art benchmarks without
compromising on solution quality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09115</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09115</id><submitter>Haitao Wang</submitter><version version="v1"><date>Sun, 18 Oct 2020 21:54:15 GMT</date><size>97kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 18:23:23 GMT</date><size>96kb</size><source_type>D</source_type></version><title>Shortest Paths Among Obstacles in the Plane Revisited</title><authors>Haitao Wang</authors><categories>cs.CG cs.DS</categories><comments>Published in SODA 2021. Observation 2 in the previous version (and
  also in SODA proceedings) is not correct. The issue is addressed in this
  version with slightly different analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of pairwise disjoint polygonal obstacles in the plane, finding an
obstacle-avoiding Euclidean shortest path between two points is a classical
problem in computational geometry and has been studied extensively. The
previous best algorithm was given by Hershberger and Suri [FOCS 1993, SIAM J.
Comput. 1999] and the algorithm runs in $O(n\log n)$ time and $O(n\log n)$
space, where $n$ is the total number of vertices of all obstacles. The
algorithm is time-optimal because $\Omega(n\log n)$ is a lower bound. It has
been an open problem for over two decades whether the space can be reduced to
$O(n)$. In this paper, we settle it by solving the problem in $O(n\log n)$ time
and $O(n)$ space, which is optimal in both time and space; we achieve this by
modifying the algorithm of Hershberger and Suri. Like their original algorithm,
our new algorithm can build a shortest path map for a source point $s$ in
$O(n\log n)$ time and $O(n)$ space, such that given any query point $t$, the
length of a shortest path from $s$ to $t$ can be computed in $O(\log n)$ time
and a shortest path can be produced in additional time linear in the number of
edges of the path.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09181</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09181</id><submitter>Tina Mai</submitter><version version="v1"><date>Mon, 19 Oct 2020 03:11:54 GMT</date><size>442kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 06:57:28 GMT</date><size>512kb</size><source_type>D</source_type></version><title>Multiscale simulations for multi-continuum Richards equations</title><authors>Jun Sur Richard Park, Siu Wun Cheung, Tina Mai</authors><categories>math.NA cs.NA math.AP</categories><comments>45 pages, 4 figures, 2 tables, major revision. This is the accepted
  manuscript by Journal of Computational and Applied Mathematics (2021). The
  published journal article is available at
  https://doi.org/10.1016/j.cam.2021.113648 (2021)</comments><msc-class>65N30, 65N99</msc-class><journal-ref>Journal of Computational and Applied Mathematics, 397:113648, 2021</journal-ref><doi>10.1016/j.cam.2021.113648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a multiscale method for simulating a dual-continuum
unsaturated flow problem within complex heterogeneous fractured porous media.
Mathematically, each of the dual continua is modeled by a multiscale Richards
equation (for pressure head), and these equations are coupled to one another by
transfer terms. On its own, Richards equation is already a nonlinear partial
differential equation, and it is exceedingly difficult to solve numerically due
to the extra nonlinear dependencies involving the soil water. To deal with
multiple scales, our strategy is that starting from a microscopic scale, we
upscale the coupled system of dual-continuum Richards equations via
homogenization by the two-scale asymptotic expansion, to obtain a homogenized
system, at an intermediate scale (level). Based on a hierarchical approach, the
homogenization's effective coefficients are computed through solving the
arising cell problems. To tackle the nonlinearity, after time discretization,
we use Picard iteration procedure for linearization of the homogenized Richards
equations. At each Picard iteration, some degree of multiscale still remains
from the intermediate level, so we utilize the generalized multiscale finite
element method (GMsFEM) combining with a multi-continuum approach, to upscale
the homogenized system to a macroscopic (coarse-grid) level. This scheme
involves building uncoupled and coupled multiscale basis functions, which are
used not only to construct coarse-grid solution approximation with high
accuracy but also (with the coupled multiscale basis) to capture the
interactions among continua. These prospects and convergence are demonstrated
by several numerical results for the proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09322</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09322</id><submitter>Anuj Diwan</submitter><version version="v1"><date>Mon, 19 Oct 2020 08:59:58 GMT</date><size>83kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 10:11:49 GMT</date><size>137kb</size><source_type>D</source_type></version><title>Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages</title><authors>Anuj Diwan, Preethi Jyothi</authors><categories>eess.AS cs.AI cs.CL cs.LG cs.SD</categories><comments>5 pages, 1 figure. Accepted at INTERSPEECH 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a seemingly simple but effective technique to improve
low-resource ASR systems for phonetic languages. By identifying sets of
acoustically similar graphemes in these languages, we first reduce the output
alphabet of the ASR system using linguistically meaningful reductions and then
reconstruct the original alphabet using a standalone module. We demonstrate
that this lessens the burden and improves the performance of low-resource
end-to-end ASR systems (because only reduced-alphabet predictions are needed)
and that it is possible to design a very simple but effective reconstruction
module that recovers sequences in the original alphabet from sequences in the
reduced alphabet. We present a finite state transducer-based reconstruction
module that operates on the 1-best ASR hypothesis in the reduced alphabet. We
demonstrate the efficacy of our proposed technique using ASR systems for two
Indian languages, Gujarati and Telugu. With access to only 10 hrs of speech
data, we obtain relative WER reductions of up to 7% compared to systems that do
not use any reduction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09559</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09559</id><submitter>Cristi\'an Bravo</submitter><version version="v1"><date>Mon, 19 Oct 2020 14:39:53 GMT</date><size>1813kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 11 Feb 2021 22:57:28 GMT</date><size>1807kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 23:01:03 GMT</date><size>1829kb</size><source_type>D</source_type></version><title>Multilayer Network Analysis for Improved Credit Risk Prediction</title><authors>Mar\'ia \'Oskarsd\'ottir and Cristi\'an Bravo</authors><categories>cs.SI cs.LG</categories><comments>25 pages, 13 figures. v3 - revised and resubmitted</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present a multilayer network model for credit risk assessment. Our model
accounts for multiple connections between borrowers (such as their geographic
location and their economic activity) and allows for explicitly modelling the
interaction between connected borrowers. We develop a multilayer personalized
PageRank algorithm that allows quantifying the strength of the default exposure
of any borrower in the network. We test our methodology in an agricultural
lending framework, where it has been suspected for a long time default
correlates between borrowers when they are subject to the same structural
risks. Our results show there are significant predictive gains just by
including centrality multilayer network information in the model, and these
gains are increased by more complex information such as the multilayer PageRank
variables. The results suggest default risk is highest when an individual is
connected to many defaulters, but this risk is mitigated by the size of the
neighbourhood of the individual, showing both default risk and financial
stability propagate throughout the network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09895</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09895</id><submitter>Sarala Padi Dr</submitter><version version="v1"><date>Mon, 19 Oct 2020 22:15:03 GMT</date><size>718kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 28 Oct 2020 00:13:07 GMT</date><size>2595kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 18:17:46 GMT</date><size>8090kb</size><source_type>D</source_type></version><title>Multi-Window Data Augmentation Approach for Speech Emotion Recognition</title><authors>Sarala Padi, Dinesh Manocha, Ram D.Sriram</authors><categories>cs.SD cs.AI cs.LG eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Multi-Window Data Augmentation (MWA-SER) approach for speech
emotion recognition. MWA-SER is a unimodal approach that focuses on two key
concepts; designing the speech augmentation method and building the deep
learning model to recognize the underlying emotion of an audio signal. Our
proposed multi-window augmentation approach generates additional data samples
from the speech signal by employing multiple window sizes in the audio feature
extraction process. We show that our augmentation method, combined with a deep
learning model, improves speech emotion recognition performance. We evaluate
the performance of our approach on three benchmark datasets: IEMOCAP, SAVEE,
and RAVDESS. We show that the multi-window model improves the SER performance
and outperforms a single-window model. The notion of finding the best window
size is an essential step in audio feature extraction. We perform extensive
experimental evaluations to find the best window choice and explore the
windowing effect for SER analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10035</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10035</id><submitter>Neha Srikanth</submitter><version version="v1"><date>Tue, 20 Oct 2020 05:06:23 GMT</date><size>904kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 01:45:26 GMT</date><size>5470kb</size><source_type>D</source_type></version><title>Elaborative Simplification: Content Addition and Explanation Generation
  in Text Simplification</title><authors>Neha Srikanth, Junyi Jessy Li</authors><categories>cs.CL</categories><comments>Findings of ACL 2021 Camera-Ready</comments><journal-ref>Findings of Association of Computational Linguistics 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of modern-day text simplification research focuses on sentence-level
simplification, transforming original, more complex sentences into simplified
versions. However, adding content can often be useful when difficult concepts
and reasoning need to be explained. In this work, we present the first
data-driven study of content addition in text simplification, which we call
elaborative simplification. We introduce a new annotated dataset of 1.3K
instances of elaborative simplification in the Newsela corpus, and analyze how
entities, ideas, and concepts are elaborated through the lens of contextual
specificity. We establish baselines for elaboration generation using
large-scale pre-trained language models, and demonstrate that considering
contextual specificity during generation can improve performance. Our results
illustrate the complexities of elaborative simplification, suggesting many
interesting directions for future work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10391</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10391</id><submitter>Georgios Michalopoulos</submitter><version version="v1"><date>Tue, 20 Oct 2020 15:56:31 GMT</date><size>1018kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 12 Apr 2021 07:21:08 GMT</date><size>715kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 28 Apr 2021 21:30:38 GMT</date><size>715kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 19 May 2021 19:59:23 GMT</date><size>689kb</size><source_type>D</source_type></version><version version="v5"><date>Thu, 3 Jun 2021 15:07:58 GMT</date><size>723kb</size><source_type>D</source_type></version><title>UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual
  Embeddings Using the Unified Medical Language System Metathesaurus</title><authors>George Michalopoulos, Yuanxin Wang, Hussam Kaka, Helen Chen and
  Alexander Wong</authors><categories>cs.CL cs.AI cs.LG</categories><comments>10 pages, 3 figures, accepted in NAACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have
achieved state-of-the-art results in biomedical natural language processing
tasks by focusing their pre-training process on domain-specific corpora.
However, such models do not take into consideration expert domain knowledge.
  In this work, we introduced UmlsBERT, a contextual embedding model that
integrates domain knowledge during the pre-training process via a novel
knowledge augmentation strategy. More specifically, the augmentation on
UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was
performed in two ways: i) connecting words that have the same underlying
`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to
create clinically meaningful input embeddings. By applying these two
strategies, UmlsBERT can encode clinical domain knowledge into word embeddings
and outperform existing domain-specific models on common named-entity
recognition (NER) and clinical natural language inference clinical NLP tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10468</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10468</id><submitter>Sherif Abdulatif</submitter><version version="v1"><date>Tue, 20 Oct 2020 17:28:07 GMT</date><size>1502kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 01:56:54 GMT</date><size>1502kb</size><source_type>D</source_type></version><title>Investigating Cross-Domain Losses for Speech Enhancement</title><authors>Sherif Abdulatif, Karim Armanious, Jayasankar T. Sajeev, Karim
  Guirguis, Bin Yang</authors><categories>cs.SD cs.LG eess.AS</categories><comments>5 pages, 3 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen a surge in the number of available frameworks for
speech enhancement (SE) and recognition. Whether model-based or constructed via
deep learning, these frameworks often rely in isolation on either time-domain
signals or time-frequency (TF) representations of speech data. In this study,
we investigate the advantages of each set of approaches by separately examining
their impact on speech intelligibility and quality. Furthermore, we combine the
fragmented benefits of time-domain and TF speech representations by introducing
two new cross-domain SE frameworks. A quantitative comparative analysis against
recent model-based and deep learning SE approaches is performed to illustrate
the merit of the proposed frameworks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10805</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10805</id><submitter>Jianlei Chi</submitter><version version="v1"><date>Wed, 21 Oct 2020 07:49:08 GMT</date><size>2123kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 06:17:30 GMT</date><size>2931kb</size></version><title>SeqTrans: Automatic Vulnerability Fix via Sequence to Sequence Learning</title><authors>Jianlei Chi, Yu Qu, Ting Liu, Qinghua Zheng, Heng Yin</authors><categories>cs.CR cs.SE</categories><comments>20 pages, 18 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software vulnerabilities are now reported at an unprecedented speed due to
the recent development of automated vulnerability hunting tools. However,
fixing vulnerabilities still mainly depends on programmers' manual efforts.
Developers need to deeply understand the vulnerability and try to affect the
system's functions as little as possible.
  In this paper, with the advancement of Neural Machine Translation (NMT)
techniques, we provide a novel approach called SeqTrans to exploit historical
vulnerability fixes to provide suggestions and automatically fix the source
code. To capture the contextual information around the vulnerable code, we
propose to leverage data flow dependencies to construct code sequences and fed
them into the state-of-the-art transformer model. The fine-tuning strategy has
been introduced to overcome the small sample size problem. We evaluate SeqTrans
on a dataset containing 1,282 commits that fix 624 vulnerabilities in 205 Java
projects. Results show that the accuracy of SeqTrans outperforms the latest
techniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level
fix. In the meantime, we look deep inside the result and observe that NMT model
performs very well in certain kinds of vulnerabilities like CWE-287 (Improper
Authentication) and CWE-863 (Incorrect Authorization).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.11075</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.11075</id><submitter>Nils Barlaug</submitter><version version="v1"><date>Wed, 21 Oct 2020 15:36:03 GMT</date><size>780kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 21:51:58 GMT</date><size>788kb</size><source_type>D</source_type></version><title>Neural Networks for Entity Matching: A Survey</title><authors>Nils Barlaug, Jon Atle Gulla</authors><categories>cs.DB cs.CL cs.LG</categories><comments>Published in ACM Transactions on Knowledge Discovery from Data (TKDD)</comments><journal-ref>ACM Transactions on Knowledge Discovery from Data, Volume 15,
  Issue 3, April 2021</journal-ref><doi>10.1145/3442200</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity matching is the problem of identifying which records refer to the same
real-world entity. It has been actively researched for decades, and a variety
of different approaches have been developed. Even today, it remains a
challenging problem, and there is still generous room for improvement. In
recent years we have seen new methods based upon deep learning techniques for
natural language processing emerge.
  In this survey, we present how neural networks have been used for entity
matching. Specifically, we identify which steps of the entity matching process
existing work have targeted using neural networks, and provide an overview of
the different techniques used at each step. We also discuss contributions from
deep learning in entity matching compared to traditional methods, and propose a
taxonomy of deep neural networks for entity matching.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.11660</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.11660</id><submitter>Ido Greenberg</submitter><version version="v1"><date>Thu, 22 Oct 2020 12:45:55 GMT</date><size>1669kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 09:41:28 GMT</date><size>1756kb</size><source_type>D</source_type></version><title>Detecting Rewards Deterioration in Episodic Reinforcement Learning</title><authors>Ido Greenberg, Shie Mannor</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In many RL applications, once training ends, it is vital to detect any
deterioration in the agent performance as soon as possible. Furthermore, it
often has to be done without modifying the policy and under minimal assumptions
regarding the environment. In this paper, we address this problem by focusing
directly on the rewards and testing for degradation. We consider an episodic
framework, where the rewards within each episode are not independent, nor
identically-distributed, nor Markov. We present this problem as a multivariate
mean-shift detection problem with possibly partial observations. We define the
mean-shift in a way corresponding to deterioration of a temporal signal (such
as the rewards), and derive a test for this problem with optimal statistical
power. Empirically, on deteriorated rewards in control problems (generated
using various environment modifications), the test is demonstrated to be more
powerful than standard tests - often by orders of magnitude. We also suggest a
novel Bootstrap mechanism for False Alarm Rate control (BFAR), applicable to
episodic (non-i.i.d) signal and allowing our test to run sequentially in an
online manner. Our method does not rely on a learned model of the environment,
is entirely external to the agent, and in fact can be applied to detect changes
or drifts in any episodic signal.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.11929</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.11929</id><submitter>Alexey Dosovitskiy</submitter><version version="v1"><date>Thu, 22 Oct 2020 17:55:59 GMT</date><size>3194kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 13:08:56 GMT</date><size>3033kb</size><source_type>D</source_type></version><title>An Image is Worth 16x16 Words: Transformers for Image Recognition at
  Scale</title><authors>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
  Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
  Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit and Neil Houlsby</authors><categories>cs.CV cs.AI cs.LG</categories><comments>Fine-tuning code and pre-trained models are available at
  https://github.com/google-research/vision_transformer. ICLR camera-ready
  version with 2 small modifications: 1) Added a discussion of CLS vs GAP
  classifier in the appendix, 2) Fixed an error in exaFLOPs computation in
  Figure 5 and Table 6 (relative performance of models is basically not
  affected)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.11939</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.11939</id><submitter>Chu-Cheng Lin</submitter><version version="v1"><date>Thu, 22 Oct 2020 17:59:09 GMT</date><size>240kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 26 May 2021 14:24:32 GMT</date><size>324kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 02:09:15 GMT</date><size>859kb</size><source_type>D</source_type></version><title>Limitations of Autoregressive Models and Their Alternatives</title><authors>Chu-Cheng Lin and Aaron Jaech and Xin Li and Matthew R. Gormley and
  Jason Eisner</authors><categories>cs.LG cs.CL stat.ML</categories><comments>NAACL 2021 (same content, more relaxed layout)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Standard autoregressive language models perform only polynomial-time
computation to compute the probability of the next symbol. While this is
attractive, it means they cannot model distributions whose next-symbol
probability is hard to compute. Indeed, they cannot even model them well enough
to solve associated easy decision problems for which an engineer might want to
consult a language model. These limitations apply no matter how much
computation and data are used to train the model, unless the model is given
access to oracle parameters that grow superpolynomially in sequence length.
  Thus, simply training larger autoregressive language models is not a panacea
for NLP. Alternatives include energy-based models (which give up efficient
sampling) and latent-variable autoregressive models (which give up efficient
scoring of a given string). Both are powerful enough to escape the above
limitations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.12008</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.12008</id><submitter>Siamak Shakeri</submitter><version version="v1"><date>Thu, 22 Oct 2020 19:59:37 GMT</date><size>14309kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 16 Apr 2021 21:24:02 GMT</date><size>20357kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 May 2021 21:07:33 GMT</date><size>24055kb</size><source_type>D</source_type></version><title>Towards Zero-Shot Multilingual Synthetic Question and Answer Generation
  for Cross-Lingual Reading Comprehension</title><authors>Siamak Shakeri, Noah Constant, Mihir Sanjay Kale, Linting Xue</authors><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple method to generate multilingual question and answer pairs
on a large scale through the use of a single generative model. These synthetic
samples can be used to improve the zero-shot performance of multilingual QA
models on target languages. Our proposed multi-task training of the generative
model only requires the labeled training samples in English, thus removing the
need for such samples in the target languages, making it applicable to far more
languages than those with labeled data. Human evaluations indicate the majority
of such samples are grammatically correct and sensible. Experimental results
show our proposed approach can achieve large gains on the XQuAD dataset,
reducing the gap between zero-shot and supervised performance of smaller QA
models on various languages.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.12165</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.12165</id><submitter>Xiao Li</submitter><version version="v1"><date>Fri, 23 Oct 2020 04:57:47 GMT</date><size>1097kb</size></version><title>Maximum bound principle preserving integrating factor Runge-Kutta
  methods for semilinear parabolic equations</title><authors>Lili Ju, Xiao Li, Zhonghua Qiao, Jiang Yang</authors><categories>math.NA cs.NA</categories><doi>10.1016/j.jcp.2021.110405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large class of semilinear parabolic equations satisfy the maximum bound
principle (MBP) in the sense that the time-dependent solution preserves for any
time a uniform pointwise bound imposed by its initial and boundary conditions.
Investigation on numerical schemes of these equations with preservation of the
MBP has attracted increasingly attentions in recent years, especially for the
temporal discretizations. In this paper, we study high-order MBP-preserving
time integration schemes by means of the integrating factor Runge-Kutta (IFRK)
method. Beginning with the space-discrete system of semilinear parabolic
equations, we present the IFRK method in general form and derive the sufficient
conditions for the method to preserve the MBP. In particular, we show that the
classic four-stage, fourth-order IFRK scheme is MBP-preserving for some typical
semilinear systems although not strong stability preserving, which can be
instantly applied to the Allen-Cahn type of equations. In addition, error
estimates for these numerical schemes are proved theoretically and verified
numerically, as well as their efficiency by simulations of long-time
evolutional behavior.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.12175</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.12175</id><submitter>Yahya Sattar</submitter><version version="v1"><date>Fri, 23 Oct 2020 05:40:04 GMT</date><size>213kb</size><source_type>D</source_type></version><title>Estimation of Groundwater Storage Variations in Indus River Basin using
  GRACE Data</title><authors>Yahya Sattar and Zubair Khalid</authors><categories>eess.SP cs.IR</categories><journal-ref>IEEE ICASSP, 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The depletion and variations of groundwater storage~(GWS) are of critical
importance for sustainable groundwater management. In this work, we use Gravity
Recovery and Climate Experiment (GRACE) to estimate variations in the
terrestrial water storage~(TWS) and use it in conjunction with the Global Land
Data Assimilation System~(GLDAS) data to extract GWS variations over time for
Indus river basin~(IRB). We present a data processing framework that processes
and combines these data-sets to provide an estimate of GWS changes. We also
present the design of a band-limited optimally concentrated window function for
spatial localization of the data in the region of interest. We construct the
so-called optimal window for the IRB region and use it in our processing
framework to analyze the GWS variations from 2005 to 2015. Our analysis reveals
the expected seasonal variations in GWS and signifies groundwater depletion on
average over the time period. Our proposed processing framework can be used to
analyze spatio-temporal variations in TWS and GWS for any region of interest.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.12461</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.12461</id><submitter>Harald Bayerlein</submitter><version version="v1"><date>Fri, 23 Oct 2020 14:59:30 GMT</date><size>2886kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 15 Mar 2021 09:07:39 GMT</date><size>3725kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 11:38:05 GMT</date><size>2172kb</size><source_type>D</source_type></version><title>Multi-UAV Path Planning for Wireless Data Harvesting with Deep
  Reinforcement Learning</title><authors>Harald Bayerlein, Mirco Theile, Marco Caccamo, David Gesbert</authors><categories>cs.MA cs.IT cs.LG cs.RO cs.SY eess.SY math.IT</categories><comments>Modifications: final formatting; Code available under
  https://github.com/hbayerlein/uav_data_harvesting, article extends on
  arXiv:2007.00544</comments><journal-ref>IEEE Open Journal of the Communications Society, vol. 2, pp.
  1171-1187, 2021</journal-ref><doi>10.1109/OJCOMS.2021.3081996</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Harvesting data from distributed Internet of Things (IoT) devices with
multiple autonomous unmanned aerial vehicles (UAVs) is a challenging problem
requiring flexible path planning methods. We propose a multi-agent
reinforcement learning (MARL) approach that, in contrast to previous work, can
adapt to profound changes in the scenario parameters defining the data
harvesting mission, such as the number of deployed UAVs, number, position and
data amount of IoT devices, or the maximum flying time, without the need to
perform expensive recomputations or relearn control policies. We formulate the
path planning problem for a cooperative, non-communicating, and homogeneous
team of UAVs tasked with maximizing collected data from distributed IoT sensor
nodes subject to flying time and collision avoidance constraints. The path
planning problem is translated into a decentralized partially observable Markov
decision process (Dec-POMDP), which we solve through a deep reinforcement
learning (DRL) approach, approximating the optimal UAV control policy without
prior knowledge of the challenging wireless channel characteristics in dense
urban environments. By exploiting a combination of centered global and local
map representations of the environment that are fed into convolutional layers
of the agents, we show that our proposed network architecture enables the
agents to cooperate effectively by carefully dividing the data collection task
among themselves, adapt to large complex environments and state spaces, and
make movement decisions that balance data collection goals, flight-time
efficiency, and navigation constraints. Finally, learning a control policy that
generalizes over the scenario parameter space enables us to analyze the
influence of individual parameters on collection performance and provide some
intuition about system-level benefits.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.12725</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.12725</id><submitter>Peter Shaw</submitter><version version="v1"><date>Sat, 24 Oct 2020 00:38:27 GMT</date><size>48kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 21:25:04 GMT</date><size>66kb</size><source_type>D</source_type></version><title>Compositional Generalization and Natural Language Variation: Can a
  Semantic Parsing Approach Handle Both?</title><authors>Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence-to-sequence models excel at handling natural language variation, but
have been shown to struggle with out-of-distribution compositional
generalization. This has motivated new specialized architectures with stronger
compositional biases, but most of these approaches have only been evaluated on
synthetically-generated datasets, which are not representative of natural
language variation. In this work we ask: can we develop a semantic parsing
approach that handles both natural language variation and compositional
generalization? To better assess this capability, we propose new train and test
splits of non-synthetic datasets. We demonstrate that strong existing
approaches do not perform well across a broad set of evaluations. We also
propose NQG-T5, a hybrid model that combines a high-precision grammar-based
approach with a pre-trained sequence-to-sequence model. It outperforms existing
approaches across several compositional generalization challenges on
non-synthetic data, while also being competitive with the state-of-the-art on
standard evaluations. While still far from solving this problem, our study
highlights the importance of diverse evaluations and the open challenge of
handling both compositional generalization and natural language variation in
semantic parsing.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.13046</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.13046</id><submitter>Louise Gillian Bautista</submitter><version version="v1"><date>Sun, 25 Oct 2020 06:12:06 GMT</date><size>6033kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 14:14:23 GMT</date><size>7048kb</size><source_type>D</source_type></version><title>CLRGaze: Contrastive Learning of Representations for Eye Movement
  Signals</title><authors>Louise Gillian C. Bautista and Prospero C. Naval Jr</authors><categories>cs.CV</categories><comments>Accepted to 29th European Signal Processing Conference (EUSIPCO 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eye movements are intricate and dynamic biosignals that contain a wealth of
cognitive information about the subject. However, these are ambiguous signals
and therefore require meticulous feature engineering to be used by machine
learning algorithms. We instead propose to learn feature vectors of eye
movements in a self-supervised manner. We adopt a contrastive learning approach
and propose a set of data transformations that encourage a deep neural network
to discern salient and granular gaze patterns. This paper presents a novel
experiment utilizing six eye-tracking data sets despite different data
specifications and experimental conditions. We assess the learned features on
biometric tasks with only a linear classifier, achieving 84.6% accuracy on a
mixed dataset, and up to 97.3% accuracy on a single dataset. Our work advances
the state of machine learning for eye movements and provides insights into a
general representation learning method not only for eye movements but also for
similar biosignals.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.13232</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.13232</id><submitter>Mehmet Ozan Unal</submitter><version version="v1"><date>Sun, 25 Oct 2020 22:02:14 GMT</date><size>2664kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 17 Apr 2021 18:58:01 GMT</date><size>14023kb</size><source_type>D</source_type></version><title>Self-Supervised Training For Low Dose CT Reconstruction</title><authors>Mehmet Ozan Unal, Metin Ertas, Isa Yildirim</authors><categories>eess.IV cs.CV</categories><journal-ref>2021 IEEE 18th International Symposium on Biomedical Imaging
  (ISBI)</journal-ref><doi>10.1109/ISBI48211.2021.9433944</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ionizing radiation has been the biggest concern in CT imaging. To reduce the
dose level without compromising the image quality, low-dose CT reconstruction
has been offered with the availability of compressed sensing based
reconstruction methods. Recently, data-driven methods got attention with the
rise of deep learning, the availability of high computational power, and big
datasets. Deep learning based methods have also been used in low-dose CT
reconstruction problem in different manners. Usually, the success of these
methods depends on labeled data. However, recent studies showed that training
can be achieved successfully with noisy datasets. In this study, we defined a
training scheme to use low-dose sinograms as their own training targets. We
applied the self-supervision principle in the projection domain where the noise
is element-wise independent which is a requirement for self-supervised training
methods. Using the self-supervised training, the filtering part of the FBP
method and the parameters of a denoiser neural network are optimized. We
demonstrate that our method outperforms both conventional and compressed
sensing based iterative reconstruction methods qualitatively and quantitatively
in the reconstruction of analytic CT phantoms and real-world CT images in
low-dose CT reconstruction task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.14110</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.14110</id><submitter>Soumyadeep Datta</submitter><version version="v1"><date>Tue, 27 Oct 2020 07:43:16 GMT</date><size>2318kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 02:38:46 GMT</date><size>3791kb</size></version><title>Full-Duplex Cell-Free mMIMO Systems: Analysis and Decentralized
  Optimization</title><authors>Soumyadeep Datta, Ekant Sharma, Dheeraj Naidu Amudala, Rohit Budhiraja
  and Shivendra S. Panwar</authors><categories>eess.SP cs.IT math.IT</categories><comments>Under review for possible publication to IEEE Transactions in
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell-free (CF) massive multiple-input-multiple-output (mMIMO) deployments are
usually investigated with half-duplex (HD) nodes and high-capacity fronthaul
links. To leverage the possible gains in throughput and energy efficiency (EE)
of full-duplex (FD) communications, we consider a FD CF mMIMO system with
practical limited-capacity fronthaul links. We derive closed-form spectral
efficiency (SE) lower bounds for this system with maximum-ratio
transmission/maximum-ratio combining (MRT/MRC) processing and optimal uniform
quantization. We then optimize the weighted sum EE (WSEE) via downlink and
uplink power control by using a two-layered approach: the first layer
formulates the optimization as a generalized convex program (GCP), while the
second layer solves the optimization decentrally using alternating direction
method of multipliers. We analytically show that the proposed two-layered
formulation yields a Karush-Kuhn-Tucker point of the original WSEE
optimization. We numerically show the influence of weights on the individual EE
of the users, which demonstrates the utility of WSEE metric to incorporate
heterogeneous EE requirements of users. We also show that with low fronthaul
capacity, the system requires a higher number of fronthaul quantization bits to
achieve high SE and WSEE. For high fronthaul capacity, higher number of bits,
however, achieves high SE and a reduced WSEE.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.15032</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.15032</id><submitter>Daniel Barcelona-Pons</submitter><version version="v1"><date>Wed, 28 Oct 2020 15:09:55 GMT</date><size>26064kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 29 Oct 2020 13:21:59 GMT</date><size>26069kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 13:45:30 GMT</date><size>41555kb</size><source_type>D</source_type></version><title>Benchmarking Parallelism in FaaS Platforms</title><authors>Daniel Barcelona-Pons and Pedro Garc\'ia-L\'opez</authors><categories>cs.DC</categories><comments>19 pages, 15 figures, submitted to FGCS, revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Serverless computing has seen a myriad of work exploring its potential. Some
systems tackle Function-as-a-Service (FaaS) properties on automatic elasticity
and scale to run highly-parallel computing jobs. However, they focus on
specific platforms and convey that their ideas can be extrapolated to any FaaS
runtime.
  An important question arises: do all FaaS platforms fit parallel
computations? In this paper, we argue that not all of them provide the
necessary means to host highly-parallel applications. To validate our
hypothesis, we create a comparative framework and categorize the architectures
of four cloud FaaS offerings, emphasizing parallel performance. We attest and
extend this description with an empirical experiment that consists in plotting
in deep detail the evolution of a parallel computing job on each service.
  The analysis of our results evinces that FaaS is not inherently good for
parallel computations and architectural differences across platforms are
decisive to categorize their performance. A key insight is the importance of
virtualization technologies and the scheduling approach of FaaS platforms.
Parallelism improves with lighter virtualization and proactive scheduling due
to finer resource allocation and faster elasticity. This causes some platforms
like AWS and IBM to perform well for highly-parallel computations, while others
such as Azure present difficulties to achieve the required parallelism degree.
Consequently, the information in this paper becomes of special interest to help
users choose the most adequate infrastructure for their parallel applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.15120</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.15120</id><submitter>Andrew Bailey</submitter><version version="v1"><date>Wed, 28 Oct 2020 14:53:56 GMT</date><size>191kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 10:58:53 GMT</date><size>1051kb</size><source_type>D</source_type></version><title>Gender Bias in Depression Detection Using Audio Features</title><authors>Andrew Bailey and Mark D. Plumbley</authors><categories>cs.SD cs.LG eess.AS eess.SP</categories><comments>5 pages, 2 figures, to be published at EUSIPCO 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depression is a large-scale mental health problem and a challenging area for
machine learning researchers in detection of depression. Datasets such as
Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created
to aid research in this area. However, on top of the challenges inherent in
accurately detecting depression, biases in datasets may result in skewed
classification performance. In this paper we examine gender bias in the
DAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an
overreporting of performance. By different concepts from Fair Machine Learning,
such as data re-distribution, and using raw audio features, we can mitigate
against the harmful effects of bias.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.15672</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.15672</id><submitter>Soumyadeep Datta</submitter><version version="v1"><date>Tue, 27 Oct 2020 07:33:17 GMT</date><size>3743kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 02:41:01 GMT</date><size>3023kb</size></version><title>FD Cell-Free mMIMO: Analysis and Optimization</title><authors>Soumyadeep Datta, Ekant Sharma, Dheeraj Naidu Amudala, Rohit Budhiraja
  and Shivendra S. Panwar</authors><categories>eess.SP cs.IT math.IT</categories><comments>Accepted in IEEE International Conference on Communications (ICC)
  2021. arXiv admin note: substantial text overlap with arXiv:2010.14110</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell-free (CF) massive multiple-input-multiple-output (mMIMO) deployments are
usually investigated with half-duplex nodes and high-capacity fronthaul links.
To leverage the possible gains in throughput and energy efficiency (EE) of
full-duplex (FD) communications, we consider a FD CF mMIMO system with
practical limited-capacity fronthaul links. We derive closed-form spectral
efficiency (SE) lower bounds for this system with maximum-ratio
combining/maximum-ratio transmission processing and optimal uniform
quantization. We then optimize the weighted sum EE (WSEE) via downlink and
uplink power control by using a {two-layered} approach: the first layer
formulates the optimization as a generalized convex program, while the second
layer solves the optimization decentrally using alternating direction method of
multipliers. We analytically show that the proposed two-layered formulation
yields a Karush-Kuhn-Tucker point of the original WSEE optimization. We
numerically show the influence of weights on the individual EE of the users,
which demonstrates the utility of WSEE metric to incorporate heterogeneous EE
requirements of users. We show that the low fronthaul capacity reduces the
number of users each AP can support, and the cell-free system, consequently,
becomes user-centric.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.15843</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.15843</id><submitter>T. Lucas Makinen</submitter><version version="v1"><date>Thu, 29 Oct 2020 18:00:02 GMT</date><size>5979kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 20:12:44 GMT</date><size>12909kb</size><source_type>D</source_type></version><title>deep21: a Deep Learning Method for 21cm Foreground Removal</title><authors>T. Lucas Makinen, Lachlan Lancaster, Francisco Villaescusa-Navarro,
  Peter Melchior, Shirley Ho, Laurence Perreault-Levasseur, and David N.
  Spergel</authors><categories>astro-ph.CO cs.LG</categories><comments>Published in JCAP 30 April 2021. 30 pages, 11 figures</comments><doi>10.1088/1475-7516/2021/04/081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek to remove foreground contaminants from 21cm intensity mapping
observations. We demonstrate that a deep convolutional neural network (CNN)
with a UNet architecture and three-dimensional convolutions, trained on
simulated observations, can effectively separate frequency and spatial patterns
of the cosmic neutral hydrogen (HI) signal from foregrounds in the presence of
noise. Cleaned maps recover cosmological clustering statistics within 10% at
all relevant angular scales and frequencies. This amounts to a reduction in
prediction variance of over an order of magnitude on small angular scales
($\ell &gt; 300$), and improved accuracy for small radial scales ($k_{\parallel} &gt;
0.17\ \rm h\ Mpc^{-1})$ compared to standard Principal Component Analysis (PCA)
methods. We estimate posterior confidence intervals for the network's
prediction by training an ensemble of UNets. Our approach demonstrates the
feasibility of analyzing 21cm intensity maps, as opposed to derived summary
statistics, for upcoming radio experiments, as long as the simulated foreground
model is sufficiently realistic. We provide the code used for this analysis on
Github https://github.com/tlmakinen/deep21 as well as a browser-based tutorial
for the experiment and UNet model via the accompanying
http://bit.ly/deep21-colab Colab notebook.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.16019</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.16019</id><submitter>Brennan Klein</submitter><version version="v1"><date>Fri, 30 Oct 2020 01:50:04 GMT</date><size>123kb</size><source_type>D</source_type></version><title>netrd: A library for network reconstruction and graph distances</title><authors>Stefan McCabe, Leo Torres, Timothy LaRock, Syed Arefinul Haque,
  Chia-Hung Yang, Harrison Hartle, Brennan Klein</authors><categories>cs.SI physics.soc-ph</categories><journal-ref>Journal of Open Source Software, 2021</journal-ref><doi>10.21105/joss.02990</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last two decades, alongside the increased availability of large
network datasets, we have witnessed the rapid rise of network science. For many
systems, however, the data we have access to is not a direct description of the
underlying network. More and more, we see the drive to study networks that have
been inferred or reconstructed from non-network data---in particular, using
time series data from the nodes in a system to infer likely connections between
them. Selecting the most appropriate technique for this task is a challenging
problem in network science. Different reconstruction techniques usually have
different assumptions, and their performance varies from system to system in
the real world. One way around this problem could be to use several different
reconstruction techniques and compare the resulting networks. However, network
comparison is also not an easy problem, as it is not obvious how best to
quantify the differences between two networks, in part because of the diversity
of tools for doing so. The netrd Python package seeks to address these two
parallel problems in network science by providing, to our knowledge, the most
extensive collection of both network reconstruction techniques and network
comparison techniques (often referred to as graph distances) in a single
library (https://github.com/netsiphd/netrd). In this article, we detail the two
main functionalities of the netrd package. Along the way, we describe some of
its other useful features. This package builds on commonly used Python packages
and is already a widely used resource for network scientists and other
multidisciplinary researchers. With ongoing open-source development, we see
this as a tool that will continue to be used by all sorts of researchers to
come.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.16046</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.16046</id><submitter>Fuli Luo</submitter><version version="v1"><date>Fri, 30 Oct 2020 03:41:38 GMT</date><size>9718kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 13:15:11 GMT</date><size>9041kb</size><source_type>D</source_type></version><title>VECO: Variable and Flexible Cross-lingual Pre-training for Language
  Understanding and Generation</title><authors>Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei
  Huang, Luo Si</authors><categories>cs.CL</categories><comments>Accepted by ACL 2021 (long paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing work in multilingual pretraining has demonstrated the potential of
cross-lingual transferability by training a unified Transformer encoder for
multiple languages. However, much of this work only relies on the shared
vocabulary and bilingual contexts to encourage the correlation across
languages, which is loose and implicit for aligning the contextual
representations between languages. In this paper, we plug a cross-attention
module into the Transformer encoder to explicitly build the interdependence
between languages. It can effectively avoid the degeneration of predicting
masked words only conditioned on the context in its own language. More
importantly, when fine-tuning on downstream tasks, the cross-attention module
can be plugged in or out on-demand, thus naturally benefiting a wider range of
cross-lingual tasks, from language understanding to generation.
  As a result, the proposed cross-lingual model delivers new state-of-the-art
results on various cross-lingual understanding tasks of the XTREME benchmark,
covering text classification, sequence labeling, question answering, and
sentence retrieval. For cross-lingual generation tasks, it also outperforms all
existing cross-lingual models and state-of-the-art Transformer variants on
WMT14 English-to-German and English-to-French translation datasets, with gains
of up to 1~2 BLEU.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.00032</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.00032</id><submitter>Ruben Grandia</submitter><version version="v1"><date>Fri, 30 Oct 2020 18:27:00 GMT</date><size>15623kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 11:20:33 GMT</date><size>39234kb</size><source_type>D</source_type></version><title>Multi-Layered Safety for Legged Robots via Control Barrier Functions and
  Model Predictive Control</title><authors>Ruben Grandia, Andrew J. Taylor, Aaron D. Ames, Marco Hutter</authors><categories>cs.RO</categories><journal-ref>IEEE International Conference on Robotics and Automation (ICRA)
  2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of dynamic locomotion over rough terrain requires both accurate
foot placement together with an emphasis on dynamic stability. Existing
approaches to this problem prioritize immediate safe foot placement over longer
term dynamic stability considerations, or relegate the coordination of foot
placement and dynamic stability to heuristic methods. We propose a
multi-layered locomotion framework that unifies Control Barrier Functions
(CBFs) with Model Predictive Control (MPC) to simultaneously achieve safe foot
placement and dynamic stability. Our approach incorporates CBF based safety
constraints both in a low frequency kino-dynamic MPC formulation and a high
frequency inverse dynamics tracking controller. This ensures that
safety-critical execution is considered when optimizing locomotion over a
longer horizon. We validate the proposed method in a 3D stepping-stone scenario
in simulation and experimentally on the ANYmal quadruped platform.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.00190</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.00190</id><submitter>Miao Zhu</submitter><version version="v1"><date>Sat, 31 Oct 2020 05:04:56 GMT</date><size>380kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 22 Apr 2021 02:14:22 GMT</date><size>761kb</size><source_type>D</source_type></version><title>Epidemic Spreading in a Social Network with Facial Masks wearing
  Individuals</title><authors>Duan-Shin Lee and Miao Zhu</authors><categories>physics.soc-ph cs.SI</categories><comments>14 pages,10 figures</comments><journal-ref>IEEE Transactions on Computational Social Systems 28 May 2021</journal-ref><doi>10.1109/TCSS.2021.3081148</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a susceptible-infected-recovered (SIR) model with
individuals wearing facial masks and individuals who do not. The disease
transmission rates, the recovering rates and the fraction of individuals who
wear masks are all time dependent in the model. We develop a progressive
estimation of the disease transmission rates and the recovering rates based on
the COVID-19 data published by John Hopkins University. We determine the
fraction of individual who wear masks by a maximum likelihood estimation, which
maximizes the transition probability of a stochastic
susceptible-infected-recovered model. The transition probability is numerically
difficult to compute if the number of infected individuals is large. We develop
an approximation for the transition probability based on central limit theorem
and mean field approximation. We show through numerical study that our
approximation works well. We develop a bond percolation analysis to predict the
eventual fraction of population who are infected, assuming that parameters of
the SIR model do not change anymore. We predict the outcome of COVID-19
pandemic using our theory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.00382</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.00382</id><submitter>Dong-Ki Kim</submitter><version version="v1"><date>Sat, 31 Oct 2020 22:50:21 GMT</date><size>3528kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Dec 2020 22:08:50 GMT</date><size>5219kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 6 Feb 2021 17:45:17 GMT</date><size>5483kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Jun 2021 16:46:58 GMT</date><size>5483kb</size><source_type>D</source_type></version><title>A Policy Gradient Algorithm for Learning to Learn in Multiagent
  Reinforcement Learning</title><authors>Dong-Ki Kim, Miao Liu, Matthew Riemer, Chuangchuang Sun, Marwa
  Abdulhai, Golnaz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, Jonathan P. How</authors><categories>cs.LG cs.AI cs.MA</categories><comments>Accepted to ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental challenge in multiagent reinforcement learning is to learn
beneficial behaviors in a shared environment with other simultaneously learning
agents. In particular, each agent perceives the environment as effectively
non-stationary due to the changing policies of other agents. Moreover, each
agent is itself constantly learning, leading to natural non-stationarity in the
distribution of experiences encountered. In this paper, we propose a novel
meta-multiagent policy gradient theorem that directly accounts for the
non-stationary policy dynamics inherent to multiagent learning settings. This
is achieved by modeling our gradient updates to consider both an agent's own
non-stationary policy dynamics and the non-stationary policy dynamics of other
agents in the environment. We show that our theoretically grounded approach
provides a general solution to the multiagent learning problem, which
inherently comprises all key aspects of previous state of the art approaches on
this topic. We test our method on a diverse suite of multiagent benchmarks and
demonstrate a more efficient ability to adapt to new agents as they learn than
baseline methods across the full spectrum of mixed incentive, competitive, and
cooperative domains.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.00454</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.00454</id><submitter>Xiaoyu Cui</submitter><version version="v1"><date>Sun, 1 Nov 2020 09:23:16 GMT</date><size>1171kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 21 Dec 2020 08:33:59 GMT</date><size>838kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 07:52:06 GMT</date><size>2465kb</size><source_type>D</source_type></version><title>Dynamic radiomics: a new methodology to extract quantitative
  time-related features from tomographic images</title><authors>Fengying Che, Ruichuan Shi, Jian Wu, Haoran Li, Shuqin Li, Weixing
  Chen, Hao Zhang, Zhi Li, and Xiaoyu Cui (Member, IEEE)</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The feature extraction methods of radiomics are mainly based on static
tomographic images at a certain moment, while the occurrence and development of
disease is a dynamic process that cannot be fully reflected by only static
characteristics. This study proposes a new dynamic radiomics feature extraction
workflow that uses time-dependent tomographic images of the same patient,
focuses on the changes in image features over time, and then quantifies them as
new dynamic features for diagnostic or prognostic evaluation. We first define
the mathematical paradigm of dynamic radiomics and introduce three specific
methods that can describe the transformation process of features over time.
Three different clinical problems are used to validate the performance of the
proposed dynamic feature with conventional 2D and 3D static features.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.00619</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.00619</id><submitter>Stephen White</submitter><version version="v1"><date>Sun, 1 Nov 2020 20:07:24 GMT</date><size>671kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 16:33:29 GMT</date><size>790kb</size><source_type>D</source_type></version><title>Support Recovery for Sparse Multidimensional Phase Retrieval</title><authors>Alexei Novikov, Stephen White</authors><categories>math.CO cs.NA eess.SP math.NA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the \textit{phase retrieval} problem of recovering a sparse
signal $\mathbf{x}$ in $\mathbb{R}^d$ from intensity-only measurements in
dimension $d \geq 2$. Phase retrieval can be equivalently formulated as the
problem of recovering a signal from its autocorrelation, which is in turn
directly related to the combinatorial problem of recovering a set from its
pairwise differences. In one spatial dimension, this problem is well studied
and known as the \textit{turnpike problem}. In this work, we present MISTR
(Multidimensional Intersection Sparse supporT Recovery), an algorithm which
exploits this formulation to recover the support of a multidimensional signal
from magnitude-only measurements. MISTR takes advantage of the structure of
multiple dimensions to provably achieve the same accuracy as the best
one-dimensional algorithms in dramatically less time. We prove theoretically
that MISTR correctly recovers the support of signals distributed as a Gaussian
point process with high probability as long as sparsity is at most
$\mathcal{O}\left(n^{d\theta}\right)$ for any $\theta &lt; 1/2$, where $n^d$
represents pixel size in a fixed image window. In the case that magnitude
measurements are corrupted by noise, we provide a thresholding scheme with
theoretical guarantees for sparsity at most
$\mathcal{O}\left(n^{d\theta}\right)$ for $\theta &lt; 1/4$ that obviates the need
for MISTR to explicitly handle noisy autocorrelation data. Detailed and
reproducible numerical experiments demonstrate the effectiveness of our
algorithm, showing that in practice MISTR enjoys time complexity which is
nearly linear in the size of the input.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.01143</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.01143</id><submitter>Scott Wisdom</submitter><version version="v1"><date>Mon, 2 Nov 2020 17:36:13 GMT</date><size>7894kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 03:47:08 GMT</date><size>7937kb</size><source_type>D</source_type></version><title>Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of
  On-Screen Sounds</title><authors>Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez,
  Daniel P. W. Ellis, John R. Hershey</authors><categories>cs.SD cs.CV eess.AS</categories><comments>ICLR 2021, 27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in deep learning has enabled many advances in sound
separation and visual scene understanding. However, extracting sound sources
which are apparent in natural videos remains an open problem. In this work, we
present AudioScope, a novel audio-visual sound separation framework that can be
trained without supervision to isolate on-screen sound sources from real
in-the-wild videos. Prior audio-visual separation work assumed artificial
limitations on the domain of sound classes (e.g., to speech or music),
constrained the number of sources, and required strong sound separation or
visual segmentation labels. AudioScope overcomes these limitations, operating
on an open domain of sounds, with variable numbers of sources, and without
labels or prior visual segmentation. The training procedure for AudioScope uses
mixture invariant training (MixIT) to separate synthetic mixtures of mixtures
(MoMs) into individual sources, where noisy labels for mixtures are provided by
an unsupervised audio-visual coincidence model. Using the noisy labels, along
with attention between video and audio features, AudioScope learns to identify
audio-visual similarity and to suppress off-screen sounds. We demonstrate the
effectiveness of our approach using a dataset of video clips extracted from
open-domain YFCC100m video data. This dataset contains a wide diversity of
sound classes recorded in unconstrained conditions, making the application of
previous methods unsuitable. For evaluation and semi-supervised experiments, we
collected human labels for presence of on-screen and off-screen sounds on a
small subset of clips.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.01454</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.01454</id><submitter>Xianyi Cheng</submitter><version version="v1"><date>Tue, 3 Nov 2020 03:45:33 GMT</date><size>2511kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 05:30:53 GMT</date><size>2502kb</size><source_type>D</source_type></version><title>Contact Mode Guided Sampling-Based Planning for Quasistatic Dexterous
  Manipulation in 2D</title><authors>Xianyi Cheng, Eric Huang, Yifan Hou, Matthew T. Mason</authors><categories>cs.RO</categories><comments>IEEE International Conference on Robotics and Automation (ICRA) 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discontinuities and multi-modality introduced by contacts make
manipulation planning challenging. Many previous works avoid this problem by
pre-designing a set of high-level motion primitives like grasping and pushing.
However, such motion primitives are often not adequate to describe dexterous
manipulation motions. In this work, we propose a method for dexterous
manipulation planning at a more primitive level. The key idea is to use contact
modes to guide the search in a sampling-based planning framework. Our method
can automatically generate contact transitions and motion trajectories under
the quasistatic assumption. In the experiments, this method sometimes generates
motions that are often pre-designed as motion primitives, as well as dexterous
motions that are more task-specific.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.01880</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.01880</id><submitter>Nikos Pitsillos</submitter><version version="v1"><date>Tue, 3 Nov 2020 17:57:28 GMT</date><size>2328kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 17:44:16 GMT</date><size>2333kb</size><source_type>D</source_type></version><title>Intrinsic Robotic Introspection: Learning Internal States From Neuron
  Activations</title><authors>Nikos Pitsillos, Ameya Pore, Bjorn Sand Jensen, Gerardo
  Aragon-Camarasa</authors><categories>cs.RO cs.AI</categories><comments>Paper accepted at the International Conference on Development and
  Learning (ICDL) 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an introspective framework inspired by the process of how humans
perform introspection. Our working assumption is that neural network
activations encode information, and building internal states from these
activations can improve the performance of an actor-critic model. We perform
experiments where we first train a Variational Autoencoder model to reconstruct
the activations of a feature extraction network and use the latent space to
improve the performance of an actor-critic when deciding which low-level
robotic behaviour to execute. We show that internal states reduce the number of
episodes needed by about 1300 episodes while training an actor-critic, denoting
faster convergence to get a high success value while completing a robotic task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02061</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02061</id><submitter>Zhichao Liu</submitter><version version="v1"><date>Tue, 3 Nov 2020 23:30:41 GMT</date><size>10683kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 22 Mar 2021 00:22:11 GMT</date><size>10683kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 01:34:29 GMT</date><size>10684kb</size><source_type>D</source_type></version><title>Toward Impact-resilient Quadrotor Design, Collision Characterization and
  Recovery Control to Sustain Flight after Collisions</title><authors>Zhichao Liu and Konstantinos Karydis</authors><categories>cs.RO</categories><comments>7 pages, 11 figures, Accepted to ICRA 2021, Supplementary video
  https://youtu.be/Kp4K7T1uvrU</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collision detection and recovery for aerial robots remain a challenge because
of the limited space for sensors and local stability of the flight controller.
We introduce a novel collision-resilient quadrotor that features a compliant
arm design to enable free flight while allowing for one passive degree of
freedom to absorb shocks. We further propose a novel collision detection and
characterization method based on Hall sensors, as well as a new recovery
control method to generate and track a smooth trajectory after a collision
occurs. Experimental results demonstrate that the robot can detect and recover
from high-speed collisions with various obstacles such as walls and poles.
Moreover, it can survive collisions that are hard to detect with existing
methods based on IMU data and contact models, for example, when colliding with
unstructured surfaces, or being hit by a moving obstacle while hovering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02100</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02100</id><submitter>Lin Meng</submitter><version version="v1"><date>Wed, 4 Nov 2020 02:26:53 GMT</date><size>900kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 19:30:24 GMT</date><size>1110kb</size><source_type>D</source_type></version><title>Deoscillated Graph Collaborative Filtering</title><authors>Zhiwei Liu, Lin Meng, Fei Jiang, Jiawei Zhang, Philip S. Yu</authors><categories>cs.IR cs.CY</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative Filtering (CF) signals are crucial for a Recommender
System~(RS) model to learn user and item embeddings. High-order information can
alleviate the cold-start issue of CF-based methods, which is modelled through
propagating the information over the user-item bipartite graph. Recent Graph
Neural Networks~(GNNs) propose to stack multiple aggregation layers to
propagate high-order signals. However, the oscillation problem, varying
locality of bipartite graph, and the fix propagation pattern spoil the ability
of multi-layer structure to propagate information. The oscillation problem
results from the bipartite structure, as the information from users only
propagates to items. Besides oscillation problem, varying locality suggests the
density of nodes should be considered in the propagation process. Moreover, the
layer-fixed propagation pattern introduces redundant information between
layers. In order to tackle these problems, we propose a new RS model, named as
\textbf{D}eoscillated \textbf{G}raph \textbf{C}ollaborative
\textbf{F}iltering~(DGCF). We introduce cross-hop propagation layers in it to
break the bipartite propagating structure, thus resolving the oscillation
problem. Additionally, we design innovative locality-adaptive layers which
adaptively propagate information. Stacking multiple cross-hop propagation
layers and locality layers constitutes the DGCF model, which models high-order
CF signals adaptively to the locality of nodes and layers. Extensive
experiments on real-world datasets show the effectiveness of DGCF. Detailed
analyses indicate that DGCF solves oscillation problem, adaptively learns local
factor, and has layer-wise propagation pattern. Our code is available online at
https://github.com/JimLiu96/DeosciRec.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02313</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02313</id><submitter>Suthee Ruangwises</submitter><version version="v1"><date>Tue, 3 Nov 2020 10:51:00 GMT</date><size>12kb</size></version><version version="v2"><date>Thu, 5 Nov 2020 13:36:32 GMT</date><size>12kb</size></version><version version="v3"><date>Tue, 16 Feb 2021 17:33:40 GMT</date><size>13kb</size></version><version version="v4"><date>Sun, 30 May 2021 15:50:24 GMT</date><size>13kb</size></version><title>Physical ZKP for Connected Spanning Subgraph: Applications to Bridges
  Puzzle and Other Problems</title><authors>Suthee Ruangwises, Toshiya Itoh</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An undirected graph $G$ is known to both the prover $P$ and the verifier $V$,
but only $P$ knows a subgraph $H$ of $G$. Without revealing any information
about $H$, $P$ wants to convince $V$ that $H$ is a connected spanning subgraph
of $G$, i.e. $H$ is connected and contains all vertices of $G$. In this paper,
we propose a physical protocol of zero-knowledge proof for this condition using
a deck of cards, which enables $P$ to physically show that $H$ satisfies the
condition without revealing it. We also show applications of this protocol to
verify solutions of three well-known NP-complete problems: the Hamiltonian
cycle problem, the maximum leaf spanning tree problem, and a logic puzzle
called Bridges.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02407</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02407</id><submitter>Jiahao Chen</submitter><version version="v1"><date>Wed, 4 Nov 2020 17:00:54 GMT</date><size>1005kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 00:06:53 GMT</date><size>496kb</size><source_type>D</source_type></version><title>Debiasing classifiers: is reality at variance with expectation?</title><authors>Ashrya Agrawal and Florian Pfisterer and Bernd Bischl and Francois
  Buet-Golfouse and Srijan Sood and Jiahao Chen and Sameena Shah and Sebastian
  Vollmer</authors><categories>cs.LG cs.CY econ.EM</categories><comments>13 pages, under review</comments><msc-class>68T01, 68Q32, 68T05</msc-class><acm-class>G.4; I.2.0; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an empirical study of debiasing methods for classifiers, showing
that debiasers often fail in practice to generalize out-of-sample, and can in
fact make fairness worse rather than better. A rigorous evaluation of the
debiasing treatment effect requires extensive cross-validation beyond what is
usually done. We demonstrate that this phenomenon can be explained as a
consequence of bias-variance trade-off, with an increase in variance
necessitated by imposing a fairness constraint. Follow-up experiments validate
the theoretical prediction that the estimation variance depends strongly on the
base rates of the protected class. Considering fairness--performance trade-offs
justifies the counterintuitive notion that partial debiasing can actually yield
better results in practice on out-of-sample data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02593</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02593</id><submitter>Chunting Zhou</submitter><version version="v1"><date>Thu, 5 Nov 2020 00:18:53 GMT</date><size>267kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 25 Dec 2020 21:05:03 GMT</date><size>302kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 20:26:55 GMT</date><size>300kb</size><source_type>D</source_type></version><title>Detecting Hallucinated Content in Conditional Neural Sequence Generation</title><authors>Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke
  Zettlemoyer, Marjan Ghazvininejad</authors><categories>cs.CL cs.AI</categories><comments>Accepted by ACL-Finding 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural sequence models can generate highly fluent sentences, but recent
studies have also shown that they are also prone to hallucinate additional
content not supported by the input. These variety of fluent but wrong outputs
are particularly problematic, as it will not be possible for users to tell they
are being presented incorrect content. To detect these errors, we propose a
task to predict whether each token in the output sequence is hallucinated (not
contained in the input) and collect new manually annotated evaluation sets for
this task. We also introduce a method for learning to detect hallucinations
using pretrained language models fine tuned on synthetic data that includes
automatically inserted hallucinations Experiments on machine translation (MT)
and abstractive summarization demonstrate that our proposed approach
consistently outperforms strong baselines on all benchmark datasets. We further
demonstrate how to use the token-level hallucination labels to define a
fine-grained loss over the target sequence in low-resource MT and achieve
significant improvements over strong baseline methods. We also apply our method
to word-level quality estimation for MT and show its effectiveness in both
supervised and unsupervised settings. Codes and data available at
https://github.com/violet-zct/fairseq-detect-hallucination.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02930</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02930</id><submitter>Ranya Aloufi</submitter><version version="v1"><date>Wed, 4 Nov 2020 14:11:35 GMT</date><size>4392kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 20:32:21 GMT</date><size>2864kb</size><source_type>D</source_type></version><title>Paralinguistic Privacy Protection at the Edge</title><authors>Ranya Aloufi, Hamed Haddadi, David Boyle</authors><categories>cs.CL</categories><comments>14 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2007.15064</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice user interfaces and digital assistants are rapidly entering our lives
and becoming singular touch points spanning our devices. These always-on
services capture and transmit our audio data to powerful cloud services for
further processing and subsequent actions. Our voices and raw audio signals
collected through these devices contain a host of sensitive paralinguistic
information that is transmitted to service providers regardless of deliberate
or false triggers. As our emotional patterns and sensitive attributes like our
identity, gender, mental well-being, are easily inferred using deep acoustic
models, we encounter a new generation of privacy risks by using these services.
One approach to mitigate the risk of paralinguistic-based privacy breaches is
to exploit a combination of cloud-based processing with privacy-preserving,
on-device paralinguistic information learning and filtering before transmitting
voice data. In this paper we introduce EDGY, a configurable, lightweight,
disentangled representation learning framework that transforms and filters
high-dimensional voice data to identify and contain sensitive attributes at the
edge prior to offloading to the cloud. We evaluate EDGY's on-device performance
and explore optimization techniques, including model quantization and knowledge
distillation, to enable private, accurate and efficient representation learning
on resource-constrained devices. Our results show that EDGY runs in tens of
milliseconds with 0.2% relative improvement in ABX score or minimal performance
penalties in learning linguistic representations from raw voice signals, using
a CPU and a single-core ARM processor without specialized hardware.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02980</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02980</id><submitter>Suthee Ruangwises</submitter><version version="v1"><date>Thu, 5 Nov 2020 17:12:09 GMT</date><size>9kb</size></version><version version="v2"><date>Fri, 25 Dec 2020 20:57:44 GMT</date><size>10kb</size></version><version version="v3"><date>Sun, 30 May 2021 15:53:09 GMT</date><size>10kb</size></version><title>Using Five Cards to Encode Each Integer in $\mathbb{Z}/6\mathbb{Z}$</title><authors>Suthee Ruangwises</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in secure multi-party computation using a deck of playing cards,
often called card-based cryptography, dates back to 1989 when Den Boer
introduced the &quot;five-card trick&quot; to compute the logical AND function. Since
then, many protocols to compute different functions have been developed. In
this paper, we propose a new encoding scheme using five cards to encode each
integer in $\mathbb{Z}/6\mathbb{Z}$. Using this encoding scheme, we develop
protocols that can copy a commitment with 13 cards, add two integers with 10
cards, and multiply two integers with 16 cards. All of our protocols are the
currently best known protocols in terms of the required number of cards. Our
encoding scheme can also be generalized to encode integers in
$\mathbb{Z}/n\mathbb{Z}$ for other values of $n$ as well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.03778</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.03778</id><submitter>Karol W\k{e}grzycki</submitter><version version="v1"><date>Sat, 7 Nov 2020 13:50:43 GMT</date><size>870kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 14:59:33 GMT</date><size>2229kb</size><source_type>D</source_type></version><title>A Gap-ETH-Tight Approximation Scheme for Euclidean TSP</title><authors>S\'andor Kisfaludi-Bak, Jesper Nederlof, Karol W\k{e}grzycki</authors><categories>cs.CG cs.CC cs.DS</categories><comments>43 pages, faster algorithms for Euclidean and Rectilinear Steiner
  Tree</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classic task of finding the shortest tour of $n$ points in
$d$-dimensional Euclidean space, for any fixed constant $d \geq 2$. We
determine the optimal dependence on $\varepsilon$ in the running time of an
algorithm that computes a $(1+\varepsilon)$-approximate tour, under a plausible
assumption. Specifically, we give an algorithm that runs in
$2^{O(1/\varepsilon^{d-1})} n\log n$ time. This improves the previously
smallest dependence on $\varepsilon$ in the running time
$(1/\varepsilon)^{O(1/\varepsilon^{d-1})}n \log n$ of the algorithm by Rao and
Smith (STOC 1998). We also show that a
$2^{o(1/\varepsilon^{d-1})}\mathrm{poly}(n)$ algorithm would violate the
Gap-Exponential Time Hypothesis (Gap-ETH).
  Our new algorithm builds upon the celebrated quadtree-based methods initially
proposed by Arora (J. ACM 1998), but it adds a new idea that we call
sparsity-sensitive patching. On a high level this lets the granularity with
which we simplify the tour depend on how sparse it is locally. We demonstrate
that our technique extends to other problems, by showing that for Steiner Tree
and Rectilinear Steiner Tree it yields the same running time. We complement our
results with a matching Gap-ETH lower bound for Rectilinear Steiner Tree.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.04026</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.04026</id><submitter>Alexander Terenin</submitter><version version="v1"><date>Sun, 8 Nov 2020 17:09:37 GMT</date><size>1050kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:38:11 GMT</date><size>751kb</size><source_type>D</source_type></version><title>Pathwise Conditioning of Gaussian Processes</title><authors>James T. Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter
  Mostowsky, and Marc Peter Deisenroth</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><journal-ref>Journal of Machine Learning Research (2021)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As Gaussian processes are used to answer increasingly complex questions,
analytic solutions become scarcer and scarcer. Monte Carlo methods act as a
convenient bridge for connecting intractable mathematical expressions with
actionable estimates via sampling. Conventional approaches for simulating
Gaussian process posteriors view samples as draws from marginal distributions
of process values at finite sets of input locations. This distribution-centric
characterization leads to generative strategies that scale cubically in the
size of the desired random vector. These methods are prohibitively expensive in
cases where we would, ideally, like to draw high-dimensional vectors or even
continuous sample paths. In this work, we investigate a different line of
reasoning: rather than focusing on distributions, we articulate Gaussian
conditionals at the level of random variables. We show how this pathwise
interpretation of conditioning gives rise to a general family of approximations
that lend themselves to efficiently sampling Gaussian process posteriors.
Starting from first principles, we derive these methods and analyze the
approximation errors they introduce. We, then, ground these results by
exploring the practical implications of pathwise conditioning in various
applied settings, such as global optimization and reinforcement learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.04820</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.04820</id><submitter>Shuijing Liu</submitter><version version="v1"><date>Mon, 9 Nov 2020 23:15:31 GMT</date><size>3950kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 25 Mar 2021 20:53:01 GMT</date><size>3840kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 15:51:50 GMT</date><size>3840kb</size><source_type>D</source_type></version><title>Decentralized Structural-RNN for Robot Crowd Navigation with Deep
  Reinforcement Learning</title><authors>Shuijing Liu, Peixin Chang, Weihang Liang, Neeloy Chakraborty,
  Katherine Driggs-Campbell</authors><categories>cs.RO cs.AI cs.LG</categories><comments>Published as a conference paper in IEEE International Conference on
  Robotics and Automation (ICRA), 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safe and efficient navigation through human crowds is an essential capability
for mobile robots. Previous work on robot crowd navigation assumes that the
dynamics of all agents are known and well-defined. In addition, the performance
of previous methods deteriorates in partially observable environments and
environments with dense crowds. To tackle these problems, we propose
decentralized structural-Recurrent Neural Network (DS-RNN), a novel network
that reasons about spatial and temporal relationships for robot decision making
in crowd navigation. We train our network with model-free deep reinforcement
learning without any expert supervision. We demonstrate that our model
outperforms previous methods in challenging crowd navigation scenarios. We
successfully transfer the policy learned in the simulator to a real-world
TurtleBot 2i.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.04917</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.04917</id><submitter>Ramaravind Kommiya Mothilal</submitter><version version="v1"><date>Tue, 10 Nov 2020 05:41:43 GMT</date><size>992kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Feb 2021 12:18:19 GMT</date><size>1159kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 17:49:39 GMT</date><size>12450kb</size><source_type>D</source_type></version><title>Towards Unifying Feature Attribution and Counterfactual Explanations:
  Different Means to the Same End</title><authors>Ramaravind Kommiya Mothilal and Divyat Mahajan and Chenhao Tan and
  Amit Sharma</authors><categories>cs.LG cs.CY</categories><comments>15 pages, 10 figures</comments><doi>10.1145/3461702.3462597</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature attributions and counterfactual explanations are popular approaches
to explain a ML model. The former assigns an importance score to each input
feature, while the latter provides input examples with minimal changes to alter
the model's predictions. To unify these approaches, we provide an
interpretation based on the actual causality framework and present two key
results in terms of their use. First, we present a method to generate feature
attribution explanations from a set of counterfactual examples. These feature
attributions convey how important a feature is to changing the classification
outcome of a model, especially on whether a subset of features is necessary
and/or sufficient for that change, which attribution-based methods are unable
to provide. Second, we show how counterfactual examples can be used to evaluate
the goodness of an attribution-based explanation in terms of its necessity and
sufficiency. As a result, we highlight the complementarity of these two
approaches. Our evaluation on three benchmark datasets - Adult-Income,
LendingClub, and German-Credit - confirms the complementarity. Feature
attribution methods like LIME and SHAP and counterfactual explanation methods
like Wachter et al. and DiCE often do not agree on feature importance rankings.
In addition, by restricting the features that can be modified for generating
counterfactual examples, we find that the top-k features from LIME or SHAP are
often neither necessary nor sufficient explanations of a model's prediction.
Finally, we present a case study of different explanation methods on a
real-world hospital triage problem
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.04942</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.04942</id><submitter>Philipp Schlicht</submitter><version version="v1"><date>Tue, 10 Nov 2020 07:06:34 GMT</date><size>20kb</size></version><version version="v2"><date>Fri, 28 May 2021 19:10:33 GMT</date><size>20kb</size></version><title>Decision times of infinite computations</title><authors>Merlin Carl, Philipp Schlicht, Philip Welch</authors><categories>math.LO cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The decision time of an infinite time algorithm is the supremum of its
halting times over all real inputs. The decision time of a set of reals is the
least decision time of an algorithm that decides the set; semidecision times of
semidecidable sets are defined similary. It is not hard to see that $\omega_1$
is the maximal decision time of sets of reals. Our main results determine the
supremum of countable decision times as $\sigma$ and that of countable
semidecision times as $\tau$, where $\sigma$ and $\tau$ denote the suprema of
$\Sigma_1$- and $\Sigma_2$-definable ordinals, respectively, over
$L_{\omega_1}$. We further compute analogous suprema for singletons.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05049</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05049</id><submitter>Zongheng Tang</submitter><version version="v1"><date>Tue, 10 Nov 2020 11:23:38 GMT</date><size>9236kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 06:51:34 GMT</date><size>12193kb</size><source_type>D</source_type></version><title>Human-centric Spatio-Temporal Video Grounding With Visual Transformers</title><authors>Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu
  Jiang, Qian Yu, Dong Xu</authors><categories>cs.CV cs.AI cs.MM</categories><comments>Accept at TCSVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we introduce a novel task - Humancentric Spatio-Temporal Video
Grounding (HC-STVG). Unlike the existing referring expression tasks in images
or videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal
tube of the target person from an untrimmed video based on a given textural
description. This task is useful, especially for healthcare and
security-related applications, where the surveillance videos can be extremely
long but only a specific person during a specific period of time is concerned.
HC-STVG is a video grounding task that requires both spatial (where) and
temporal (when) localization. Unfortunately, the existing grounding methods
cannot handle this task well. We tackle this task by proposing an effective
baseline method named Spatio-Temporal Grounding with Visual Transformers
(STGVT), which utilizes Visual Transformers to extract cross-modal
representations for video-sentence matching and temporal localization. To
facilitate this task, we also contribute an HC-STVG dataset consisting of 5,660
video-sentence pairs on complex multi-person scenes. Specifically, each video
lasts for 20 seconds, pairing with a natural query sentence with an average of
17.25 words. Extensive experiments are conducted on this dataset, demonstrating
the newly-proposed method outperforms the existing baseline methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05357</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05357</id><submitter>Barbara Franci Dott.</submitter><version version="v1"><date>Tue, 10 Nov 2020 19:13:42 GMT</date><size>1498kb</size></version><version version="v2"><date>Mon, 31 May 2021 18:02:29 GMT</date><size>1321kb</size></version><title>Stochastic generalized Nash equilibrium seeking under partial-decision
  information</title><authors>Barbara Franci and Sergio Grammatico</authors><categories>math.OC cs.GT cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider for the first time a stochastic generalized Nash equilibrium
problem, i.e., with expected-value cost functions and joint feasibility
constraints, under partial-decision information, meaning that the agents
communicate only with some trusted neighbours. We propose several distributed
algorithms for network games and aggregative games that we show being special
instances of a preconditioned forward-backward splitting method. We prove that
the algorithms converge to a generalized Nash equilibrium when the forward
operator is restricted cocoercive by using the stochastic approximation scheme
with variance reduction to estimate the expected value of the pseudogradient.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05551</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05551</id><submitter>Jagadeesh Malla Sree Mr</submitter><version version="v1"><date>Wed, 11 Nov 2020 05:20:39 GMT</date><size>600kb</size></version><title>NIT COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for
  Identify Informative COVID-19 English Tweets</title><authors>Jagadeesh M S, Alphonse P J A</authors><categories>cs.CL</categories><comments>5 pages, one figures, conference</comments><doi>10.18653/v1/2020.wnut-1.66</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the model submitted by the NIT_COVID-19 team for
identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared
task addresses the problem of automatically identifying whether an English
tweet related to informative (novel coronavirus) or not. These informative
tweets provide information about recovered, confirmed, suspected, and death
cases as well as the location or travel history of the cases. The proposed
approach includes pre-processing techniques and pre-trained RoBERTa with
suitable hyperparameters for English coronavirus tweet classification. The
performance achieved by the proposed model for shared task WNUT 2020 Task2 is
89.14% in the F1-score metric.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05606</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05606</id><submitter>Giulio Rossetti</submitter><version version="v1"><date>Wed, 11 Nov 2020 07:37:39 GMT</date><size>3225kb</size><source_type>D</source_type></version><title>UTLDR: an agent-based framework for modeling infectious diseases and
  public interventions</title><authors>Giulio Rossetti, Letizia Milli, Salvatore Citraro, Virginia Morini</authors><categories>cs.SI physics.soc-ph</categories><msc-class>05C85, 60J60, 90C35</msc-class><acm-class>G.2.2; F.2.1</acm-class><journal-ref>Journal of Intelligent Information Systems, 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, due to the SARS-CoV-2 pandemic, epidemic modelling is experiencing
a constantly growing interest from researchers of heterogeneous fields of
study. Indeed, the vast literature on computational epidemiology offers solid
grounds for analytical studies and the definition of novel models aimed at both
predictive and prescriptive scenario descriptions. To ease the access to
diffusion modelling, several programming libraries and tools have been proposed
during the last decade: however, to the best of our knowledge, none of them is
explicitly designed to allow its users to integrate public interventions in
their model. In this work, we introduce UTLDR, a framework that can simulate
the effects of several public interventions (and their combinations) on the
unfolding of epidemic processes. UTLDR enables the design of compartmental
models incrementally and to simulate them over complex interaction network
topologies. Moreover, it allows integrating external information on the
analyzed population (e.g., age, gender, geographical allocation, and mobility
patterns\dots) and to use it to stratify and refine the designed model. After
introducing the framework, we provide a few case studies to underline its
flexibility and expressive power.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05707</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05707</id><submitter>Goeric Huybrechts</submitter><version version="v1"><date>Wed, 11 Nov 2020 11:22:37 GMT</date><size>231kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 20:18:08 GMT</date><size>232kb</size><source_type>D</source_type></version><title>Low-resource expressive text-to-speech using data augmentation</title><authors>Goeric Huybrechts, Thomas Merritt, Giulia Comini, Bartek Perz, Raahil
  Shah, Jaime Lorenzo-Trueba</authors><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While recent neural text-to-speech (TTS) systems perform remarkably well,
they typically require a substantial amount of recordings from the target
speaker reading in the desired speaking style. In this work, we present a novel
3-step methodology to circumvent the costly operation of recording large
amounts of target data in order to build expressive style voices with as little
as 15 minutes of such recordings. First, we augment data via voice conversion
by leveraging recordings in the desired speaking style from other speakers.
Next, we use that synthetic data on top of the available recordings to train a
TTS model. Finally, we fine-tune that model to further increase quality. Our
evaluations show that the proposed changes bring significant improvements over
non-augmented models across many perceived aspects of synthesised speech. We
demonstrate the proposed approach on 2 styles (newscaster and conversational),
on various speakers, and on both single and multi-speaker models, illustrating
the robustness of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05869</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05869</id><submitter>Tengyu Xu</submitter><version version="v1"><date>Wed, 11 Nov 2020 16:05:14 GMT</date><size>63kb</size></version><version version="v2"><date>Tue, 17 Nov 2020 21:24:18 GMT</date><size>64kb</size></version><version version="v3"><date>Mon, 31 May 2021 04:41:09 GMT</date><size>111kb</size></version><title>CRPO: A New Approach for Safe Reinforcement Learning with Convergence
  Guarantee</title><authors>Tengyu Xu, Yingbin Liang, Guanghui Lan</authors><categories>cs.LG stat.ML</categories><comments>Published in ICML 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In safe reinforcement learning (SRL) problems, an agent explores the
environment to maximize an expected total reward and meanwhile avoids violation
of certain constraints on a number of expected total costs. In general, such
SRL problems have nonconvex objective functions subject to multiple nonconvex
constraints, and hence are very challenging to solve, particularly to provide a
globally optimal policy. Many popular SRL algorithms adopt a primal-dual
structure which utilizes the updating of dual variables for satisfying the
constraints. In contrast, we propose a primal approach, called
constraint-rectified policy optimization (CRPO), which updates the policy
alternatingly between objective improvement and constraint satisfaction. CRPO
provides a primal-type algorithmic framework to solve SRL problems, where each
policy update can take any variant of policy optimization step. To demonstrate
the theoretical performance of CRPO, we adopt natural policy gradient (NPG) for
each policy update step and show that CRPO achieves an
$\mathcal{O}(1/\sqrt{T})$ convergence rate to the global optimal policy in the
constrained policy set and an $\mathcal{O}(1/\sqrt{T})$ error bound on
constraint satisfaction. This is the first finite-time analysis of primal SRL
algorithms with global optimality guarantee. Our empirical results demonstrate
that CRPO can outperform the existing primal-dual baseline algorithms
significantly.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05878</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05878</id><submitter>Gregory Gutin</submitter><version version="v1"><date>Wed, 11 Nov 2020 16:18:09 GMT</date><size>5kb</size></version><version version="v2"><date>Sun, 13 Dec 2020 17:01:21 GMT</date><size>7kb</size></version><version version="v3"><date>Wed, 2 Jun 2021 16:04:25 GMT</date><size>7kb</size></version><title>Kings in Multipartite Hypertournaments</title><authors>Jiangdong Ai, Stefanie Gerke, Gregory Gutin</authors><categories>math.CO cs.DM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In his paper &quot;Kings in Bipartite Hypertournaments&quot; (Graphs $\&amp;$ Combinatorics
35, 2019), Petrovic stated two conjectures on 4-kings in multipartite
hypertournaments. We prove one of these conjectures and give counterexamples
for the other.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.06077</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.06077</id><submitter>Sai Mang Pun</submitter><version version="v1"><date>Wed, 11 Nov 2020 21:09:00 GMT</date><size>3536kb</size></version><version version="v2"><date>Tue, 16 Mar 2021 22:56:31 GMT</date><size>3699kb</size></version><title>Temporal Splitting algorithms for non-stationary multiscale problems</title><authors>Yalchin Efendiev, Sai-Mang Pun, Petr N. Vabishchevich</authors><categories>math.NA cs.NA</categories><comments>16 pages, 8 figures</comments><msc-class>65M22</msc-class><doi>10.1016/j.jcp.2021.110375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study temporal splitting algorithms for multiscale
problems. The exact fine-grid spatial problems typically require some reduction
in degrees of freedom. Multiscale algorithms are designed to represent the
fine-scale details on a coarse grid and, thus, reduce the problems' size. When
solving time-dependent problems, one can take advantage of the multiscale
decomposition of the solution and perform temporal splitting by solving
smaller-dimensional problems, which is studied in the paper. In the proposed
approach, we consider the temporal splitting based on various low dimensional
spatial approximations. Because a multiscale spatial splitting gives a &quot;good&quot;
decomposition of the solution space, one can achieve an efficient
implicit-explicit temporal discretization. We present a recently developed
theoretical result in our earlier work and adopt it in this paper for
multiscale problems. Numerical results are presented to demonstrate the
efficiency of the proposed splitting algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.06763</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.06763</id><submitter>Xuan Zhang</submitter><version version="v1"><date>Fri, 13 Nov 2020 05:03:35 GMT</date><size>43kb</size></version><version version="v2"><date>Mon, 31 May 2021 19:18:18 GMT</date><size>45kb</size></version><title>Affinely representable lattices, stable matchings, and choice functions</title><authors>Yuri Faenza, Xuan Zhang</authors><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Birkhoff's representation theorem (Birkhoff, 1937) defines a bijection
between elements of a distributive lattice and the family of upper sets of an
associated poset. Although not used explicitly, this result is at the backbone
of the combinatorial algorithm by Irving et al. (1987) for maximizing a linear
function over the set of stable matchings in Gale and Shapley's stable marriage
model (Gale and Shapley, 1962). In this paper, we introduce a property of
distributive lattices, which we term as affine representability, and show its
role in efficiently solving linear optimization problems over the elements of a
distributive lattice, as well as describing the convex hull of the
characteristic vectors of the lattice elements. We apply this concept to the
stable matching model with path-independent quota-filling choice functions,
thus giving efficient algorithms and a compact polyhedral description for this
model. To the best of our knowledge, this model generalizes all models from the
literature for which similar results were known, and our paper is the first
that proposes efficient algorithms for stable matchings with choice functions,
beyond classical extensions of the Deferred Acceptance algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.07143</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.07143</id><submitter>Gabriele Fici</submitter><version version="v1"><date>Fri, 13 Nov 2020 21:57:18 GMT</date><size>84kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:15:15 GMT</date><size>17kb</size></version><title>Substring Query Complexity of String Reconstruction</title><authors>Gabriele Fici, Nicola Prezza, Rossano Venturini</authors><categories>cs.DS</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose an oracle knows a string $S$ that is unknown to us and that we want
to determine. The oracle can answer queries of the form ``Is $s$ a substring of
$S$?''. The \emph{Substring Query Complexity} of a string $S$, denoted
$\chi(S)$, is the minimum number of adaptive substring queries that are needed
to exactly reconstruct (or learn) $S$. It has been introduced in 1995 by Skiena
and Sundaram, who showed that $\chi(S) \geq \sigma n/4 -O(n)$ in the worst
case, where $\sigma$ is the size of the alphabet of $S$ and $n$ its length, and
gave an algorithm that spends $(\sigma-1)n+O(\sigma \sqrt{n})$ queries to
reconstruct $S$.
  We show that for any binary string $S$, $\chi(S)$ is asymptotically equal to
the Kolmogorov complexity of $S$ and therefore lower bounds any other measure
of compressibility. However, since this result does not yield an efficient
algorithm for the reconstruction, we also present new algorithms which require
a number of substring queries bounded by other known measures of complexity,
e.g., the number $rle$ of runs in $S$, the size $g$ of the smallest grammar
producing (only) $S$, or the size $z_{no}$ of the non-overlapping LZ77
factorization of $S$. We first show that any string of length $n$ over an
integer alphabet of size $\sigma$ with $rle$ runs can be reconstructed with
$q=O(rle (\sigma + \log \frac{n}{rle}))$ substring queries in linear time and
space. We then present an algorithm that spends $q \in O(\sigma g\log n)
\subseteq O(\sigma z_{no}\log (n/z_{no})\log n)$ substring queries and runs in
$O(n(\log n + \log \sigma)+ q)$ time using linear space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.07542</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.07542</id><submitter>Ina Kodrasi</submitter><version version="v1"><date>Sun, 15 Nov 2020 14:48:28 GMT</date><size>76kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 8 Feb 2021 12:16:22 GMT</date><size>76kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 08:36:58 GMT</date><size>79kb</size><source_type>D</source_type></version><title>Automatic and perceptual discrimination between dysarthria, apraxia of
  speech, and neurotypical speech</title><authors>I. Kodrasi and M. Pernon and M. Laganaro and H. Bourlard</authors><categories>cs.SD eess.AS</categories><comments>ICASSP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic techniques in the context of motor speech disorders (MSDs) are
typically two-class techniques aiming to discriminate between dysarthria and
neurotypical speech or between dysarthria and apraxia of speech (AoS). Further,
although such techniques are proposed to support the perceptual assessment of
clinicians, the automatic and perceptual classification accuracy has never been
compared. In this paper, we investigate a three-class automatic technique and a
set of handcrafted features for the discrimination of dysarthria, AoS and
neurotypical speech. Instead of following the commonly used One-versus-One or
One-versus-Rest approaches for multi-class classification, a hierarchical
approach is proposed. Further, a perceptual study is conducted where speech and
language pathologists are asked to listen to recordings of dysarthria, AoS, and
neurotypical speech and decide which class the recordings belong to. The
proposed automatic technique is evaluated on the same recordings and the
automatic and perceptual classification performance are compared. The presented
results show that the hierarchical classification approach yields a higher
classification accuracy than baseline One-versus-One and One-versus-Rest
approaches. Further, the presented results show that the automatic approach
yields a higher classification accuracy than the perceptual assessment of
speech and language pathologists, demonstrating the potential advantages of
integrating automatic tools in clinical practice.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.08019</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.08019</id><submitter>Anjith George</submitter><version version="v1"><date>Mon, 16 Nov 2020 15:14:59 GMT</date><size>2312kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 10:37:30 GMT</date><size>13793kb</size><source_type>D</source_type></version><title>On the Effectiveness of Vision Transformers for Zero-shot Face
  Anti-Spoofing</title><authors>Anjith George and Sebastien Marcel</authors><categories>cs.CV</categories><comments>8 pages, 3 figures, Accepted for Publication in IJCB2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The vulnerability of face recognition systems to presentation attacks has
limited their application in security-critical scenarios. Automatic methods of
detecting such malicious attempts are essential for the safe use of facial
recognition technology. Although various methods have been suggested for
detecting such attacks, most of them over-fit the training set and fail in
generalizing to unseen attacks and environments. In this work, we use transfer
learning from the vision transformer model for the zero-shot anti-spoofing
task. The effectiveness of the proposed approach is demonstrated through
experiments in publicly available datasets. The proposed approach outperforms
the state-of-the-art methods in the zero-shot protocols in the HQ-WMCA and
SiW-M datasets by a large margin. Besides, the model achieves a significant
boost in cross-database performance as well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.08826</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.08826</id><submitter>Udaranga Wickramasinghe</submitter><version version="v1"><date>Tue, 17 Nov 2020 18:48:28 GMT</date><size>15104kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 18 Nov 2020 18:25:58 GMT</date><size>15196kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 20 Nov 2020 23:19:36 GMT</date><size>18277kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 21:31:20 GMT</date><size>18284kb</size><source_type>D</source_type></version><title>Deep Active Surface Models</title><authors>Udaranga Wickramasinghe and Graham Knott and Pascal Fua</authors><categories>cs.CV cs.GR cs.LG</categories><comments>11 pages, 7 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09414</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09414</id><submitter>Burhaneddin Yaman</submitter><version version="v1"><date>Wed, 18 Nov 2020 17:22:21 GMT</date><size>571kb</size><source_type>D</source_type></version><title>Self-Supervised Physics-Guided Deep Learning Reconstruction For
  High-Resolution 3D LGE CMR</title><authors>Burhaneddin Yaman, Chetan Shenoy, Zilin Deng, Steen Moeller, Hossam
  El-Rewaidy, Reza Nezafat, and Mehmet Ak\c{c}akaya</authors><categories>eess.IV cs.CV cs.LG eess.SP physics.med-ph</categories><journal-ref>Proceedings of IEEE ISBI, 2021</journal-ref><doi>10.1109/ISBI48211.2021.9434054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Late gadolinium enhancement (LGE) cardiac MRI (CMR) is the clinical standard
for diagnosis of myocardial scar. 3D isotropic LGE CMR provides improved
coverage and resolution compared to 2D imaging. However, image acceleration is
required due to long scan times and contrast washout. Physics-guided deep
learning (PG-DL) approaches have recently emerged as an improved accelerated
MRI strategy. Training of PG-DL methods is typically performed in supervised
manner requiring fully-sampled data as reference, which is challenging in 3D
LGE CMR. Recently, a self-supervised learning approach was proposed to enable
training PG-DL techniques without fully-sampled data. In this work, we extend
this self-supervised learning approach to 3D imaging, while tackling challenges
related to small training database sizes of 3D volumes. Results and a reader
study on prospectively accelerated 3D LGE show that the proposed approach at
6-fold acceleration outperforms the clinically utilized compressed sensing
approach at 3-fold acceleration.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09545</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09545</id><submitter>Bo Xiong</submitter><version version="v1"><date>Wed, 18 Nov 2020 20:54:28 GMT</date><size>2646kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 09:07:44 GMT</date><size>680kb</size><source_type>D</source_type></version><title>MOFA: Modular Factorial Design for Hyperparameter Optimization</title><authors>Bo Xiong, Yimin Huang, Hanrong Ye, Steffen Staab, Zhenguo Li</authors><categories>cs.LG</categories><comments>13 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a novel and lightweight hyperparameter optimization (HPO)
method, MOdular FActorial Design (MOFA). MOFA pursues several rounds of HPO,
where each round alternates between exploration of hyperparameter space by
factorial design and exploitation of evaluation results by factorial analysis.
Each round first explores the configuration space by constructing a
low-discrepancy set of hyperparameters that cover this space well while
de-correlating hyperparameters, and then exploits evaluation results through
factorial analysis that determines which hyperparameters should be further
explored and which should become fixed in the next round. We prove that the
inference of MOFA achieves higher confidence than other sampling schemes. Each
individual round is highly parallelizable and hence offers major improvements
of efficiency compared to model-based methods. Empirical results show that MOFA
achieves better effectiveness and efficiency compared with state-of-the-art
methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09553</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09553</id><submitter>Yue Feng</submitter><version version="v1"><date>Wed, 18 Nov 2020 21:42:44 GMT</date><size>353kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 02:48:28 GMT</date><size>497kb</size><source_type>D</source_type></version><title>A Sequence-to-Sequence Approach to Dialogue State Tracking</title><authors>Yue Feng, Yang Wang, Hang Li</authors><categories>cs.CL cs.AI cs.IR</categories><comments>Accepted by ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is concerned with dialogue state tracking (DST) in a task-oriented
dialogue system. Building a DST module that is highly effective is still a
challenging issue, although significant progresses have been made recently.
This paper proposes a new approach to dialogue state tracking, referred to as
Seq2Seq-DU, which formalizes DST as a sequence-to-sequence problem. Seq2Seq-DU
employs two BERT-based encoders to respectively encode the utterances in the
dialogue and the descriptions of schemas, an attender to calculate attentions
between the utterance embeddings and the schema embeddings, and a decoder to
generate pointers to represent the current state of dialogue. Seq2Seq-DU has
the following advantages. It can jointly model intents, slots, and slot values;
it can leverage the rich representations of utterances and schemas based on
BERT; it can effectively deal with categorical and non-categorical slots, and
unseen schemas. In addition, Seq2Seq-DU can also be used in the NLU (natural
language understanding) module of a dialogue system. Experimental results on
benchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1,
WOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the
existing methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09641</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09641</id><submitter>Jos\'e Verschae</submitter><version version="v1"><date>Thu, 19 Nov 2020 04:15:48 GMT</date><size>57kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 16:20:10 GMT</date><size>70kb</size></version><title>On the geometry of symmetry breaking inequalities</title><authors>Jos\'e Verschae, Mat\'ias Villagra and L\'eonard von Niederh\&quot;ausern</authors><categories>cs.DM math.OC</categories><msc-class>52B15 (Primary), 90C10 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Breaking symmetries is a popular way of speeding up the branch-and-bound
method for symmetric integer programs. We study fundamental domains, which are
minimal and closed symmetry breaking polyhedra. Our long-term goal is to
understand the relationship between the complexity of such polyhedra and their
symmetry breaking capability.
  Borrowing ideas from geometric group theory, we provide structural properties
that relate the action of the group with the geometry of the facets of
fundamental domains. Inspired by these insights, we provide a new generalized
construction for fundamental domains, which we call generalized Dirichlet
domain (GDD). Our construction is recursive and exploits the coset
decomposition of the subgroups that fix given vectors in $\mathbb{R}^n$. We use
this construction to analyze a recently introduced set of symmetry breaking
inequalities by Salvagnin (2018) and Liberti and Ostrowski (2014), called
Schreier-Sims inequalities. In particular, this shows that every permutation
group admits a fundamental domain with less than $n$ facets. We also show that
this bound is tight.
  Finally, we prove that the Schreier-Sims inequalities can contain an
exponential number of isomorphic binary vectors for a given permutation group
$G$, which provides evidence of the lack of symmetry breaking effectiveness of
this fundamental domain. Conversely, a suitably constructed GDD for this $G$
has linearly many inequalities and contains unique representatives for
isomorphic binary vectors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09821</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09821</id><submitter>Jerry Swan</submitter><version version="v1"><date>Thu, 19 Nov 2020 13:49:05 GMT</date><size>410kb</size></version><version version="v2"><date>Fri, 20 Nov 2020 20:24:40 GMT</date><size>410kb</size></version><version version="v3"><date>Fri, 18 Dec 2020 10:52:28 GMT</date><size>410kb</size></version><version version="v4"><date>Thu, 3 Jun 2021 11:04:10 GMT</date><size>101kb</size></version><title>Metaheuristics &quot;In the Large&quot;</title><authors>Jerry Swan, Steven Adriaensen, Alexander E. I. Brownlee, Kevin
  Hammond, Colin G. Johnson, Ahmed Kheiri, Faustyna Krawiec, J. J. Merelo,
  Leandro L. Minku, Ender \&quot;Ozcan, Gisele L. Pappa, Pablo Garc\'ia-S\'anchez,
  Kenneth S\&quot;orensen, Stefan Vo{\ss}, Markus Wagner, David R. White</authors><categories>cs.NE cs.AI</categories><msc-class>68W99</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Following decades of sustained improvement, metaheuristics are one of the
great success stories of optimization research. However, in order for research
in metaheuristics to avoid fragmentation and a lack of reproducibility, there
is a pressing need for stronger scientific and computational infrastructure to
support the development, analysis and comparison of new approaches. We argue
that, via principled choice of infrastructure support, the field can pursue a
higher level of scientific enquiry. We describe our vision and report on
progress, showing how the adoption of common protocols for all metaheuristics
can help liberate the potential of the field, easing the exploration of the
design space of metaheuristics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10173</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10173</id><submitter>Ziyang Wang</submitter><version version="v1"><date>Fri, 20 Nov 2020 02:22:31 GMT</date><size>860kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 16:23:58 GMT</date><size>674kb</size><source_type>D</source_type></version><title>Exploring Global Information for Session-based Recommendation</title><authors>Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui
  Qiu, Shanshan Feng</authors><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session-based recommendation (SBR) is a challenging task, which aims at
recommending items based on anonymous behavior sequences. Most existing SBR
studies model the user preferences based only on the current session while
neglecting the item-transition information from the other sessions, which
suffer from the inability of modeling the complicated item-transition pattern.
To address the limitations, we introduce global item-transition information to
strength the modeling of the dynamic item-transition. For fully exploiting the
global item-transition information, two ways of exploring global information
for SBR are studied in this work. Specifically, we first propose a basic
GNN-based framework (BGNN), which solely uses session-level item-transition
information on session graph. Based on BGNN, we propose a novel approach,
called Session-based Recommendation with Global Information (SRGI), which
infers the user preferences via fully exploring global item-transitions over
all sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM),
which recursively incorporates the neighbor embeddings of each node on global
graph into the learning process of session level item representation; and (ii)
Constrained-based Model (SRGI-CM), which treats the global-level
item-transition information as a constraint to ensure the learned item
embeddings are consistent with the global item-transition. Extensive
experiments conducted on three popular benchmark datasets demonstrate that both
SRGI-FM and SRGI-CM outperform the state-of-the-art methods consistently.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10185</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10185</id><submitter>Zhouyong Liu</submitter><version version="v1"><date>Fri, 20 Nov 2020 02:52:53 GMT</date><size>23841kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 09:01:10 GMT</date><size>28778kb</size><source_type>D</source_type></version><title>ConvTransformer: A Convolutional Transformer Network for Video Frame
  Synthesis</title><authors>Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, Shilei Sun,
  Chunguo Li, Luxi Yang</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (CNNs) are powerful models that have
achieved excellent performance on difficult computer vision tasks. Although
CNNs perform well whenever large labeled training samples are available, they
work badly on video frame synthesis due to objects deforming and moving, scene
lighting changes, and cameras moving in video sequence. In this paper, we
present a novel and general end-to-end architecture, called convolutional
Transformer or ConvTransformer, for video frame sequence learning and video
frame synthesis. The core ingredient of ConvTransformer is the proposed
attention layer, i.e., multi-head convolutional self-attention layer, that
learns the sequential dependence of video sequence. ConvTransformer uses an
encoder, built upon multi-head convolutional self-attention layer, to encode
the sequential dependence between the input frames, and then a decoder decodes
the long-term dependence between the target synthesized frames and the input
frames. Experiments on video future frame extrapolation task show
ConvTransformer to be superior in quality while being more parallelizable to
recent approaches built upon convolutional LSTM (ConvLSTM). To the best of our
knowledge, this is the first time that ConvTransformer architecture is proposed
and applied to video frame synthesis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10282</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10282</id><submitter>Hang Liu</submitter><version version="v1"><date>Fri, 20 Nov 2020 08:54:13 GMT</date><size>368kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 8 Dec 2020 14:59:36 GMT</date><size>369kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 19 Mar 2021 09:25:59 GMT</date><size>416kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 14:47:11 GMT</date><size>444kb</size><source_type>D</source_type></version><title>Reconfigurable Intelligent Surface Enabled Federated Learning: A Unified
  Communication-Learning Design Approach</title><authors>Hang Liu, Xiaojun Yuan, Ying-Jun Angela Zhang</authors><categories>cs.IT cs.LG cs.NI eess.SP math.IT</categories><comments>Simulation codes are available at
  https://github.com/liuhang1994/RIS-FL. This work has been accepted by IEEE
  Transactions on Wireless Communications. Copyright may be transferred without
  notice, after which this version may no longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To exploit massive amounts of data generated at mobile edge networks,
federated learning (FL) has been proposed as an attractive substitute for
centralized machine learning (ML). By collaboratively training a shared
learning model at edge devices, FL avoids direct data transmission and thus
overcomes high communication latency and privacy issues as compared to
centralized ML. To improve the communication efficiency in FL model
aggregation, over-the-air computation has been introduced to support a large
number of simultaneous local model uploading by exploiting the inherent
superposition property of wireless channels. However, due to the heterogeneity
of communication capacities among edge devices, over-the-air FL suffers from
the straggler issue in which the device with the weakest channel acts as a
bottleneck of the model aggregation performance. This issue can be alleviated
by device selection to some extent, but the latter still suffers from a
tradeoff between data exploitation and model communication. In this paper, we
leverage the reconfigurable intelligent surface (RIS) technology to relieve the
straggler issue in over-the-air FL. Specifically, we develop a learning
analysis framework to quantitatively characterize the impact of device
selection and model aggregation error on the convergence of over-the-air FL.
Then, we formulate a unified communication-learning optimization problem to
jointly optimize device selection, over-the-air transceiver design, and RIS
configuration. Numerical experiments show that the proposed design achieves
substantial learning accuracy improvement compared with the state-of-the-art
approaches, especially when channel conditions vary dramatically across edge
devices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10406</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10406</id><submitter>Alex Bogatu</submitter><version version="v1"><date>Fri, 20 Nov 2020 13:47:11 GMT</date><size>1943kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 22 Feb 2021 19:29:35 GMT</date><size>1954kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 26 Feb 2021 11:13:27 GMT</date><size>1954kb</size><source_type>D</source_type></version><title>Cost-effective Variational Active Entity Resolution</title><authors>Alex Bogatu, Norman W. Paton, Mark Douthwaite, Stuart Davie, Andre
  Freitas</authors><categories>cs.LG cs.DB</categories><journal-ref>2021 IEEE 37th International Conference on Data Engineering (ICDE)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Accurately identifying different representations of the same real-world
entity is an integral part of data cleaning and many methods have been proposed
to accomplish it. The challenges of this entity resolution task that demand so
much research attention are often rooted in the task-specificity and
user-dependence of the process. Adopting deep learning techniques has the
potential to lessen these challenges. In this paper, we set out to devise an
entity resolution method that builds on the robustness conferred by deep
autoencoders to reduce human-involvement costs. Specifically, we reduce the
cost of training deep entity resolution models by performing unsupervised
representation learning. This unveils a transferability property of the
resulting model that can further reduce the cost of applying the approach to
new datasets by means of transfer learning. Finally, we reduce the cost of
labelling training data through an active learning approach that builds on the
properties conferred by the use of deep autoencoders. Empirical evaluation
confirms the accomplishment of our cost-reduction desideratum while achieving
comparable effectiveness with state-of-the-art alternatives.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10838</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10838</id><submitter>Raman Goyal</submitter><version version="v1"><date>Sat, 21 Nov 2020 18:23:31 GMT</date><size>2316kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 19 Dec 2020 01:03:11 GMT</date><size>2316kb</size><source_type>D</source_type></version><title>Integrating Structure, Information Architecture and Control Design:
  Application to Tensegrity Systems</title><authors>Raman Goyal and Manoranjan Majji and Robert E. Skelton</authors><categories>eess.SY cs.SY math.OC</categories><doi>10.1016/j.ymssp.2021.107913</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel unified approach to jointly optimize structural design parameters,
actuator and sensor precision and controller parameters is presented in this
paper. The joint optimization problem is posed as a covariance control problem,
where feasibility is achieved by bounding the covariance of the output as well
as that of the control signals. The formulation is used to design a tensegrity
system, where the initial prestress parameters, sensor and actuator precisions,
and the control law are jointly optimized. Tensegrity system dynamics models
linearized about an equilibrium point are used for system design, where
minimality is ensured by constraint projection. The feedback loop is assumed to
have a full-order dynamic compensator with its characteristic matrices chosen
as optimization variables. The suboptimal solution of this non-convex system
design problem is found by iterating over an approximated convex problem
through the use of a convexifying potential function that enables the
convergence to a stationary point. It is shown that for a linear dynamical
system, the approximated joint optimization problem can be formulated using
Linear Matrix Inequalities (LMIs).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10931</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10931</id><submitter>Feiran Zhao</submitter><version version="v1"><date>Sun, 22 Nov 2020 04:40:15 GMT</date><size>128kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Dec 2020 02:18:16 GMT</date><size>129kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 10 Dec 2020 03:37:12 GMT</date><size>129kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 30 May 2021 14:11:51 GMT</date><size>209kb</size><source_type>D</source_type></version><title>Primal-dual Learning for the Model-free Risk-constrained Linear
  Quadratic Regulator</title><authors>Feiran Zhao, Keyou You</authors><categories>eess.SY cs.LG cs.SY</categories><comments>To appear in the Annual Conference on Learning for Dynamics and
  Control (L4DC) 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Risk-aware control, though with promise to tackle unexpected events, requires
a known exact dynamical model. In this work, we propose a model-free framework
to learn a risk-aware controller with a focus on the linear system. We
formulate it as a discrete-time infinite-horizon LQR problem with a state
predictive variance constraint. To solve it, we parameterize the policy with a
feedback gain pair and leverage primal-dual methods to optimize it by solely
using data. We first study the optimization landscape of the Lagrangian
function and establish the strong duality in spite of its non-convex nature.
Alongside, we find that the Lagrangian function enjoys an important local
gradient dominance property, which is then exploited to develop a convergent
random search algorithm to learn the dual function. Furthermore, we propose a
primal-dual algorithm with global convergence to learn the optimal
policy-multiplier pair. Finally, we validate our results via simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.11928</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.11928</id><submitter>Dayiheng Liu</submitter><version version="v1"><date>Tue, 24 Nov 2020 06:59:45 GMT</date><size>189kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 19 May 2021 11:54:23 GMT</date><size>882kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 08:01:50 GMT</date><size>858kb</size><source_type>D</source_type></version><title>GLGE: A New General Language Generation Evaluation Benchmark</title><authors>Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao,
  Weizhu Chen, Jie Fu, Linjun Shou, Ming Gong, Pengcheng Wang, Jiusheng Chen,
  Daxin Jiang, Jiancheng Lv, Ruofei Zhang, Winnie Wu, Ming Zhou, Nan Duan</authors><categories>cs.CL</categories><comments>Findings of Association for Computational Linguistics. ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multi-task benchmarks such as GLUE and SuperGLUE have driven great progress
of pretraining and transfer learning in Natural Language Processing (NLP).
These benchmarks mostly focus on a range of Natural Language Understanding
(NLU) tasks, without considering the Natural Language Generation (NLG) models.
In this paper, we present the General Language Generation Evaluation (GLGE), a
new multi-task benchmark for evaluating the generalization capabilities of NLG
models across eight language generation tasks. For each task, we continue to
design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and
GLGE-Hard). This introduces 24 subtasks to comprehensively compare model
performance. To encourage research on pretraining and transfer learning on NLG
models, we make GLGE publicly available and build a leaderboard with strong
baselines including MASS, BART, and ProphetNet (The source code and dataset are
publicly available at https://github.com/microsoft/glge).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.12865</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.12865</id><submitter>Christian Schiffer</submitter><version version="v1"><date>Wed, 25 Nov 2020 16:44:23 GMT</date><size>8947kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 28 Jan 2021 10:17:01 GMT</date><size>4346kb</size><source_type>D</source_type></version><title>Contrastive Representation Learning for Whole Brain Cytoarchitectonic
  Mapping in Histological Human Brain Sections</title><authors>Christian Schiffer, Katrin Amunts, Stefan Harmeling, Timo Dickscheid</authors><categories>eess.IV cs.CV q-bio.NC</categories><comments>Accepted to ISBI 2021</comments><doi>10.1109/ISBI48211.2021.9433986</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cytoarchitectonic maps provide microstructural reference parcellations of the
brain, describing its organization in terms of the spatial arrangement of
neuronal cell bodies as measured from histological tissue sections. Recent work
provided the first automatic segmentations of cytoarchitectonic areas in the
visual system using Convolutional Neural Networks. We aim to extend this
approach to become applicable to a wider range of brain areas, envisioning a
solution for mapping the complete human brain. Inspired by recent success in
image classification, we propose a contrastive learning objective for encoding
microscopic image patches into robust microstructural features, which are
efficient for cytoarchitectonic area classification. We show that a model
pre-trained using this learning task outperforms a model trained from scratch,
as well as a model pre-trained on a recently proposed auxiliary task. We
perform cluster analysis in the feature space to show that the learned
representations form anatomically meaningful groups.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.13055</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.13055</id><submitter>Sameera Ramasinghe Mr.</submitter><version version="v1"><date>Wed, 25 Nov 2020 22:54:11 GMT</date><size>6249kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 30 Nov 2020 06:00:21 GMT</date><size>6249kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 11:50:46 GMT</date><size>21721kb</size><source_type>D</source_type></version><title>Rethinking conditional GAN training: An approach using geometrically
  structured latent manifolds</title><authors>Sameera Ramasinghe, Moshiur Farazi, Salman Khan, Nick Barnes, Stephen
  Gould</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional GANs (cGAN), in their rudimentary form, suffer from critical
drawbacks such as the lack of diversity in generated outputs and distortion
between the latent and output manifolds. Although efforts have been made to
improve results, they can suffer from unpleasant side-effects such as the
topology mismatch between latent and output spaces. In contrast, we tackle this
problem from a geometrical perspective and propose a novel training mechanism
that increases both the diversity and the visual quality of a vanilla cGAN, by
systematically encouraging a bi-lipschitz mapping between the latent and the
output manifolds. We validate the efficacy of our solution on a baseline cGAN
(i.e., Pix2Pix) which lacks diversity, and show that by only modifying its
training mechanism (i.e., with our proposed Pix2Pix-Geo), one can achieve more
diverse and realistic outputs on a broad set of image-to-image translation
tasks. Codes are available at https://github.com/samgregoost/Rethinking-CGANs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.13137</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.13137</id><submitter>Yifan Gao</submitter><version version="v1"><date>Thu, 26 Nov 2020 05:48:55 GMT</date><size>179kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 07:07:19 GMT</date><size>5822kb</size><source_type>D</source_type></version><title>Answering Ambiguous Questions through Generative Evidence Fusion and
  Round-Trip Prediction</title><authors>Yifan Gao, Henghui Zhu, Patrick Ng, Cicero Nogueira dos Santos, Zhiguo
  Wang, Feng Nan, Dejiao Zhang, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang</authors><categories>cs.CL cs.AI cs.LG</categories><comments>ACL 2021 main conference, 14 pages, 7 figures. Code will be released
  at https://github.com/amzn/refuel-open-domain-qa</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In open-domain question answering, questions are highly likely to be
ambiguous because users may not know the scope of relevant topics when
formulating them. Therefore, a system needs to find possible interpretations of
the question, and predict one or multiple plausible answers. When multiple
plausible answers are found, the system should rewrite the question for each
answer to resolve the ambiguity. In this paper, we present a model that
aggregates and combines evidence from multiple passages to adaptively predict a
single answer or a set of question-answer pairs for ambiguous questions. In
addition, we propose a novel round-trip prediction approach to iteratively
generate additional interpretations that our model fails to find in the first
pass, and then verify and filter out the incorrect question-answer pairs to
arrive at the final disambiguated output. Our model, named Refuel, achieves a
new state-of-the-art performance on the AmbigQA dataset, and shows competitive
performance on NQ-Open and TriviaQA. The proposed round-trip prediction is a
model-agnostic general approach for answering ambiguous open-domain questions,
which improves our Refuel as well as several baseline models. We release source
code for our models and experiments at
https://github.com/amzn/refuel-open-domain-qa.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.14051</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.14051</id><submitter>Jing Li</submitter><version version="v1"><date>Sat, 28 Nov 2020 03:09:25 GMT</date><size>6143kb</size></version><version version="v2"><date>Tue, 20 Apr 2021 07:29:17 GMT</date><size>6159kb</size></version><version version="v3"><date>Mon, 31 May 2021 21:29:24 GMT</date><size>6313kb</size><source_type>D</source_type></version><title>Close Latency--Security Trade-off for the Nakamoto Consensus</title><authors>Jing Li, Ling Ren, Dongning Guo</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a peer-to-peer electronic cash system invented by Nakamoto in
2008. While it has attracted much research interest, its exact latency and
security properties remain open. Existing analyses provide security and latency
(or confirmation time) guarantees that are too loose for practical use. In fact
the best known upper bounds are several orders of magnitude larger than a lower
bound due to a well-known private-mining attack. This paper describes a
continuous-time model for blockchains and develops a rigorous analysis that
yields close upper and lower bounds for the latency--security trade-off. For
example, when the adversary controls 10\% of the total mining power and the
block propagation delays are within 10 seconds, a Bitcoin block is secured with
less than $10^{-3}$ error probability if it is confirmed after four hours, or
with less than $10^{-9}$ error probability if confirmed after ten hours. These
confirmation times are about two hours away from their corresponding lower
bounds. To establish such close bounds, the blockchain security question is
reduced to a race between the Poisson adversarial mining process and a renewal
process formed by a certain species of honest blocks. The moment generation
functions of relevant renewal times are derived in closed form. The general
formulas from the analysis are then applied to study the latency--security
trade-off of several well-known proof-of-work longest-chain cryptocurrencies.
Guidance is also provided on how to set parameters for different purposes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.14062</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.14062</id><submitter>Man-Ling Sung</submitter><version version="v1"><date>Sat, 28 Nov 2020 03:52:38 GMT</date><size>467kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 21:06:44 GMT</date><size>936kb</size><source_type>D</source_type></version><title>Unsupervised Spoken Term Discovery Based on Re-clustering of
  Hypothesized Speech Segments with Siamese and Triplet Networks</title><authors>Man-Ling Sung, Tan Lee</authors><categories>eess.AS cs.CL cs.SD</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spoken term discovery from untranscribed speech audio could be achieved via a
two-stage process. In the first stage, the unlabelled speech is decoded into a
sequence of subword units that are learned and modelled in an unsupervised
manner. In the second stage, partial sequence matching and clustering are
performed on the decoded subword sequences, resulting in a set of discovered
words or phrases. A limitation of this approach is that the results of subword
decoding could be erroneous, and the errors would impact the subsequent steps.
While Siamese/Triplet network is one approach to learn segment representations
that can improve the discovery process, the challenge in spoken term discovery
under a complete unsupervised scenario is that training examples are
unavailable. In this paper, we propose to generate training examples from
initial hypothesized sequence clusters. The Siamese/Triplet network is trained
on the hypothesized examples to measure the similarity between two speech
segments and hereby perform re-clustering of all hypothesized subword sequences
to achieve spoken term discovery. Experimental results show that the proposed
approach is effective in obtaining training examples for Siamese and Triplet
networks, improving the efficacy of spoken term discovery as compared with the
original two-stage method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.14195</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.14195</id><submitter>Abdelkader Mokkadem</submitter><version version="v1"><date>Sat, 28 Nov 2020 18:53:03 GMT</date><size>25kb</size></version><version version="v2"><date>Sat, 29 May 2021 16:54:45 GMT</date><size>27kb</size></version><title>Recursive Association Rule Mining</title><authors>Abdelkader Mokkadem, Mariane Pelletier, Louis Raimbault</authors><categories>cs.DB math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Mining frequent itemsets and association rules is an essential task within
data mining and data analysis. In this paper, we introduce PrefRec, a recursive
algorithm for finding frequent itemsets and association rules. Its main
advantage is its recursiveness with respect to the items. It is particularly
efficient for updating the mining process when new items are added to the
database or when some are excluded. We present in a complete way the logic of
the algorithm as well as its various applications. Finally we present
experiments carried out in the R language comparing PrefRec with Apriori and
Eclat the two most powerful algorithms in this language. To achieve this we
built an R package to run our algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.14387</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.14387</id><submitter>Marija Vella</submitter><version version="v1"><date>Sun, 29 Nov 2020 15:19:41 GMT</date><size>1867kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 10:28:45 GMT</date><size>1975kb</size><source_type>D</source_type></version><title>Overcoming Measurement Inconsistency in Deep Learning for Linear Inverse
  Problems: Applications in Medical Imaging</title><authors>Marija Vella, Jo\~ao F. C. Mota</authors><categories>eess.IV cs.CV eess.SP math.OC</categories><comments>Accepted for publication at ICASSP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkable performance of deep neural networks (DNNs) currently makes
them the method of choice for solving linear inverse problems. They have been
applied to super-resolve and restore images, as well as to reconstruct MR and
CT images. In these applications, DNNs invert a forward operator by finding,
via training data, a map between the measurements and the input images. It is
then expected that the map is still valid for the test data. This framework,
however, introduces measurement inconsistency during testing. We show that such
inconsistency, which can be critical in domains like medical imaging or
defense, is intimately related to the generalization error. We then propose a
framework that post-processes the output of DNNs with an optimization algorithm
that enforces measurement consistency. Experiments on MR images show that
enforcing measurement consistency via our method can lead to large gains in
reconstruction performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.14858</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.14858</id><submitter>Aditya Jyoti Paul</submitter><version version="v1"><date>Mon, 30 Nov 2020 14:56:23 GMT</date><size>7883kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 22 Dec 2020 18:52:33 GMT</date><size>15716kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 12:55:21 GMT</date><size>15716kb</size><source_type>D</source_type></version><title>A Tiny CNN Architecture for Medical Face Mask Detection for
  Resource-Constrained Endpoints</title><authors>Puranjay Mohan, Aditya Jyoti Paul, Abhay Chirania</authors><categories>cs.CV cs.CY cs.LG eess.IV</categories><comments>11 pages, Published in Springer LNEE at
  http://link.springer.com/chapter/10.1007%2F978-981-16-0749-3_52</comments><msc-class>68T45, 68T10, 68T07, 68U10</msc-class><acm-class>C.3; I.2.6; I.2.10; I.4.9; I.5.1; I.5.2; I.5.4; I.5.5; K.4.1; K.4.3</acm-class><journal-ref>Innovations in Electrical and Electronic Engineering. Lecture
  Notes in Electrical Engineering, vol 756, pp 657-670, Springer, Singapore,
  2021</journal-ref><doi>10.1007/978-981-16-0749-3_52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The world is going through one of the most dangerous pandemics of all time
with the rapid spread of the novel coronavirus (COVID-19). According to the
World Health Organisation, the most effective way to thwart the transmission of
coronavirus is to wear medical face masks. Monitoring the use of face masks in
public places has been a challenge because manual monitoring could be unsafe.
This paper proposes an architecture for detecting medical face masks for
deployment on resource-constrained endpoints having extremely low memory
footprints. A small development board with an ARM Cortex-M7 microcontroller
clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for
the deployment of the model. Using the TensorFlow Lite framework, the model is
quantized to further reduce its size. The proposed model is 138 KB post
quantization and runs at the inference speed of 30 FPS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.15021</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.15021</id><submitter>Daniel Gratzer</submitter><version version="v1"><date>Mon, 30 Nov 2020 17:23:34 GMT</date><size>106kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 13:08:02 GMT</date><size>108kb</size><source_type>D</source_type></version><title>Multimodal Dependent Type Theory</title><authors>Daniel Gratzer, G.A. Kavvos, Andreas Nuyts, Lars Birkedal</authors><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce MTT, a dependent type theory which supports multiple modalities.
MTT is parametrized by a mode theory which specifies a collection of modes,
modalities, and transformations between them. We show that different choices of
mode theory allow us to use the same type theory to compute and reason in many
modal situations, including guarded recursion, axiomatic cohesion, and
parametric quantification. We reproduce examples from prior work in guarded
recursion and axiomatic cohesion -- demonstrating that MTT constitutes a simple
and usable syntax whose instantiations intuitively correspond to previous
handcrafted modal type theories. In some cases, instantiating MTT to a
particular situation unearths a previously unknown type theory that improves
upon prior systems. Finally, we investigate the metatheory of MTT. We prove the
consistency of MTT and establish canonicity through an extension of recent
type-theoretic gluing techniques. These results hold irrespective of the choice
of mode theory, and thus apply to a wide variety of modal situations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.15069</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.15069</id><submitter>R\'emy Brossard</submitter><version version="v1"><date>Mon, 30 Nov 2020 18:05:22 GMT</date><size>294kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 07:58:32 GMT</date><size>274kb</size><source_type>D</source_type></version><title>Graph convolutions that can finally model local structure</title><authors>R\'emy Brossard, Oriel Frigo, David Dehaene</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite quick progress in the last few years, recent studies have shown that
modern graph neural networks can still fail at very simple tasks, like
detecting small cycles. This hints at the fact that current networks fail to
catch information about the local structure, which is problematic if the
downstream task heavily relies on graph substructure analysis, as in the
context of chemistry. We propose a very simple correction to the now standard
GIN convolution that enables the network to detect small cycles with nearly no
cost in terms of computation time and number of parameters. Tested on real life
molecule property datasets, our model consistently improves performance on
large multi-tasked datasets over all baselines, both globally and on a per-task
setting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.15124</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.15124</id><submitter>Emanuele Bugliarello</submitter><version version="v1"><date>Mon, 30 Nov 2020 18:55:24 GMT</date><size>1253kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 23:37:58 GMT</date><size>1383kb</size><source_type>D</source_type></version><title>Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework
  of Vision-and-Language BERTs</title><authors>Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott</authors><categories>cs.CL cs.CV</categories><comments>To appear in TACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale pretraining and task-specific fine-tuning is now the standard
methodology for many tasks in computer vision and natural language processing.
Recently, a multitude of methods have been proposed for pretraining vision and
language BERTs to tackle challenges at the intersection of these two key areas
of AI. These models can be categorised into either single-stream or dual-stream
encoders. We study the differences between these two categories, and show how
they can be unified under a single theoretical framework. We then conduct
controlled experiments to discern the empirical differences between five V&amp;L
BERTs. Our experiments show that training data and hyperparameters are
responsible for most of the differences between the reported results, but they
also reveal that the embedding layer plays a crucial role in these massive
models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00059</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00059</id><submitter>Shobhit Jain</submitter><version version="v1"><date>Mon, 30 Nov 2020 19:21:59 GMT</date><size>744kb</size><source_type>D</source_type></version><title>Integral Equations &amp; Model Reduction For Fast Computation of Nonlinear
  Periodic Response</title><authors>Gergely Buza, George Haller, Shobhit Jain</authors><categories>cs.CE math.DS</categories><journal-ref>International Journal of Numerical Methods in Engineering (2021)</journal-ref><doi>10.1002/nme.6740</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose a reformulation for the integral equations approach of Jain,
Breunung \&amp; Haller [Nonlinear Dyn. 97, 313--341 (2019)] to steady-state
response computation for periodically forced nonlinear mechanical systems. This
reformulation results in additional speed-up and better convergence. We show
that the solutions of the reformulated equations are in one-to-one
correspondence with those of the original integral equations and derive
conditions under which a collocation type approximation converges to the exact
solution in the reformulated setting. Furthermore, we observe that model
reduction using a selected set of vibration modes of the linearized system
substantially enhances the computational performance. Finally, we discuss an
open-source implementation of this approach and demonstrate the gains in
computational performance using three examples that also include nonlinear
finite-element models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00212</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00212</id><submitter>Shuaicheng Liu Prof.</submitter><version version="v1"><date>Tue, 1 Dec 2020 01:57:46 GMT</date><size>9374kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 05:26:17 GMT</date><size>8700kb</size><source_type>D</source_type></version><title>UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning</title><authors>Kunming Luo, Chuan Wang, Shuaicheng Liu, Haoqiang Fan, Jue Wang, Jian
  Sun</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present an unsupervised learning approach for optical flow estimation by
improving the upsampling and learning of pyramid network. We design a
self-guided upsample module to tackle the interpolation blur problem caused by
bilinear upsampling between pyramid levels. Moreover, we propose a pyramid
distillation loss to add supervision for intermediate levels via distilling the
finest flow as pseudo labels. By integrating these two components together, our
method achieves the best performance for unsupervised optical flow learning on
multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015.
In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015,
which outperform the previous state-of-the-art methods by 22.2% and 15.7%,
respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00235</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00235</id><submitter>Qianli Zhou</submitter><version version="v1"><date>Tue, 1 Dec 2020 03:13:57 GMT</date><size>4197kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 17:05:03 GMT</date><size>6337kb</size><source_type>D</source_type></version><title>Fractal-based belief entropy</title><authors>Qianli Zhou, Yong Deng</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total uncertainty measurement of basic probability assignment (BPA) in
evidence theory has always been an open issue. Although many scholars have put
forward various measures and requirements of bodies of evidence (BoE), none of
them are widely recognized. So in order to express the uncertainty in evidence
theory, transforming basic probability assignment (BPA) into probability
distribution is a widely used method, but all the previous methods of
probability transformation are directly allocating focal elements in evidence
theory to their elements without specific transformation process. Based on
above, this paper simulates the pignistic probability transformation (PPT)
process based on the idea of fractal, making the PPT process and the
information volume lost during transformation more intuitive. Then apply this
idea to the total uncertainty measure in evidence theory. A new belief entropy
called Fractal-based (FB) entropy is proposed, which is the first time to apply
fractal idea in belief entropy. After verification, the new entropy is superior
to all existing total uncertainty measurements.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00425</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00425</id><submitter>Shashi Raj Pandey</submitter><version version="v1"><date>Tue, 1 Dec 2020 11:46:03 GMT</date><size>642kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Mar 2021 03:08:31 GMT</date><size>853kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 06:34:12 GMT</date><size>853kb</size><source_type>D</source_type></version><title>Edge-assisted Democratized Learning Towards Federated Analytics</title><authors>Shashi Raj Pandey, Minh N.H. Nguyen, Tri Nguyen Dang, Nguyen H. Tran,
  Kyi Thar, Zhu Han, Choong Seon Hong</authors><categories>cs.LG cs.NI</categories><comments>Accepted for publication in IEEE Internet of Things Journal</comments><doi>10.1109/JIOT.2021.3085429</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent take towards Federated Analytics (FA), which allows analytical
insights of distributed datasets, reuses the Federated Learning (FL)
infrastructure to evaluate the summary of model performances across the
training devices. However, the current realization of FL adopts single
server-multiple client architecture with limited scope for FA, which often
results in learning models with poor generalization, i.e., an ability to handle
new/unseen data, for real-world applications. Moreover, a hierarchical FL
structure with distributed computing platforms demonstrates incoherent model
performances at different aggregation levels. Therefore, we need to design a
robust learning mechanism than the FL that (i) unleashes a viable
infrastructure for FA and (ii) trains learning models with better
generalization capability. In this work, we adopt the novel democratized
learning (Dem-AI) principles and designs to meet these objectives. Firstly, we
show the hierarchical learning structure of the proposed edge-assisted
democratized learning mechanism, namely Edge-DemLearn, as a practical framework
to empower generalization capability in support of FA. Secondly, we validate
Edge-DemLearn as a flexible model training mechanism to build a distributed
control and aggregation methodology in regions by leveraging the distributed
computing infrastructure. The distributed edge computing servers construct
regional models, minimize the communication loads, and ensure distributed data
analytic application's scalability. To that end, we adhere to a near-optimal
two-sided many-to-one matching approach to handle the combinatorial constraints
in Edge-DemLearn and solve it for fast knowledge acquisition with optimization
of resource allocation and associations between multiple servers and devices.
Extensive simulation results on real datasets demonstrate the effectiveness of
the proposed methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00489</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00489</id><submitter>Massimiliano Luca</submitter><version version="v1"><date>Tue, 1 Dec 2020 13:49:46 GMT</date><size>6021kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 30 Dec 2020 19:42:54 GMT</date><size>13359kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 13:46:17 GMT</date><size>13360kb</size><source_type>D</source_type></version><title>Deep Gravity: enhancing mobility flows generation with deep neural
  networks and geographic information</title><authors>Filippo Simini, Gianni Barlacchi, Massimiliano Luca, Luca Pappalardo</authors><categories>cs.LG cs.SI</categories><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The movements of individuals within and among cities influence key aspects of
our society, such as the objective and subjective well-being, the diffusion of
innovations, the spreading of epidemics, and the quality of the environment.
For this reason, there is increasing interest around the challenging problem of
flow generation, which consists in generating the flows between a set of
geographic locations, given the characteristics of the locations and without
any information about the real flows. Existing solutions to flow generation are
mainly based on mechanistic approaches, such as the gravity model and the
radiation model, which suffer from underfitting and overdispersion, neglect
important variables such as land use and the transportation network, and cannot
describe non-linear relationships between these variables. In this paper, we
propose the Multi-Feature Deep Gravity (MFDG) model as an effective solution to
flow generation. On the one hand, the MFDG model exploits a large number of
variables (e.g., characteristics of land use and the road network; transport,
food, and health facilities) extracted from voluntary geographic information
data (OpenStreetMap). On the other hand, our model exploits deep neural
networks to describe complex non-linear relationships between those variables.
Our experiments, conducted on commuting flows in England, show that the MFDG
model achieves a significant increase in the performance (up to 250\% for
highly populated areas) than mechanistic models that do not use deep neural
networks, or that do not exploit geographic voluntary data. Our work presents a
precise definition of the flow generation problem, which is a novel task for
the deep learning community working with spatio-temporal data, and proposes a
deep neural network model that significantly outperforms current
state-of-the-art statistical models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00517</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00517</id><submitter>Tuomo Sipola</submitter><version version="v1"><date>Tue, 1 Dec 2020 14:27:28 GMT</date><size>704kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 16 Dec 2020 09:42:34 GMT</date><size>723kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 12:06:48 GMT</date><size>782kb</size><source_type>D</source_type></version><title>One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer</title><authors>Joni Korpihalkola, Tuomo Sipola, Samir Puuska, Tero Kokkonen</authors><categories>cs.CV cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision and machine learning can be used to automate various tasks in
cancer diagnostic and detection. If an attacker can manipulate the automated
processing, the results can be devastating and in the worst case lead to wrong
diagnosis and treatment. In this research, the goal is to demonstrate the use
of one-pixel attacks in a real-life scenario with a real pathology dataset,
TUPAC16, which consists of digitized whole-slide images. We attack against the
IBM CODAIT's MAX breast cancer detector using adversarial images. These
adversarial examples are found using differential evolution to perform the
one-pixel modification to the images in the dataset. The results indicate that
a minor one-pixel modification of a whole slide image under analysis can affect
the diagnosis by reversing the automatic diagnosis result. The attack poses a
threat from the cyber security perspective: the one-pixel method can be used as
an attack vector by a motivated attacker.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00533</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00533</id><submitter>Jialong Xu</submitter><version version="v1"><date>Mon, 30 Nov 2020 04:28:55 GMT</date><size>19564kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 02:31:27 GMT</date><size>18289kb</size><source_type>D</source_type></version><title>Wireless Image Transmission Using Deep Source Channel Coding With
  Attention Modules</title><authors>Jialong Xu, Bo Ai, Wei Chen, Ang Yang, Peng Sun, Miguel Rodrigues</authors><categories>cs.IT math.IT</categories><comments>13 pages, 13 figures, journal paper</comments><doi>10.1109/TCSVT.2021.3082521</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent research on joint source channel coding (JSCC) for wireless
communications has achieved great success owing to the employment of deep
learning (DL). However, the existing work on DL based JSCC usually trains the
designed network to operate under a specific signal-to-noise ratio (SNR)
regime, without taking into account that the SNR level during the deployment
stage may differ from that during the training stage. A number of networks are
required to cover the scenario with a broad range of SNRs, which is
computational inefficiency (in the training stage) and requires large storage.
To overcome these drawbacks our paper proposes a novel method called Attention
DL based JSCC (ADJSCC) that can successfully operate with different SNR levels
during transmission. This design is inspired by the resource assignment
strategy in traditional JSCC, which dynamically adjusts the compression ratio
in source coding and the channel coding rate according to the channel SNR. This
is achieved by resorting to attention mechanisms because these are able to
allocate computing resources to more critical tasks. Instead of applying the
resource allocation strategy in traditional JSCC, the ADJSCC uses the
channel-wise soft attention to scaling features according to SNR conditions. We
compare the ADJSCC method with the state-of-the-art DL based JSCC method
through extensive experiments to demonstrate its adaptability, robustness and
versatility. Compared with the existing methods, the proposed method takes less
storage and is more robust in the presence of channel mismatch.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00572</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00572</id><submitter>Juan Vidal Alegr\'ia</submitter><version version="v1"><date>Tue, 1 Dec 2020 15:27:13 GMT</date><size>2002kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 8 Apr 2021 15:50:04 GMT</date><size>3941kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 17:44:03 GMT</date><size>1605kb</size><source_type>D</source_type></version><title>Trade-offs in Decentralized Multi-Antenna Architectures: The WAX
  Decomposition</title><authors>Juan Vidal Alegr\'ia and Fredrik Rusek and Ove Edfors</authors><categories>eess.SP cs.IT math.IT</categories><comments>14 pages, 9 figures, submitted to IEEE Transactions on Signal
  Processing. arXiv admin note: text overlap with arXiv:2003.01961</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research on multi-antenna architectures is trending towards
increasing the amount of antennas in the base stations (BSs) so as to increase
the spectral efficiency. As a result, the interconnection bandwidth and
computational complexity required to process the data using centralized
architectures is becoming prohibitively high. Decentralized architectures can
reduce these requirements by pre-processing the data before it arrives at a
central processing unit (CPU). However, performing decentralized processing
introduces also cost in complexity/interconnection bandwidth at the antenna end
which is in general being ignored. This paper aims at studying the interplay
between level of decentralization and the associated complexity/interconnection
bandwidth requirement at the antenna end. To do so, we propose a general
framework for centralized/decentralized architectures that can explore said
interplay by adjusting some system parameters, namely the number of connections
to the CPU (level of decentralization), and the number of
multiplications/outputs per antenna (complexity/interconnection bandwidth). We
define a novel matrix decomposition, the WAX decomposition, that allows
information-lossless processing within our proposed framework, and we use it to
obtain the operational limits of the interplay under study. We also look into
some of the limitations of the WAX decomposition.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01065</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01065</id><submitter>Weijie He</submitter><version version="v1"><date>Wed, 2 Dec 2020 10:12:49 GMT</date><size>458kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 14:53:00 GMT</date><size>121kb</size><source_type>D</source_type></version><title>FIT: a Fast and Accurate Framework for Solving Medical Inquiring and
  Diagnosing Tasks</title><authors>Weijie He, Xiaohao Mao, Chao Ma, Yu Huang, Jos\'e Miguel
  Hern\'andez-Lobato, Ting Chen</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic self-diagnosis provides low-cost and accessible healthcare via an
agent that queries the patient and makes predictions about possible diseases.
From a machine learning perspective, symptom-based self-diagnosis can be viewed
as a sequential feature selection and classification problem. Reinforcement
learning methods have shown good performance in this task but often suffer from
large search spaces and costly training. To address these problems, we propose
a competitive bipartite framework, called FIT, which uses an
information-theoretic reward to determine what data to collect next. FIT
improves over previous information-based approaches by using a multimodal
variational autoencoder (MVAE) model and a two-step sampling strategy for
disease prediction. Furthermore, we propose novel methods to substantially
reduce the computational cost of FIT to a level that is acceptable for
practical online self-diagnosis. Our results in two simulated datasets show
that FIT can effectively deal with large search space problems, outperforming
existing RL baselines. Moreover, using several public medical datasets, we show
that FIT is a competitive alternative in various real-world settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01417</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01417</id><submitter>Gaurav Bhardwaj</submitter><version version="v1"><date>Wed, 2 Dec 2020 05:50:42 GMT</date><size>11728kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 12:48:49 GMT</date><size>11730kb</size><source_type>D</source_type></version><title>Cycloidal Trajectory Realization on Staircase with Optimal Trajectory
  Tracking Control based on Neural Network Temporal Quantized Lagrange Dynamics
  (NNTQLD)</title><authors>Gaurav Bhardwaj, Utkarsh A. Mishra, N. Sukavanam and R.
  Balasubramanian</authors><categories>cs.RO cs.NE cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, a novel optimal technique for joint angles trajectory tracking
control of a biped robot with toe foot is proposed. For the task of climbing
stairs by a 9 link biped model, a cycloid trajectory for swing phase is
proposed in such a way that the cycloid variables depend on the staircase
dimensions. Zero Moment Point(ZMP) criteria is taken for satisfying stability
constraint. This paper mainly can be divided into 4 steps: 1) Planning stable
cycloid trajectory for initial step and subsequent step for climbing upstairs.
2) Inverse Kinematics using unsupervised artificial neural network with knot
shifting procedure for jerk minimization. 3) Modeling Dynamics for Toe foot
biped model using Lagrange Dynamics along with contact modeling using spring
damper system , and finally 4) Real time joint angle trajectory tracking
optimization using Temporal Quantized Lagrange Dynamics which takes inverse
kinematics output from neural network as its inputs. Generated patterns have
been simulated in MATLAB.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01696</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01696</id><submitter>Yuji Roh</submitter><version version="v1"><date>Thu, 3 Dec 2020 04:36:04 GMT</date><size>2628kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 14:55:19 GMT</date><size>2635kb</size><source_type>D</source_type></version><title>FairBatch: Batch Selection for Model Fairness</title><authors>Yuji Roh, Kangwook Lee, Steven Euijong Whang, Changho Suh</authors><categories>cs.LG cs.AI stat.ML</categories><comments>In Proceedings of the 9th International Conference on Learning
  Representations (ICLR), 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training a fair machine learning model is essential to prevent demographic
disparity. Existing techniques for improving model fairness require broad
changes in either data preprocessing or model training, rendering themselves
difficult-to-adopt for potentially already complex machine learning systems. We
address this problem via the lens of bilevel optimization. While keeping the
standard training algorithm as an inner optimizer, we incorporate an outer
optimizer so as to equip the inner problem with an additional functionality:
Adaptively selecting minibatch sizes for the purpose of improving model
fairness. Our batch selection algorithm, which we call FairBatch, implements
this optimization and supports prominent fairness measures: equal opportunity,
equalized odds, and demographic parity. FairBatch comes with a significant
implementation benefit -- it does not require any modification to data
preprocessing or model training. For instance, a single-line change of PyTorch
code for replacing batch selection part of model training suffices to employ
FairBatch. Our experiments conducted both on synthetic and benchmark real data
demonstrate that FairBatch can provide such functionalities while achieving
comparable (or even greater) performances against the state of the arts.
Furthermore, FairBatch can readily improve fairness of any pre-trained model
simply via fine-tuning. It is also compatible with existing batch selection
techniques intended for different purposes, such as faster convergence, thus
gracefully achieving multiple purposes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01707</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01707</id><submitter>Pavan Kapanipathi</submitter><version version="v1"><date>Thu, 3 Dec 2020 05:17:55 GMT</date><size>21495kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 16:04:04 GMT</date><size>19203kb</size><source_type>D</source_type></version><title>Leveraging Abstract Meaning Representation for Knowledge Base Question
  Answering</title><authors>Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim
  Roukos, Alexander Gray, Ramon Astudillo, Maria Chang, Cristina Cornelio,
  Saswati Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, Sairam Gurajada,
  Hima Karanam, Naweed Khan, Dinesh Khandelwal, Young-Suk Lee, Yunyao Li,
  Francois Luus, Ndivhuwo Makondo, Nandana Mihindukulasooriya, Tahira Naseem,
  Sumit Neelam, Lucian Popa, Revanth Reddy, Ryan Riegel, Gaetano Rossiello,
  Udit Sharma, G P Shrivatsa Bhargav, Mo Yu</authors><categories>cs.CL cs.AI</categories><comments>Accepted to Findings of ACL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge base question answering (KBQA)is an important task in Natural
Language Processing. Existing approaches face significant challenges including
complex question understanding, necessity for reasoning, and lack of large
end-to-end training datasets. In this work, we propose Neuro-Symbolic Question
Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning
Representation (AMR) parses for task-independent question understanding; (2) a
simple yet effective graph transformation approach to convert AMR parses into
candidate logical queries that are aligned to the KB; (3) a pipeline-based
approach which integrates multiple, reusable modules that are trained
specifically for their individual tasks (semantic parser, entity
andrelationship linkers, and neuro-symbolic reasoner) and do not require
end-to-end training data. NSQA achieves state-of-the-art performance on two
prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore,
our analysis emphasizes that AMR is a powerful tool for KBQA systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01708</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01708</id><submitter>Najam Nazar Dr</submitter><version version="v1"><date>Thu, 3 Dec 2020 05:21:04 GMT</date><size>1689kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 02:28:23 GMT</date><size>2098kb</size><source_type>D</source_type></version><title>Feature-Based Software Design Pattern Detection</title><authors>Najam Nazar, Aldeida Aleti, Yaokun Zheng</authors><categories>cs.SE</categories><comments>Submitted to the Journal of Systems and Software (JSS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software design patterns are standard solutions to common problems in
software design and architecture. Knowing that a particular module implements a
design pattern is a shortcut to design comprehension. Manually detecting design
patterns is a time consuming and challenging task; therefore, researchers have
proposed automatic design patterns detection techniques to facilitate software
developers. However, these techniques show low performance for certain design
patterns. In this work, we introduce an approach that improves the performance
over the state-of-the-art by using code features with machine learning
classifiers to automatically train a design pattern detection. We create a
semantic representation of source code from the code features and the call
graph, and apply the Word2Vec algorithm on the semantic representation to
construct the word-space geometric model of the Java source code. DPD_F then
uses a Machine Learning approach trained using the word-space model and
identifies software design patterns with 74% Precision and 71% Recall.
Additionally, we have compared our results with two existing design pattern
detection approaches namely FeatureMaps &amp; MARPLE-DPD. Empirical results
demonstrate that our approach outperforms the benchmark approaches by 30\% and
10\% respectively in terms of Precision. The runtime performance also supports
its practical applicability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01764</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01764</id><submitter>Louis Esperet</submitter><version version="v1"><date>Thu, 3 Dec 2020 08:51:44 GMT</date><size>24kb</size></version><version version="v2"><date>Fri, 19 Mar 2021 20:15:15 GMT</date><size>21kb</size></version><version version="v3"><date>Mon, 10 May 2021 07:40:14 GMT</date><size>20kb</size></version><version version="v4"><date>Thu, 3 Jun 2021 14:34:21 GMT</date><size>20kb</size></version><title>Optimal labelling schemes for adjacency, comparability, and reachability</title><authors>Marthe Bonamy, Louis Esperet, Carla Groenland, Alex Scott</authors><categories>math.CO cs.DS</categories><comments>17 pages - to appear in the proceedings of STOC 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct asymptotically optimal adjacency labelling schemes for every
hereditary class containing $2^{\Omega(n^2)}$ $n$-vertex graphs as $n\to
\infty$. This regime contains many classes of interest, for instance perfect
graphs or comparability graphs, for which we obtain an adjacency labelling
scheme with labels of $n/4+o(n)$ bits per vertex. This implies the existence of
a reachability labelling scheme for digraphs with labels of $n/4+o(n)$ bits per
vertex and comparability labelling scheme for posets with labels of $n/4+o(n)$
bits per element. All these results are best possible, up to the lower order
term.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01937</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01937</id><submitter>Rajdip Nayek</submitter><version version="v1"><date>Thu, 3 Dec 2020 14:14:31 GMT</date><size>2241kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 7 Dec 2020 08:45:57 GMT</date><size>2246kb</size><source_type>D</source_type></version><title>On spike-and-slab priors for Bayesian equation discovery of nonlinear
  dynamical systems via sparse linear regression</title><authors>Rajdip Nayek, Ramon Fuentes, Keith Worden, Elizabeth J. Cross</authors><categories>stat.ME cs.SY eess.SY stat.AP</categories><doi>10.1016/j.ymssp.2021.107986</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents the use of spike-and-slab (SS) priors for discovering
governing differential equations of motion of nonlinear structural dynamic
systems. The problem of discovering governing equations is cast as that of
selecting relevant variables from a predetermined dictionary of basis variables
and solved via sparse Bayesian linear regression. The SS priors, which belong
to a class of discrete-mixture priors and are known for their strong
sparsifying (or shrinkage) properties, are employed to induce sparse solutions
and select relevant variables. Three different variants of SS priors are
explored for performing Bayesian equation discovery. As the posteriors with SS
priors are analytically intractable, a Markov chain Monte Carlo (MCMC)-based
Gibbs sampler is employed for drawing posterior samples of the model
parameters; the posterior samples are used for variable selection and parameter
estimation in equation discovery. The proposed algorithm has been applied to
four systems of engineering interest, which include a baseline linear system,
and systems with cubic stiffness, quadratic viscous damping, and Coulomb
damping. The results demonstrate the effectiveness of the SS priors in
identifying the presence and type of nonlinearity in the system. Additionally,
comparisons with the Relevance Vector Machine (RVM) - that uses a Student's-t
prior - indicate that the SS priors can achieve better model selection
consistency, reduce false discoveries, and derive models that have superior
predictive accuracy. Finally, the Silverbox experimental benchmark is used to
validate the proposed methodology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.02011</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.02011</id><submitter>Tomas Pippia</submitter><version version="v1"><date>Thu, 3 Dec 2020 15:45:33 GMT</date><size>257kb</size></version><version version="v2"><date>Thu, 20 May 2021 10:13:25 GMT</date><size>307kb</size></version><title>Scenario-based Nonlinear Model Predictive Control for Building Heating
  Systems</title><authors>Tomas Pippia, Jesus Lago, Roel De Coninck, Bart De Schutter</authors><categories>eess.SY cs.SY</categories><doi>10.1016/j.enbuild.2021.111108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art Model Predictive Control (MPC) applications for building
heating adopt either a deterministic controller together with a nonlinear model
or a linearized model with a stochastic MPC controller. However, deterministic
MPC only considers one single realization of the disturbances and its
performance strongly depends on the quality of the forecast of the
disturbances, which can lead to low performance. In fact, inadequate building
energy management can lead to high energy costs and CO$_2$ emissions. On the
other hand, a linearized model can fail to capture some dynamics and behavior
of the building under control. In this article, we combine a stochastic
scenario-based MPC (SBMPC) controller together with a nonlinear Modelica model
that is able to provide a richer building description and to capture the
dynamics of the building more accurately than linear models. The adopted SBMPC
controller considers multiple realizations of the external disturbances
obtained through a statistically accurate model, so as to consider different
possible disturbance evolutions and to robustify the control action. To this
purpose, we present a scenario generation method for building temperature
control that can be applied to several exogenous perturbations, e.g.\ solar
irradiance, outside temperature, and that satisfies several important
stastistical properties, in contrast with simpler and less accurate methods
adopted in the literature. We show the benefits of our proposed approach
through several simulations in which we compare our method against the standard
ones from the literature, for several combinations of a trade-off parameter
between comfort and energy cost. We show how our SBMPC controller approach
outperforms the standard controllers available in the literature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.02086</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.02086</id><submitter>Warut Suksompong</submitter><version version="v1"><date>Thu, 3 Dec 2020 17:20:29 GMT</date><size>32kb</size></version><version version="v2"><date>Tue, 4 May 2021 15:39:03 GMT</date><size>43kb</size></version><title>Welfare Guarantees in Schelling Segregation</title><authors>Martin Bullinger, Warut Suksompong, Alexandros A. Voudouris</authors><categories>cs.GT</categories><comments>Appears in the 35th AAAI Conference on Artificial Intelligence
  (AAAI), 2021</comments><journal-ref>Journal of Artificial Intelligence Research, 71:143-174 (2021)</journal-ref><doi>10.1613/jair.1.12771</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schelling's model is an influential model that reveals how individual
perceptions and incentives can lead to residential segregation. Inspired by a
recent stream of work, we study welfare guarantees and complexity in this model
with respect to several welfare measures. First, we show that while maximizing
the social welfare is NP-hard, computing an assignment of agents to the nodes
of any topology graph with approximately half of the maximum welfare can be
done in polynomial time. We then consider Pareto optimality, introduce two new
optimality notions based on it, and establish mostly tight bounds on the
worst-case welfare loss for assignments satisfying these notions as well as the
complexity of computing such assignments. In addition, we show that for tree
topologies, it is possible to decide whether there exists an assignment that
gives every agent a positive utility in polynomial time; moreover, when every
node in the topology has degree at least $2$, such an assignment always exists
and can be found efficiently.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.02190</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.02190</id><submitter>Alex Yu</submitter><version version="v1"><date>Thu, 3 Dec 2020 18:59:54 GMT</date><size>9768kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 5 May 2021 05:41:26 GMT</date><size>8642kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 30 May 2021 17:52:24 GMT</date><size>8810kb</size><source_type>D</source_type></version><title>pixelNeRF: Neural Radiance Fields from One or Few Images</title><authors>Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa</authors><categories>cs.CV cs.GR cs.LG</categories><comments>CVPR 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose pixelNeRF, a learning framework that predicts a continuous neural
scene representation conditioned on one or few input images. The existing
approach for constructing neural radiance fields involves optimizing the
representation to every scene independently, requiring many calibrated views
and significant compute time. We take a step towards resolving these
shortcomings by introducing an architecture that conditions a NeRF on image
inputs in a fully convolutional manner. This allows the network to be trained
across multiple scenes to learn a scene prior, enabling it to perform novel
view synthesis in a feed-forward manner from a sparse set of views (as few as
one). Leveraging the volume rendering approach of NeRF, our model can be
trained directly from images with no explicit 3D supervision. We conduct
extensive experiments on ShapeNet benchmarks for single image novel view
synthesis tasks with held-out objects as well as entire unseen categories. We
further demonstrate the flexibility of pixelNeRF by demonstrating it on
multi-object ShapeNet scenes and real scenes from the DTU dataset. In all
cases, pixelNeRF outperforms current state-of-the-art baselines for novel view
synthesis and single image 3D reconstruction. For the video and code, please
visit the project website: https://alexyu.net/pixelnerf
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.02405</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.02405</id><submitter>Houwang Tu</submitter><version version="v1"><date>Fri, 4 Dec 2020 05:06:02 GMT</date><size>3183kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 14 Jan 2021 07:08:47 GMT</date><size>3183kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 26 May 2021 10:56:05 GMT</date><size>16496kb</size><source_type>D</source_type></version><title>Applying the Chebyshev-Tau spectral method to solve the parabolic
  equation model of wide-angle rational approximation in ocean acoustics</title><authors>Houwang Tu, Yongxian Wang, Xian Ma and Xunjiang Zhu</authors><categories>cs.CE cs.NA math.NA physics.flu-dyn</categories><comments>19 pages, 7 pages, 2 tables</comments><doi>10.1142/S2591728521500134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving an acoustic wave equation using a parabolic approximation is a
popular approach for many existing ocean acoustic models. Commonly used
parabolic equation (PE) model programs, such as the range-dependent acoustic
model (RAM), are discretized by the finite difference method (FDM). Considering
the idea and theory of the wide-angle rational approximation, a discrete PE
model using the Chebyshev spectral method (CSM) is derived, and the code is
developed. This method is currently suitable only for range-independent
waveguides. Taking three ideal fluid waveguides as examples, the correctness of
using the CSM discrete PE model in solving the underwater acoustic propagation
problem is verified. The test results show that compared with the RAM, the
method proposed in this paper can achieve higher accuracy in computational
underwater acoustics and requires fewer discrete grid points. After
optimization, this method is more advantageous than the FDM in terms of speed.
Thus, the CSM provides high-precision reference standards for benchmark
examples of the range-independent PE model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.02466</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.02466</id><submitter>Cen Liu</submitter><version version="v1"><date>Fri, 4 Dec 2020 08:37:58 GMT</date><size>10262kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 11:08:46 GMT</date><size>11240kb</size></version><title>RIS-Assisted Secure Transmission Exploiting Statistical CSI of
  Eavesdropper</title><authors>Cen Liu, Chang Tian, Peixi Liu</authors><categories>cs.IT eess.SP math.IT</categories><comments>6 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate the reconfigurable intelligent surface (RIS) assisted downlink
secure transmission where only the statistical channel of eavesdropper is
available. To handle the stochastic ergodic secrecy rate (ESR) maximization
problem, a deterministic lower bound of ESR (LESR) is derived. We aim to
maximize the LESR by jointly designing the transmit beamforming at the access
point (AP) and reflect beamforming by the phase shifts at the RIS. To solve the
non-convex LESR maximization problem, we develop a novel penalty dual convex
approximation (PDCA) algorithm based on the penalty dual decomposition (PDD)
optimization framework, where the exacting constraints are penalized and
dualized into the objective function as augmented Lagrangian components. The
proposed PDCA algorithm performs double-loop iterations, i.e., the inner loop
resorts to the block successive convex approximation (BSCA) to update the
optimization variables; while the outer loop adjusts the Lagrange multipliers
and penalty parameter of the augmented Lagrangian cost function. The
convergence to a Karush-Kuhn-Tucker (KKT) solution is theoretically guaranteed
with low computational complexity. Simulation results show that the proposed
PDCA scheme is better than the commonly adopted alternating optimization (AO)
scheme with the knowledge of statistical channel of eavesdropper.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03058</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03058</id><submitter>Xingyu Zhao</submitter><version version="v1"><date>Sat, 5 Dec 2020 15:41:52 GMT</date><size>634kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 20:17:32 GMT</date><size>2626kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 19 May 2021 12:28:47 GMT</date><size>2626kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 20 May 2021 07:46:30 GMT</date><size>2626kb</size><source_type>D</source_type></version><version version="v5"><date>Sat, 29 May 2021 07:49:00 GMT</date><size>2626kb</size><source_type>D</source_type></version><title>BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations</title><authors>Xingyu Zhao, Wei Huang, Xiaowei Huang, Valentin Robu, David Flynn</authors><categories>cs.AI</categories><comments>Preprint accepted by UAI2021. The final version to appear in the
  UAI2021 volume of Proceedings of Machine Learning Research</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given the pressing need for assuring algorithmic transparency, Explainable AI
(XAI) has emerged as one of the key areas of AI research. In this paper, we
develop a novel Bayesian extension to the LIME framework, one of the most
widely used approaches in XAI -- which we call BayLIME. Compared to LIME,
BayLIME exploits prior knowledge and Bayesian reasoning to improve both the
consistency in repeated explanations of a single prediction and the robustness
to kernel settings. BayLIME also exhibits better explanation fidelity than the
state-of-the-art (LIME, SHAP and GradCAM) by its ability to integrate prior
knowledge from, e.g., a variety of other XAI techniques, as well as
verification and validation (V&amp;V) methods. We demonstrate the desirable
properties of BayLIME through both theoretical analysis and extensive
experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03129</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03129</id><submitter>Saeed Khaki</submitter><version version="v1"><date>Sat, 5 Dec 2020 22:09:07 GMT</date><size>1747kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 4 Mar 2021 18:10:16 GMT</date><size>3988kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 15:20:12 GMT</date><size>2916kb</size></version><title>Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data
  Using Deep Transfer Learning</title><authors>Saeed Khaki, Hieu Pham and Lizhi Wang</authors><categories>cs.CV cs.LG eess.IV q-bio.QM stat.AP</categories><comments>14 pages, 8 figures, 7 tables</comments><journal-ref>Scientific Reports, 11(1), 1-14 (2021)</journal-ref><doi>10.1038/s41598-021-89779-z</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Large-scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout their growth cycle. Having this information allows
stakeholders the ability to make real-time decisions to maximize yield
potential. Although various models exist that predict yield from remote sensing
data, there currently does not exist an approach that can estimate yield for
multiple crops simultaneously, and thus leads to more accurate predictions. A
model that predicts the yield of multiple crops and concurrently considers the
interaction between multiple crop yields. We propose a new convolutional neural
network model called YieldNet which utilizes a novel deep learning framework
that uses transfer learning between corn and soybean yield predictions by
sharing the weights of the backbone feature extractor. Additionally, to
consider the multi-target response variable, we propose a new loss function. We
conduct our experiment using data from 1,132 counties for corn and 1,076
counties for soybean across the United States. Numerical results demonstrate
that our proposed method accurately predicts corn and soybean yield from one to
four months before the harvest with a MAE being 8.74% and 8.70% of the average
yield, respectively, and is competitive to other state-of-the-art approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03208</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03208</id><submitter>Byeonghwi Kim</submitter><version version="v1"><date>Sun, 6 Dec 2020 07:59:22 GMT</date><size>4470kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 15:49:23 GMT</date><size>4470kb</size><source_type>D</source_type></version><title>MOCA: A Modular Object-Centric Approach for Interactive Instruction
  Following</title><authors>Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi,
  Jonghyun Choi</authors><categories>cs.AI cs.CV cs.RO</categories><comments>12 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Performing simple household tasks based on language directives is very
natural to humans, yet it remains an open challenge for an AI agent. Recently,
an 'interactive instruction following' task has been proposed to foster
research in reasoning over long instruction sequences that requires object
interactions in a simulated environment. It involves solving open problems in
vision, language and navigation literature at each step. To address this
multifaceted problem, we propose a modular architecture that decouples the task
into visual perception and action policy, and name it as MOCA, a Modular
Object-Centric Approach. We evaluate our method on the ALFRED benchmark and
empirically validate that it outperforms prior arts by significant margins in
all metrics with good generalization performance (high success rate in unseen
environments). Our code is available at https://github.com/gistvision/moca.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03322</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03322</id><submitter>Aymene Mohammed Bouayed</submitter><version version="v1"><date>Sun, 6 Dec 2020 17:03:34 GMT</date><size>1019kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 17:16:14 GMT</date><size>1011kb</size><source_type>D</source_type></version><title>A Pseudo-labelling Auto-Encoder for unsupervised image classification</title><authors>Aymene Mohammed Bouayed and Karim Atif and Rachid Deriche and
  Abdelhakim Saim</authors><categories>cs.CV cs.AI</categories><comments>13 pages, 17 figures, 9 tables, title simplified, references added</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we introduce a unique variant of the denoising Auto-Encoder
and combine it with the perceptual loss to classify images in an unsupervised
manner. The proposed method, called Pseudo Labelling, consists of first
applying a randomly sampled set of data augmentation transformations to each
training image. As a result, each initial image can be considered as a
pseudo-label to its corresponding augmented ones. Then, an Auto-Encoder is used
to learn the mapping between each set of the augmented images and its
corresponding pseudo-label. Furthermore, the perceptual loss is employed to
take into consideration the existing dependencies between the pixels in the
same neighbourhood of an image. This combination encourages the encoder to
output richer encodings that are highly informative of the input's class.
Consequently, the Auto-Encoder's performance on unsupervised image
classification is improved in terms of stability, accuracy and consistency
across all tested datasets. Previous state-of-the-art accuracy on the MNIST,
CIFAR-10 and SVHN datasets is improved by 0.3\%, 3.11\% and 9.21\%
respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03929</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03929</id><submitter>Lin Pan</submitter><version version="v1"><date>Mon, 7 Dec 2020 18:58:57 GMT</date><size>109kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 18:17:44 GMT</date><size>989kb</size><source_type>D</source_type></version><title>Benchmarking Commercial Intent Detection Services with Practice-Driven
  Evaluations</title><authors>Haode Qi, Lin Pan, Atin Sood, Abhishek Shah, Ladislav Kunc, Mo Yu,
  Saloni Potdar</authors><categories>cs.CL</categories><comments>Accepted at NAACL2021 Industry Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intent detection is a key component of modern goal-oriented dialog systems
that accomplish a user task by predicting the intent of users' text input.
There are three primary challenges in designing robust and accurate intent
detection models. First, typical intent detection models require a large amount
of labeled data to achieve high accuracy. Unfortunately, in practical scenarios
it is more common to find small, unbalanced, and noisy datasets. Secondly, even
with large training data, the intent detection models can see a different
distribution of test data when being deployed in the real world, leading to
poor accuracy. Finally, a practical intent detection model must be
computationally efficient in both training and single query inference so that
it can be used continuously and re-trained frequently. We benchmark intent
detection methods on a variety of datasets. Our results show that Watson
Assistant's intent detection model outperforms other commercial solutions and
is comparable to large pretrained language models while requiring only a
fraction of computational resources and training data. Watson Assistant
demonstrates a higher degree of robustness when the training and test
distributions differ.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03979</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03979</id><submitter>Erel Segal-Halevi</submitter><version version="v1"><date>Mon, 7 Dec 2020 19:00:19 GMT</date><size>28kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 15:37:01 GMT</date><size>27kb</size></version><title>Computing Welfare-Maximizing Fair Allocations of Indivisible Goods</title><authors>Haris Aziz, Xin Huang, Nicholas Mattei, Erel Segal-Halevi</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of computing allocations that are both
fair and maximize the utilitarian social welfare, i.e., the sum of agents'
utilities. We focus on two tractable fairness concepts: envy-freeness up to one
item (EF1) and proportionality up to one item (PROP1). In particular, we
consider the following two computational problems: (1) Among the
utilitarian-maximal allocations, decide whether there exists one that is also
fair according to either EF1 or PROP1; (2) among the fair allocations, compute
one that maximizes the utilitarian welfare. We show that both problems are
strongly NP-hard when the number of agents is variable, and remain NP-hard for
a fixed number of agents greater than two. For the special case of two agents,
we find that problem (1) is polynomial-time solvable, while problem (2) remains
NP-hard. Finally, with a fixed number of agents, we design
pseudopolynomial-time algorithms for both problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04012</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04012</id><submitter>Timo Bolkart</submitter><version version="v1"><date>Mon, 7 Dec 2020 19:30:45 GMT</date><size>25984kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 17:52:42 GMT</date><size>23420kb</size><source_type>D</source_type></version><title>Learning an Animatable Detailed 3D Face Model from In-The-Wild Images</title><authors>Yao Feng and Haiwen Feng and Michael J. Black and Timo Bolkart</authors><categories>cs.CV</categories><comments>SIGGRAPH 2021</comments><journal-ref>ACM Transactions on Graphics (ToG), Vol. 40, No. 4, Article 88.
  Publication date: August 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While current monocular 3D face reconstruction methods can recover fine
geometric details, they suffer several limitations. Some methods produce faces
that cannot be realistically animated because they do not model how wrinkles
vary with expression. Other methods are trained on high-quality face scans and
do not generalize well to in-the-wild images. We present the first approach
that regresses 3D face shape and animatable details that are specific to an
individual but change with expression. Our model, DECA (Detailed Expression
Capture and Animation), is trained to robustly produce a UV displacement map
from a low-dimensional latent representation that consists of person-specific
detail parameters and generic expression parameters, while a regressor is
trained to predict detail, shape, albedo, expression, pose and illumination
parameters from a single image. To enable this, we introduce a novel
detail-consistency loss that disentangles person-specific details from
expression-dependent wrinkles. This disentanglement allows us to synthesize
realistic person-specific wrinkles by controlling expression parameters while
keeping person-specific details unchanged. DECA is learned from in-the-wild
images with no paired 3D supervision and achieves state-of-the-art shape
reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild
data demonstrate DECA's robustness and its ability to disentangle identity- and
expression-dependent details enabling animation of reconstructed faces. The
model and code are publicly available at https://deca.is.tue.mpg.de.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04028</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04028</id><submitter>Oliver Speidel</submitter><version version="v1"><date>Mon, 7 Dec 2020 20:05:09 GMT</date><size>26230kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Feb 2021 16:16:57 GMT</date><size>23238kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 09:08:06 GMT</date><size>30493kb</size><source_type>D</source_type></version><title>On-Road Motion Planning for Automated Vehicles at Ulm University</title><authors>Maximilian Graf, Oliver Speidel, Jona Ruof and Klaus Dietmayer</authors><categories>cs.RO</categories><comments>Maximilian Graf and Oliver Speidel contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Institute of Measurement, Control and Microtechnology at Ulm University
investigates advanced driver assistance systems for decades and concentrates in
large parts on autonomous driving. It is well known that motion planning is a
key technology for autonomous driving. It is first and foremost responsible for
the safety of the vehicle passengers as well as of all surrounding traffic
participants. However, a further task consists in providing a smooth and
comfortable driving behavior. In Ulm, we have the grateful opportunity to test
our algorithms under real conditions in public traffic and diversified
scenarios. In this paper, we would like to give the readers an insight of our
work, about the vehicle, the test track, as well as of the related problems,
challenges and solutions. Therefore, we will describe the motion planning
system and explain the implemented functionalities. Furthermore, we will show
how our vehicle moves through public road traffic and how it deals with
challenging scenarios like e.g. driving through roundabouts and intersections.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04174</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04174</id><submitter>Ninad Jadhav</submitter><version version="v1"><date>Tue, 8 Dec 2020 02:31:06 GMT</date><size>8890kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 21:56:08 GMT</date><size>8890kb</size><source_type>D</source_type></version><title>WSR: A WiFi Sensor for Collaborative Robotics</title><authors>Ninad Jadhav, Weiying Wang, Diana Zhang, Oussama Khatib, Swarun Kumar
  and Stephanie Gil</authors><categories>cs.RO cs.MA</categories><comments>28 pages, 25 figures, *co-primary authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive a new capability for robots to measure relative
direction, or Angle-of-Arrival (AOA), to other robots operating in
non-line-of-sight and unmapped environments with occlusions, without requiring
external infrastructure. We do so by capturing all of the paths that a WiFi
signal traverses as it travels from a transmitting to a receiving robot, which
we term an AOA profile. The key intuition is to &quot;emulate antenna arrays in the
air&quot; as the robots move in 3D space, a method akin to Synthetic Aperture Radar
(SAR). The main contributions include development of i) a framework to
accommodate arbitrary 3D trajectories, as well as continuous mobility all
robots, while computing AOA profiles and ii) an accompanying analysis that
provides a lower bound on variance of AOA estimation as a function of robot
trajectory geometry based on the Cramer Rao Bound. This is a critical
distinction with previous work on SAR that restricts robot mobility to
prescribed motion patterns, does not generalize to 3D space, and/or requires
transmitting robots to be static during data acquisition periods. Our method
results in more accurate AOA profiles and thus better AOA estimation, and
formally characterizes this observation as the informativeness of the
trajectory; a computable quantity for which we derive a closed form. All
theoretical developments are substantiated by extensive simulation and hardware
experiments. We also show that our formulation can be used with an
off-the-shelf trajectory estimation sensor. Finally, we demonstrate the
performance of our system on a multi-robot dynamic rendezvous task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04345</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04345</id><submitter>Jicong Fan</submitter><version version="v1"><date>Tue, 8 Dec 2020 10:34:21 GMT</date><size>506kb</size></version><version version="v2"><date>Sun, 30 May 2021 15:37:45 GMT</date><size>803kb</size></version><title>Large-Scale Subspace Clustering via k-Factorization</title><authors>Jicong Fan</authors><categories>cs.LG cs.AI</categories><comments>Accepted to KDD'21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering (SC) aims to cluster data lying in a union of
low-dimensional subspaces. Usually, SC learns an affinity matrix and then
performs spectral clustering. Both steps suffer from high time and space
complexity, which leads to difficulty in clustering large datasets. This paper
presents a method called k-Factorization Subspace Clustering (k-FSC) for
large-scale subspace clustering. K-FSC directly factorizes the data into k
groups via pursuing structured sparsity in the matrix factorization model.
Thus, k-FSC avoids learning affinity matrix and performing eigenvalue
decomposition, and has low (linear) time and space complexity on large
datasets. This paper proves the effectiveness of the k-FSC model theoretically.
An efficient algorithm with convergence guarantee is proposed to solve the
optimization of k-FSC. In addition, k-FSC is able to handle sparse noise,
outliers, and missing data, which are pervasive in real applications. This
paper also provides online extension and out-of-sample extension for k-FSC to
handle streaming data and cluster arbitrarily large datasets. Extensive
experiments on large-scale real datasets show that k-FSC and its extensions
outperform state-of-the-art methods of subspace clustering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04705</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04705</id><submitter>Srinivas Kota Reddy</submitter><version version="v1"><date>Tue, 8 Dec 2020 19:47:10 GMT</date><size>291kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 03:50:29 GMT</date><size>427kb</size><source_type>D</source_type></version><title>Structured Index Coding Problem and Multi-access Coded Caching</title><authors>Kota Srinivas Reddy and Nikhil Karamchandani</authors><categories>cs.IT cs.DC math.CO math.IT</categories><comments>42 pages, single column</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Index coding and coded caching are two active research topics in information
theory with strong ties to each other. Motivated by the multi-access coded
caching problem, we study a new class of structured index coding problems
(ICPs) which are formed by the union of several symmetric ICPs. We derive upper
and lower bounds on the optimal server transmission rate for this class of ICPs
and demonstrate that they differ by at most a factor of two. Finally, we apply
these results to the multi-access coded caching problem to derive better bounds
than the state of the art.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04731</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04731</id><submitter>Sena Kiciroglu</submitter><version version="v1"><date>Tue, 8 Dec 2020 20:45:51 GMT</date><size>6300kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 14:35:08 GMT</date><size>23941kb</size></version><title>Long Term Motion Prediction Using Keyposes</title><authors>Sena Kiciroglu, Wei Wang, Mathieu Salzmann, Pascal Fua</authors><categories>cs.CV</categories><comments>See supplementary video at: https://youtu.be/T1WjdF8ux6o</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long term human motion prediction is essential in safety-critical
applications such as human-robot interaction and autonomous driving. In this
paper, we show that, to achieve long term forecasting, predicting human pose at
every time instant is unnecessary. Instead, it is more effective to predict a
few keyposes and approximate intermediate ones by linearly interpolating the
keyposes.
  We will demonstrate that our approach enables us to predict realistic motions
for up to 5 seconds in the future, which is far larger than the typical 1
second encountered in the literature. Over this extended time period, our
predictions are more realistic and better preserve the motion dynamics than
those state-of-the-art methods yield.
  Furthermore, because we model future keyposes probabilistically, we can
generate multiple plausible future motions by sampling at inference time. This
is useful to model because people usually can do one of several things given
what they have already done.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04735</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04735</id><submitter>Muhammad Usman</submitter><version version="v1"><date>Tue, 8 Dec 2020 20:51:56 GMT</date><size>12717kb</size><source_type>D</source_type></version><title>Graph-Based Generative Representation Learning of Semantically and
  Behaviorally Augmented Floorplans</title><authors>Vahid Azizi, Muhammad Usman, Honglu Zhou, Petros Faloutsos and
  Mubbasir Kapadia</authors><categories>cs.LG cs.AI</categories><doi>10.1007/s00371-021-02155-w</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Floorplans are commonly used to represent the layout of buildings. In
computer aided-design (CAD) floorplans are usually represented in the form of
hierarchical graph structures. Research works towards computational techniques
that facilitate the design process, such as automated analysis and
optimization, often use simple floorplan representations that ignore the
semantics of the space and do not take into account usage related analytics. We
present a floorplan embedding technique that uses an attributed graph to
represent the geometric information as well as design semantics and behavioral
features of the inhabitants as node and edge attributes. A Long Short-Term
Memory (LSTM) Variational Autoencoder (VAE) architecture is proposed and
trained to embed attributed graphs as vectors in a continuous space. A user
study is conducted to evaluate the coupling of similar floorplans retrieved
from the embedding space with respect to a given input (e.g., design layout).
The qualitative, quantitative and user-study evaluations show that our
embedding framework produces meaningful and accurate vector representations for
floorplans. In addition, our proposed model is a generative model. We studied
and showcased its effectiveness for generating new floorplans. We also release
the dataset that we have constructed and which, for each floorplan, includes
the design semantics attributes as well as simulation generated human
behavioral features for further study in the community.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04808</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04808</id><submitter>Yichong Xu</submitter><version version="v1"><date>Wed, 9 Dec 2020 00:57:49 GMT</date><size>878kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:11:10 GMT</date><size>5476kb</size><source_type>D</source_type></version><title>Fusing Context Into Knowledge Graph for Commonsense Question Answering</title><authors>Yichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu, Michael Zeng, Xuedong
  Huang</authors><categories>cs.CL</categories><comments>To appear at ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Commonsense question answering (QA) requires a model to grasp commonsense and
factual knowledge to answer questions about world events. Many prior methods
couple language modeling with knowledge graphs (KG). However, although a KG
contains rich structural information, it lacks the context to provide a more
precise understanding of the concepts. This creates a gap when fusing knowledge
graphs into language modeling, especially when there is insufficient labeled
data. Thus, we propose to employ external entity descriptions to provide
contextual information for knowledge understanding. We retrieve descriptions of
related concepts from Wiktionary and feed them as additional input to
pre-trained language models. The resulting model achieves state-of-the-art
result in the CommonsenseQA dataset and the best result among non-generative
models in OpenBookQA.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.05211</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.05211</id><submitter>Shih-Hao Tseng</submitter><version version="v1"><date>Wed, 9 Dec 2020 18:09:52 GMT</date><size>1588kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 03:47:08 GMT</date><size>1654kb</size><source_type>D</source_type></version><title>Synthesis to Deployment: Cyber-Physical Control Architectures</title><authors>Shih-Hao Tseng and James Anderson</authors><categories>math.OC cs.SY eess.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1911.01510</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of how to deploy a controller to a (networked)
cyber-physical system (CPS). Controlling a CPS is an involved task, and
synthesizing a controller to respect sensing, actuation, and communication
constraints is only part of the challenge. In addition to controller synthesis,
one should also consider how the controller will be incorporated within the
CPS. Put another way, the cyber layer and its interaction with the physical
layer need to be taken into account.
  In this work, we aim to bridge the gap between theoretical controller
synthesis and practical CPS deployment. We adopt the system level synthesis
(SLS) framework to synthesize a controller, either state-feedback or
output-feedback, and provide deployment architectures for the standard SLS
controllers. Furthermore, we derive new controller realizations for open-loop
stable systems and introduce different state-feedback and output-feedback
architectures for deployment, ranging from fully centralized to fully
distributed. Finally, we compare the trade-offs among them in terms of
robustness, memory, computation, and communication overhead.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.05395</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.05395</id><submitter>Zhaofeng Wu</submitter><version version="v1"><date>Thu, 10 Dec 2020 01:27:24 GMT</date><size>3603kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 20 Dec 2020 19:56:30 GMT</date><size>3603kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 8 Feb 2021 07:40:54 GMT</date><size>3605kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 19:09:55 GMT</date><size>3603kb</size><source_type>D</source_type></version><title>Infusing Finetuning with Semantic Dependencies</title><authors>Zhaofeng Wu, Hao Peng, Noah A. Smith</authors><categories>cs.CL</categories><comments>TACL 2021</comments><doi>10.1162/tacl_a_00363</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For natural language processing systems, two kinds of evidence support the
use of text representations from neural language models &quot;pretrained&quot; on large
unannotated corpora: performance on application-inspired benchmarks (Peters et
al., 2018, inter alia), and the emergence of syntactic abstractions in those
representations (Tenney et al., 2019, inter alia). On the other hand, the lack
of grounded supervision calls into question how well these representations can
ever capture meaning (Bender and Koller, 2020). We apply novel probes to recent
language models -- specifically focusing on predicate-argument structure as
operationalized by semantic dependencies (Ivanova et al., 2012) -- and find
that, unlike syntax, semantics is not brought to the surface by today's
pretrained models. We then use convolutional graph encoders to explicitly
incorporate semantic parses into task-specific finetuning, yielding benefits to
natural language understanding (NLU) tasks in the GLUE benchmark. This approach
demonstrates the potential for general-purpose (rather than task-specific)
linguistic supervision, above and beyond conventional pretraining and
finetuning. Several diagnostics help to localize the benefits of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.05422</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.05422</id><submitter>Ziyang Wang</submitter><version version="v1"><date>Thu, 10 Dec 2020 02:45:33 GMT</date><size>953kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 16:04:00 GMT</date><size>1350kb</size><source_type>D</source_type></version><title>Exploiting Group-level Behavior Pattern forSession-based Recommendation</title><authors>Ziyang Wang, Wei Wei, Xian-Ling Mao, Xiao-Li Li and Shanshan Feng</authors><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session-based recommendation (SBR) is a challenging task, which aims to
predict users' future interests based on anonymous behavior sequences. Existing
methods leverage powerful representation learning approaches to encode sessions
into a low-dimensional space. However, despite such achievements, all the
existing studies focus on the instance-level session learning, while neglecting
the group-level users' preference, which is significant to model the users'
behavior. To this end, we propose a novel Repeat-aware Neural Mechanism for
Session-based Recommendation (RNMSR). In RNMSR, we propose to learn the user
preference from both instance-level and group-level, respectively: (i)
instance-level, which employs GNNs on a similarity-based item-pairwise session
graph to capture the users' preference in instance-level. (ii) group-level,
which converts sessions into group-level behavior patterns to model the
group-level users' preference. In RNMSR, we combine instance-level user
preference and group-level user preference to model the repeat consumption of
users, \ie whether users take repeated consumption and which items are
preferred by users. Extensive experiments are conducted on three real-world
datasets, \ie Diginetica, Yoochoose, and Nowplaying, demonstrating that the
proposed method consistently achieves state-of-the-art performance in all the
tests.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.05471</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.05471</id><submitter>Vitaly Cheptsov</submitter><version version="v1"><date>Thu, 10 Dec 2020 06:21:44 GMT</date><size>22kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 10:57:49 GMT</date><size>23kb</size><source_type>D</source_type></version><title>Securing the EDK II Image Loader</title><authors>Marvin H\&quot;auser, Vitaly Cheptsov</authors><categories>cs.CR cs.SE</categories><comments>10 pages, 2 tables</comments><journal-ref>2020 Ivannikov Ispras Open Conference (ISPRAS), 2020, pp. 16-25</journal-ref><doi>10.1109/ISPRAS51486.2020.00010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Extensible Firmware Interface (UEFI) is a standardised interface
between the firmware and the operating system used in all x86-based platforms
over the past ten years, which continues to spread to other architectures such
as ARM and RISC-V. The UEFI incorporates a modular design based on images
containing a driver or an application in a Common Object File Format (COFF)
either as a Portable Executable (PE) or as a Terse Executable (TE). The
de-facto standard generic UEFI services implementation, including the image
loading functionality, is TianoCore EDK II. Its track of security issues shows
numerous design and implementation flaws some of which are yet to be addressed.
In this paper we outline both the requirements for a secure UEFI Image Loader
and the issues of the existing implementation. As an alternative we propose a
formally verified Image Loader supporting both PE and TE images with
fine-grained hardening enabling a seamless integration with EDK II and
subsequently with the other firmwares.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.06390</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.06390</id><submitter>Ferhat Ozgur Catak</submitter><version version="v1"><date>Fri, 11 Dec 2020 14:44:59 GMT</date><size>2113kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 10:02:47 GMT</date><size>2376kb</size><source_type>D</source_type></version><title>Closeness and Uncertainty Aware Adversarial Examples Detection in
  Adversarial Machine Learning</title><authors>Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil</authors><categories>cs.LG cs.AI</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While state-of-the-art Deep Neural Network (DNN) models are considered to be
robust to random perturbations, it was shown that these architectures are
highly vulnerable to deliberately crafted perturbations, albeit being
quasi-imperceptible. These vulnerabilities make it challenging to deploy DNN
models in security-critical areas. In recent years, many research studies have
been conducted to develop new attack methods and come up with new defense
techniques that enable more robust and reliable models. In this work, we
explore and assess the usage of different type of metrics for detecting
adversarial samples. We first leverage the usage of moment-based predictive
uncertainty estimates of a DNN classifier obtained using Monte-Carlo Dropout
Sampling. And we also introduce a new method that operates in the subspace of
deep features extracted by the model. We verified the effectiveness of our
approach on a range of standard datasets like MNIST (Digit), MNIST (Fashion)
and CIFAR-10. Our experiments show that these two different approaches
complement each other, and the combined usage of all the proposed metrics
yields up to 99 \% ROC-AUC scores regardless of the attack algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.06508</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.06508</id><submitter>Charles Corbi\`ere</submitter><version version="v1"><date>Fri, 11 Dec 2020 17:21:12 GMT</date><size>3854kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 17:24:34 GMT</date><size>2733kb</size><source_type>D</source_type></version><title>Confidence Estimation via Auxiliary Models</title><authors>Charles Corbi\`ere, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu,
  Matthieu Cord, Patrick P\'erez</authors><categories>cs.CV cs.LG stat.ML</categories><comments>Accepted to TPAMI 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reliably quantifying the confidence of deep neural classifiers is a
challenging yet fundamental requirement for deploying such models in
safety-critical applications. In this paper, we introduce a novel target
criterion for model confidence, namely the true class probability (TCP). We
show that TCP offers better properties for confidence estimation than standard
maximum class probability (MCP). Since the true class is by essence unknown at
test time, we propose to learn TCP criterion from data with an auxiliary model,
introducing a specific learning scheme adapted to this context. We evaluate our
approach on the task of failure prediction and of self-training with
pseudo-labels for domain adaptation, which both necessitate effective
confidence estimates. Extensive experiments are conducted for validating the
relevance of the proposed approach in each task. We study various network
architectures and experiment with small and large datasets for image
classification and semantic segmentation. In every tested benchmark, our
approach outperforms strong baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.06881</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.06881</id><submitter>Muhammad Fayaz</submitter><version version="v1"><date>Sat, 12 Dec 2020 18:26:55 GMT</date><size>2283kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 10:05:13 GMT</date><size>2642kb</size><source_type>D</source_type></version><title>Transmit Power Pool Design for Grant-Free NOMA-IoT Networks via Deep
  Reinforcement Learning</title><authors>Muhammad Fayaz, Wenqiang Yi, Yuanwei Liu, and Arumugam Nallanathan</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grant-free non-orthogonal multiple access (GF-NOMA) is a potential multiple
access framework for short-packet internet-of-things (IoT) networks to enhance
connectivity. However, the resource allocation problem in GF-NOMA is
challenging due to the absence of closed-loop power control. We design a
prototype of transmit power pool (PP) to provide open-loop power control. IoT
users acquire their transmit power in advance from this prototype PP solely
according to their communication distances. Firstly, a multi-agent deep
Q-network (DQN) aided GF-NOMA algorithm is proposed to determine the optimal
transmit power levels for the prototype PP. More specifically, each IoT user
acts as an agent and learns a policy by interacting with the wireless
environment that guides them to select optimal actions. Secondly, to prevent
the Q-learning model overestimation problem, double DQN based GF-NOMA algorithm
is proposed. Numerical results confirm that the double DQN based algorithm
finds out the optimal transmit power levels that form the PP. Comparing with
the conventional online learning approach, the proposed algorithm with the
prototype PP converges faster under changing environments due to limiting the
action space based on previous learning. The considered GF-NOMA system
outperforms the networks with fixed transmission power, namely all the users
have the same transmit power and the traditional GF with orthogonal multiple
access techniques, in terms of throughput.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.06925</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.06925</id><submitter>Leighton Wilson</submitter><version version="v1"><date>Sun, 13 Dec 2020 00:09:10 GMT</date><size>2591kb</size><source_type>D</source_type></version><title>A GPU-Accelerated Fast Summation Method Based on Barycentric Lagrange
  Interpolation and Dual Tree Traversal</title><authors>Leighton Wilson, Nathan Vaughn, and Robert Krasny</authors><categories>physics.comp-ph cs.DC</categories><comments>31 pages, 36 figures</comments><doi>10.1016/j.cpc.2021.108017</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present the barycentric Lagrange dual tree traversal (BLDTT) fast
summation method for particle interactions. The scheme replaces well-separated
particle-particle interactions by adaptively chosen particle-cluster,
cluster-particle, and cluster-cluster approximations given by barycentric
Lagrange interpolation at proxy particles on a Chebyshev grid in each cluster.
The BLDTT is kernel-independent and the approximations can be efficiently
mapped onto GPUs, where target particles provide an outer level of parallelism
and source particles provide an inner level of parallelism. We present an
OpenACC GPU implementation of the BLDTT with MPI remote memory access for
distributed memory parallelization. The performance of the GPU-accelerated
BLDTT is demonstrated for calculations with different problem sizes, particle
distributions, geometric domains, and interaction kernels, as well as for
unequal target and source particles. Comparison with our earlier
particle-cluster barycentric Lagrange treecode (BLTC) demonstrates the superior
performance of the BLDTT. In particular, on a single GPU for problem sizes
ranging from $N$=1E5 to 1E8, the BLTC has $O(N\log N)$ scaling, while the BLDTT
has $O(N)$ scaling. In addition, MPI strong scaling results are presented for
the BLTC and BLDTT using $N$=64E6 particles on up to 32 GPUs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.07297</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.07297</id><submitter>Jian Liang</submitter><version version="v1"><date>Mon, 14 Dec 2020 07:28:50 GMT</date><size>1210kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 09:32:39 GMT</date><size>2253kb</size><source_type>D</source_type></version><title>Source Data-absent Unsupervised Domain Adaptation through Hypothesis
  Transfer and Labeling Transfer</title><authors>Jian Liang and Dapeng Hu and Yunbo Wang and Ran He and Jiashi Feng</authors><categories>cs.CV cs.LG</categories><comments>More interesting results are further shown in
  https://github.com/tim-learn/SHOT-plus/blob/master/supp/shot%2B%2B_supp.pdf.
  arXiv admin note: text overlap with arXiv:2002.08546</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
related but different well-labeled source domain to a new unlabeled target
domain. Most existing UDA methods require access to the source data, and thus
are not applicable when the data are confidential and not shareable due to
privacy concerns. This paper aims to tackle a realistic setting with only a
classification model available trained over, instead of accessing to, the
source data. To effectively utilize the source model for adaptation, we propose
a novel approach called Source HypOthesis Transfer (SHOT), which learns the
feature extraction module for the target domain by fitting the target data
features to the frozen source classification module (representing
classification hypothesis). Specifically, SHOT exploits both information
maximization and self-supervised learning for the feature extraction module
learning to ensure the target features are implicitly aligned with the features
of unseen source data via the same hypothesis. Furthermore, we propose a new
labeling transfer strategy, which separates the target data into two splits
based on the confidence of predictions (labeling information), and then employ
semi-supervised learning to improve the accuracy of less-confident predictions
in the target domain. We denote labeling transfer as SHOT++ if the predictions
are obtained by SHOT. Extensive experiments on both digit classification and
object recognition tasks show that SHOT and SHOT++ achieve results surpassing
or comparable to the state-of-the-arts, demonstrating the effectiveness of our
approaches for various visual domain adaptation problems. Code will be
available at \url{https://github.com/tim-learn/SHOT-plus}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.07719</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.07719</id><submitter>Dongxiao Zhang</submitter><version version="v1"><date>Sun, 29 Nov 2020 10:55:58 GMT</date><size>2620kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 06:32:43 GMT</date><size>2165kb</size></version><title>Digital rock reconstruction with user-defined properties using
  conditional generative adversarial networks</title><authors>Qiang Zheng and Dongxiao Zhang</authors><categories>cs.CV cs.AI cs.LG physics.data-an</categories><comments>36 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncertainty is ubiquitous with flow in subsurface rocks because of their
inherent heterogeneity and lack of in-situ measurements. To complete
uncertainty analysis in a multi-scale manner, it is a prerequisite to provide
sufficient rock samples. Even though the advent of digital rock technology
offers opportunities to reproduce rocks, it still cannot be utilized to provide
massive samples due to its high cost, thus leading to the development of
diversified mathematical methods. Among them, two-point statistics (TPS) and
multi-point statistics (MPS) are commonly utilized, which feature incorporating
low-order and high-order statistical information, respectively. Recently,
generative adversarial networks (GANs) are becoming increasingly popular since
they can reproduce training images with excellent visual and consequent
geologic realism. However, standard GANs can only incorporate information from
data, while leaving no interface for user-defined properties, and thus may
limit the representativeness of reconstructed samples. In this study, we
propose conditional GANs for digital rock reconstruction, aiming to reproduce
samples not only similar to the real training data, but also satisfying
user-specified properties. In fact, the proposed framework can realize the
targets of MPS and TPS simultaneously by incorporating high-order information
directly from rock images with the GANs scheme, while preserving low-order
counterparts through conditioning. We conduct three reconstruction experiments,
and the results demonstrate that rock type, rock porosity, and correlation
length can be successfully conditioned to affect the reconstructed rock images.
Furthermore, in contrast to existing GANs, the proposed conditioning enables
learning of multiple rock types simultaneously, and thus invisibly saves
computational cost.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.08346</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.08346</id><submitter>Daniel Dadush</submitter><version version="v1"><date>Tue, 15 Dec 2020 14:59:44 GMT</date><size>40kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 15:15:36 GMT</date><size>37kb</size></version><title>On the Integrality Gap of Binary Integer Programs with Gaussian Data</title><authors>Sander Borst, Daniel Dadush, Sophie Huiberts, Samarth Tiwari</authors><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a binary integer program (IP) ${\rm max} ~ c^\mathsf{T} x, Ax \leq b, x
\in \{0,1\}^n$, where $A \in \mathbb{R}^{m \times n}$ and $c \in \mathbb{R}^n$
have independent Gaussian entries and the right-hand side $b \in \mathbb{R}^m$
satisfies that its negative coordinates have $\ell_2$ norm at most $n/10$, we
prove that the gap between the value of the linear programming relaxation and
the IP is upper bounded by $\operatorname{poly}(m)(\log n)^2 / n$ with
probability at least $1-2/n^7-2^{-\operatorname{poly}(m)}$. Our results give a
Gaussian analogue of the classical integrality gap result of Dyer and Frieze
(Math. of O.R., 1989) in the case of random packing IPs. In constrast to the
packing case, our integrality gap depends only polynomially on $m$ instead of
exponentially. Building upon recent breakthrough work of Dey, Dubey and
Molinaro (SODA, 2021), we show that the integrality gap implies that
branch-and-bound requires $n^{\operatorname{poly}(m)}$ time on random Gaussian
IPs with good probability, which is polynomial when the number of constraints
$m$ is fixed. We derive this result via a novel meta-theorem, which relates the
size of branch-and-bound trees and the integrality gap for random logconcave
IPs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.08890</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.08890</id><submitter>Dan Jia</submitter><version version="v1"><date>Wed, 16 Dec 2020 12:10:04 GMT</date><size>8137kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 13:30:48 GMT</date><size>8140kb</size><source_type>D</source_type></version><title>Self-Supervised Person Detection in 2D Range Data using a Calibrated
  Camera</title><authors>Dan Jia and Mats Steinweg and Alexander Hermans and Bastian Leibe</authors><categories>cs.CV cs.RO</categories><comments>2021 IEEE International Conference on Robotics and Automation (ICRA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning is the essential building block of state-of-the-art person
detectors in 2D range data. However, only a few annotated datasets are
available for training and testing these deep networks, potentially limiting
their performance when deployed in new environments or with different LiDAR
models. We propose a method, which uses bounding boxes from an image-based
detector (e.g. Faster R-CNN) on a calibrated camera to automatically generate
training labels (called pseudo-labels) for 2D LiDAR-based person detectors.
Through experiments on the JackRabbot dataset with two detector models, DROW3
and DR-SPAAM, we show that self-supervised detectors, trained or fine-tuned
with pseudo-labels, outperform detectors trained only on a different dataset.
Combined with robust training techniques, the self-supervised detectors reach a
performance close to the ones trained using manual annotations of the target
dataset. Our method is an effective way to improve person detectors during
deployment without any additional labeling effort, and we release our source
code to support relevant robotic applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09331</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09331</id><submitter>Brian Reily</submitter><version version="v1"><date>Thu, 17 Dec 2020 00:12:58 GMT</date><size>31694kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 07:51:19 GMT</date><size>6273kb</size><source_type>D</source_type></version><title>Team Assignment for Heterogeneous Multi-Robot Sensor Coverage through
  Graph Representation Learning</title><authors>Brian Reily, Hao Zhang</authors><categories>cs.RO cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor coverage is the critical multi-robot problem of maximizing the
detection of events in an environment through the deployment of multiple
robots. Large multi-robot systems are often composed of simple robots that are
typically not equipped with a complete set of sensors, so teams with
comprehensive sensing abilities are required to properly cover an area. Robots
also exhibit multiple forms of relationships (e.g., communication connections
or spatial distribution) that need to be considered when assigning robot teams
for sensor coverage. To address this problem, in this paper we introduce a
novel formulation of sensor coverage by multi-robot systems with heterogeneous
relationships as a graph representation learning problem. We propose a
principled approach based on the mathematical framework of regularized
optimization to learn a unified representation of the multi-robot system from
the graphs describing the heterogeneous relationships and to identify the
learned representation's underlying structure in order to assign the robots to
teams. To evaluate the proposed approach, we conduct extensive experiments on
simulated multi-robot systems and a physical multi-robot system as a case
study, demonstrating that our approach is able to effectively assign teams for
heterogeneous multi-robot sensor coverage.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09334</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09334</id><submitter>Brian Reily</submitter><version version="v1"><date>Thu, 17 Dec 2020 00:22:18 GMT</date><size>4039kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 07:44:30 GMT</date><size>4189kb</size><source_type>D</source_type></version><title>Adaptation to Team Composition Changes for Heterogeneous Multi-Robot
  Sensor Coverage</title><authors>Brian Reily, Terran Mott, Hao Zhang</authors><categories>cs.RO cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of multi-robot sensor coverage, which deals with
deploying a multi-robot team in an environment and optimizing the sensing
quality of the overall environment. As real-world environments involve a
variety of sensory information, and individual robots are limited in their
available number of sensors, successful multi-robot sensor coverage requires
the deployment of robots in such a way that each individual team member's
sensing quality is maximized. Additionally, because individual robots have
varying complements of sensors and both robots and sensors can fail, robots
must be able to adapt and adjust how they value each sensing capability in
order to obtain the most complete view of the environment, even through changes
in team composition. We introduce a novel formulation for sensor coverage by
multi-robot teams with heterogeneous sensing capabilities that maximizes each
robot's sensing quality, balancing the varying sensing capabilities of
individual robots based on the overall team composition. We propose a solution
based on regularized optimization that uses sparsity-inducing terms to ensure a
robot team focuses on all possible event types, and which we show is proven to
converge to the optimal solution. Through extensive simulation, we show that
our approach is able to effectively deploy a multi-robot team to maximize the
sensing quality of an environment, responding to failures in the multi-robot
team more robustly than non-adaptive approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09422</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09422</id><submitter>Andrew Bennett</submitter><version version="v1"><date>Thu, 17 Dec 2020 07:21:06 GMT</date><size>42kb</size></version><version version="v2"><date>Fri, 28 May 2021 20:57:53 GMT</date><size>64kb</size></version><title>The Variational Method of Moments</title><authors>Andrew Bennett, Nathan Kallus</authors><categories>cs.LG econ.EM math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The conditional moment problem is a powerful formulation for describing
structural causal parameters in terms of observables, a prominent example being
instrumental variable regression. A standard approach is to reduce the problem
to a finite set of marginal moment conditions and apply the optimally weighted
generalized method of moments (OWGMM), but this requires we know a finite set
of identifying moments, can still be inefficient even if identifying, or can be
theoretically efficient but practically unwieldy if we use a growing sieve of
moment conditions. Motivated by a variational minimax reformulation of OWGMM,
we define a very general class of estimators for the conditional moment
problem, which we term the variational method of moments (VMM) and which
naturally enables controlling infinitely-many moments. We provide a detailed
theoretical analysis of multiple VMM estimators, including ones based on kernel
methods and neural nets, and provide appropriate conditions under which these
estimators are consistent, asymptotically normal, and semiparametrically
efficient in the full conditional moment model. This is in contrast to other
recently proposed methods for solving conditional moment problems based on
adversarial machine learning, which do not incorporate optimal weighting, do
not establish asymptotic normality, and are not semiparametrically efficient.
In addition, we provide corresponding inference algorithms based on the same
kind of variational reformulations, both for kernel- and neural net-based
varieties. Finally, we demonstrate the strong performance of our proposed
estimation and inference algorithms in a detailed series of synthetic
experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09432</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09432</id><submitter>Sanjaya Lohani</submitter><version version="v1"><date>Thu, 17 Dec 2020 07:51:47 GMT</date><size>2198kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 03:36:43 GMT</date><size>1786kb</size><source_type>D</source_type></version><title>On the experimental feasibility of quantum state reconstruction via
  machine learning</title><authors>Sanjaya Lohani, Thomas A. Searles, Brian T. Kirby, and Ryan T. Glasser</authors><categories>quant-ph cs.AI cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the resource scaling of machine learning-based quantum state
reconstruction methods, in terms of inference and training, for systems of up
to four qubits when constrained to pure states. Further, we examine system
performance in the low-count regime, likely to be encountered in the tomography
of high-dimensional systems. Finally, we implement our quantum state
reconstruction method on an IBM Q quantum computer, and compare against both
unconstrained and constrained MLE state reconstruction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09778</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09778</id><submitter>Marco De Angelis</submitter><version version="v1"><date>Thu, 17 Dec 2020 17:40:33 GMT</date><size>348kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 21:42:07 GMT</date><size>570kb</size><source_type>D</source_type></version><title>Interval propagation through the discrete Fourier transform</title><authors>Marco De Angelis, Marco Behrendt, Liam Comerford, Yuanjin Zhang,
  Michael Beer</authors><categories>eess.SP cs.NA cs.SY eess.SY math.NA</categories><comments>This work will appear in proceedings of the 9th international
  workshop on Reliable Engineering Computing</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present an algorithm for the forward propagation of intervals through the
discrete Fourier transform. The algorithm yields best-possible bounds when
computing the amplitude of the Fourier transform for real and complex valued
sequences. We show that computing the exact bounds of the amplitude can be
achieved with an exhaustive examination of all possible corners of the interval
domain. However, because the number of corners increases exponentially with the
number of intervals, such method is infeasible for large interval signals. We
provide an algorithm that does not need such an exhaustive search, and show
that the best possible bounds can be obtained propagating complex pairs only
from the convex hull of endpoints at each term of the Fourier series. Because
the convex hull is always tightly inscribed in the respective rigorous bounding
box resulting from interval arithmetic, we conclude that the obtained bounds
are guaranteed to enclose the true values.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09984</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09984</id><submitter>Z Yan</submitter><version version="v1"><date>Fri, 18 Dec 2020 00:09:21 GMT</date><size>1089kb</size></version><title>Data-driven rogue waves and parameter discovery in the defocusing NLS
  equation with a potential using the PINN deep learning</title><authors>Li Wang, Zhenya Yan</authors><categories>nlin.PS cs.LG quant-ph</categories><comments>12 pages, 5 figures</comments><doi>10.1016/j.physleta.2021.127408</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The physics-informed neural networks (PINNs) can be used to deep learn the
nonlinear partial differential equations and other types of physical models. In
this paper, we use the multi-layer PINN deep learning method to study the
data-driven rogue wave solutions of the defocusing nonlinear Schr\&quot;odinger
(NLS) equation with the time-dependent potential by considering several initial
conditions such as the rogue wave, Jacobi elliptic cosine function,
two-Gaussian function, or three-hyperbolic-secant function, and periodic
boundary conditions. Moreover, the multi-layer PINN algorithm can also be used
to learn the parameter in the defocusing NLS equation with the time-dependent
potential under the sense of the rogue wave solution. These results will be
useful to further discuss the rogue wave solutions of the defocusing NLS
equation with a potential in the study of deep learning neural networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.10146</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.10146</id><submitter>Conor McMenamin</submitter><version version="v1"><date>Fri, 18 Dec 2020 10:13:35 GMT</date><size>85kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:35:54 GMT</date><size>233kb</size><source_type>D</source_type></version><title>Achieving State Machine Replication without Honest Players</title><authors>Conor McMenamin and Vanesa Daza and Matteo Pontecorvi</authors><categories>cs.GT cs.DC cs.MA</categories><comments>14 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing standards for player characterisation in tokenised state machine
replication protocols depend on honest players who will always follow the
protocol, regardless of possible token increases for deviating. Given the
ever-increasing market capitalisation of these tokenised protocols, honesty is
becoming more expensive and more unrealistic. As such, this out-dated player
characterisation must be removed to provide true guarantees of safety and
liveness in a major stride towards universal trust in state machine replication
protocols and a new scale of adoption. As all current state machine replication
protocols are built on these legacy standards, it is imperative that a new
player model is identified and utilised to reflect the true nature of players
in tokenised protocols, now and into the future.
  To this effect, we propose the ByRa player model for state machine
replication protocols. In the ByRa model, players either attempt to maximise
their tokenised rewards, or behave adversarially. This merges the fields of
game theory and distributed systems, an intersection in which tokenised state
machine replication protocols exist, but on which little formalisation has been
carried out. In the ByRa model, we identify the properties of strong incentive
compatibility in expectation and fairness that all protocols must satisfy in
order to achieve state machine replication. We then provide Tenderstake, a
protocol which provably satisfies these properties, and by doing so, achieves
state machine replication in the ByRa model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.10219</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.10219</id><submitter>Fidan Mehmeti</submitter><version version="v1"><date>Fri, 18 Dec 2020 13:30:06 GMT</date><size>142kb</size></version><version version="v2"><date>Mon, 21 Dec 2020 21:51:09 GMT</date><size>142kb</size></version><version version="v3"><date>Sat, 2 Jan 2021 19:02:24 GMT</date><size>142kb</size></version><version version="v4"><date>Sat, 29 May 2021 17:52:31 GMT</date><size>139kb</size></version><title>Resource Allocation for Improved User Experience with Live Video
  Streaming in 5G</title><authors>Fidan Mehmeti and Thomas F. La Porta</authors><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Providing a high-quality real-time video streaming experience to mobile users
is one of the biggest challenges in cellular networks. This is due to the need
of these services for high rates with low variability, which is not easy to
accomplish given the competition among (usually a high number of) users for
constrained network resources and the high variability of their channel
characteristics. A way of improving the user experience is by exploiting their
buffers and the ability to provide a constant data rate to everyone, as one of
the features of 5G networks. However, the latter is not very efficient. To this
end, in this paper we provide a theoretical-analysis framework for resource
allocation in 5G networks that leads to an improved user experience when
watching live video. We do this by solving three problems, in which the
objectives are to provide the highest achievable video resolution to all
one-class and two-class users, and to maximize the number of users that
experience a given resolution. The analysis is validated by simulations that
are run on traces. We also compare the performance of our approach against
other techniques for different QoE metrics. Results show that the performance
can be improved by at least 15% with our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.10584</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.10584</id><submitter>Matthew Kwan</submitter><version version="v1"><date>Sat, 19 Dec 2020 03:22:26 GMT</date><size>11kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 19:53:14 GMT</date><size>12kb</size></version><title>List-decodability with large radius for Reed-Solomon codes</title><authors>Asaf Ferber, Matthew Kwan, Lisa Sauermann</authors><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  List-decodability of Reed-Solomon codes has received a lot of attention, but
the best-possible dependence between the parameters is still not
well-understood. In this work, we focus on the case where the list-decoding
radius is of the form $r=1-\varepsilon$ for $\varepsilon$ tending to zero. Our
main result states that there exist Reed-Solomon codes with rate
$\Omega(\varepsilon)$ which are $(1-\varepsilon,
O(1/\varepsilon))$-list-decodable, meaning that any Hamming ball of radius
$1-\varepsilon$ contains at most $O(1/\varepsilon)$ codewords. This trade-off
between rate and list-decoding radius is best-possible for any code with list
size less than exponential in the block length.
  By achieving this trade-off between rate and list-decoding radius we improve
a recent result of Guo, Li, Shangguan, Tamo, and Wootters, and resolve the main
motivating question of their work. Moreover, while their result requires the
field to be exponentially large in the block length, we only need the field
size to be polynomially large (and in fact, almost-linear suffices). We deduce
our main result from a more general theorem, in which we prove good
list-decodability properties of random puncturings of any given code with very
large distance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.10589</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.10589</id><submitter>Yibin Wu</submitter><version version="v1"><date>Sat, 19 Dec 2020 04:11:18 GMT</date><size>1424kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 03:21:54 GMT</date><size>1441kb</size></version><version version="v3"><date>Wed, 2 Jun 2021 02:02:16 GMT</date><size>1441kb</size></version><title>A Comparison of Three Measurement Models for the Wheel-mounted MEMS
  IMU-based Dead Reckoning System</title><authors>Yibin Wu, Xiaoji Niu, Jian Kuang</authors><categories>cs.RO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1912.07805</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A self-contained autonomous dead reckoning (DR) system is desired to
complement the Global Navigation Satellite System (GNSS) for land vehicles, for
which odometer-aided inertial navigation system (ODO/INS) is a classical
solution. In this study, we use a wheel-mounted MEMS IMU (Wheel-IMU) to
substitute the odometer, and further, investigate three types of measurement
models, including the velocity measurement, displacement increment measurement,
and contact point zero-velocity measurement, in the Wheel-IMU based DR system.
The measurement produced by the Wheel-IMU along with the non-holonomic
constraint (NHC) are fused with INS through an error-state extended Kalman
filter (EKF). Theoretical discussion and field tests illustrate the feasibility
and equivalence of the three measurements in terms of the overall DR
performance. The maximum horizontal position drifts are all less than 2% of the
total travelled distance. Additionally, the displacement increment measurement
model is less sensitive to the lever arm error between the Wheel-IMU and the
wheel center.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.10644</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.10644</id><submitter>Aniq Ur Rahman</submitter><version version="v1"><date>Sat, 19 Dec 2020 10:06:31 GMT</date><size>837kb</size></version><version version="v2"><date>Sun, 30 May 2021 20:46:32 GMT</date><size>643kb</size></version><title>A Game-Theoretic Framework for Coexistence of WiFi and Cellular Networks
  in the 6-GHz Unlicensed Spectrum</title><authors>Aniq Ur Rahman, Mustafa A. Kishk, Mohamed-Slim Alouini</authors><categories>eess.SY cs.GT cs.IT cs.SY math.IT</categories><comments>25 pages, 13 figures, 1 table. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behaviour of WiFi and 5G cellular networks as they exploit the
recently unlocked 6 GHz spectrum for unlicensed access, while conforming to the
constraints imposed by the incumbent users. We use tools from stochastic
geometry to derive the theoretical performance metrics for users of each radio
access technology, which helps us in capturing the aggregate behaviour of the
network in a snapshot. We propose a framework where the portions of cellular
and WiFi networks are grouped together to form entities. These entities
interact to satisfy their Quality of Service demands by playing a
non-cooperative game. The action of an entity corresponds to the fraction of
its network elements (WiFi access point and cellular base stations) operating
in the 6 GHz band. Due to the decentralized nature of the entities, we find the
Nash equilibrium using distributed Best Response Algorithm (D-BRA), where each
entity takes actions without any centralized scheduling. D-BRA improves the
average datarate by $11.37\%$ and $18.59\%$ for cellular and WiFi networks
respectively with a random strategy as baseline. The results demonstrate how
the system parameters affect the performance of a network at equilibrium and
highlight the throughput gains of the networks as a result of using the 6 GHz
bands, which offer considerably larger bandwidths. The proposed framework is
flexible and can be used to model a variety of scenarios for feasibility and
performance assessment of the networks involved.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.10861</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.10861</id><submitter>Lei Liu</submitter><version version="v1"><date>Sun, 20 Dec 2020 07:42:15 GMT</date><size>202kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 10 Feb 2021 14:31:38 GMT</date><size>305kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 04:08:05 GMT</date><size>307kb</size><source_type>D</source_type></version><title>Memory AMP</title><authors>Lei Liu, Shunqi Huang and Brian M. Kurkoski</authors><categories>cs.IT cs.AI cs.LG eess.SP math.IT math.ST stat.TH</categories><comments>30 pages, 9 figures, submitted to IEEE Trans. on Information Theory
  for possible publication. [Memory AMP inherits the strengths of AMP and
  OAMP/VAMP such as low complexity, Bayes optimality and applicability to
  unitarily-inavariant matrices, while avoiding the weakness of AMP (e.g.
  limited to IID matrices) and OAMP/VAMP (e.g. needs high-complexity LMMSE).]</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Approximate message passing (AMP) is a low-cost iterative
parameter-estimation technique for certain high-dimensional linear systems with
non-Gaussian distributions. However, AMP only applies to independent
identically distributed (IID) transform matrices, but may become unreliable
(e.g. perform poorly or even diverge) for other matrix ensembles, especially
for ill-conditioned ones. To handle this difficulty, orthogonal/vector AMP
(OAMP/VAMP) was proposed for general right-unitarily-invariant matrices.
However, the Bayes-optimal OAMP/VAMP requires high-complexity linear minimum
mean square error (MMSE) estimator. This limits the application of OAMP/VAMP to
large-scale systems.
  To solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory
AMP (MAMP), in which a long-memory matched filter is proposed for interference
suppression. The complexity of MAMP is comparable to AMP. The asymptotic
Gaussianity of estimation errors in MAMP is guaranteed by the orthogonality
principle. A state evolution is derived to asymptotically characterize the
performance of MAMP. Based on state evolution, the relaxation parameters and
damping vector in MAMP are optimized. For all right-unitarily-invariant
matrices, the optimized MAMP converges to the high-complexity OAMP/VAMP, and
thus is Bayes-optimal if it has a unique fixed point. Finally, simulations are
provided to verify the validity and accuracy of the theoretical results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.11207</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.11207</id><submitter>Zhengyu Zhao</submitter><version version="v1"><date>Mon, 21 Dec 2020 09:41:29 GMT</date><size>8458kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 6 Feb 2021 15:18:35 GMT</date><size>6292kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 May 2021 20:50:00 GMT</date><size>6851kb</size><source_type>D</source_type></version><title>On Success and Simplicity: A Second Look at Transferable Targeted
  Attacks</title><authors>Zhengyu Zhao, Zhuoran Liu, Martha Larson</authors><categories>cs.LG cs.CR cs.CV</categories><comments>Code available at https://github.com/ZhengyuZhao/Targeted-Tansfer</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Achieving transferability of targeted attacks is reputed to be remarkably
difficult. Currently, state-of-the-art approaches are resource-intensive
because they necessitate training model(s) for each target class with
additional data. In our investigation, we find, however, that simple
transferable attacks which require neither additional data nor model training
can achieve surprisingly high targeted transferability. This insight has been
overlooked until now, mainly due to the widespread practice of unreasonably
restricting attack optimization to a limited number of iterations. In
particular, we, for the first time, identify that a simple logit loss can yield
competitive results with the state of the arts. Our analysis spans a variety of
transfer settings, especially including three new, realistic settings: an
ensemble transfer setting with little model similarity, a worse-case setting
with low-ranked target classes, and also a real-world attack against the Google
Cloud Vision API. Results in these new settings demonstrate that the commonly
adopted, easy settings cannot fully reveal the actual properties of different
attacks and may cause misleading comparisons. We also show the usefulness of
the simple logit loss for generating targeted universal adversarial
perturbations in a data-free and training-free manner. Overall, the aim of our
analysis is to inspire a more meaningful evaluation on targeted
transferability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.12624</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.12624</id><submitter>Jinhyuk Lee</submitter><version version="v1"><date>Wed, 23 Dec 2020 12:28:17 GMT</date><size>221kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 2 Jan 2021 00:42:50 GMT</date><size>356kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 12:20:23 GMT</date><size>630kb</size><source_type>D</source_type></version><title>Learning Dense Representations of Phrases at Scale</title><authors>Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen</authors><categories>cs.CL</categories><comments>ACL 2021. Code available at
  https://github.com/princeton-nlp/DensePhrases</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open-domain question answering can be reformulated as a phrase retrieval
problem, without the need for processing documents on-demand during inference
(Seo et al., 2019). However, current phrase retrieval models heavily depend on
sparse representations and still underperform retriever-reader approaches. In
this work, we show for the first time that we can learn dense representations
of phrases alone that achieve much stronger performance in open-domain QA. We
present an effective method to learn phrase representations from the
supervision of reading comprehension tasks, coupled with novel negative
sampling methods. We also propose a query-side fine-tuning strategy, which can
support transfer learning and reduce the discrepancy between training and
inference. On five popular open-domain QA datasets, our model DensePhrases
improves over previous phrase retrieval models by 15%-25% absolute accuracy and
matches the performance of state-of-the-art retriever-reader models. Our model
is easy to parallelize due to pure dense representations and processes more
than 10 questions per second on CPUs. Finally, we directly use our pre-indexed
dense phrase representations for two slot filling tasks, showing the promise of
utilizing DensePhrases as a dense knowledge base for downstream tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.13294</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.13294</id><submitter>Xuhui Meng</submitter><version version="v1"><date>Sat, 19 Dec 2020 02:03:53 GMT</date><size>6159kb</size><source_type>D</source_type></version><title>Multi-fidelity Bayesian Neural Networks: Algorithms and Applications</title><authors>Xuhui Meng, Hessam Babaee, and George Em Karniadakis</authors><categories>cs.LG physics.comp-ph</categories><comments>31 pages, 11 figures</comments><doi>10.1016/j.jcp.2021.110361</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a new class of Bayesian neural networks (BNNs) that can be trained
using noisy data of variable fidelity, and we apply them to learn function
approximations as well as to solve inverse problems based on partial
differential equations (PDEs). These multi-fidelity BNNs consist of three
neural networks: The first is a fully connected neural network, which is
trained following the maximum a posteriori probability (MAP) method to fit the
low-fidelity data; the second is a Bayesian neural network employed to capture
the cross-correlation with uncertainty quantification between the low- and
high-fidelity data; and the last one is the physics-informed neural network,
which encodes the physical laws described by PDEs. For the training of the last
two neural networks, we use the Hamiltonian Monte Carlo method to estimate
accurately the posterior distributions for the corresponding hyperparameters.
We demonstrate the accuracy of the present method using synthetic data as well
as real measurements. Specifically, we first approximate a one- and
four-dimensional function, and then infer the reaction rates in one- and
two-dimensional diffusion-reaction systems. Moreover, we infer the sea surface
temperature (SST) in the Massachusetts and Cape Cod Bays using satellite images
and in-situ measurements. Taken together, our results demonstrate that the
present method can capture both linear and nonlinear correlation between the
low- and high-fideilty data adaptively, identify unknown parameters in PDEs,
and quantify uncertainties in predictions, given a few scattered noisy
high-fidelity data. Finally, we demonstrate that we can effectively and
efficiently reduce the uncertainties and hence enhance the prediction accuracy
with an active learning approach, using as examples a specific one-dimensional
function approximation and an inverse PDE problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.13869</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.13869</id><submitter>Abhinav Gupta</submitter><version version="v1"><date>Sun, 27 Dec 2020 05:55:33 GMT</date><size>7855kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 23:56:42 GMT</date><size>10817kb</size><source_type>D</source_type></version><title>Neural Closure Models for Dynamical Systems</title><authors>Abhinav Gupta and Pierre F.J. Lermusiaux</authors><categories>cs.LG math.DS physics.flu-dyn</categories><comments>29 pages, 9 figures, 13 pages of supplementary information</comments><msc-class>68T01 (Primary) 37M05, 34A99, 86-08 (Secondary)</msc-class><acm-class>J.2; I.2.m</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Complex dynamical systems are used for predictions in many domains. Because
of computational costs, models are however often truncated, coarsened, or
aggregated. As the neglected and unresolved terms along with their interactions
with the resolved ones become important, the usefulness of model predictions
diminishes. We develop a novel, versatile, and rigorous methodology to learn
non-Markovian closure parameterizations for low-fidelity models using data from
high-fidelity simulations. The new &quot;neural closure models&quot; augment low-fidelity
models with neural delay differential equations (nDDEs), motivated by the
Mori-Zwanzig formulation and the inherent delays in complex dynamical systems.
We demonstrate that neural closures efficiently account for truncated modes in
reduced-order-models, capture the effects of subgrid-scale processes in coarse
models, and augment the simplification of complex biological and
physical-biogeochemical models. We find that using non-Markovian over Markovian
closures improves long-term prediction accuracy and requires smaller networks.
We derive adjoint equations and network architectures needed to efficiently
implement the new discrete and distributed nDDEs. The performance of discrete
over distributed delays in closure models is explained using information
theory, and we find an optimal amount of past information for a specified
architecture. Finally, we analyze computational complexity and explain the
limited additional cost due to neural closure models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.13962</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.13962</id><submitter>Felix Leibfried</submitter><version version="v1"><date>Sun, 27 Dec 2020 15:25:13 GMT</date><size>561kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 30 Dec 2020 12:25:12 GMT</date><size>561kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 3 Jan 2021 17:18:26 GMT</date><size>571kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 11 Jan 2021 08:44:30 GMT</date><size>571kb</size><source_type>D</source_type></version><version version="v5"><date>Fri, 15 Jan 2021 09:58:34 GMT</date><size>571kb</size><source_type>D</source_type></version><version version="v6"><date>Fri, 29 Jan 2021 11:51:16 GMT</date><size>570kb</size><source_type>D</source_type></version><version version="v7"><date>Tue, 2 Feb 2021 17:02:43 GMT</date><size>572kb</size><source_type>D</source_type></version><version version="v8"><date>Fri, 16 Apr 2021 11:21:38 GMT</date><size>572kb</size><source_type>D</source_type></version><version version="v9"><date>Wed, 2 Jun 2021 19:29:18 GMT</date><size>572kb</size><source_type>D</source_type></version><title>A Tutorial on Sparse Gaussian Processes and Variational Inference</title><authors>Felix Leibfried, Vincent Dutordoir, ST John, Nicolas Durrande</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GPs) provide a framework for Bayesian inference that can
offer principled uncertainty estimates for a large range of problems. For
example, if we consider regression problems with Gaussian likelihoods, a GP
model enjoys a posterior in closed form. However, identifying the posterior GP
scales cubically with the number of training examples and requires to store all
examples in memory. In order to overcome these obstacles, sparse GPs have been
proposed that approximate the true posterior GP with pseudo-training examples.
Importantly, the number of pseudo-training examples is user-defined and enables
control over computational and memory complexity. In the general case, sparse
GPs do not enjoy closed-form solutions and one has to resort to approximate
inference. In this context, a convenient choice for approximate inference is
variational inference (VI), where the problem of Bayesian inference is cast as
an optimization problem -- namely, to maximize a lower bound of the log
marginal likelihood. This paves the way for a powerful and versatile framework,
where pseudo-training examples are treated as optimization arguments of the
approximate posterior that are jointly identified together with hyperparameters
of the generative model (i.e. prior and likelihood). The framework can
naturally handle a wide scope of supervised learning problems, ranging from
regression with heteroscedastic and non-Gaussian likelihoods to classification
problems with discrete labels, but also multilabel problems. The purpose of
this tutorial is to provide access to the basic matter for readers without
prior knowledge in both GPs and VI. A proper exposition to the subject enables
also access to more recent advances (like importance-weighted VI as well as
interdomain, multioutput and deep GPs) that can serve as an inspiration for new
research ideas.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.13972</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.13972</id><submitter>Lun-Pin Yuan</submitter><version version="v1"><date>Sun, 27 Dec 2020 16:31:05 GMT</date><size>3917kb</size><source_type>D</source_type></version><title>Recomposition vs. Prediction: A Novel Anomaly Detection for Discrete
  Events Based On Autoencoder</title><authors>Lun-Pin Yuan, Peng Liu, Sencun Zhu</authors><categories>cs.LG</categories><doi>10.1145/3433210.3453098</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  One of the most challenging problems in the field of intrusion detection is
anomaly detection for discrete event logs. While most earlier work focused on
applying unsupervised learning upon engineered features, most recent work has
started to resolve this challenge by applying deep learning methodology to
abstraction of discrete event entries. Inspired by natural language processing,
LSTM-based anomaly detection models were proposed. They try to predict upcoming
events, and raise an anomaly alert when a prediction fails to meet a certain
criterion. However, such a predict-next-event methodology has a fundamental
limitation: event predictions may not be able to fully exploit the distinctive
characteristics of sequences. This limitation leads to high false positives
(FPs) and high false negatives (FNs). It is also critical to examine the
structure of sequences and the bi-directional causality among individual
events. To this end, we propose a new methodology: Recomposing event sequences
as anomaly detection. We propose DabLog, a Deep Autoencoder-Based anomaly
detection method for discrete event Logs. The fundamental difference is that,
rather than predicting upcoming events, our approach determines whether a
sequence is normal or abnormal by analyzing (encoding) and reconstructing
(decoding) the given sequence. Our evaluation results show that our new
methodology can significantly reduce the numbers of FPs and FNs, hence
achieving a higher $F_1$ score.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14116</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14116</id><submitter>Zenan Xu</submitter><version version="v1"><date>Mon, 28 Dec 2020 06:48:04 GMT</date><size>227kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 08:13:49 GMT</date><size>386kb</size><source_type>D</source_type></version><title>Syntax-Enhanced Pre-trained Model</title><authors>Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong,
  Wanjun Zhong, Xiaojun Quan, Nan Duan and Daxin Jiang</authors><categories>cs.CL</categories><comments>Accepted by ACL-IJCNLP 2021: The Joint Conference of the 59th Annual
  Meeting of the Association for Computational Linguistics and the 11th
  International Joint Conference on Natural Language Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of leveraging the syntactic structure of text to enhance
pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of
text either in the pre-training stage or in the fine-tuning stage, so that they
suffer from discrepancy between the two stages. Such a problem would lead to
the necessity of having human-annotated syntactic information, which limits the
application of existing methods to broader scenarios. To address this, we
present a model that utilizes the syntax of text in both pre-training and
fine-tuning stages. Our model is based on Transformer with a syntax-aware
attention layer that considers the dependency tree of the text. We further
introduce a new pre-training task of predicting the syntactic distance among
tokens in the dependency tree. We evaluate the model on three downstream tasks,
including relation classification, entity typing, and question answering.
Results show that our model achieves state-of-the-art performance on six public
benchmark datasets. We have two major findings. First, we demonstrate that
infusing automatically produced syntax of text improves pre-trained models.
Second, global syntactic distances among tokens bring larger performance gains
compared to local head relations between contiguous tokens.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14117</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14117</id><submitter>Mundher Al-Shabi</submitter><version version="v1"><date>Mon, 28 Dec 2020 06:49:09 GMT</date><size>363kb</size></version><version version="v2"><date>Sat, 2 Jan 2021 06:52:30 GMT</date><size>363kb</size></version><version version="v3"><date>Tue, 1 Jun 2021 10:43:26 GMT</date><size>509kb</size></version><title>3D Axial-Attention for Lung Nodule Classification</title><authors>Mundher Al-Shabi, Kelvin Shak, Maxine Tan</authors><categories>eess.IV cs.CV stat.ML</categories><journal-ref>International Journal of Computer Assisted Radiology and Surgery,
  1-6 (2021)</journal-ref><doi>10.1007/s11548-021-02415-z</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Purpose: In recent years, Non-Local based methods have been successfully
applied to lung nodule classification. However, these methods offer 2D
attention or limited 3D attention to low-resolution feature maps. Moreover,
they still depend on a convenient local filter such as convolution as full 3D
attention is expensive to compute and requires a big dataset, which might not
be available.
  Methods: We propose to use 3D Axial-Attention, which requires a fraction of
the computing power of a regular Non-Local network (i.e., self-attention).
Unlike a regular Non-Local network, the 3D Axial-Attention network applies the
attention operation to each axis separately. Additionally, we solve the
invariant position problem of the Non-Local network by proposing to add 3D
positional encoding to shared embeddings.
  Results: We validated the proposed method on 442 benign nodules and 406
malignant nodules, extracted from the public LIDC-IDRI dataset by following a
rigorous experimental setup using only nodules annotated by at least three
radiologists. Our results show that the 3D Axial-Attention model achieves
state-of-the-art performance on all evaluation metrics, including AUC and
Accuracy.
  Conclusions: The proposed model provides full 3D attention, whereby every
element (i.e., pixel) in the 3D volume space attends to every other element in
the nodule effectively. Thus, the 3D Axial-Attention network can be used in all
layers without the need for local filters. The experimental results show the
importance of full 3D attention for classifying lung nodules.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14193</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14193</id><submitter>Stanislaw Jastrzebski</submitter><version version="v1"><date>Mon, 28 Dec 2020 11:17:46 GMT</date><size>721kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 19:47:02 GMT</date><size>733kb</size><source_type>D</source_type></version><title>Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts
  Generalization</title><authors>Stanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo Kerg,
  Huan Wang, Caiming Xiong, Richard Socher, Kyunghyun Cho, Krzysztof Geras</authors><categories>cs.LG stat.ML</categories><comments>The last two authors contributed equally. Accepted to the
  International Conference on Machine Learning 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The early phase of training of deep neural networks has a dramatic effect on
the local curvature of the loss function. For instance, using a small learning
rate does not guarantee stable optimization because the optimization trajectory
has a tendency to steer towards regions of the loss surface with increasing
local curvature. We ask whether this tendency is connected to the widely
observed phenomenon that the choice of the learning rate strongly influences
generalization. We first show that stochastic gradient descent (SGD) implicitly
penalizes the trace of the Fisher Information Matrix (FIM), a measure of the
local curvature, from the beginning of training. We argue it is an implicit
regularizer in SGD by showing that explicitly penalizing the trace of the FIM
can significantly improve generalization. We highlight that poor final
generalization coincides with the trace of the FIM increasing to a large value
early in training, to which we refer as catastrophic Fisher explosion. Finally,
to gain insight into the regularization effect of penalizing the trace of the
FIM, we show that it limits memorization by reducing the learning speed of
examples with noisy labels more than that of the clean examples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14240</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14240</id><submitter>Marko Mihajlovic</submitter><version version="v1"><date>Mon, 28 Dec 2020 14:13:33 GMT</date><size>10514kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 16:37:04 GMT</date><size>26342kb</size><source_type>D</source_type></version><title>DeepSurfels: Learning Online Appearance Fusion</title><authors>Marko Mihajlovic, Silvan Weder, Marc Pollefeys, Martin R. Oswald</authors><categories>cs.CV</categories><comments>In Proceedings IEEE Conference on Computer Vision and Pattern
  Recognition. CVPR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present DeepSurfels, a novel hybrid scene representation for geometry and
appearance information. DeepSurfels combines explicit and neural building
blocks to jointly encode geometry and appearance information. In contrast to
established representations, DeepSurfels better represents high-frequency
textures, is well-suited for online updates of appearance information, and can
be easily combined with machine learning methods. We further present an
end-to-end trainable online appearance fusion pipeline that fuses information
from RGB images into the proposed scene representation and is trained using
self-supervision imposed by the reprojection error with respect to the input
images. Our method compares favorably to classical texture mapping approaches
as well as recent learning-based techniques. Moreover, we demonstrate lower
runtime, im-proved generalization capabilities, and better scalability to
larger scenes compared to existing methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14681</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14681</id><submitter>Sathish Indurthi</submitter><version version="v1"><date>Tue, 29 Dec 2020 09:43:27 GMT</date><size>763kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 03:45:18 GMT</date><size>958kb</size><source_type>D</source_type></version><title>Faster Re-translation Using Non-Autoregressive Model For Simultaneous
  Neural Machine Translation</title><authors>Hyojung Han, Sathish Indurthi, Mohd Abbas Zaidi, Nikhil Kumar
  Lakumarapu, Beomseok Lee, Sangha Kim, Chanwoo Kim, Inchul Hwang</authors><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, simultaneous translation has gathered a lot of attention since it
enables compelling applications such as subtitle translation for a live event
or real-time video-call translation. Some of these translation applications
allow editing of partial translation giving rise to re-translation approaches.
The current re-translation approaches are based on autoregressive sequence
generation models (ReTA), which generate tar-get tokens in the (partial)
translation sequentially. The multiple re-translations with sequential
generation inReTAmodelslead to an increased inference time gap between the
incoming source input and the corresponding target output as the source input
grows. Besides, due to the large number of inference operations involved, the
ReTA models are not favorable for resource-constrained devices. In this work,
we propose a faster re-translation system based on a non-autoregressive
sequence generation model (FReTNA) to overcome the aforementioned limitations.
We evaluate the proposed model on multiple translation tasks and our model
reduces the inference times by several orders and achieves a competitive
BLEUscore compared to the ReTA and streaming (Wait-k) models.The proposed model
reduces the average computation time by a factor of 20 when compared to the
ReTA model by incurring a small drop in the translation quality. It also
outperforms the streaming-based Wait-k model both in terms of computation time
(1.5 times lower) and translation quality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14710</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14710</id><submitter>Hongqiu Wu</submitter><version version="v1"><date>Tue, 29 Dec 2020 11:37:43 GMT</date><size>8519kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 02:54:32 GMT</date><size>1474kb</size><source_type>D</source_type></version><title>Code Summarization with Structure-induced Transformer</title><authors>Hongqiu Wu and Hai Zhao and Min Zhang</authors><categories>cs.CL</categories><comments>Findings of ACL2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code summarization (CS) is becoming a promising area in recent language
understanding, which aims to generate sensible human language automatically for
programming language in the format of source code, serving in the most
convenience of programmer developing. It is well known that programming
languages are highly structured. Thus previous works attempt to apply
structure-based traversal (SBT) or non-sequential models like Tree-LSTM and
graph neural network (GNN) to learn structural program semantics. However, it
is surprising that incorporating SBT into advanced encoder like Transformer
instead of LSTM has been shown no performance gain, which lets GNN become the
only rest means modeling such necessary structural clue in source code. To
release such inconvenience, we propose structure-induced Transformer, which
encodes sequential code inputs with multi-view structural clues in terms of a
newly-proposed structure-induced self-attention mechanism. Extensive
experiments show that our proposed structure-induced Transformer helps achieve
new state-of-the-art results on benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14763</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14763</id><submitter>Wei Zhu</submitter><version version="v1"><date>Tue, 29 Dec 2020 14:23:50 GMT</date><size>970kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 12:44:39 GMT</date><size>0kb</size><source_type>I</source_type></version><title>CMV-BERT: Contrastive multi-vocab pretraining of BERT</title><authors>Wei Zhu, Daniel Cheung</authors><categories>cs.CL</categories><comments>will add more detailed technical contents, and more detailed
  experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we represent CMV-BERT, which improves the pretraining of a
language model via two ingredients: (a) contrastive learning, which is well
studied in the area of computer vision; (b) multiple vocabularies, one of which
is fine-grained and the other is coarse-grained. The two methods both provide
different views of an original sentence, and both are shown to be beneficial.
Downstream tasks demonstrate our proposed CMV-BERT are effective in improving
the pretrained language models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14774</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14774</id><submitter>Yumo Xu</submitter><version version="v1"><date>Tue, 29 Dec 2020 14:39:35 GMT</date><size>348kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 21:34:37 GMT</date><size>5337kb</size><source_type>D</source_type></version><title>Generating Query Focused Summaries from Query-Free Resources</title><authors>Yumo Xu and Mirella Lapata</authors><categories>cs.CL cs.IR cs.LG</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of large-scale datasets has driven the development of neural
models that create generic summaries from single or multiple documents. In this
work we consider query focused summarization (QFS), a task for which training
data in the form of queries, documents, and summaries is not readily available.
We propose to decompose QFS into (1) query modeling (i.e., finding supportive
evidence within a set of documents for a query) and (2) conditional language
modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE
Regression framework for evidence estimation and ranking which relies on a
unified representation for summaries and queries, so that summaries in generic
data can be converted into proxy queries for learning a query model.
Experiments across QFS benchmarks and query types show that our model achieves
state-of-the-art performance despite learning from weak supervision.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14827</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14827</id><submitter>Siru Ouyang</submitter><version version="v1"><date>Tue, 29 Dec 2020 16:08:36 GMT</date><size>904kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 22 May 2021 09:49:27 GMT</date><size>6550kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 11:15:08 GMT</date><size>6550kb</size><source_type>D</source_type></version><title>Dialogue Graph Modeling for Conversational Machine Reading</title><authors>Siru Ouyang, Zhuosheng Zhang, Hai Zhao</authors><categories>cs.CL cs.AI</categories><comments>Findings of ACL: ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conversational Machine Reading (CMR) aims at answering questions in a
complicated manner. Machine needs to answer questions through interactions with
users based on given rule document, user scenario and dialogue history, and ask
questions to clarify if necessary. In this paper, we propose a dialogue graph
modeling framework to improve the understanding and reasoning ability of
machine on CMR task. There are three types of graph in total. Specifically,
Discourse Graph is designed to learn explicitly and extract the discourse
relation among rule texts as well as the extra knowledge of scenario;
Decoupling Graph is used for understanding local and contextualized connection
within rule texts. And finally a global graph for fusing the information
together and reply to the user with our final decision being either
&quot;Yes/No/Irrelevant&quot; or to ask a follow-up question to clarify.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14862</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14862</id><submitter>Si Sun</submitter><version version="v1"><date>Tue, 29 Dec 2020 17:28:53 GMT</date><size>8472kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 16:35:46 GMT</date><size>5839kb</size><source_type>D</source_type></version><title>Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision</title><authors>Si Sun, Yingzhuo Qian, Zhenghao Liu, Chenyan Xiong, Kaitao Zhang, Jie
  Bao, Zhiyuan Liu and Paul Bennett</authors><categories>cs.IR cs.CL</categories><comments>14 pages, accepted by ACL-IJCNLP 2021 (long paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a
large scale of in-domain relevance training signals, which are not always
available in real-world ranking scenarios. To democratize the benefits of
Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method
that generalizes Neu-IR models from label-rich source domains to few-shot
target domains. Drawing on source-domain massive relevance supervision,
MetaAdaptRank contrastively synthesizes a large number of weak supervision
signals for target domains and meta-learns to reweight these synthetic &quot;weak&quot;
data based on their benefits to the target-domain ranking accuracy of Neu-IR
models. Experiments on three TREC benchmarks in the web, news, and biomedical
domains show that MetaAdaptRank significantly improves the few-shot ranking
accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives
from both its contrastive weak data synthesis and meta-reweighted data
selection. The code and data of this paper can be obtained from
https://github.com/thunlp/MetaAdaptRank.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14898</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14898</id><submitter>Clayton Thomas</submitter><version version="v1"><date>Tue, 29 Dec 2020 18:41:44 GMT</date><size>141kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 22:10:42 GMT</date><size>143kb</size></version><title>Exponential Communication Separations between Notions of Selfishness</title><authors>Aviad Rubinstein, Raghuvansh R. Saxena, Clayton Thomas, S. Mathew
  Weinberg, Junyao Zhao</authors><categories>cs.GT econ.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of implementing a fixed social choice function
between multiple players (which takes as input a type $t_i$ from each player
$i$ and outputs an outcome $f(t_1,\ldots, t_n)$), in which each player must be
incentivized to follow the protocol. In particular, we study the communication
requirements of a protocol which: (a) implements $f$, (b) implements $f$ and
computes payments that make it ex-post incentive compatible (EPIC) to follow
the protocol, and (c) implements $f$ and computes payments in a way that makes
it dominant-strategy incentive compatible (DSIC) to follow the protocol.
  We show exponential separations between all three of these quantities,
already for just two players. That is, we first construct an $f$ such that $f$
can be implemented in communication $c$, but any EPIC implementation of $f$
(with any choice of payments) requires communication $\exp(c)$. This answers an
open question of [FS09, BBS13]. Second, we construct an $f$ such that an EPIC
protocol implements $f$ with communication $C$, but all DSIC implementations of
$f$ require communication $\exp(C)$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.14919</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.14919</id><submitter>Mingda Chen</submitter><version version="v1"><date>Tue, 29 Dec 2020 19:35:34 GMT</date><size>7758kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 00:42:42 GMT</date><size>347kb</size><source_type>D</source_type></version><title>WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia
  Article Sections</title><authors>Mingda Chen, Sam Wiseman, Kevin Gimpel</authors><categories>cs.CL</categories><comments>Findings of ACL 2021, camera-ready version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Datasets for data-to-text generation typically focus either on multi-domain,
single-sentence generation or on single-domain, long-form generation. In this
work, we cast generating Wikipedia sections as a data-to-text generation task
and create a large-scale dataset, WikiTableT, that pairs Wikipedia sections
with their corresponding tabular data and various metadata. WikiTableT contains
millions of instances, covering a broad range of topics, as well as a variety
of flavors of generation tasks with different levels of flexibility. We
benchmark several training and decoding strategies on WikiTableT. Our
qualitative analysis shows that the best approaches can generate fluent and
high quality texts but they struggle with coherence and factuality, showing the
potential for our dataset to inspire future work on long-form generation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15005</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15005</id><submitter>Xiaoming Liu</submitter><version version="v1"><date>Wed, 30 Dec 2020 02:03:25 GMT</date><size>788kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 01:31:39 GMT</date><size>1214kb</size><source_type>D</source_type></version><title>Infer-AVAE: An Attribute Inference Model Based on Adversarial
  Variational Autoencoder</title><authors>Yadong Zhou, Zhihao Ding, Xiaoming Liu, Chao Shen, Lingling Tong,
  Xiaohong Guan</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  User attributes, such as gender and education, face severe incompleteness in
social networks. In order to make this kind of valuable data usable for
downstream tasks like user profiling and personalized recommendation, attribute
inference aims to infer users' missing attribute labels based on observed data.
Recently, variational autoencoder (VAE), an end-to-end deep generative model,
has shown promising performance by handling the problem in a semi-supervised
way. However, VAEs can easily suffer from over-fitting and over-smoothing when
applied to attribute inference. To be specific, VAE implemented with
multi-layer perceptron (MLP) can only reconstruct input data but fail in
inferring missing parts. While using the trending graph neural networks (GNNs)
as encoder has the problem that GNNs aggregate redundant information from
neighborhood and generate indistinguishable user representations, which is
known as over-smoothing. In this paper, we propose an attribute
\textbf{Infer}ence model based on \textbf{A}dversarial \textbf{VAE}
(Infer-AVAE) to cope with these issues. Specifically, to overcome
over-smoothing, Infer-AVAE unifies MLP and GNNs in encoder to learn positive
and negative latent representations respectively. Meanwhile, an adversarial
network is trained to distinguish the two representations and GNNs are trained
to aggregate less noise for more robust representations through adversarial
training. Finally, to relieve over-fitting, mutual information constraint is
introduced as a regularizer for decoder, so that it can make better use of
auxiliary information in representations and generate outputs not limited by
observations. We evaluate our model on 4 real-world social network datasets,
experimental results demonstrate that our model averagely outperforms baselines
by 7.0$\%$ in accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15015</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15015</id><submitter>Jiwei Li</submitter><version version="v1"><date>Wed, 30 Dec 2020 03:02:50 GMT</date><size>3957kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 13:15:07 GMT</date><size>5523kb</size><source_type>D</source_type></version><title>OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual
  Contexts</title><authors>Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei Wu, Rui Yan
  and Jiwei Li</authors><categories>cs.CL</categories><comments>Dataset, visual features and code are found at
  https://github.com/ShannonAI/OpenViDial</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When humans converse, what a speaker will say next significantly depends on
what he sees. Unfortunately, existing dialogue models generate dialogue
utterances only based on preceding textual contexts, and visual contexts are
rarely considered. This is due to a lack of a large-scale multi-module dialogue
dataset with utterances paired with visual contexts. In this paper, we release
{\bf OpenViDial}, a large-scale multi-module dialogue dataset. The dialogue
turns and visual contexts are extracted from movies and TV series, where each
dialogue turn is paired with the corresponding visual context in which it takes
place. OpenViDial contains a total number of 1.1 million dialogue turns, and
thus 1.1 million visual contexts stored in images. Based on this dataset, we
propose a family of encoder-decoder models leveraging both textual and visual
contexts, from coarse-grained image features extracted from CNNs to
fine-grained object features extracted from Faster R-CNNs. We observe that
visual information significantly improves dialogue generation qualities,
verifying the necessity of integrating multi-modal features for dialogue
learning. Our work marks an important step towards large-scale multi-modal
dialogue learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15045</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15045</id><submitter>Sheng Shen</submitter><version version="v1"><date>Wed, 30 Dec 2020 05:20:16 GMT</date><size>388kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:32:18 GMT</date><size>914kb</size><source_type>D</source_type></version><title>Reservoir Transformers</title><authors>Sheng Shen, Alexei Baevski, Ari S. Morcos, Kurt Keutzer, Michael Auli,
  Douwe Kiela</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that transformers obtain impressive performance even when some
of the layers are randomly initialized and never updated. Inspired by old and
well-established ideas in machine learning, we explore a variety of non-linear
&quot;reservoir&quot; layers interspersed with regular transformer layers, and show
improvements in wall-clock compute time until convergence, as well as overall
performance, on various machine translation and (masked) language modelling
tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15229</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15229</id><submitter>Nada Aldarrab</submitter><version version="v1"><date>Wed, 30 Dec 2020 17:16:33 GMT</date><size>262kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 18:38:07 GMT</date><size>1765kb</size><source_type>D</source_type></version><title>Can Sequence-to-Sequence Models Crack Substitution Ciphers?</title><authors>Nada Aldarrab and Jonathan May</authors><categories>cs.CL</categories><comments>ACL 2021 main conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decipherment of historical ciphers is a challenging problem. The language of
the target plaintext might be unknown, and ciphertext can have a lot of noise.
State-of-the-art decipherment methods use beam search and a neural language
model to score candidate plaintext hypotheses for a given cipher, assuming the
plaintext language is known. We propose an end-to-end multilingual model for
solving simple substitution ciphers. We test our model on synthetic and real
historical ciphers and show that our proposed method can decipher text without
explicit language identification while still being robust to noise.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15243</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15243</id><submitter>Hongming Zhang</submitter><version version="v1"><date>Wed, 30 Dec 2020 17:47:24 GMT</date><size>530kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 04:18:40 GMT</date><size>530kb</size><source_type>D</source_type></version><title>Unsupervised Label-aware Event Trigger and Argument Classification</title><authors>Hongming Zhang, Haoyu Wang, Dan Roth</authors><categories>cs.CL</categories><comments>ACL 2021 Findings</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Identifying events and mapping them to pre-defined event types has long been
an important natural language processing problem. Most previous work has been
heavily relying on labor-intensive and domain-specific annotations while
ignoring the semantic meaning contained in the labels of the event types. As a
result, the learned models cannot effectively generalize to new domains, where
new event types could be introduced. In this paper, we propose an unsupervised
event extraction pipeline, which first identifies events with available tools
(e.g., SRL) and then automatically maps them to pre-defined event types with
our proposed unsupervised classification model. Rather than relying on
annotated data, our model matches the semantics of identified events with those
of event type labels. Specifically, we leverage pre-trained language models to
contextually represent pre-defined types for both event triggers and arguments.
After we map identified events to the target types via representation
similarity, we use the event ontology (e.g., argument type &quot;Victim&quot; can only
appear as the argument of event type &quot;Attack&quot;) as global constraints to
regularize the prediction. The proposed approach is shown to be very effective
when tested on the ACE-2005 dataset, which has 33 trigger and 22 argument
types. Without using any annotation, we successfully map 83% of the triggers
and 54% of the arguments to the correct types, almost doubling the performance
of previous zero-shot approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15355</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15355</id><submitter>Peng Xu</submitter><version version="v1"><date>Wed, 30 Dec 2020 22:53:49 GMT</date><size>417kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 19 May 2021 17:12:23 GMT</date><size>459kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 27 May 2021 16:53:14 GMT</date><size>912kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 16:45:47 GMT</date><size>913kb</size><source_type>D</source_type></version><title>Optimizing Deeper Transformers on Small Datasets</title><authors>Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang,
  Jackie Chi Kit Cheung, Simon J.D. Prince, Yanshuai Cao</authors><categories>cs.CL cs.LG</categories><comments>Accepted at ACL 2021 main conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  It is a common belief that training deep transformers from scratch requires
large datasets. Consequently, for small datasets, people usually use shallow
and simple additional layers on top of pre-trained models during fine-tuning.
This work shows that this does not always need to be the case: with proper
initialization and optimization, the benefits of very deep transformers can
carry over to challenging tasks with small datasets, including Text-to-SQL
semantic parsing and logical reading comprehension. In particular, we
successfully train $48$ layers of transformers, comprising $24$ fine-tuned
layers from pre-trained RoBERTa and $24$ relation-aware layers trained from
scratch. With fewer training steps and no task-specific pre-training, we obtain
the state-of-the-art performance on the challenging cross-domain Text-to-SQL
parsing benchmark Spider. We achieve this by deriving a novel Data-dependent
Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the
prior T-Fixup work. Further error analysis shows that increasing depth can help
improve generalization on small datasets for hard cases that require reasoning
and structural understanding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15613</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15613</id><submitter>Phillip Rust</submitter><version version="v1"><date>Thu, 31 Dec 2020 14:11:00 GMT</date><size>9160kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 18:17:59 GMT</date><size>8232kb</size><source_type>D</source_type></version><title>How Good is Your Tokenizer? On the Monolingual Performance of
  Multilingual Language Models</title><authors>Phillip Rust, Jonas Pfeiffer, Ivan Vuli\'c, Sebastian Ruder, Iryna
  Gurevych</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we provide a systematic and comprehensive empirical comparison
of pretrained multilingual language models versus their monolingual
counterparts with regard to their monolingual task performance. We study a set
of nine typologically diverse languages with readily available pretrained
monolingual models on a set of five diverse monolingual downstream tasks. We
first aim to establish, via fair and controlled comparisons, if a gap between
the multilingual and the corresponding monolingual representation of that
language exists, and subsequently investigate the reason for any performance
difference. To disentangle conflating factors, we train new monolingual models
on the same data, with monolingually and multilingually trained tokenizers. We
find that while the pretraining data size is an important factor, a designated
monolingual tokenizer plays an equally important role in the downstream
performance. Our results show that languages that are adequately represented in
the multilingual model's vocabulary exhibit negligible performance decreases
over their monolingual counterparts. We further find that replacing the
original multilingual tokenizer with the specialized monolingual tokenizer
improves the downstream performance of the multilingual model for almost every
task and language.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15682</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15682</id><submitter>Mengjie Zhao</submitter><version version="v1"><date>Thu, 31 Dec 2020 16:03:48 GMT</date><size>102kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:34:52 GMT</date><size>194kb</size><source_type>D</source_type></version><title>A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots
  Matters</title><authors>Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vuli\'c, Roi Reichart, Anna
  Korhonen, Hinrich Sch\&quot;utze</authors><categories>cs.CL</categories><comments>ACL-IJCNLP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Few-shot crosslingual transfer has been shown to outperform its zero-shot
counterpart with pretrained encoders like multilingual BERT. Despite its
growing popularity, little to no attention has been paid to standardizing and
analyzing the design of few-shot experiments. In this work, we highlight a
fundamental risk posed by this shortcoming, illustrating that the model
exhibits a high degree of sensitivity to the selection of few shots. We conduct
a large-scale experimental study on 40 sets of sampled few shots for six
diverse NLP tasks across up to 40 languages. We provide an analysis of success
and failure cases of few-shot transfer, which highlights the role of lexical
features. Additionally, we show that a straightforward full model finetuning
approach is quite effective for few-shot transfer, outperforming several
state-of-the-art few-shot approaches. As a step towards standardizing few-shot
crosslingual experimental designs, we make our sampled few shots publicly
available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15723</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15723</id><submitter>Tianyu Gao</submitter><version version="v1"><date>Thu, 31 Dec 2020 17:21:26 GMT</date><size>1597kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:41:36 GMT</date><size>2767kb</size><source_type>D</source_type></version><title>Making Pre-trained Language Models Better Few-shot Learners</title><authors>Tianyu Gao, Adam Fisch, Danqi Chen</authors><categories>cs.CL cs.LG</categories><comments>Accepted to ACL 2021. The code is publicly available at
  https://github.com/princeton-nlp/LM-BFF</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot
performance solely by leveraging a natural-language prompt and a few task
demonstrations as input context. Inspired by their findings, we study few-shot
learning in a more practical scenario, where we use smaller language models for
which fine-tuning is computationally efficient. We present LM-BFF--better
few-shot fine-tuning of language models--a suite of simple and complementary
techniques for fine-tuning language models on a small number of annotated
examples. Our approach includes (1) prompt-based fine-tuning together with a
novel pipeline for automating prompt generation; and (2) a refined strategy for
dynamically and selectively incorporating demonstrations into each context.
Finally, we present a systematic evaluation for analyzing few-shot performance
on a range of NLP tasks, including classification and regression. Our
experiments demonstrate that our methods combine to dramatically outperform
standard fine-tuning procedures in this low resource setting, achieving up to
30% absolute improvement, and 11% on average across all tasks. Our approach
makes minimal assumptions on task resources and domain expertise, and hence
constitutes a strong task-agnostic method for few-shot learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15761</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15761</id><submitter>Bertie Vidgen Dr</submitter><version version="v1"><date>Thu, 31 Dec 2020 17:36:48 GMT</date><size>7362kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 08:05:32 GMT</date><size>7396kb</size><source_type>D</source_type></version><title>Learning from the Worst: Dynamically Generated Datasets to Improve
  Online Hate Detection</title><authors>Bertie Vidgen, Tristan Thrush, Zeerak Waseem, Douwe Kiela</authors><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a human-and-model-in-the-loop process for dynamically generating
datasets and training better performing and more robust hate detection models.
We provide a new dataset of ~40,000 entries, generated and labelled by trained
annotators over four rounds of dynamic data creation. It includes ~15,000
challenging perturbations and each hateful entry has fine-grained labels for
the type and target of hate. Hateful entries make up 54% of the dataset, which
is substantially higher than comparable datasets. We show that model
performance is substantially improved using this approach. Models trained on
later rounds of data collection perform better on test sets and are harder for
annotators to trick. They also perform better on HateCheck, a suite of
functional tests for online hate detection. We provide the code, dataset and
annotation guidelines for other researchers to use. Accepted at ACL 2021.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15832</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15832</id><submitter>Ofir Press</submitter><version version="v1"><date>Thu, 31 Dec 2020 18:52:59 GMT</date><size>162kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 02:14:46 GMT</date><size>123kb</size><source_type>D</source_type></version><title>Shortformer: Better Language Modeling using Shorter Inputs</title><authors>Ofir Press, Noah A. Smith, Mike Lewis</authors><categories>cs.CL</categories><comments>To appear at ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing the input length has been a driver of progress in language
modeling with transformers. We identify conditions where shorter inputs are not
harmful, and achieve perplexity and efficiency improvements through two new
methods that decrease input length. First, we show that initially training a
model on short subsequences before moving on to longer ones both reduces
overall training time and, surprisingly, substantially improves perplexity.
Second, we show how to improve the efficiency of recurrence methods in
transformers, which let models condition on previously processed tokens when
generating sequences that exceed the maximal length the transformer can handle
at once. Existing methods require computationally expensive relative position
embeddings; we introduce a simple alternative of adding absolute position
embeddings to queries and keys instead of to word embeddings, which efficiently
produces superior results. We show that these recurrent models also benefit
from short input lengths. Combining these techniques speeds up training by a
factor of 1.65, reduces memory usage, and substantially improves perplexity on
WikiText-103, without adding any parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15859</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15859</id><submitter>Seraphina Goldfarb-Tarrant</submitter><version version="v1"><date>Thu, 31 Dec 2020 18:59:44 GMT</date><size>1185kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 2 Jan 2021 11:41:05 GMT</date><size>1460kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 10:14:20 GMT</date><size>1741kb</size><source_type>D</source_type></version><title>Intrinsic Bias Metrics Do Not Correlate with Application Bias</title><authors>Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Mu\~noz Sanchez,
  Mugdha Pandya, Adam Lopez</authors><categories>cs.CL</categories><comments>In Proceedings of ACL 2021, 9 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Natural Language Processing (NLP) systems learn harmful societal biases that
cause them to amplify inequality as they are deployed in more and more
situations. To guide efforts at debiasing these systems, the NLP community
relies on a variety of metrics that quantify bias in models. Some of these
metrics are intrinsic, measuring bias in word embedding spaces, and some are
extrinsic, measuring bias in downstream tasks that the word embeddings enable.
Do these intrinsic and extrinsic metrics correlate with each other? We compare
intrinsic and extrinsic metrics across hundreds of trained models covering
different tasks and experimental conditions. Our results show no reliable
correlation between these metrics that holds in all scenarios across tasks and
languages. We urge researchers working on debiasing to focus on extrinsic
measures of bias, and to make using these measures more feasible via creation
of new challenge sets and annotated test data. To aid this effort, we release
code, a new intrinsic metric, and an annotated test set focused on gender bias
in hate speech.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00121</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00121</id><submitter>Karen Hambardzumyan</submitter><version version="v1"><date>Fri, 1 Jan 2021 00:41:03 GMT</date><size>5226kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 13:13:01 GMT</date><size>6429kb</size><source_type>D</source_type></version><title>WARP: Word-level Adversarial ReProgramming</title><authors>Karen Hambardzumyan, Hrant Khachatrian, Jonathan May</authors><categories>cs.CL</categories><comments>Accepted ACL 2021 Long Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer learning from pretrained language models recently became the
dominant approach for solving many NLP tasks. A common approach to transfer
learning for multiple tasks that maximize parameter sharing trains one or more
task-specific layers on top of the language model. In this paper, we present an
alternative approach based on adversarial reprogramming, which extends earlier
work on automatic prompt generation. Adversarial reprogramming attempts to
learn task-specific word embeddings that, when concatenated to the input text,
instruct the language model to solve the specified task. Using up to 25K
trainable parameters per task, this approach outperforms all existing methods
with up to 25M trainable parameters on the public leaderboard of the GLUE
benchmark. Our method, initialized with task-specific human-readable prompts,
also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks
with just 32 training samples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00178</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00178</id><submitter>Hao Cheng</submitter><version version="v1"><date>Fri, 1 Jan 2021 06:36:16 GMT</date><size>152kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 18:07:48 GMT</date><size>5355kb</size><source_type>D</source_type></version><title>UnitedQA: A Hybrid Approach for Open Domain Question Answering</title><authors>Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen,
  Jianfeng Gao</authors><categories>cs.CL cs.AI</categories><comments>ACL 2021 camera-ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, most of recent work under the retrieval-reader framework for
open-domain QA focuses on either extractive or generative reader exclusively.
In this paper, we study a hybrid approach for leveraging the strengths of both
models. We apply novel techniques to enhance both extractive and generative
readers built upon recent pretrained neural language models, and find that
proper training methods can provide large improvement over previous
state-of-the-art models. We demonstrate that a simple hybrid approach by
combining answers from both readers can efficiently take advantages of
extractive and generative answer inference strategies and outperforms single
models as well as homogeneous ensembles. Our approach outperforms previous
state-of-the-art models by 3.3 and 2.7 points in exact match on
NaturalQuestions and TriviaQA respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00288</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00288</id><submitter>Tongshuang Wu</submitter><version version="v1"><date>Fri, 1 Jan 2021 18:34:22 GMT</date><size>4313kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 17:13:45 GMT</date><size>5708kb</size><source_type>D</source_type></version><title>Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and
  Improving Models</title><authors>Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel S. Weld</authors><categories>cs.CL</categories><comments>ACL 2021, main conference, long paper</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  While counterfactual examples are useful for analysis and training of NLP
models, current generation methods either rely on manual labor to create very
few counterfactuals, or only instantiate limited types of perturbations such as
paraphrases or word substitutions. We present Polyjuice, a general-purpose
counterfactual generator that allows for control over perturbation types and
locations, trained by finetuning GPT-2 on multiple datasets of paired
sentences. We show that Polyjuice produces diverse sets of realistic
counterfactuals, which in turn are useful in various distinct applications:
improving training and evaluation on three different tasks (with around 70%
less annotation effort than manual generation), augmenting state-of-the-art
explanation techniques, and supporting systematic counterfactual error analysis
by revealing behaviors easily missed by human experts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00294</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00294</id><submitter>Yuning Mao</submitter><version version="v1"><date>Fri, 1 Jan 2021 18:54:19 GMT</date><size>36kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 22:17:15 GMT</date><size>38kb</size><source_type>D</source_type></version><title>Reader-Guided Passage Reranking for Open-Domain Question Answering</title><authors>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao,
  Jiawei Han, Weizhu Chen</authors><categories>cs.CL cs.AI cs.IR</categories><comments>Findings of ACL 2021 Camera-ready. TLDR: Reranking retrieved passages
  by reader predictions can achieve 10~20 gains in top-1 retrieval accuracy and
  1~4 gains in Exact Match (EM) without any training</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current open-domain question answering systems often follow a
Retriever-Reader architecture, where the retriever first retrieves relevant
passages and the reader then reads the retrieved passages to form an answer. In
this paper, we propose a simple and effective passage reranking method, named
Reader-guIDEd Reranker (RIDER), which does not involve training and reranks the
retrieved passages solely based on the top predictions of the reader before
reranking. We show that RIDER, despite its simplicity, achieves 10 to 20
absolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains
without refining the retriever or reader. In addition, RIDER, without any
training, outperforms state-of-the-art transformer-based supervised rerankers.
Remarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM
on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are
used as the reader input after passage reranking.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00345</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00345</id><submitter>Yasumasa Onoe</submitter><version version="v1"><date>Sat, 2 Jan 2021 00:59:10 GMT</date><size>497kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 05:51:55 GMT</date><size>1404kb</size><source_type>D</source_type></version><title>Modeling Fine-Grained Entity Types with Box Embeddings</title><authors>Yasumasa Onoe, Michael Boratko, Andrew McCallum, Greg Durrett</authors><categories>cs.CL cs.AI cs.LG</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural entity typing models typically represent fine-grained entity types as
vectors in a high-dimensional space, but such spaces are not well-suited to
modeling these types' complex interdependencies. We study the ability of box
embeddings, which embed concepts as d-dimensional hyperrectangles, to capture
hierarchies of types even when these relationships are not defined explicitly
in the ontology. Our model represents both types and entity mentions as boxes.
Each mention and its context are fed into a BERT-based model to embed that
mention in our box space; essentially, this model leverages typological clues
present in the surface text to hypothesize a type representation for the
mention. Box containment can then be used to derive both the posterior
probability of a mention exhibiting a given type and the conditional
probability relations between types themselves. We compare our approach with a
vector-based typing model and observe state-of-the-art performance on several
entity typing benchmarks. In addition to competitive typing performance, our
box-based model shows better performance in prediction consistency (predicting
a supertype and a subtype together) and confidence (i.e., calibration),
demonstrating that the box-based model captures the latent type hierarchies
better than the vector-based model does.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00403</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00403</id><submitter>Valentin Hofmann</submitter><version version="v1"><date>Sat, 2 Jan 2021 08:36:48 GMT</date><size>743kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 6 May 2021 16:05:02 GMT</date><size>1644kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 14:00:26 GMT</date><size>1704kb</size><source_type>D</source_type></version><title>Superbizarre Is Not Superb: Derivational Morphology Improves BERT's
  Interpretation of Complex Words</title><authors>Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\&quot;utze</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does the input segmentation of pretrained language models (PLMs) affect
their interpretations of complex words? We present the first study
investigating this question, taking BERT as the example PLM and focusing on its
semantic representations of English derivatives. We show that PLMs can be
interpreted as serial dual-route models, i.e., the meanings of complex words
are either stored or else need to be computed from the subwords, which implies
that maximally meaningful input tokens should allow for the best generalization
on new words. This hypothesis is confirmed by a series of semantic probing
tasks on which DelBERT (Derivation leveraging BERT), a model with derivational
input segmentation, substantially outperforms BERT with WordPiece segmentation.
Our results suggest that the generalization capabilities of PLMs could be
further improved if a morphologically-informed vocabulary of input tokens were
used.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00408</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00408</id><submitter>Devendra Singh Sachan</submitter><version version="v1"><date>Sat, 2 Jan 2021 09:05:34 GMT</date><size>7560kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 02:46:38 GMT</date><size>7644kb</size><source_type>D</source_type></version><title>End-to-End Training of Neural Retrievers for Open-Domain Question
  Answering</title><authors>Devendra Singh Sachan and Mostofa Patwary and Mohammad Shoeybi and
  Neel Kant and Wei Ping and William L Hamilton and Bryan Catanzaro</authors><categories>cs.CL cs.AI</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on training neural retrievers for open-domain question answering
(OpenQA) has employed both supervised and unsupervised approaches. However, it
remains unclear how unsupervised and supervised methods can be used most
effectively for neural retrievers. In this work, we systematically study
retriever pre-training. We first propose an approach of unsupervised
pre-training with the Inverse Cloze Task and masked salient spans, followed by
supervised finetuning using question-context pairs. This approach leads to
absolute gains of 2+ points over the previous best result in the top-20
retrieval accuracy on Natural Questions and TriviaQA datasets.
  We also explore two approaches for end-to-end supervised training of the
reader and retriever components in OpenQA models. In the first approach, the
reader considers each retrieved document separately while in the second
approach, the reader considers all the retrieved documents together. Our
experiments demonstrate the effectiveness of these approaches as we obtain new
state-of-the-art results. On the Natural Questions dataset, we obtain a top-20
retrieval accuracy of 84, an improvement of 5 points over the recent DPR model.
In addition, we achieve good results on answer extraction, outperforming recent
models like REALM and RAG by 3+ points. We further scale up end-to-end training
to large models and show consistent gains in performance over smaller models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00434</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00434</id><submitter>Yuval Kirstain</submitter><version version="v1"><date>Sat, 2 Jan 2021 11:46:51 GMT</date><size>109kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:57:00 GMT</date><size>5286kb</size><source_type>D</source_type></version><title>Coreference Resolution without Span Representations</title><authors>Yuval Kirstain, Ori Ram, Omer Levy</authors><categories>cs.CL</categories><comments>Accepted to ACL 2021</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The introduction of pretrained language models has reduced many complex
task-specific NLP models to simple lightweight layers. An exception to this
trend is coreference resolution, where a sophisticated task-specific model is
appended to a pretrained transformer encoder. While highly effective, the model
has a very large memory footprint -- primarily due to dynamically-constructed
span and span-pair representations -- which hinders the processing of complete
documents and the ability to train on multiple instances in a single batch. We
introduce a lightweight end-to-end coreference model that removes the
dependency on span representations, handcrafted features, and heuristics. Our
model performs competitively with the current standard model, while being
simpler and more efficient.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00438</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00438</id><submitter>Ori Ram</submitter><version version="v1"><date>Sat, 2 Jan 2021 11:58:44 GMT</date><size>8210kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 13:24:01 GMT</date><size>8355kb</size><source_type>D</source_type></version><title>Few-Shot Question Answering by Pretraining Span Selection</title><authors>Ori Ram and Yuval Kirstain and Jonathan Berant and Amir Globerson and
  Omer Levy</authors><categories>cs.CL</categories><comments>Accepted to ACL 2021</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In several question answering benchmarks, pretrained models have reached
human parity through fine-tuning on an order of 100,000 annotated questions and
answers. We explore the more realistic few-shot setting, where only a few
hundred training examples are available, and observe that standard models
perform poorly, highlighting the discrepancy between current pretraining
objectives and question answering. We propose a new pretraining scheme tailored
for question answering: recurring span selection. Given a passage with multiple
sets of recurring spans, we mask in each set all recurring spans but one, and
ask the model to select the correct span in the passage for each masked span.
Masked spans are replaced with a special token, viewed as a question
representation, that is later used during fine-tuning to select the answer
span. The resulting model obtains surprisingly good results on multiple
benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while
maintaining competitive performance in the high-resource setting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00536</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00536</id><submitter>Guanrong Chen Professor</submitter><version version="v1"><date>Sun, 3 Jan 2021 01:09:43 GMT</date><size>1153kb</size></version><version version="v2"><date>Mon, 31 May 2021 11:35:17 GMT</date><size>1710kb</size></version><title>Computing Cliques and Cavities in Networks</title><authors>Dinghua Shi, Zhifeng Chen, Xiang Sun, Qinghua Chen, Chuang Ma, Yang
  Lou and Guanrong Chen</authors><categories>cs.NE</categories><comments>20 pages, 4+2 figures, 3+3 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Complex networks have complete subgraphs such as nodes, edges, triangles,
etc., referred to as cliques of different orders. Notably, cavities consisting
of higher-order cliques have been found playing an important role in brain
functions. Since searching for the maximum clique in a large network is an
NP-complete problem, we propose using k-core decomposition to determine the
computability of a given network subject to limited computing resources. For a
computable network, we design a search algorithm for finding cliques of
different orders, which also provides the Euler characteristic number. Then, we
compute the Betti number by using the ranks of the boundary matrices of
adjacent cliques. Furthermore, we design an optimized algorithm for finding
cavities of different orders. Finally, we apply the algorithm to the neuronal
network of C. elegans in one dataset, and find all of its cliques and some
cavities of different orders therein, providing a basis for further
mathematical analysis and computation of the structure and function of the C.
elegans neuronal network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00588</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00588</id><submitter>Xin Jin</submitter><version version="v1"><date>Sun, 3 Jan 2021 09:01:39 GMT</date><size>4076kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 11:33:36 GMT</date><size>1881kb</size><source_type>D</source_type></version><title>Style Normalization and Restitution for Domain Generalization and
  Adaptation</title><authors>Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen</authors><categories>cs.CV</categories><comments>We have extended our SNR for domain generalization and adaptation to
  various computer vision tasks, e.g., image classification, object detection,
  semantic segmentation</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  For many practical computer vision applications, the learned models usually
have high performance on the datasets used for training but suffer from
significant performance degradation when deployed in new environments, where
there are usually style differences between the training images and the testing
images. An effective domain generalizable model is expected to be able to learn
feature representations that are both generalizable and discriminative. In this
paper, we design a novel Style Normalization and Restitution module (SNR) to
simultaneously ensure both high generalization and discrimination capability of
the networks. In the SNR module, particularly, we filter out the style
variations (e.g, illumination, color contrast) by performing Instance
Normalization (IN) to obtain style normalized features, where the discrepancy
among different samples and domains is reduced. However, such a process is
task-ignorant and inevitably removes some task-relevant discriminative
information, which could hurt the performance. To remedy this, we propose to
distill task-relevant discriminative features from the residual (i.e, the
difference between the original feature and the style normalized feature) and
add them back to the network to ensure high discrimination. Moreover, for
better disentanglement, we enforce a dual causality loss constraint in the
restitution step to encourage the better separation of task-relevant and
task-irrelevant features. We validate the effectiveness of our SNR on different
computer vision tasks, including classification, semantic segmentation, and
object detection. Experiments demonstrate that our SNR module is capable of
improving the performance of networks for domain generalization (DG) and
unsupervised domain adaptation (UDA) on many tasks. Code are available at
https://github.com/microsoft/SNR.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00942</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00942</id><submitter>Henning Urbat</submitter><version version="v1"><date>Mon, 4 Jan 2021 13:17:59 GMT</date><size>103kb</size></version><version version="v2"><date>Mon, 31 May 2021 13:15:32 GMT</date><size>170kb</size></version><title>Reiterman's Theorem on Finite Algebras for a Monad</title><authors>Jiri Adamek and Liang-Ting Chen and Stefan Milius and Henning Urbat</authors><categories>math.CT cs.FL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Profinite equations are an indispensable tool for the algebraic
classification of formal languages. Reiterman's theorem states that they
precisely specify pseudovarieties, i.e.~classes of finite algebras closed under
finite products, subalgebras and quotients. In this paper, Reiterman's theorem
is generalized to finite Eilenberg-Moore algebras for a monad T on a category
D: we prove that a class of finite T-algebras is a pseudovariety iff it is
presentable by profinite equations. As a key technical tool, we introduce the
concept of a profinite monad associated to the monad T, which gives a
categorical view of the construction of the space of profinite terms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.01041</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.01041</id><submitter>Xiangyuan Zhang</submitter><version version="v1"><date>Mon, 4 Jan 2021 16:00:46 GMT</date><size>3784kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 02:41:16 GMT</date><size>10056kb</size><source_type>D</source_type></version><title>Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust
  Control Design: Implicit Regularization and Sample Complexity</title><authors>Kaiqing Zhang, Xiangyuan Zhang, Bin Hu, Tamer Ba\c{s}ar</authors><categories>math.OC cs.AI cs.LG cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct policy search serves as one of the workhorses in modern reinforcement
learning (RL), and its applications in continuous control tasks have recently
attracted increasing attention. In this work, we investigate the convergence
theory of policy gradient (PG) methods for learning the linear risk-sensitive
and robust controller. In particular, we develop PG methods that can be
implemented in a derivative-free fashion by sampling system trajectories, and
establish both global convergence and sample complexity results in the
solutions of two fundamental settings in risk-sensitive and robust control: the
finite-horizon linear exponential quadratic Gaussian, and the finite-horizon
linear-quadratic disturbance attenuation problems. As a by-product, our results
also provide the first sample complexity for the global convergence of PG
methods on solving zero-sum linear-quadratic dynamic games, a
nonconvex-nonconcave minimax optimization problem that serves as a baseline
setting in multi-agent reinforcement learning (MARL) with continuous spaces.
One feature of our algorithms is that during the learning phase, a certain
level of robustness/risk-sensitivity of the controller is preserved, which we
termed as the implicit regularization property, and is an essential requirement
in safety-critical control systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.01300</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.01300</id><submitter>Waheed Bajwa</submitter><version version="v1"><date>Tue, 5 Jan 2021 00:51:14 GMT</date><size>1540kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:04:10 GMT</date><size>1527kb</size><source_type>D</source_type></version><title>A Linearly Convergent Algorithm for Distributed Principal Component
  Analysis</title><authors>Arpita Gang and Waheed U. Bajwa</authors><categories>cs.LG cs.DC cs.MA eess.SP stat.ML</categories><comments>33 pages; 15 figures; preprint of a journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal Component Analysis (PCA) is the workhorse tool for dimensionality
reduction in this era of big data. While often overlooked, the purpose of PCA
is not only to reduce data dimensionality, but also to yield features that are
uncorrelated. Furthermore, the ever-increasing volume of data in the modern
world often requires storage of data samples across multiple machines, which
precludes the use of centralized PCA algorithms. This paper focuses on the dual
objective of PCA, namely, dimensionality reduction and decorrelation of
features, but in a distributed setting. This requires estimating the
eigenvectors of the data covariance matrix, as opposed to only estimating the
subspace spanned by the eigenvectors, when data is distributed across a network
of machines. Although a few distributed solutions to the PCA problem have been
proposed recently, convergence guarantees and/or communications overhead of
these solutions remain a concern. With an eye towards communications
efficiency, this paper introduces a feedforward neural network-based one
time-scale distributed PCA algorithm termed Distributed Sanger's Algorithm
(DSA) that estimates the eigenvectors of the data covariance matrix when data
is distributed across an undirected and arbitrarily connected network of
machines. Furthermore, the proposed algorithm is shown to converge linearly to
a neighborhood of the true solution. Numerical results are also provided to
demonstrate the efficacy of the proposed solution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.01308</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.01308</id><submitter>Guankai Li</submitter><version version="v1"><date>Tue, 5 Jan 2021 01:35:19 GMT</date><size>5149kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 08:00:36 GMT</date><size>11764kb</size><source_type>D</source_type></version><title>CycleSegNet: Object Co-segmentation with Cycle Refinement and Region
  Correspondence</title><authors>Chi Zhang, Guankai Li, Guosheng Lin, Qingyao Wu, Rui Yao</authors><categories>cs.CV</categories><comments>Accept to TIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image co-segmentation is an active computer vision task that aims to segment
the common objects from a set of images. Recently, researchers design various
learning-based algorithms to undertake the co-segmentation task. The main
difficulty in this task is how to effectively transfer information between
images to make conditional predictions. In this paper, we present CycleSegNet,
a novel framework for the co-segmentation task. Our network design has two key
components: a region correspondence module which is the basic operation for
exchanging information between local image regions, and a cycle refinement
module, which utilizes ConvLSTMs to progressively update image representations
and exchange information in a cycle and iterative manner. Extensive experiments
demonstrate that our proposed method significantly outperforms the
state-of-the-art methods on four popular benchmark datasets -- PASCAL VOC
dataset, MSRC dataset, Internet dataset, and iCoseg dataset, by 2.6%, 7.7%,
2.2%, and 2.9%, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.01558</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.01558</id><submitter>Mirko Trisolini</submitter><version version="v1"><date>Sun, 27 Dec 2020 17:48:29 GMT</date><size>702kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 21 Jan 2021 19:07:13 GMT</date><size>702kb</size><source_type>D</source_type></version><title>Constrained optimisation of preliminary spacecraft configurations under
  the design-for-demise paradigm</title><authors>Mirko Trisolini and Hugh G. Lewis and Camilla Colombo</authors><categories>eess.SY cs.CE cs.NE cs.SY</categories><comments>Pre-print submitted to the Journal of Space Safety Engineering</comments><journal-ref>Journal of Space Safety Engineering. 8. 1. (2021) 63-74</journal-ref><doi>10.1016/j.jsse.2021.01.005</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In the past few years, the interest towards the implementation of
design-for-demise measures has increased steadily. Most mid-sized satellites
currently launched and already in orbit fail to comply with the casualty risk
threshold of 0.0001. Therefore, satellites manufacturers and mission operators
need to perform a disposal through a controlled re-entry, which has a higher
cost and increased complexity. Through the design-for-demise paradigm, this
additional cost and complexity can be removed as the spacecraft is directly
compliant with the casualty risk regulations. However, building a spacecraft
such that most of its parts will demise may lead to designs that are more
vulnerable to space debris impacts, thus compromising the reliability of the
mission. In fact, the requirements connected to the demisability and the
survivability are in general competing. Given this competing nature, trade-off
solutions can be found, which favour the implementation of design-for-demise
measures while still maintaining the spacecraft resilient to space debris
impacts. A multi-objective optimisation framework has been developed by the
authors in previous works. The framework's objective is to find preliminary
design solutions considering the competing nature of the demisability and the
survivability of a spacecraft since the early stages of the mission design. In
this way, a more integrated design can be achieved. The present work focuses on
the improvement of the multi-objective optimisation framework by including
constraints. The paper shows the application of the constrained optimisation to
two relevant examples: the optimisation of a tank assembly and the optimisation
of a typical satellite configuration.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.01785</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.01785</id><submitter>El Moatez Billah Nagoudi</submitter><version version="v1"><date>Sun, 27 Dec 2020 06:32:55 GMT</date><size>266kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 03:44:27 GMT</date><size>521kb</size><source_type>D</source_type></version><title>ARBERT &amp; MARBERT: Deep Bidirectional Transformers for Arabic</title><authors>Muhammad Abdul-Mageed, AbdelRahim Elmadany, El Moatez Billah Nagoudi</authors><categories>cs.CL</categories><comments>All authors contributed equally. The order is alphabetical</comments><journal-ref>ACL-IJCNLP 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Pre-trained language models (LMs) are currently integral to many natural
language processing systems. Although multilingual LMs were also introduced to
serve many languages, these have limitations such as being costly at inference
time and the size and diversity of non-English data involved in their
pre-training. We remedy these issues for a collection of diverse Arabic
varieties by introducing two powerful deep bidirectional transformer-based
models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a
new benchmark for multi-dialectal Arabic language understanding evaluation.
ARLUE is built using $42$ datasets targeting six different task clusters,
allowing us to offer a series of standardized experiments under rich
conditions. When fine-tuned on ARLUE, our models collectively achieve new
state-of-the-art results across the majority of tasks (37 out of 48
classification tasks, on the 42 datasets). Our best model acquires the highest
ARLUE score (77.40) across all six task clusters, outperforming all other
models including XLM-R Large (~ 3.4 x larger size). Our models are publicly
available at https://github.com/UBC-NLP/marbert and ARLUE will be released
through the same repository.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.02285</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.02285</id><submitter>Kathleen M Lewis</submitter><version version="v1"><date>Wed, 6 Jan 2021 22:01:46 GMT</date><size>43268kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 19:38:57 GMT</date><size>19646kb</size><source_type>D</source_type></version><title>TryOnGAN: Body-Aware Try-On via Layered Interpolation</title><authors>Kathleen M Lewis, Srivatsan Varadharajan, Ira Kemelmacher-Shlizerman</authors><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a pair of images-target person and garment on another person-we
automatically generate the target person in the given garment. Previous methods
mostly focused on texture transfer via paired data training, while overlooking
body shape deformations, skin color, and seamless blending of garment with the
person. This work focuses on those three components, while also not requiring
paired data training. We designed a pose conditioned StyleGAN2 architecture
with a clothing segmentation branch that is trained on images of people wearing
garments. Once trained, we propose a new layered latent space interpolation
method that allows us to preserve and synthesize skin color and target body
shape while transferring the garment from a different person. We demonstrate
results on high resolution 512x512 images, and extensively compare to state of
the art in try-on on both latent space generated and real images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.02780</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.02780</id><submitter>Tanujay Saha</submitter><version version="v1"><date>Thu, 7 Jan 2021 22:01:30 GMT</date><size>3317kb</size><source_type>D</source_type></version><title>SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning</title><authors>Tanujay Saha, Najwa Aaraj, Neel Ajjarapu, Niraj K. Jha</authors><categories>cs.CR cs.AI cs.LG</categories><comments>This article has been accepted in IEEE Transactions on Emerging
  Topics in Computing. 17 pages, 12 figures, IEEE copyright</comments><journal-ref>IEEE Transactions on Emerging Topics in Computing, 2021</journal-ref><doi>10.1109/TETC.2021.3050733</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.02854</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.02854</id><submitter>Sai Sandeep</submitter><version version="v1"><date>Fri, 8 Jan 2021 05:31:16 GMT</date><size>57kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 17:05:13 GMT</date><size>60kb</size><source_type>D</source_type></version><title>Almost Optimal Inapproximability of Multidimensional Packing Problems</title><authors>Sai Sandeep</authors><categories>cs.DS cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multidimensional packing problems generalize the classical packing problems
such as Bin Packing, Multiprocessor Scheduling by allowing the jobs to be
$d$-dimensional vectors. While the approximability of the scalar problems is
well understood, there has been a significant gap between the approximation
algorithms and the hardness results for the multidimensional variants. In this
paper, we close this gap by giving almost tight hardness results for these
problems.
  1. We show that Vector Bin Packing has no polynomial time $\Omega( \log d)$
factor asymptotic approximation algorithm when $d$ is a large constant,
assuming $\textsf{P}\neq \textsf{NP}$. This matches the $\ln d + O(1)$ factor
approximation algorithms (Chekuri, Khanna SICOMP 2004, Bansal, Caprara,
Sviridenko SICOMP 2009, Bansal, Eli\'{a}s, Khan SODA 2016) upto constants.
  2. We show that Vector Scheduling has no polynomial time algorithm with an
approximation ratio of $\Omega\left( (\log d)^{1-\epsilon}\right)$ when $d$ is
part of the input, assuming $\textsf{NP}\nsubseteq \textsf{ZPTIME}\left(
n^{(\log n)^{O(1)}}\right)$. This almost matches the $O\left( \frac{\log
d}{\log \log d}\right)$ factor algorithms(Harris, Srinivasan JACM 2019, Im,
Kell, Kulkarni, Panigrahi SICOMP 2019). We also show that the problem is
NP-hard to approximate within $(\log \log d)^{\omega(1)}$.
  3. We show that Vector Bin Covering is NP-hard to approximate within
$\Omega\left( \frac{\log d}{\log \log d}\right)$ when $d$ is part of the input,
almost matching the $O(\log d)$ factor algorithm (Alon et al., Algorithmica
1998).
  Previously, no hardness results that grow with $d$ were known for Vector
Scheduling and Vector Bin Covering when $d$ is part of the input and for Vector
Bin Packing when $d$ is a fixed constant.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.03187</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.03187</id><submitter>Yingzhao Lian</submitter><version version="v1"><date>Fri, 8 Jan 2021 19:12:20 GMT</date><size>42kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 08:35:57 GMT</date><size>44kb</size></version><title>Nonlinear Data-Enabled Prediction and Control</title><authors>Yingzhao Lian and Colin N.Jones</authors><categories>math.OC cs.SY eess.SY</categories><comments>Accepted to L4DC 2021, Zurich</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Willems' fundamental lemma, which characterizes linear dynamics with
measured trajectories, has found successful applications in controller design
and signal processing, which has driven a broad research interest in its
extension to nonlinear systems. In this work, we propose to apply the
fundamental lemma to a reproducing kernel Hilbert space in order to extend its
application to a class of nonlinear systems and we show its application in
prediction and in predictive control.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.03196</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.03196</id><submitter>Bei Wang</submitter><version version="v1"><date>Fri, 8 Jan 2021 19:38:46 GMT</date><size>38079kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 23:39:30 GMT</date><size>39855kb</size><source_type>D</source_type></version><title>Sketching Merge Trees for Scientific Data Visualization</title><authors>Mingzhe Li, Sourabh Palande, Lin Yan, Bei Wang</authors><categories>cs.CG cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Merge trees are a type of topological descriptors that record the
connectivity among the sublevel sets of scalar fields. They are among the most
widely used topological tools in visualization. In this paper, we are
interested in sketching a set of merge trees. That is, given a large set T of
merge trees, we would like to find a much smaller basis set S such that each
tree in T can be approximately reconstructed from a linear combination of merge
trees in S. A set of high-dimensional vectors can be sketched via matrix
sketching techniques such as principal component analysis and column subset
selection. However, up until now, topological descriptors such as merge trees
have not been known to be sketchable. We develop a framework for sketching a
set of merge trees that combines the Gromov-Wasserstein probabilistic matching
with techniques from matrix sketching. We demonstrate the applications of our
framework in sketching merge trees that arise from time-varying scientific
simulations. Specifically, our framework obtains a much smaller representation
of a large set of merge trees for downstream analysis and visualization. It is
shown to be useful in identifying good representatives and outliers with
respect to a chosen basis. Finally, our work shows a promising direction of
utilizing randomized linear algebra within scientific visualization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.03618</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.03618</id><submitter>Sergio G\'omez</submitter><version version="v1"><date>Sun, 10 Jan 2021 20:05:25 GMT</date><size>723kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 25 Mar 2021 19:15:13 GMT</date><size>759kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 10:05:31 GMT</date><size>737kb</size><source_type>D</source_type></version><title>Network clique cover approximation to analyze complex contagions through
  group interactions</title><authors>Giulio Burgio, Alex Arenas, Sergio G\'omez, Joan T. Matamalas</authors><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>26 pages, 10 figures</comments><journal-ref>Communications Physics 4 (2021) 111</journal-ref><doi>10.1038/s42005-021-00618-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contagion processes have been proven to fundamentally depend on the
structural properties of the interaction networks conveying them. Many real
networked systems are characterized by clustered substructures representing
either collections of all-to-all pair-wise interactions (cliques) and/or group
interactions, involving many of their members at once. In this work, focusing
on interaction structures represented as simplicial complexes, we present a
discrete-time microscopic model of complex contagion for a
susceptible-infected-susceptible dynamics. Introducing a particular edge clique
cover and a heuristic to find it, the model accounts for the higher-order
dynamical correlations among the members of the substructures
(cliques/simplices). The analytical computation of the critical point reveals
that higher-order correlations are responsible for its dependence on the
higher-order couplings. While such dependence eludes any mean-field model, the
possibility of a bi-stable region is extended to structured populations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.03624</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.03624</id><submitter>Dongting Li</submitter><version version="v1"><date>Sun, 10 Jan 2021 20:44:08 GMT</date><size>4177kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 16 Jan 2021 17:47:57 GMT</date><size>4177kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 26 May 2021 14:14:10 GMT</date><size>5632kb</size><source_type>D</source_type></version><title>Compliant Fins for Locomotion in Granular Media</title><authors>Dongting Li, Sichuan Huang, Yong Tang, Hamidreza Marvi, Junliang Tao
  and Daniel M. Aukes</authors><categories>cs.RO</categories><comments>\c{opyright} 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><doi>10.1109/LRA.2021.3084877</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach to study the behavior of compliant
plates in granular media and optimize the performance of a robot that utilizes
this technique for mobility. From previous work and fundamental tests on thin
plate force generation inside granular media, we introduce an origami-inspired
mechanism with non-linear compliance in the joints that can be used in granular
propulsion. This concept utilizes one-sided joint limits to create an
asymmetric gait cycle that avoids more complicated alternatives often found in
other swimming/digging robots. To analyze its locomotion as well as its shape
and propulsive force, we utilize granular Resistive Force Theory (RFT) as a
starting point. Adding compliance to this theory enables us to predict the
time-based evolution of compliant plates when they are dragged and rotated. It
also permits more rational design of swimming robots where fin design variables
may be optimized against the characteristics of the granular medium. This is
done using a Python-based dynamic simulation library to model the deformation
of the plates and optimize aspects of the robot's gait. Finally, we prototype
and test robot with a gait optimized using the modelling techniques mentioned
above.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04017</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04017</id><submitter>Antonio Lieto</submitter><version version="v1"><date>Mon, 11 Jan 2021 16:44:38 GMT</date><size>1868kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 13:58:59 GMT</date><size>2256kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 26 May 2021 13:48:08 GMT</date><size>2252kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 20:53:30 GMT</date><size>2256kb</size><source_type>D</source_type></version><version version="v5"><date>Wed, 2 Jun 2021 11:10:56 GMT</date><size>2256kb</size><source_type>D</source_type></version><title>A Commonsense Reasoning Framework for Explanatory Emotion Attribution,
  Generation and Re-classification</title><authors>Antonio Lieto, Gian Luca Pozzato, Stefano Zoia, Viviana Patti, Rossana
  Damiano</authors><categories>cs.AI</categories><comments>50 pages. This work has been partially funded from the European
  Research Council (ERC) under the European Union'sHorizon 2020 research and
  innovation programme, grant agreement n{\deg}870811</comments><acm-class>I.2; I.2.1; I.2.3; I.2.4</acm-class><journal-ref>Knowledge-Based Systems, 2021</journal-ref><doi>10.1016/j.knosys.2021.107166</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present DEGARI (Dynamic Emotion Generator And ReclassIfier), an
explainable system for emotion attribution and recommendation. This system
relies on a recently introduced commonsense reasoning framework, the TCL logic,
which is based on a human-like procedure for the automatic generation of novel
concepts in a Description Logics knowledge base. Starting from an ontological
formalization of emotions based on the Plutchik model, known as ArsEmotica, the
system exploits the logic TCL to automatically generate novel commonsense
semantic representations of compound emotions (e.g. Love as derived from the
combination of Joy and Trust according to Plutchik). The generated emotions
correspond to prototypes, i.e. commonsense representations of given concepts,
and have been used to reclassify emotion-related contents in a variety of
artistic domains, ranging from art datasets to the editorial contents available
in RaiPlay, the online platform of RAI Radiotelevisione Italiana (the Italian
public broadcasting company). We show how the reported results (evaluated in
the light of the obtained reclassifications, the user ratings assigned to such
reclassifications, and their explainability) are encouraging, and pave the way
to many further research directions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04108</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04108</id><submitter>Umang Gupta</submitter><version version="v1"><date>Mon, 11 Jan 2021 18:57:33 GMT</date><size>2931kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 8 Apr 2021 17:54:43 GMT</date><size>2932kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 17:21:06 GMT</date><size>5859kb</size><source_type>D</source_type></version><title>Controllable Guarantees for Fair Outcomes via Contrastive Information
  Estimation</title><authors>Umang Gupta and Aaron M Ferber and Bistra Dilkina and Greg Ver Steeg</authors><categories>cs.LG stat.ML</categories><comments>This version fixes an error in Theorem 2 of the original manuscript
  that appeared at the Proceedings of the 35th AAAI Conference on Artificial
  Intelligence (AAAI-21). Code is available at
  https://github.com/umgupta/fairness-via-contrastive-estimation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlling bias in training datasets is vital for ensuring equal treatment,
or parity, between different groups in downstream applications. A naive
solution is to transform the data so that it is statistically independent of
group membership, but this may throw away too much information when a
reasonable compromise between fairness and accuracy is desired. Another common
approach is to limit the ability of a particular adversary who seeks to
maximize parity. Unfortunately, representations produced by adversarial
approaches may still retain biases as their efficacy is tied to the complexity
of the adversary used during training. To this end, we theoretically establish
that by limiting the mutual information between representations and protected
attributes, we can assuredly control the parity of any downstream classifier.
We demonstrate an effective method for controlling parity through mutual
information based on contrastive information estimators and show that they
outperform approaches that rely on variational bounds based on complex
generative models. We test our approach on UCI Adult and Heritage Health
datasets and demonstrate that our approach provides more informative
representations across a range of desired parity thresholds while providing
strong theoretical guarantees on the parity of any downstream algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04144</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04144</id><submitter>Raviraj Joshi</submitter><version version="v1"><date>Mon, 11 Jan 2021 19:10:57 GMT</date><size>315kb</size></version><version version="v2"><date>Wed, 13 Jan 2021 14:25:06 GMT</date><size>220kb</size></version><version version="v3"><date>Tue, 9 Mar 2021 16:27:40 GMT</date><size>1087kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 7 Apr 2021 06:44:47 GMT</date><size>1087kb</size><source_type>D</source_type></version><title>Evaluation of Deep Learning Models for Hostility Detection in Hindi Text</title><authors>Ramchandra Joshi, Rushabh Karnavat, Kaustubh Jirapure, Raviraj Joshi</authors><categories>cs.CL cs.LG</categories><comments>Accepted at IEEE I2CT 2021</comments><doi>10.1109/I2CT51068.2021.9418073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social media platform is a convenient medium to express personal thoughts
and share useful information. It is fast, concise, and has the ability to reach
millions. It is an effective place to archive thoughts, share artistic content,
receive feedback, promote products, etc. Despite having numerous advantages
these platforms have given a boost to hostile posts. Hate speech and derogatory
remarks are being posted for personal satisfaction or political gain. The
hostile posts can have a bullying effect rendering the entire platform
experience hostile. Therefore detection of hostile posts is important to
maintain social media hygiene. The problem is more pronounced languages like
Hindi which are low in resources. In this work, we present approaches for
hostile text detection in the Hindi language. The proposed approaches are
evaluated on the Constraint@AAAI 2021 Hindi hostility detection dataset. The
dataset consists of hostile and non-hostile texts collected from social media
platforms. The hostile posts are further segregated into overlapping classes of
fake, offensive, hate, and defamation. We evaluate a host of deep learning
approaches based on CNN, LSTM, and BERT for this multi-label classification
problem. The pre-trained Hindi fast text word embeddings by IndicNLP and
Facebook are used in conjunction with CNN and LSTM models. Two variations of
pre-trained multilingual transformer language models mBERT and IndicBERT are
used. We show that the performance of BERT based models is best. Moreover, CNN
and LSTM models also perform competitively with BERT based models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04306</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04306</id><submitter>Andrew McDonald</submitter><version version="v1"><date>Tue, 12 Jan 2021 05:46:13 GMT</date><size>1648kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 5 Feb 2021 05:08:18 GMT</date><size>1648kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 00:01:32 GMT</date><size>889kb</size><source_type>D</source_type></version><title>Multi-Robot Gaussian Process Estimation and Coverage: A Deterministic
  Sequencing Algorithm and Regret Analysis</title><authors>Lai Wei, Andrew McDonald, Vaibhav Srivastava</authors><categories>cs.RO cs.SY eess.SY math.OC stat.ML</categories><comments>7 pages, 2 figures, accepted to IEEE ICRA'21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of distributed multi-robot coverage over an unknown,
nonuniform sensory field. Modeling the sensory field as a realization of a
Gaussian Process and using Bayesian techniques, we devise a policy which aims
to balance the tradeoff between learning the sensory function and covering the
environment. We propose an adaptive coverage algorithm called Deterministic
Sequencing of Learning and Coverage (DSLC) that schedules learning and coverage
epochs such that its emphasis gradually shifts from exploration to exploitation
while never fully ceasing to learn. Using a novel definition of coverage regret
which characterizes overall coverage performance of a multi-robot team over a
time horizon $T$, we analyze DSLC to provide an upper bound on expected
cumulative coverage regret. Finally, we illustrate the empirical performance of
the algorithm through simulations of the coverage task over an unknown
distribution of wildfires.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04427</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04427</id><submitter>Amoldeep Singh Mr.</submitter><version version="v1"><date>Tue, 12 Jan 2021 11:57:04 GMT</date><size>3761kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 17:03:20 GMT</date><size>4511kb</size><source_type>D</source_type></version><title>Quantum Internet- Applications, Functionalities, Enabling Technologies,
  Challenges, and Research Directions</title><authors>Amoldeep Singh, Kapal Dev, Harun Siljak, Hem Dutt Joshi and Maurizio
  Magarini</authors><categories>quant-ph cs.CR cs.NI</categories><comments>This survey paper is submitted in IEEE Communications Surveys and
  Tutorials and revised on 27th May 2021. It includes 31 pages, 14 figures, and
  5 tables</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The advanced notebooks, mobile phones, and internet applications in today's
world that we use are all entrenched in classical communication bits of zeros
and ones. Classical internet has laid its foundation originating from the
amalgamation of mathematics and Claude Shannon's theory of information. But
today's internet technology is a playground for eavesdroppers. This poses a
serious challenge to various applications that relies on classical internet
technology. This has motivated the researchers to switch to new technologies
that are fundamentally more secure. Exploring the quantum effects, researchers
paved the way into quantum networks that provide security, privacy and range of
capabilities such as quantum computation, communication and metrology. The
realization of quantum internet requires quantum communication between various
remote nodes through quantum channels guarded by quantum cryptographic
protocols. Such networks rely upon quantum bits (qubits) that can
simultaneously take the value of zeros and ones. Due to extraordinary
properties of qubits such as entanglement, teleportation and superposition, it
gives an edge to quantum networks over traditional networks in many ways. But
at the same time transmitting qubits over long distances is a formidable task
and extensive research is going on quantum teleportation over such distances,
which will become a breakthrough in physically realizing quantum internet in
near future. In this paper, quantum internet functionalities, technologies,
applications and open challenges have been extensively surveyed to help readers
gain a basic understanding of infrastructure required for the development of
global quantum internet.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04631</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04631</id><submitter>Majed El Helou</submitter><version version="v1"><date>Tue, 12 Jan 2021 17:38:32 GMT</date><size>24030kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 22 Jan 2021 11:05:15 GMT</date><size>24031kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 17:25:08 GMT</date><size>24031kb</size><source_type>D</source_type></version><title>Deep Gaussian Denoiser Epistemic Uncertainty and Decoupled
  Dual-Attention Fusion</title><authors>Xiaoqi Ma, Xiaoyu Lin, Majed El Helou, Sabine S\&quot;usstrunk</authors><categories>eess.IV cs.CV</categories><comments>Code and models are publicly available on https://github.com/IVRL/DEU</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the performance breakthrough of denoising networks, improvements
have come chiefly through novel architecture designs and increased depth. While
novel denoising networks were designed for real images coming from different
distributions, or for specific applications, comparatively small improvement
was achieved on Gaussian denoising. The denoising solutions suffer from
epistemic uncertainty that can limit further advancements. This uncertainty is
traditionally mitigated through different ensemble approaches. However, such
ensembles are prohibitively costly with deep networks, which are already large
in size.
  Our work focuses on pushing the performance limits of state-of-the-art
methods on Gaussian denoising. We propose a model-agnostic approach for
reducing epistemic uncertainty while using only a single pretrained network. We
achieve this by tapping into the epistemic uncertainty through augmented and
frequency-manipulated images to obtain denoised images with varying error. We
propose an ensemble method with two decoupled attention paths, over the pixel
domain and over that of our different manipulations, to learn the final fusion.
Our results significantly improve over the state-of-the-art baselines and
across varying noise levels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04968</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04968</id><submitter>Dominic Richards</submitter><version version="v1"><date>Wed, 13 Jan 2021 09:58:06 GMT</date><size>579kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 14:43:44 GMT</date><size>70kb</size><source_type>D</source_type></version><title>Learning with Gradient Descent and Weakly Convex Losses</title><authors>Dominic Richards, Mike Rabbat</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Updated References</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We study the learning performance of gradient descent when the empirical risk
is weakly convex, namely, the smallest negative eigenvalue of the empirical
risk's Hessian is bounded in magnitude. By showing that this eigenvalue can
control the stability of gradient descent, generalisation error bounds are
proven that hold under a wider range of step sizes compared to previous work.
Out of sample guarantees are then achieved by decomposing the test error into
generalisation, optimisation and approximation errors, each of which can be
bounded and traded off with respect to algorithmic parameters, sample size and
magnitude of this eigenvalue. In the case of a two layer neural network, we
demonstrate that the empirical risk can satisfy a notion of local weak
convexity, specifically, the Hessian's smallest eigenvalue during training can
be controlled by the normalisation of the layers, i.e., network scaling. This
allows test error guarantees to then be achieved when the population risk
minimiser satisfies a complexity assumption. By trading off the network
complexity and scaling, insights are gained into the implicit bias of neural
network scaling, which are further supported by experimental findings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.05260</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.05260</id><submitter>Nathanael Lemessa Baisa</submitter><version version="v1"><date>Wed, 13 Jan 2021 18:47:47 GMT</date><size>1708kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 19 Jan 2021 22:12:18 GMT</date><size>1708kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 21 Feb 2021 14:18:19 GMT</date><size>1708kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 12:34:42 GMT</date><size>1710kb</size><source_type>D</source_type></version><title>Hand-Based Person Identification using Global and Part-Aware Deep
  Feature Representation Learning</title><authors>Nathanael L. Baisa, Zheheng Jiang, Ritesh Vyas, Bryan Williams,
  Hossein Rahmani, Plamen Angelov, Sue Black</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cases of serious crime, including sexual abuse, often the only available
information with demonstrated potential for identification is images of the
hands. Since this evidence is captured in uncontrolled situations, it is
difficult to analyse. As global approaches to feature comparison are limited in
this case, it is important to extend to consider local information. In this
work, we propose hand-based person identification by learning both global and
local deep feature representation. Our proposed method, Global and Part-Aware
Network (GPA-Net), creates global and local branches on the conv-layer for
learning robust discriminative global and part-level features. For learning the
local (part-level) features, we perform uniform partitioning on the conv-layer
in both horizontal and vertical directions. We retrieve the parts by conducting
a soft partition without explicitly partitioning the images or requiring
external cues such as pose estimation. We make extensive evaluations on two
large multi-ethnic and publicly available hand datasets, demonstrating that our
proposed method significantly outperforms competing approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.05853</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.05853</id><submitter>Manish Raghavan</submitter><version version="v1"><date>Thu, 14 Jan 2021 20:18:23 GMT</date><size>1224kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 14:46:26 GMT</date><size>1203kb</size><source_type>D</source_type></version><title>Algorithmic Monoculture and Social Welfare</title><authors>Jon Kleinberg, Manish Raghavan</authors><categories>cs.GT cs.CY cs.LG</categories><comments>A version of this paper appears in Proceedings of the National
  Academy of Sciences at https://www.pnas.org/content/118/22/e2018340118</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As algorithms are increasingly applied to screen applicants for high-stakes
decisions in employment, lending, and other domains, concerns have been raised
about the effects of algorithmic monoculture, in which many decision-makers all
rely on the same algorithm. This concern invokes analogies to agriculture,
where a monocultural system runs the risk of severe harm from unexpected
shocks. Here we show that the dangers of algorithmic monoculture run much
deeper, in that monocultural convergence on a single algorithm by a group of
decision-making agents, even when the algorithm is more accurate for any one
agent in isolation, can reduce the overall quality of the decisions being made
by the full collection of agents. Unexpected shocks are therefore not needed to
expose the risks of monoculture; it can hurt accuracy even under &quot;normal&quot;
operations, and even for algorithms that are more accurate when used by only a
single decision-maker. Our results rely on minimal assumptions, and involve the
development of a probabilistic framework for analyzing systems that use
multiple noisy estimates of a set of alternatives.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.05951</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.05951</id><submitter>Houwang Tu</submitter><version version="v1"><date>Fri, 15 Jan 2021 03:16:24 GMT</date><size>1218kb</size></version><title>Two Chebyshev Spectral Methods for Solving Normal Modes in Atmospheric
  Acoustics</title><authors>Tu Houwang, Wang Yongxian, Xiao Wenbin, Lan Qiang, Liu Wei</authors><categories>cs.CE</categories><comments>10 pages, 8 figures and 3 tables</comments><doi>10.3390/e23060705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The normal mode model is important in computational atmospheric acoustics. It
is often used to compute the atmospheric acoustic field under a harmonic point
source. Its solution consists of a set of discrete modes radiating into the
upper atmosphere, usually related to the continuous spectrum. In this article,
we present two spectral methods, the Chebyshev--Tau and Chebyshev--Collocation
methods, to solve for the atmospheric acoustic normal modes, and corresponding
programs were developed. The two spectral methods successfully transform the
problem of searching for the modal wavenumbers in the complex plane into a
simple dense matrix eigenvalue problem by projecting the governing equation
onto a set of orthogonal bases, which can be easily solved through linear
algebra methods. After obtaining the eigenvalues and eigenvectors, the
horizontal wavenumbers and their corresponding modes can be obtained with
simple processing. Numerical experiments were examined for both downwind and
upwind conditions to verify the effectiveness of the methods. The running time
data indicated that both spectral methods proposed in this article are faster
than the Legendre--Galerkin spectral method proposed previously.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06005</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06005</id><submitter>Yifeng Jiang</submitter><version version="v1"><date>Fri, 15 Jan 2021 07:54:31 GMT</date><size>434kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 08:04:23 GMT</date><size>831kb</size><source_type>D</source_type></version><title>SimGAN: Hybrid Simulator Identification for Domain Adaptation via
  Adversarial Reinforcement Learning</title><authors>Yifeng Jiang, Tingnan Zhang, Daniel Ho, Yunfei Bai, C. Karen Liu,
  Sergey Levine, Jie Tan</authors><categories>cs.RO</categories><comments>ICRA 2021, Code Available at: https://github.com/jyf588/SimGAN ;
  Accompanying Video: https://youtu.be/McKOGllO7nc</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  As learning-based approaches progress towards automating robot controllers
design, transferring learned policies to new domains with different dynamics
(e.g. sim-to-real transfer) still demands manual effort. This paper introduces
SimGAN, a framework to tackle domain adaptation by identifying a hybrid physics
simulator to match the simulated trajectories to the ones from the target
domain, using a learned discriminative loss to address the limitations
associated with manual loss design. Our hybrid simulator combines neural
networks and traditional physics simulation to balance expressiveness and
generalizability, and alleviates the need for a carefully selected parameter
set in System ID. Once the hybrid simulator is identified via adversarial
reinforcement learning, it can be used to refine policies for the target
domain, without the need to interleave data collection and policy refinement.
We show that our approach outperforms multiple strong baselines on six robotic
locomotion tasks for domain adaptation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06291</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06291</id><submitter>Mahsa Moosavi</submitter><version version="v1"><date>Fri, 15 Jan 2021 20:16:37 GMT</date><size>2130kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:49:42 GMT</date><size>2174kb</size><source_type>D</source_type></version><title>Trading on-chain: How Feasible is Regulators' Worst-Case Scenario?</title><authors>Mahsa Moosavi and Jeremy Clark</authors><categories>cs.CR</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  When consumers trade financial products, they typically use well-identified
service providers that operate under government regulation. In theory,
decentralized platforms like Ethereum can offer trading services 'on-chain'
without an obvious entry point for regulators. Fortunately for regulators, most
trading volume in blockchain-based assets is still on centralized service
providers for performance reasons. However this leaves the following research
questions we address in this paper: (i) is secure trading (i.e., resistant to
front-running and price manipulation) even feasible as a fully 'on-chain'
service on a public blockchain, (ii) what is its performance benchmark, and
(iii) what is the performance impact of novel techniques (e.g., 'rollups') in
closing the performance gap? To answer these questions, we 'learn by doing' and
custom design an Ethereum-based call market (or batch auction) exchange, Lissy,
with favourable security properties.We conducta variety of optimizations and
experiments to demonstrate that this technology cannot expect to exceed a few
hundred trade executions per block (i.e., 13s window of time). However this can
be scaled dramatically with off-chain execution that is not consumer-facing. We
also illustrate, with numerous examples throughout the paper, how blockchain
deployment is full of nuances that make it quite different from developing in
better understood domains (e.g., cloud-based web applications).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06615</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06615</id><submitter>Quang-Ha Pham</submitter><version version="v1"><date>Sun, 17 Jan 2021 07:51:34 GMT</date><size>1072kb</size></version><version version="v2"><date>Sun, 28 Feb 2021 11:08:06 GMT</date><size>1060kb</size></version><version version="v3"><date>Thu, 15 Apr 2021 04:31:56 GMT</date><size>869kb</size></version><version version="v4"><date>Mon, 26 Apr 2021 11:44:20 GMT</date><size>866kb</size></version><version version="v5"><date>Thu, 13 May 2021 08:34:11 GMT</date><size>869kb</size></version><version version="v6"><date>Mon, 31 May 2021 07:42:03 GMT</date><size>1101kb</size></version><title>Online Robust Sliding-Windowed LiDAR SLAM in Natural Environments</title><authors>Quang-Ha Pham, Ngoc-Huy Tran, Thanh-Toan Nguyen, Thien-Phuc Tran</authors><categories>cs.RO</categories><comments>Add figure 2 for clearer explanation</comments><journal-ref>P. Q. Ha, T. N. Huy, N. T. Toan and T. T. Phuc, &quot;Online Robust
  Sliding-Windowed LiDAR SLAM in Natural Environments,&quot; in 2021 International
  Symposium on Electrical and Electronics Engineering (ISEE), Ho Chi Minh City,
  2021</journal-ref><doi>10.1109/ISEE51682.2021.9418728</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the growing interest for autonomous environmental monitoring,
effective SLAM realization in native habitats remains largely unsolved. In this
paper, we fill this gap by presenting a novel online graph-based SLAM system
for 2D LiDAR sensor in natural environments. By taking advantage of robust
weighting scheme, sliding-windowed optimization, fast scan-matcher and parallel
computing, our system not only delivers stable performance in cluttered
surroudings but also meets real-time constraint. Simulated and experimental
results confirm the feasibility and efficiency in the overall design of the
proposed system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06718</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06718</id><submitter>Francesco Ferrante</submitter><version version="v1"><date>Sun, 17 Jan 2021 17:10:55 GMT</date><size>294kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 09:29:45 GMT</date><size>294kb</size></version><title>On the Design of Structured Stabilizers for LTI Systems</title><authors>Francesco Ferrante, Fabrizio Dabbene, Chiara Ravazzi</authors><categories>math.OC cs.SY eess.SY</categories><comments>V1 fixes a few minor typos in the published version. V2 includes a
  clarification and fixes some minor typos in the proof of Theorem 2</comments><journal-ref>IEEE Control Systems Letters, 4(2), 289-294, 2020</journal-ref><doi>10.1109/LCSYS.2019.2925524</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a static state-feedback controller subject to structural constraint
achieving asymptotic stability is a relevant problem with many applications,
including network decentralized control, coordinated control, and sparse
feedback design. Leveraging on the Projection Lemma, this work presents a new
solution to a class of state-feedback control problems, in which the controller
is constrained to belong to a given linear space. We show through extensive
discussion and numerical examples that our approach leads to several advantages
with respect to existing methods: first, it is computationally efficient;
second, it is less conservative than previous methods, since it relaxes the
requirement of restricting the Lyapunov matrix to a block-diagonal form.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06781</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06781</id><submitter>Mudabbir Kaleem</submitter><version version="v1"><date>Sun, 17 Jan 2021 20:51:29 GMT</date><size>750kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 02:29:49 GMT</date><size>651kb</size><source_type>D</source_type></version><title>Demystifying Pythia: A Survey of ChainLink Oracles Usage on Ethereum</title><authors>Mudabbir Kaleem, Weidong Shi</authors><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart contracts are dependent on oracle systems for their adoption and
usability. We perform an empirical study of oracle systems' usage trends and
adoption metrics to provide better insight into the health of the smart
contract ecosystem. We collect ChainLink usage data on the Ethereum network
using a modified Ethereum client and running a full node. We analyze the
collected data and present our findings and insights surrounding the usage
trends, adoption metrics, oracle pricing and service quality associated with
ChainLink on the Ethereum network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06927</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06927</id><submitter>Peter M\&quot;ullner</submitter><version version="v1"><date>Mon, 18 Jan 2021 08:30:00 GMT</date><size>1835kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 06:12:40 GMT</date><size>1734kb</size><source_type>D</source_type></version><title>Robustness of Meta Matrix Factorization Against Strict Privacy
  Constraints</title><authors>Peter M\&quot;ullner, Dominik Kowald, Elisabeth Lex</authors><categories>cs.IR</categories><comments>Accepted at ECIR 2021, Reproducibility Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the reproducibility of MetaMF, a meta matrix
factorization framework introduced by Lin et al. MetaMF employs meta learning
for federated rating prediction to preserve users' privacy. We reproduce the
experiments of Lin et al. on five datasets, i.e., Douban, Hetrec-MovieLens,
MovieLens 1M, Ciao, and Jester. Also, we study the impact of meta learning on
the accuracy of MetaMF's recommendations. Furthermore, in our work, we
acknowledge that users may have different tolerances for revealing information
about themselves. Hence, in a second strand of experiments, we investigate the
robustness of MetaMF against strict privacy constraints. Our study illustrates
that we can reproduce most of Lin et al.'s results. Plus, we provide strong
evidence that meta learning is essential for MetaMF's robustness against strict
privacy constraints.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06968</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06968</id><submitter>Javier Fumanal-Idocin Mr.</submitter><version version="v1"><date>Mon, 18 Jan 2021 10:14:01 GMT</date><size>3311kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 08:41:37 GMT</date><size>3310kb</size><source_type>D</source_type></version><title>Motor-Imagery-Based Brain Computer Interface using Signal Derivation and
  Aggregation Functions</title><authors>Javier Fumanal-Idocin, Yu-Kai Wang, Chin-Teng Lin, Javier Fern\'andez,
  Jose Antonio Sanz, Humberto Bustince</authors><categories>cs.HC cs.AI cs.SY eess.SY</categories><comments>IEEE Transactions on Cybernetics (2021)</comments><doi>10.1109/TCYB.2021.3073210</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Brain Computer Interface technologies are popular methods of communication
between the human brain and external devices. One of the most popular
approaches to BCI is Motor Imagery. In BCI applications, the
ElectroEncephaloGraphy is a very popular measurement for brain dynamics because
of its non-invasive nature. Although there is a high interest in the BCI topic,
the performance of existing systems is still far from ideal, due to the
difficulty of performing pattern recognition tasks in EEG signals. BCI systems
are composed of a wide range of components that perform signal pre-processing,
feature extraction and decision making. In this paper, we define a BCI
Framework, named Enhanced Fusion Framework, where we propose three different
ideas to improve the existing MI-based BCI frameworks. Firstly, we include aan
additional pre-processing step of the signal: a differentiation of the EEG
signal that makes it time-invariant. Secondly, we add an additional frequency
band as feature for the system and we show its effect on the performance of the
system. Finally, we make a profound study of how to make the final decision in
the system. We propose the usage of both up to six types of different
classifiers and a wide range of aggregation functions (including classical
aggregations, Choquet and Sugeno integrals and their extensions and overlap
functions) to fuse the information given by the considered classifiers. We have
tested this new system on a dataset of 20 volunteers performing motor
imagery-based brain-computer interface experiments. On this dataset, the new
system achieved a 88.80% of accuracy. We also propose an optimized version of
our system that is able to obtain up to 90,76%. Furthermore, we find that the
pair Choquet/Sugeno integrals and overlap functions are the ones providing the
best results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.07621</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.07621</id><submitter>Tomomi Matsui</submitter><version version="v1"><date>Tue, 19 Jan 2021 13:54:41 GMT</date><size>27kb</size></version><version version="v2"><date>Sat, 29 May 2021 10:21:53 GMT</date><size>33kb</size></version><title>Trading Transforms of Non-weighted Simple Games and Integer Weights of
  Weighted Simple Games</title><authors>Akihiro Kawana and Tomomi Matsui</authors><categories>cs.GT cs.AI</categories><comments>23 pages</comments><msc-class>91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates simple games. A fundamental research question in this
field is to determine necessary and sufficient conditions for a simple game to
be a weighted majority game. Taylor and Zwicker (1992) showed that a simple
game is non-weighted if and only if there exists a trading transform of finite
size. They also provided an upper bound on the size of such a trading
transform, if it exists. Gvozdeva and Slinko (2011) improved that upper bound;
their proof employed a property of linear inequalities demonstrated by Muroga
(1971).In this study, we provide a new proof of the existence of a trading
transform when a given simple game is non-weighted. Our proof employs Farkas'
lemma (1894), and yields an improved upper bound on the size of a trading
transform.
  We also discuss an integer-weight representation of a weighted simple game,
improving the bounds obtained by Muroga (1971). We show that our bound on the
quota is tight when the number of players is less than or equal to five, based
on the computational results obtained by Kurz (2012).
  Furthermore, we discuss the problem of finding an integer-weight
representation under the assumption that we have minimal winning coalitions and
maximal losing coalitions.In particular, we show a performance of a rounding
method.
  Lastly, we address roughly weighted simple games. Gvozdeva and Slinko (2011)
showed that a given simple game is not roughly weighted if and only if there
exists a potent certificate of non-weightedness. We give an upper bound on the
length of a potent certificate of non-weightedness. We also discuss an
integer-weight representation of a roughly weighted simple game.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.07752</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.07752</id><submitter>Asier Guti\'errez-Fandi\~no</submitter><version version="v1"><date>Tue, 19 Jan 2021 17:44:50 GMT</date><size>988kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 21 Jan 2021 13:17:49 GMT</date><size>988kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 09:43:19 GMT</date><size>292kb</size><source_type>D</source_type></version><title>Characterizing and Measuring the Similarity of Neural Networks with
  Persistent Homology</title><authors>David P\'erez-Fern\'andez and Asier Guti\'errez-Fandi\~no and Jordi
  Armengol-Estap\'e and Marta Villegas</authors><categories>cs.LG math.AT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Characterizing the structural properties of neural networks is crucial yet
poorly understood, and there are no well-established similarity measures
between networks. In this work, we observe that neural networks can be
represented as abstract simplicial complex and analyzed using their topological
'fingerprints' via Persistent Homology (PH). We then describe a PH-based
representation proposed for characterizing and measuring similarity of neural
networks. We empirically show the effectiveness of this representation as a
descriptor of different architectures in several datasets. This approach based
on Topological Data Analysis is a step towards better understanding neural
networks and serves as a useful similarity measure.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.07994</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.07994</id><submitter>Hongyu Zhou</submitter><version version="v1"><date>Wed, 20 Jan 2021 06:41:53 GMT</date><size>769kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 03:41:48 GMT</date><size>909kb</size><source_type>D</source_type></version><title>Distributed Motion Coordination Using Convex Feasible Set Based Model
  Predictive Control</title><authors>Hongyu Zhou and Changliu Liu</authors><categories>cs.RO</categories><comments>7 pages, 10 figures. ICRA 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation of optimization-based motion coordination approaches in
real world multi-agent systems remains challenging due to their high
computational complexity and potential deadlocks. This paper presents a
distributed model predictive control (MPC) approach based on convex feasible
set (CFS) algorithm for multi-vehicle motion coordination in autonomous
driving. By using CFS to convexify the collision avoidance constraints,
collision-free trajectories can be computed in real time. We analyze the
potential deadlocks and show that a deadlock can be resolved by changing
vehicles' desired speeds. The MPC structure ensures that our algorithm is
robust to low-level tracking errors. The proposed distributed method has been
tested in multiple challenging multi-vehicle environments, including
unstructured road, intersection, crossing, platoon formation, merging, and
overtaking scenarios. The numerical results and comparison with other
approaches (including a centralized MPC and reciprocal velocity obstacles) show
that the proposed method is computationally efficient and robust, and avoids
deadlocks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.07997</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.07997</id><submitter>Zhanlin Liu</submitter><version version="v1"><date>Wed, 20 Jan 2021 07:06:50 GMT</date><size>161kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 21 Jan 2021 01:44:40 GMT</date><size>161kb</size><source_type>D</source_type></version><title>Data-driven sparse polynomial chaos expansion for models with dependent
  inputs</title><authors>Zhanlin Liu and Youngjun Choe</authors><categories>eess.SY cs.SY stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial chaos expansions (PCEs) have been used in many real-world
engineering applications to quantify how the uncertainty of an output is
propagated from inputs. PCEs for models with independent inputs have been
extensively explored in the literature. Recently, different approaches have
been proposed for models with dependent inputs to expand the use of PCEs to
more real-world applications. Typical approaches include building PCEs based on
the Gram-Schmidt algorithm or transforming the dependent inputs into
independent inputs. However, the two approaches have their limitations
regarding computational efficiency and additional assumptions about the input
distributions, respectively. In this paper, we propose a data-driven approach
to build sparse PCEs for models with dependent inputs. The proposed algorithm
recursively constructs orthonormal polynomials using a set of monomials based
on their correlations with the output. The proposed algorithm on building
sparse PCEs not only reduces the number of minimally required observations but
also improves the numerical stability and computational efficiency. Four
numerical examples are implemented to validate the proposed algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08141</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08141</id><submitter>Srinivasan Arunachalam</submitter><version version="v1"><date>Wed, 20 Jan 2021 14:05:49 GMT</date><size>57kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 19:56:24 GMT</date><size>58kb</size></version><title>Positive spectrahedra: Invariance principles and Pseudorandom generators</title><authors>Srinivasan Arunachalam and Penghui Yao</authors><categories>cs.CC math.CO</categories><comments>63 pages. v2: Minor revisions and Improvements in presentation</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In a recent work, O'Donnell, Servedio and Tan (STOC 2019) gave explicit
pseudorandom generators (PRGs) for arbitrary $m$-facet polytopes in $n$
variables with seed length poly-logarithmic in $m,n$, concluding a sequence of
works in the last decade, that was started by Diakonikolas, Gopalan, Jaiswal,
Servedio, Viola (SICOMP 2010) and Meka, Zuckerman (SICOMP 2013) for fooling
linear and polynomial threshold functions, respectively. In this work, we
consider a natural extension of PRGs for intersections of positive
spectrahedrons. A positive spectrahedron is a Boolean function
$f(x)=[x_1A^1+\cdots +x_nA^n \preceq B]$ where the $A^i$s are $k\times k$
positive semidefinite matrices. We construct explicit PRGs that $\delta$-fool
&quot;regular&quot; width-$M$ positive spectrahedrons (i.e., when none of the $A^i$s are
dominant) over the Boolean space with seed length $\textsf{poly}(\log k,\log n,
M, 1/\delta)$.
  Our main technical contributions are the following: We first prove an
invariance principle for positive spectrahedra via the well-known Lindeberg
method. As far as we are aware such a generalization of the Lindeberg method
was unknown. Second, we prove an upper bound on noise sensitivity and a
Littlewood-Offord theorem for positive spectrahedra. Using these results, we
give applications for constructing PRGs for positive spectrahedra, learning
theory, discrepancy sets for positive spectrahedra (over the Boolean cube) and
PRGs for intersections of structured polynomial threshold functions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08533</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08533</id><submitter>Yunpeng Gong</submitter><version version="v1"><date>Thu, 21 Jan 2021 10:33:02 GMT</date><size>686kb</size></version><version version="v2"><date>Wed, 7 Apr 2021 08:26:49 GMT</date><size>1611kb</size></version><version version="v3"><date>Mon, 31 May 2021 15:15:14 GMT</date><size>765kb</size></version><version version="v4"><date>Tue, 1 Jun 2021 01:30:13 GMT</date><size>765kb</size></version><title>A general multi-modal data learning method for Person Re-identification</title><authors>Yunpeng Gong</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a general multi-modal data learning method, which
includes Global Homogeneous Transformation, Local Homogeneous Transformation
and their combination. During ReID model training, on the one hand, it randomly
selected a rectangular area in the RGB image and replace its color with the
same rectangular area in corresponding homogeneous image, thus it generate a
training image with different homogeneous areas; On the other hand, it convert
an image into a homogeneous image. These two methods help the model to directly
learn the relationship between different modalities in the Special ReID task.
In single-modal ReID tasks, it can be used as an effective data augmentation.
The experimental results show that our method achieves a performance
improvement of up to 3.3% in single modal ReID task, and performance
improvement in the Sketch Re-identification more than 8%. In addition, our
experiments also show that this method is also very useful in adversarial
training for adversarial defense. It can help the model learn faster and better
from adversarial examples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08609</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08609</id><submitter>Jinhai Yang</submitter><version version="v1"><date>Thu, 21 Jan 2021 13:55:29 GMT</date><size>3420kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 05:02:45 GMT</date><size>3420kb</size><source_type>D</source_type></version><title>MPASNET: Motion Prior-Aware Siamese Network for Unsupervised Deep Crowd
  Segmentation in Video Scenes</title><authors>Jinhai Yang, Hua Yang</authors><categories>cs.CV cs.LG</categories><comments>ICIP 2021 Camera Ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd segmentation is a fundamental task serving as the basis of crowded
scene analysis, and it is highly desirable to obtain refined pixel-level
segmentation maps. However, it remains a challenging problem, as existing
approaches either require dense pixel-level annotations to train deep learning
models or merely produce rough segmentation maps from optical or particle flows
with physical models. In this paper, we propose the Motion Prior-Aware Siamese
Network (MPASNET) for unsupervised crowd semantic segmentation. This model not
only eliminates the need for annotation but also yields high-quality
segmentation maps. Specially, we first analyze the coherent motion patterns
across the frames and then apply a circular region merging strategy on the
collective particles to generate pseudo-labels. Moreover, we equip MPASNET with
siamese branches for augmentation-invariant regularization and siamese feature
aggregation. Experiments over benchmark datasets indicate that our model
outperforms the state-of-the-arts by more than 12% in terms of mIoU.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08658</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08658</id><submitter>Ofer Mendelevitch</submitter><version version="v1"><date>Mon, 18 Jan 2021 23:01:27 GMT</date><size>8171kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 04:41:17 GMT</date><size>6302kb</size></version><title>Fidelity and Privacy of Synthetic Medical Data</title><authors>Ofer Mendelevitch, Michael D. Lesh</authors><categories>cs.LG cs.AI cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The digitization of medical records ushered in a new era of big data to
clinical science, and with it the possibility that data could be shared, to
multiply insights beyond what investigators could abstract from paper records.
The need to share individual-level medical data to accelerate innovation in
precision medicine continues to grow, and has never been more urgent, as
scientists grapple with the COVID-19 pandemic. However, enthusiasm for the use
of big data has been tempered by a fully appropriate concern for patient
autonomy and privacy. That is, the ability to extract private or confidential
information about an individual, in practice, renders it difficult to share
data, since significant infrastructure and data governance must be established
before data can be shared. Although HIPAA provided de-identification as an
approved mechanism for data sharing, linkage attacks were identified as a major
vulnerability. A variety of mechanisms have been established to avoid leaking
private information, such as field suppression or abstraction, strictly
limiting the amount of information that can be shared, or employing
mathematical techniques such as differential privacy. Another approach, which
we focus on here, is creating synthetic data that mimics the underlying data.
For synthetic data to be a useful mechanism in support of medical innovation
and a proxy for real-world evidence, one must demonstrate two properties of the
synthetic dataset: (1) any analysis on the real data must be matched by
analysis of the synthetic data (statistical fidelity) and (2) the synthetic
data must preserve privacy, with minimal risk of re-identification (privacy
guarantee). In this paper we propose a framework for quantifying the
statistical fidelity and privacy preservation properties of synthetic datasets
and demonstrate these metrics for synthetic data generated by Syntegra
technology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08687</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08687</id><submitter>Ties van Rozendaal</submitter><version version="v1"><date>Thu, 21 Jan 2021 15:58:58 GMT</date><size>1723kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 14:26:04 GMT</date><size>1766kb</size><source_type>D</source_type></version><title>Overfitting for Fun and Profit: Instance-Adaptive Data Compression</title><authors>Ties van Rozendaal, Iris A.M. Huijben, Taco S. Cohen</authors><categories>cs.LG</categories><comments>Accepted at International Conference on Learning Representations 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural data compression has been shown to outperform classical methods in
terms of $RD$ performance, with results still improving rapidly. At a high
level, neural compression is based on an autoencoder that tries to reconstruct
the input instance from a (quantized) latent representation, coupled with a
prior that is used to losslessly compress these latents. Due to limitations on
model capacity and imperfect optimization and generalization, such models will
suboptimally compress test data in general. However, one of the great strengths
of learned compression is that if the test-time data distribution is known and
relatively low-entropy (e.g. a camera watching a static scene, a dash cam in an
autonomous car, etc.), the model can easily be finetuned or adapted to this
distribution, leading to improved $RD$ performance. In this paper we take this
concept to the extreme, adapting the full model to a single video, and sending
model updates (quantized and compressed using a parameter-space prior) along
with the latent representation. Unlike previous work, we finetune not only the
encoder/latents but the entire model, and - during finetuning - take into
account both the effect of model quantization and the additional costs incurred
by sending the model updates. We evaluate an image compression model on
I-frames (sampled at 2 fps) from videos of the Xiph dataset, and demonstrate
that full-model adaptation improves $RD$ performance by ~1 dB, with respect to
encoder-only finetuning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08934</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08934</id><submitter>Hengrong Lan</submitter><version version="v1"><date>Fri, 22 Jan 2021 03:49:30 GMT</date><size>2147kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 01:55:16 GMT</date><size>1031kb</size></version><title>AS-Net: Fast Photoacoustic Reconstruction with Multi-feature Fusion from
  Sparse Data</title><authors>Mengjie Guo, Hengrong Lan, Changchun Yang, and Fei Gao</authors><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photoacoustic (PA) imaging is a biomedical imaging modality capable of
acquiring high-contrast images of optical absorption at depths much greater
than traditional optical imaging techniques. However, practical instrumentation
and geometry limit the number of available acoustic sensors surrounding the
imaging target, which results in the sparsity of sensor data. Conventional PA
image reconstruction methods give severe artifacts when they are applied
directly to the sparse PA data. In this paper, we firstly propose to employ a
novel signal processing method to make sparse PA raw data more suitable for the
neural network, concurrently speeding up image reconstruction. Then we propose
Attention Steered Network (AS-Net) for PA reconstruction with multi-feature
fusion. AS-Net is validated on different datasets, including simulated
photoacoustic data from fundus vasculature phantoms and experimental data from
in vivo fish and mice. Notably, the method is also able to eliminate some
artifacts present in the ground truth for in vivo data. Results demonstrated
that our method provides superior reconstructions at a faster speed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.09379</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.09379</id><submitter>Ulugbek Kamilov</submitter><version version="v1"><date>Fri, 22 Jan 2021 23:33:11 GMT</date><size>8142kb</size><source_type>D</source_type></version><title>SGD-Net: Efficient Model-Based Deep Learning with Theoretical Guarantees</title><authors>Jiaming Liu, Yu Sun, Weijie Gan, Xiaojian Xu, Brendt Wohlberg, and
  Ulugbek S. Kamilov</authors><categories>eess.IV cs.LG</categories><doi>10.1109/TCI.2021.3085534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep unfolding networks have recently gained popularity in the context of
solving imaging inverse problems. However, the computational and memory
complexity of data-consistency layers within traditional deep unfolding
networks scales with the number of measurements, limiting their applicability
to large-scale imaging inverse problems. We propose SGD-Net as a new
methodology for improving the efficiency of deep unfolding through stochastic
approximations of the data-consistency layers. Our theoretical analysis shows
that SGD-Net can be trained to approximate batch deep unfolding networks to an
arbitrary precision. Our numerical results on intensity diffraction tomography
and sparse-view computed tomography show that SGD-Net can match the performance
of the batch network at a fraction of training and testing complexity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.09469</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.09469</id><submitter>Victor Junqiu Wei</submitter><version version="v1"><date>Sat, 23 Jan 2021 10:01:28 GMT</date><size>2808kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 14:37:37 GMT</date><size>1412kb</size><source_type>D</source_type></version><title>Training Multilingual Pre-trained Language Model with Byte-level
  Subwords</title><authors>Junqiu Wei, Qun Liu, Yinpeng Guo, Xin Jiang</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pre-trained language models have achieved great successes in various
natural language understanding (NLU) tasks due to its capacity to capture the
deep contextualized information in text by pre-training on large-scale corpora.
One of the fundamental components in pre-trained language models is the
vocabulary, especially for training multilingual models on many different
languages. In the technical report, we present our practices on training
multilingual pre-trained language models with BBPE: Byte-Level BPE (i.e., Byte
Pair Encoding). In the experiment, we adopted the architecture of NEZHA as the
underlying pre-trained language model and the results show that NEZHA trained
with byte-level subwords consistently outperforms Google multilingual BERT and
vanilla NEZHA by a notable margin in several multilingual NLU tasks. We release
the source code of our byte-level vocabulary building tools and the
multilingual pre-trained language models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.09520</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.09520</id><submitter>John Fitzgerald</submitter><version version="v1"><date>Sat, 23 Jan 2021 15:06:14 GMT</date><size>18496kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 09:18:43 GMT</date><size>15359kb</size><source_type>D</source_type></version><title>Is academia becoming more localised? The growth of regional knowledge
  networks within international research collaboration</title><authors>John Fitzgerald, Sanna Ojanper\&quot;a, Neave O'Clery</authors><categories>cs.SI</categories><comments>28 pages, 7 figures, accepted to Applied Network Science</comments><journal-ref>Appl Netw Sci 6, 38 (2021)</journal-ref><doi>10.1007/s41109-021-00371-w</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  It is well-established that the process of learning and capability building
is core to economic development and structural transformation. Since knowledge
is `sticky', a key component of this process is learning-by-doing, which can be
achieved via a variety of mechanisms including international research
collaboration. Uncovering significant inter-country research ties using Scopus
co-authorship data, we show that within-region collaboration has increased over
the past five decades relative to international collaboration. Further
supporting this insight, we find that while communities present in the global
collaboration network before 2000 were often based on historical geopolitical
or colonial lines, in more recent years they increasingly align with a simple
partition of countries by regions. These findings are unexpected in light of a
presumed continual increase in globalisation, and have significant implications
for the design of programmes aimed at promoting international research
collaboration and knowledge diffusion.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.09877</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.09877</id><submitter>Mikhail Bragin</submitter><version version="v1"><date>Mon, 25 Jan 2021 03:30:07 GMT</date><size>1282kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 02:19:12 GMT</date><size>1340kb</size><source_type>D</source_type></version><title>TSO-DSO Operational Planning Coordination through &quot;$l_1$-Proximal&quot;
  Surrogate Lagrangian Relaxation</title><authors>Mikhail Bragin, Yury Dvorkin</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of distributed energy resources (DERs), located at the
Distribution System Operator (DSO) level, bring new opportunities as well as
new challenges to the operations within the grid, specifically, when it comes
to the interaction with the Transmission System Operator (TSO). To enable
interoperability, while ensuring higher flexibility and cost-efficiency, DSOs
and the TSO need to be efficiently coordinated. Difficulties behind creating
such TSO-DSO coordination include the combinatorial nature of the operational
planning problem involved at the transmission level as well as the nonlinearity
of AC power flow within both systems. These considerations significantly
increase the complexity even under the deterministic setting. In this paper, a
deterministic TSO-DSO operational planning coordination problem is considered
and a novel decomposition and coordination approach is developed. Within the
new method, the problem is decomposed into TSO and DSO subproblems, which are
efficiently coordinated by updating Lagrangian multipliers. The nonlinearities
at the TSO level caused by AC power flow constraints are resolved through a
dynamic linearization while guaranteeing feasibility through &quot;$l_1$-proximal&quot;
terms. Numerical results based on the coordination of the 118-bus TSO system
with up to 32 DSO 34-bus systems indicate that the method efficiently overcomes
the computational difficulties of the problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10092</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10092</id><submitter>Maximilian Parzen</submitter><version version="v1"><date>Fri, 15 Jan 2021 13:23:28 GMT</date><size>3349kb</size></version><version version="v2"><date>Mon, 31 May 2021 22:29:52 GMT</date><size>9216kb</size><source_type>D</source_type></version><title>Beyond cost reduction: Improving the value of energy storage in
  electricity systems</title><authors>Maximilian Parzen, Fabian Neumann, Addrian H. Van Der Weijde, Daniel
  Friedrich, Aristides Kiprakis</authors><categories>eess.SY cs.SY physics.soc-ph q-fin.PM</categories><comments>15 pages, 10 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  An energy storage technology is valuable if it makes energy systems cheaper.
Traditional ways to improve storage technologies are to reduce their costs;
however, the cheapest energy storage is not always the most valuable in energy
systems. This paper reviews techno-economic storage valuation methods and
expands them by the introduced market potential method. The market potential
method derives the value of technologies by examining common deployment signals
from energy system model outputs in a structured way. We apply and compare this
method to cost evaluation approaches in a renewables-based European power
system model, covering diverse energy storage technologies. We find that
characteristics of high-cost hydrogen storage can be equally or even more
valuable than low-cost hydrogen storage. Additionally, we show that modifying
the freedom of storage sizing and component interactions can make the energy
system 10% cheaper and impact the value of technologies. The results suggest to
look beyond the pure cost reduction paradigm and focus on developing
technologies with value approaches that can lead to cheaper electricity systems
in future. One practical and useful value method guiding energy storage
innovation could be the market potential method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10394</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10394</id><submitter>Luca Ballotta</submitter><version version="v1"><date>Mon, 25 Jan 2021 20:30:15 GMT</date><size>3667kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 12 Apr 2021 10:27:20 GMT</date><size>5561kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 16 Apr 2021 18:40:07 GMT</date><size>5561kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 15:57:02 GMT</date><size>3943kb</size><source_type>D</source_type></version><title>Optimal Network Topology of Multi-Agent Systems subject to Computation
  and Communication Latency (with proofs)</title><authors>Luca Ballotta and Mihailo R. Jovanovi\'c and Luca Schenato</authors><categories>eess.SY cs.SY</categories><comments>18 pages, 9 figures</comments><msc-class>93B70 (Primary) 93C43 (Secondary)</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study minimum-variance feedback-control design for a networked control
system with retarded dynamics, where inter-agent communication is subject to
latency. We prove that such a design can be solved efficiently for circular
formations and compute near-optimal control gains in closed form. We show that
the centralized control architecture is in general suboptimal when the
communication increase with the number of links, and propose a control-driven
optimization of the network topology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10423</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10423</id><submitter>Zheda Mai</submitter><version version="v1"><date>Mon, 25 Jan 2021 21:20:02 GMT</date><size>2301kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 21:07:30 GMT</date><size>2326kb</size><source_type>D</source_type></version><title>Online Continual Learning in Image Classification: An Empirical Survey</title><authors>Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, Scott
  Sanner</authors><categories>cs.LG cs.CV</categories><comments>Submitted to Neurocomputing. Codes available at
  https://github.com/RaptorMai/online-continual-learning</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as &quot;review&quot; trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe iCaRL remains competitive when the memory buffer is small; GDumb
outperforms many recently proposed methods in medium-size datasets and MIR
performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the &quot;review&quot; trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10478</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10478</id><submitter>Tristan Montoya</submitter><version version="v1"><date>Mon, 25 Jan 2021 23:55:44 GMT</date><size>1760kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:26:43 GMT</date><size>598kb</size><source_type>D</source_type></version><title>A unifying algebraic framework for discontinuous Galerkin and flux
  reconstruction methods based on the summation-by-parts property</title><authors>Tristan Montoya and David W. Zingg</authors><categories>math.NA cs.NA physics.comp-ph</categories><comments>41 pages, 4 figures</comments><msc-class>65M12, 65M60, 65M70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a unifying framework for the matrix-based formulation and analysis
of discontinuous Galerkin (DG) and flux reconstruction (FR) methods for
conservation laws on general unstructured grids. Within such an algebraic
framework, the multidimensional summation-by-parts (SBP) property is used to
establish the discrete equivalence of strong and weak formulations, as well as
the conservation and energy stability properties of a broad class of DG and FR
schemes. Specifically, the analysis enables the extension of the equivalence
between the strong and weak forms of the discontinuous Galerkin collocation
spectral-element method demonstrated by Kopriva and Gassner (J Sci Comput
44:136-155, 2010) to more general nodal and modal DG formulations, as well as
to the Vincent-Castonguay-Jameson-Huynh (VCJH) family of FR methods. Moreover,
new algebraic proofs of conservation and energy stability for DG and VCJH
schemes with respect to suitable quadrature rules and discrete norms are
presented, in which the SBP property serves as a unifying mechanism for
establishing such results. Numerical experiments are provided for the
two-dimensional linear advection and Euler equations, highlighting the design
choices afforded for methods within the proposed framework and corroborating
the theoretical analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10814</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10814</id><submitter>Arya Tanmay Gupta</submitter><version version="v1"><date>Tue, 5 Jan 2021 19:09:03 GMT</date><size>543kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 31 Jan 2021 14:04:56 GMT</date><size>547kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 20:10:43 GMT</date><size>560kb</size><source_type>D</source_type></version><title>Spread and defend infection in graphs</title><authors>Arya Tanmay Gupta</authors><categories>physics.soc-ph cs.DM math.PR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The spread of an infection, a contagion, meme, emotion, message and various
other spreadable objects have been discussed in several works. Burning and
firefighting have been discussed in particular on static graphs. Graph burning
simulates the notion of the spread of &quot;fire&quot; throughout a graph (plus, one
unburned node burned at each time-step); graph firefighting simulates the
defending of nodes by placing firefighters on the nodes which have not been
already burned while the fire is being spread (started by only a single fire
source).
  This article studies a combination of firefighting and burning on a graph
class which is a variation (generalization) of temporal graphs. Nodes can be
infected from &quot;outside&quot; a network. We present a notion of both upgrading (of
unburned nodes, similar to firefighting) and repairing (of infected nodes). The
nodes which are burned, firefighted, or repaired are chosen probabilistically.
So a variable amount of nodes are allowed to be infected, upgraded and repaired
in each time step.
  In the model presented in this article, both burning and firefighting proceed
concurrently, we introduce such a system to enable the community to study the
notion of spread of an infection and the notion of upgrade/repair against each
other. The graph class that we study (on which, these processes are simulated)
is a variation of temporal graph class in which at each time-step,
probabilistically, a communication takes place (iff an edge exists in that time
step). In addition, a node can be &quot;worn out&quot; and thus can be removed from the
network, and a new healthy node can be added to the network as well. This class
of graphs enables systems with high complexity to be able to be simulated and
studied.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10825</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10825</id><submitter>Mirko Trisolini</submitter><version version="v1"><date>Fri, 22 Jan 2021 10:17:52 GMT</date><size>3993kb</size><source_type>D</source_type></version><title>Propagation and reconstruction of re-entry uncertainties using
  continuity equation and simplicial interpolation</title><authors>Mirko Trisolini and Camilla Colombo</authors><categories>cs.CE astro-ph.EP astro-ph.IM cs.NA math.NA</categories><journal-ref>Journal of Guidance, Control, and Dynamics. 44. 4. (2021) 793-811</journal-ref><doi>10.2514/1.G005228</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This work proposes a continuum-based approach for the propagation of
uncertainties in the initial conditions and parameters for the analysis and
prediction of spacecraft re-entries. Using the continuity equation together
with the re-entry dynamics, the joint probability distribution of the
uncertainties is propagated in time for specific sampled points. At each time
instant, the joint probability distribution function is then reconstructed from
the scattered data using a gradient-enhanced linear interpolation based on a
simplicial representation of the state space. Uncertainties in the initial
conditions at re-entry and in the ballistic coefficient for three
representative test cases are considered: a three-state and a six-state steep
Earth re-entry and a six-state unguided lifting entry at Mars. The paper shows
the comparison of the proposed method with Monte Carlo based techniques in
terms of quality of the obtained marginal distributions and runtime as a
function of the number of samples used.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10856</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10856</id><submitter>Hao Xu</submitter><version version="v1"><date>Tue, 26 Jan 2021 15:24:22 GMT</date><size>453kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 27 Feb 2021 19:43:45 GMT</date><size>626kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 12:24:26 GMT</date><size>854kb</size><source_type>D</source_type></version><title>BE-RAN: Blockchain-enabled Open RAN with Decentralized Identity
  Management and Privacy-Preserving Communication</title><authors>Hao Xu, Lei Zhang, Yunqing Sun, and Chih-Lin I</authors><categories>cs.CR cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio Access Networks (RAN) tends to be more distributed in the 5G and
beyond, in order to provide low latency and flexible on-demanding services. In
this paper, Blockchain-enabled Radio Access Networks (BE-RAN) is proposed as a
novel decentralized RAN architecture to facilitate enhanced security and
privacy on identification and authentication. It can offer user-centric
identity management for User Equipment (UE) and RAN elements, and enable mutual
authentication to all entities while enabling on-demand point-to-point
communication with accountable billing service add-on on public network. Also,
a potential operating model with thorough decentralization of RAN is
envisioned. The paper also proposed a distributed privacy-preserving P2P
communication approach, as one of the core use cases for future mobile
networks, is presented as an essential complement to the existing core
network-based security and privacy management. The results show that BE-RAN
significantly improves communication and computation overheads compared to the
existing communication authentication protocols.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.11037</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.11037</id><submitter>Oliver Urs Lenz</submitter><version version="v1"><date>Tue, 26 Jan 2021 19:14:14 GMT</date><size>578kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 12:17:09 GMT</date><size>366kb</size><source_type>D</source_type></version><title>Average Localised Proximity: A new data descriptor with good default
  one-class classification performance</title><authors>Oliver Urs Lenz, Daniel Peralta, Chris Cornelis</authors><categories>cs.LG stat.ML</categories><comments>Accepted manuscript</comments><journal-ref>Pattern Recognition 118 (2021) 107991</journal-ref><doi>10.1016/j.patcog.2021.107991</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  One-class classification is a challenging subfield of machine learning in
which so-called data descriptors are used to predict membership of a class
based solely on positive examples of that class, and no counter-examples. A
number of data descriptors that have been shown to perform well in previous
studies of one-class classification, like the Support Vector Machine (SVM),
require setting one or more hyperparameters. There has been no systematic
attempt to date to determine optimal default values for these hyperparameters,
which limits their ease of use, especially in comparison with
hyperparameter-free proposals like the Isolation Forest (IF). We address this
issue by determining optimal default hyperparameter values across a collection
of 246 one-class classification problems derived from 50 different real-world
datasets. In addition, we propose a new data descriptor, Average Localised
Proximity (ALP) to address certain issues with existing approaches based on
nearest neighbour distances. Finally, we evaluate classification performance
using a leave-one-dataset-out procedure, and find strong evidence that ALP
outperforms IF and a number of other data descriptors, as well as weak evidence
that it outperforms SVM, making ALP a good default choice.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.11284</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.11284</id><submitter>Corinna Coupette</submitter><version version="v1"><date>Wed, 27 Jan 2021 09:38:56 GMT</date><size>3954kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 5 Apr 2021 12:05:07 GMT</date><size>4146kb</size><source_type>D</source_type></version><title>Measuring Law Over Time: A Network Analytical Framework with an
  Application to Statutes and Regulations in the United States and Germany</title><authors>Corinna Coupette, Janis Beckedorf, Dirk Hartung, Michael Bommarito,
  and Daniel Martin Katz</authors><categories>cs.SI cs.CY physics.soc-ph</categories><comments>32 pages, 13 figures (main paper); 32 pages, 14 figures
  (supplementary information)</comments><journal-ref>Frontiers in Physics 9 (2021)</journal-ref><doi>10.3389/fphy.2021.658463</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How do complex social systems evolve in the modern world? This question lies
at the heart of social physics, and network analysis has proven critical in
providing answers to it. In recent years, network analysis has also been used
to gain a quantitative understanding of law as a complex adaptive system, but
most research has focused on legal documents of a single type, and there exists
no unified framework for quantitative legal document analysis using network
analytical tools. Against this background, we present a comprehensive framework
for analyzing legal documents as multi-dimensional, dynamic document networks.
We demonstrate the utility of this framework by applying it to an original
dataset of statutes and regulations from two different countries, the United
States and Germany, spanning more than twenty years (1998-2019). Our framework
provides tools for assessing the size and connectivity of the legal system as
viewed through the lens of specific document collections as well as for
tracking the evolution of individual legal documents over time. Implementing
the framework for our dataset, we find that at the federal level, the United
States legal system is increasingly dominated by regulations, whereas the
German legal system remains governed by statutes. This holds regardless of
whether we measure the systems at the macro, the meso, or the micro level.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.11320</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.11320</id><submitter>Boro Sitnikovski</submitter><version version="v1"><date>Wed, 27 Jan 2021 11:14:37 GMT</date><size>20kb</size></version><version version="v2"><date>Wed, 17 Feb 2021 15:50:23 GMT</date><size>20kb</size></version><version version="v3"><date>Thu, 4 Mar 2021 23:14:03 GMT</date><size>21kb</size></version><version version="v4"><date>Sun, 25 Apr 2021 10:34:11 GMT</date><size>24kb</size></version><version version="v5"><date>Wed, 28 Apr 2021 20:42:29 GMT</date><size>24kb</size></version><version version="v6"><date>Tue, 4 May 2021 18:19:11 GMT</date><size>24kb</size></version><version version="v7"><date>Tue, 11 May 2021 06:50:03 GMT</date><size>24kb</size></version><version version="v8"><date>Wed, 2 Jun 2021 10:36:34 GMT</date><size>25kb</size></version><title>Tutorial on implementing Hoare logic for imperative programs in Haskell</title><authors>Boro Sitnikovski</authors><categories>cs.PL cs.LO</categories><comments>Added sample implementation for H-Consequence, H-While, and another
  example; Added CoI section, tweaks to labels for 'boptimize'; Improved Hoare
  logic implementation by relying on actual Propositional calculus and Number
  theory systems, rather than toy optimization functions; improve formula
  printer; small tweak updates. Associated files are available at
  https://github.com/bor0/hoare-imp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the programming language Haskell, we introduce an implementation of
propositional calculus, number theory, and a simple imperative language that
can evaluate arithmetic and boolean expressions. Finally, we provide an
implementation of Hoare's logic which will allow us to deduce facts about
programs without the need for a full evaluation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.11665</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.11665</id><submitter>Sebastian Farquhar</submitter><version version="v1"><date>Wed, 27 Jan 2021 19:52:24 GMT</date><size>22675kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 11:46:48 GMT</date><size>22674kb</size><source_type>D</source_type></version><title>On Statistical Bias In Active Learning: How and When To Fix It</title><authors>Sebastian Farquhar, Yarin Gal, Tom Rainforth</authors><categories>stat.ML cs.LG</categories><comments>Published at ICLR 2021 (Spotlight)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learning is a powerful tool when labelling data is expensive, but it
introduces a bias because the training data no longer follows the population
distribution. We formalize this bias and investigate the situations in which it
can be harmful and sometimes even helpful. We further introduce novel
corrective weights to remove bias when doing so is beneficial. Through this,
our work not only provides a useful mechanism that can improve the active
learning approach, but also an explanation of the empirical successes of
various existing approaches which ignore this bias. In particular, we show that
this bias can be actively helpful when training overparameterized models --
like neural networks -- with relatively little data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.11861</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.11861</id><submitter>Masahiko Ueda</submitter><version version="v1"><date>Thu, 28 Jan 2021 08:12:23 GMT</date><size>7kb</size></version><version version="v2"><date>Tue, 9 Feb 2021 08:05:11 GMT</date><size>91kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 21 May 2021 02:45:04 GMT</date><size>202kb</size></version><title>Symmetric equilibrium of multi-agent reinforcement learning in repeated
  prisoner's dilemma</title><authors>Yuki Usui, Masahiko Ueda</authors><categories>cs.GT physics.soc-ph</categories><comments>29 pages, 6 figures</comments><journal-ref>Appl. Math. Comput. 409, 126370 (2021)</journal-ref><doi>10.1016/j.amc.2021.126370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the repeated prisoner's dilemma game where both players
alternately use reinforcement learning to obtain their optimal memory-one
strategies. We theoretically solve the simultaneous Bellman optimality
equations of reinforcement learning. We find that the Win-stay Lose-shift
strategy, the Grim strategy, and the strategy which always defects can form
symmetric equilibrium of the mutual reinforcement learning process amongst all
deterministic memory-one strategies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.12173</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.12173</id><submitter>Quntao Zhuang</submitter><version version="v1"><date>Thu, 28 Jan 2021 18:27:50 GMT</date><size>2478kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 3 Apr 2021 15:38:37 GMT</date><size>3547kb</size><source_type>D</source_type></version><title>Entanglement-assisted capacity regions and protocol designs for quantum
  multiple-access channels</title><authors>Haowei Shi, Min-Hsiu Hsieh, Saikat Guha, Zheshen Zhang and Quntao
  Zhuang</authors><categories>quant-ph cs.IT math.IT</categories><comments>8+10 pages, 11 figures, accepted by npj Quantum Inf</comments><journal-ref>npj Quantum Inf. 7, 74 (2021)</journal-ref><doi>10.1038/s41534-021-00412-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the entanglement-assisted (EA) classical capacity region of quantum
multiple-access channels with an arbitrary number of senders. As an example, we
consider the bosonic thermal-loss multiple-access channel and solve the
one-shot capacity region enabled by an entanglement source composed of
sender-receiver pairwise two-mode squeezed vacuum states. The EA capacity
region is strictly larger than the capacity region without
entanglement-assistance. With two-mode squeezed vacuum states as the source and
phase modulation as the encoding, we also design practical receiver protocols
to realize the entanglement advantages. Four practical receiver designs, based
on optical parametric amplifiers, are given and analyzed. In the parameter
region of a large noise background, the receivers can enable a simultaneous
rate advantage of 82.0% for each sender. Due to teleportation and superdense
coding, our results for EA classical communication can be directly extended to
EA quantum communication at half of the rates. Our work provides a unique and
practical network communication scenario where entanglement can be beneficial.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.12370</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.12370</id><submitter>Cheuk Ting Li</submitter><version version="v1"><date>Fri, 29 Jan 2021 02:59:07 GMT</date><size>24kb</size></version><version version="v2"><date>Wed, 10 Feb 2021 18:04:29 GMT</date><size>27kb</size></version><version version="v3"><date>Fri, 28 May 2021 18:16:47 GMT</date><size>32kb</size></version><title>An Automated Theorem Proving Framework for Information-Theoretic Results</title><authors>Cheuk Ting Li</authors><categories>cs.IT math.IT</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a versatile automated theorem proving framework which is capable
of automated proofs of outer bounds in network information theory, discovery of
inner bounds in network information theory (in conjunction with the method by
Lee and Chung), simplification of capacity regions involving auxiliary random
variables, deduction of properties of information-theoretic quantities (e.g.
Wyner and G\'acs-K\&quot;orner common information), and discovery of
non-Shannon-type inequalities, under a unified framework. Our method is based
on the linear programming approach for proving Shannon-type information
inequalities studied by Yeung and Zhang, together with a novel pruning method
for searching auxiliary random variables. We introduce the concept of
existential information inequalities, which provides an axiomatic framework for
a wide range of problems in information theory. To demonstrate the use of the
framework, we present a new outer bound for the broadcast channel discovered by
the framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.12733</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.12733</id><submitter>Wei-Lin Wu</submitter><version version="v1"><date>Fri, 29 Jan 2021 18:45:23 GMT</date><size>29kb</size></version><version version="v2"><date>Mon, 31 May 2021 19:49:06 GMT</date><size>33kb</size></version><title>On the Expressive Power of Homomorphism Counts</title><authors>Albert Atserias, Phokion G. Kolaitis, Wei-Lin Wu</authors><categories>math.CO cs.LO</categories><comments>27 pages, 2 figures</comments><acm-class>F.4.1; G.2.2; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical result by Lov\'asz asserts that two graphs $G$ and $H$ are
isomorphic if and only if they have the same left profile, that is, for every
graph $F$, the number of homomorphisms from $F$ to $G$ coincides with the
number of homomorphisms from $F$ to $H$. Dvor{\'{a}}k and later on Dell, Grohe,
and Rattan showed that restrictions of the left profile to a class of graphs
can capture several different relaxations of isomorphism, including equivalence
in counting logics with a fixed number of variables (which contains fractional
isomorphism as a special case) and co-spectrality (i.e., two graphs having the
same characteristic polynomial). On the other side, a result by Chaudhuri and
Vardi asserts that isomorphism is also captured by the right profile, that is,
two graphs $G$ and $H$ are isomorphic if and only if for every graph $F$, the
number of homomorphisms from $G$ to $F$ coincides with the number of
homomorphisms from $H$ to $F$. In this paper, we embark on a study of the
restrictions of the right profile by investigating relaxations of isomorphism
that can or cannot be captured by restricting the right profile to a fixed
class of graphs. Our results unveil striking differences between the expressive
power of the left profile and the right profile. We show that fractional
isomorphism, equivalence in counting logics with a fixed number of variables,
and co-spectrality cannot be captured by restricting the right profile to a
class of graphs. In the opposite direction, we show that chromatic equivalence
cannot be captured by restricting the left profile to a class of graphs, while,
clearly, it can be captured by restricting the right profile to the class of
all cliques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01017</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01017</id><submitter>Yanai Elazar</submitter><version version="v1"><date>Mon, 1 Feb 2021 17:48:42 GMT</date><size>357kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 12:50:17 GMT</date><size>360kb</size><source_type>D</source_type></version><title>Measuring and Improving Consistency in Pretrained Language Models</title><authors>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander,
  Eduard Hovy, Hinrich Sch\&quot;utze, Yoav Goldberg</authors><categories>cs.CL</categories><comments>Accepted to the TACL journal, pre-MIT Press publication version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistency of a model -- that is, the invariance of its behavior under
meaning-preserving alternations in its input -- is a highly desirable property
in natural language processing. In this paper we study the question: Are
Pretrained Language Models (PLMs) consistent with respect to factual knowledge?
To this end, we create ParaRel, a high-quality resource of cloze-style query
English paraphrases. It contains a total of 328 paraphrases for 38 relations.
Using ParaRel, we show that the consistency of all PLMs we experiment with is
poor -- though with high variance between relations. Our analysis of the
representational spaces of PLMs suggests that they have a poor structure and
are currently not suitable for representing knowledge robustly. Finally, we
propose a method for improving model consistency and experimentally demonstrate
its effectiveness.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01189</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01189</id><submitter>Youzhi Luo</submitter><version version="v1"><date>Mon, 1 Feb 2021 21:39:41 GMT</date><size>683kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 16:15:03 GMT</date><size>683kb</size><source_type>D</source_type></version><title>GraphDF: A Discrete Flow Model for Molecular Graph Generation</title><authors>Youzhi Luo, Keqiang Yan, Shuiwang Ji</authors><categories>cs.LG cs.AI</categories><comments>Accepted by ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of molecular graph generation using deep models.
While graphs are discrete, most existing methods use continuous latent
variables, resulting in inaccurate modeling of discrete graph structures. In
this work, we propose GraphDF, a novel discrete latent variable model for
molecular graph generation based on normalizing flow methods. GraphDF uses
invertible modulo shift transforms to map discrete latent variables to graph
nodes and edges. We show that the use of discrete latent variables reduces
computational costs and eliminates the negative effect of dequantization.
Comprehensive experimental results show that GraphDF outperforms prior methods
on random generation, property optimization, and constrained optimization
tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01373</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01373</id><submitter>Wenxuan Zhou</submitter><version version="v1"><date>Tue, 2 Feb 2021 07:57:06 GMT</date><size>35kb</size></version><version version="v2"><date>Tue, 27 Apr 2021 20:02:47 GMT</date><size>35kb</size></version><version version="v3"><date>Wed, 2 Jun 2021 22:38:54 GMT</date><size>38kb</size></version><title>An Improved Baseline for Sentence-level Relation Extraction</title><authors>Wenxuan Zhou, Muhao Chen</authors><categories>cs.CL</categories><comments>Code available at https://github.com/wzhouad/RE_improved_baseline</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentence-level relation extraction (RE) aims at identifying the relationship
between two entities in a sentence. Many efforts have been devoted to this
problem, while the best performing methods are still far from perfect. In this
paper, we revisit two problems that affect the performance of existing RE
models, namely entity representation and noisy or ill-defined labels. Our
improved baseline model, incorporated with entity representations with typed
markers, achieves an F1 of 74.6% on TACRED, significantly outperforms previous
SOTA methods. Furthermore, the presented new baseline achieves an F1 of 91.1%
on the refined Re-TACRED dataset, demonstrating that the pre-trained language
models achieve unexpectedly high performance on this task. We release our code
to the community for future research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01529</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01529</id><submitter>Arthur Parzygnat</submitter><version version="v1"><date>Tue, 2 Feb 2021 14:54:53 GMT</date><size>32kb</size></version><version version="v2"><date>Sun, 30 May 2021 08:49:56 GMT</date><size>33kb</size></version><title>Conditional distributions for quantum systems</title><authors>Arthur J. Parzygnat</authors><categories>quant-ph cs.IT math.CT math.IT math.OA</categories><comments>11 pages + bibliography, expanded some results and examples</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conditional distributions, as defined by the Markov category framework, are
studied in the setting of matrix algebras (quantum systems). Their construction
as linear unital maps are obtained via a categorical Bayesian inversion
procedure. Simple criteria establishing when such linear maps are positive are
obtained. Several examples are provided, including the standard EPR scenario,
where the EPR correlations are reproduced in a purely compositional
(categorical) manner. A comparison between the Bayes map, the Petz recovery
map, and the Leifer--Spekkens acausal belief propagation is provided,
illustrating some similarities and key differences.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01547</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01547</id><submitter>Binbin Zhang</submitter><version version="v1"><date>Tue, 2 Feb 2021 15:19:41 GMT</date><size>464kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 03:49:11 GMT</date><size>728kb</size><source_type>D</source_type></version><title>WeNet: Production oriented Streaming and Non-streaming End-to-End Speech
  Recognition Toolkit</title><authors>Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang,
  Zhendong Peng, Xiaoyu Chen, Lei Xie, Xin Lei</authors><categories>cs.SD cs.CL eess.AS</categories><comments>5 pages, 2 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose an open source, production first, and production
ready speech recognition toolkit called WeNet in which a new two-pass approach
is implemented to unify streaming and non-streaming end-to-end (E2E) speech
recognition in a single model. The main motivation of WeNet is to close the gap
between the research and the production of E2E speechrecognition models. WeNet
provides an efficient way to ship ASR applications in several real-world
scenarios, which is the main difference and advantage to other open source E2E
speech recognition toolkits. In our toolkit, a new two-pass method is
implemented. Our method propose a dynamic chunk-based attention strategy of the
the transformer layers to allow arbitrary right context length modifies in
hybrid CTC/attention architecture. The inference latency could be easily
controlled by only changing the chunk size. The CTC hypotheses are then
rescored by the attention decoder to get the final result. Our experiments on
the AISHELL-1 dataset using WeNet show that, our model achieves 5.03\% relative
character error rate (CER) reduction in non-streaming ASR compared to a
standard non-streaming transformer. After model quantification, our model
perform reasonable RTF and latency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01828</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01828</id><submitter>Xiao-Shan Gao</submitter><version version="v1"><date>Wed, 3 Feb 2021 01:24:22 GMT</date><size>181kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 06:36:20 GMT</date><size>174kb</size><source_type>D</source_type></version><title>Analyzing the barren plateau phenomenon in training quantum neural
  networks with the ZX-calculus</title><authors>Chen Zhao and Xiao-Shan Gao</authors><categories>quant-ph cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a general scheme to analyze the gradient vanishing
phenomenon, also known as the barren plateau phenomenon, in training quantum
neural networks with the ZX-calculus. More precisely, we extend the barren
plateaus theorem from unitary 2-design circuits to any parameterized quantum
circuits under certain reasonable assumptions. The main technical contribution
of this paper is representing certain integrations as ZX-diagrams and computing
them with the ZX-calculus. The method is used to analyze four concrete quantum
neural networks with different structures. It is shown that, for the hardware
efficient ansatz and the MPS-inspired ansatz, there exist barren plateaus,
while for the QCNN ansatz and the tree tensor network ansatz, there exists no
barren plateau.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02494</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02494</id><submitter>Yingkai Ouyang</submitter><version version="v1"><date>Thu, 4 Feb 2021 09:12:35 GMT</date><size>95kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 12:52:58 GMT</date><size>93kb</size><source_type>D</source_type></version><title>Permutation-invariant quantum coding for quantum deletion channels</title><authors>Yingkai Ouyang</authors><categories>quant-ph cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum deletions, which are harder to correct than erasure errors, occur in
many realistic settings. It is therefore pertinent to develop quantum coding
schemes for quantum deletion channels. To date, not much is known about which
explicit quantum error correction codes can combat quantum deletions. We note
that {\em any} permutation-invariant quantum code that has a distance of $t+1$
can correct $t$ quantum deletions for any positive integer $t$ in both the
qubit and the qudit setting. Leveraging on coding properties of
permutation-invariant quantum codes under erasure errors, we derive
corresponding coding bounds for permutation-invariant quantum codes under
quantum deletions. We focus our attention on a specific family of $N$-qubit
permutation-invariant quantum codes, which we call shifted gnu codes, and show
that their encoding and decoding algorithms can be performed in $O(N)$ and
$O(N^2)$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02504</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02504</id><submitter>Pierre Alquier</submitter><version version="v1"><date>Thu, 4 Feb 2021 09:32:22 GMT</date><size>227kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 09:03:58 GMT</date><size>228kb</size><source_type>D</source_type></version><title>Meta-strategy for Learning Tuning Parameters with Guarantees</title><authors>Dimitri Meunier and Pierre Alquier</authors><categories>stat.ML cs.LG math.ST stat.CO stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online gradient methods, like the online gradient algorithm (OGA), often
depend on tuning parameters that are difficult to set in practice. We consider
an online meta-learning scenario, and we propose a meta-strategy to learn these
parameters from past tasks. Our strategy is based on the minimization of a
regret bound. It allows to learn the initialization and the step size in OGA
with guarantees. We provide a regret analysis of the strategy in the case of
convex losses. It suggests that, when there are parameters
$\theta_1,\dots,\theta_T$ solving well tasks $1,\dots,T$ respectively and that
are close enough one to each other, our strategy indeed improves on learning
each task in isolation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02727</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02727</id><submitter>Lorenz Welter</submitter><version version="v1"><date>Thu, 4 Feb 2021 16:39:56 GMT</date><size>111kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 08:12:49 GMT</date><size>126kb</size></version><title>Multiple Criss-Cross Insertion and Deletion Correcting Codes</title><authors>Lorenz Welter, Rawad Bitar, Antonia Wachter-Zeh, Eitan Yaakobi</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of correcting multiple criss-cross
insertions and deletions in arrays. More precisely, we study the unique
recovery of $n \times n$ arrays affected by $t$-criss-cross deletions defined
as any combination of $t_r$ row and $t_c$ column deletions such that $t_r + t_c
= t$ for a given $t$. We show an equivalence between correcting $t$-criss-cross
deletions and $t$-criss-cross insertions and show that a code correcting
$t$-criss-cross insertions/deletions has redundancy at least $tn + t \log n -
\log(t!)$. Then, we present an existential construction of $t$-criss-cross
insertion/deletion correcting code with redundancy bounded from above by $tn +
\mathcal{O}(t^2 \log^2 n)$. The main ingredients of the presented code
construction are systematic binary $t$-deletion correcting codes and Gabidulin
codes. The first ingredient helps locating the indices of the inserted/deleted
rows and columns, thus transforming the insertion/deletion-correction problem
into a row/column erasure-correction problem which is then solved using the
second ingredient.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02959</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02959</id><submitter>Fakrul Islam Tushar</submitter><version version="v1"><date>Fri, 5 Feb 2021 02:07:39 GMT</date><size>1611kb</size></version><version version="v2"><date>Thu, 25 Feb 2021 03:12:11 GMT</date><size>2003kb</size></version><version version="v3"><date>Sun, 30 May 2021 09:37:37 GMT</date><size>2003kb</size></version><title>Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text
  Reports Using Deep Learning</title><authors>Vincent M. D'Anniballe, Fakrul I. Tushar, Khrystyna Faryna, Songyue
  Han, Maciej A. Mazurowski, Geoffrey D. Rubin, Joseph Y. Lo</authors><categories>cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Purpose: To develop high throughput multi-label annotators for body (chest,
abdomen, and pelvis) Computed Tomography (CT) reports that can be applied
across a variety of abnormalities, organs, and disease states.
  Approach: We used a dictionary approach to develop rule-based algorithms
(RBA) for extraction of disease labels from radiology text reports. We targeted
three organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with
four diseases per system based on their prevalence in our dataset. To expand
the algorithms beyond pre-defined keywords, attention-guided recurrent neural
networks (RNN) were trained using the RBA-extracted labels to classify reports
as being positive for one or more diseases or normal for each organ system.
Confounding effects on model performance were evaluated using random
initialization or pre-trained embedding as well as different sizes of training
datasets. Performance was evaluated using the receiver operating characteristic
(ROC) area under the curve (AUC) against 2,158 manually obtained labels.
  Results: Our models extracted disease labels from 261,229 radiology reports
of 112,501 unique subjects. Pre-trained models outperformed random
initialization across all diseases. As the training dataset size was reduced,
performance was robust except for a few diseases with relatively small number
of cases. Pre-trained classification AUCs achieved &gt; 0.95 for all five disease
outcomes across all three organ systems.
  Conclusions: Our label-extracting pipeline was able to encompass a variety of
cases and diseases by generalizing beyond strict rules with exceptional
accuracy. This method can be easily adapted to enable automated labeling of
hospital-scale medical data sets for training image-based disease classifiers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02992</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02992</id><submitter>Shaojun Ma</submitter><version version="v1"><date>Fri, 5 Feb 2021 04:25:28 GMT</date><size>21862kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 8 Feb 2021 01:33:56 GMT</date><size>21862kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 22 Feb 2021 04:11:54 GMT</date><size>21862kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 30 May 2021 05:18:00 GMT</date><size>25584kb</size><source_type>D</source_type></version><title>Learning High Dimensional Wasserstein Geodesics</title><authors>Shu Liu, Shaojun Ma, Yongxin Chen, Hongyuan Zha, Haomin Zhou</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new formulation and learning strategy for computing the
Wasserstein geodesic between two probability distributions in high dimensions.
By applying the method of Lagrange multipliers to the dynamic formulation of
the optimal transport (OT) problem, we derive a minimax problem whose saddle
point is the Wasserstein geodesic. We then parametrize the functions by deep
neural networks and design a sample based bidirectional learning algorithm for
training. The trained networks enable sampling from the Wasserstein geodesic.
As by-products, the algorithm also computes the Wasserstein distance and OT map
between the marginal distributions. We demonstrate the performance of our
algorithms through a series of experiments with both synthetic and realistic
data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03183</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03183</id><submitter>Loucas Pillaud-Vivien</submitter><version version="v1"><date>Fri, 5 Feb 2021 14:02:20 GMT</date><size>76kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 18:02:06 GMT</date><size>81kb</size></version><title>Last iterate convergence of SGD for Least-Squares in the Interpolation
  regime</title><authors>Aditya Varre, Loucas Pillaud-Vivien, Nicolas Flammarion</authors><categories>cs.LG math.OC stat.ML</categories><comments>23 pages, 1 figure, 1 Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent successes of neural networks that have the ability to
fit the data perfectly and generalize well, we study the noiseless model in the
fundamental least-squares setup. We assume that an optimum predictor fits
perfectly inputs and outputs $\langle \theta_* , \phi(X) \rangle = Y$, where
$\phi(X)$ stands for a possibly infinite dimensional non-linear feature map. To
solve this problem, we consider the estimator given by the last iterate of
stochastic gradient descent (SGD) with constant step-size. In this context, our
contribution is two fold: (i) from a (stochastic) optimization perspective, we
exhibit an archetypal problem where we can show explicitly the convergence of
SGD final iterate for a non-strongly convex problem with constant step-size
whereas usual results use some form of average and (ii) from a statistical
perspective, we give explicit non-asymptotic convergence rates in the
over-parameterized setting and leverage a fine-grained parameterization of the
problem to exhibit polynomial rates that can be faster than $O(1/T)$. The link
with reproducing kernel Hilbert spaces is established.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03322</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03322</id><submitter>Ana Lucic</submitter><version version="v1"><date>Fri, 5 Feb 2021 17:58:14 GMT</date><size>333kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 15:05:08 GMT</date><size>334kb</size><source_type>D</source_type></version><title>CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks</title><authors>Ana Lucic, Maartje ter Hoeve, Gabriele Tolomei, Maarten de Rijke,
  Fabrizio Silvestri</authors><categories>cs.LG cs.AI</categories><comments>Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the increasing promise of Graph Neural Networks (GNNs) in real-world
applications, several methods have been developed for explaining their
predictions. So far, these methods have primarily focused on generating
subgraphs that are especially relevant for a particular prediction. However,
such methods do not provide a clear opportunity for recourse: given a
prediction, we want to understand how the prediction can be changed in order to
achieve a more desirable outcome. In this work, we propose a method for
generating counterfactual (CF) explanations for GNNs: the minimal perturbation
to the input (graph) data such that the prediction changes. Using only edge
deletions, we find that our method, CF-GNNExplainer can generate CF
explanations for the majority of instances across three widely used datasets
for GNN explanations, while removing less than 3 edges on average, with at
least 94\% accuracy. This indicates that CF-GNNExplainer primarily removes
edges that are crucial for the original predictions, resulting in minimal CF
explanations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03347</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03347</id><submitter>Christof Ferreira Torres</submitter><version version="v1"><date>Fri, 5 Feb 2021 18:49:50 GMT</date><size>432kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 12:37:58 GMT</date><size>4380kb</size><source_type>D</source_type></version><title>Frontrunner Jones and the Raiders of the Dark Forest: An Empirical Study
  of Frontrunning on the Ethereum Blockchain</title><authors>Christof Ferreira Torres, Ramiro Camino, Radu State</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ethereum prospered the inception of a plethora of smart contract
applications, ranging from gambling games to decentralized finance. However,
Ethereum is also considered a highly adversarial environment, where vulnerable
smart contracts will eventually be exploited. Recently, Ethereum's pool of
pending transaction has become a far more aggressive environment. In the hope
of making some profit, attackers continuously monitor the transaction pool and
try to frontrun their victims' transactions by either displacing or suppressing
them, or strategically inserting their transactions. This paper aims to shed
some light into what is known as a dark forest and uncover these predators'
actions. We present a methodology to efficiently measure the three types of
frontrunning: displacement, insertion, and suppression. We perform a
large-scale analysis on more than 11M blocks and identify almost 200K attacks
with an accumulated profit of 18.41M USD for the attackers, providing evidence
that frontrunning is both, lucrative and a prevalent issue.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03567</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03567</id><submitter>Yan Dong</submitter><version version="v1"><date>Sat, 6 Feb 2021 11:42:18 GMT</date><size>346kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 06:11:08 GMT</date><size>154kb</size><source_type>D</source_type></version><title>Standard and Event Cameras Fusion for Dense Mapping</title><authors>Yan Dong</authors><categories>cs.RO</categories><comments>4 pages, 2 figures, 6 references</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Event cameras are a kind of bio-inspired sensors that generate data when the
brightness changes, which are of low-latency and high dynamic range (HDR).
However, due to the nature of the sparse event stream, event-based mapping can
only obtain sparse or semi-dense edge 3D maps. By contrast, standard cameras
provide complete frames. To leverage the complementarity of event-based and
standard frame-based cameras, we propose a fusion strategy for dense mapping in
this paper. We first generate an edge map from events, and then fill the map
using frames to obtain the dense depth map. We propose &quot;filling score&quot; to
evaluate the quality of filled results and show that our strategy can increase
the number of existing semi-dense 3D map.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03763</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03763</id><submitter>Andrea Iannelli</submitter><version version="v1"><date>Sun, 7 Feb 2021 10:12:31 GMT</date><size>1728kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 10:35:48 GMT</date><size>1734kb</size><source_type>D</source_type></version><title>The Balanced Mode Decomposition Algorithm for Data-Driven LPV Low-Order
  Models of Aeroservoelastic Systems</title><authors>Andrea Iannelli, Urban Fasel, Roy S. Smith</authors><categories>eess.SY cs.SY</categories><comments>Accepted for publication in Aerospace Science and Technology
  (open-access version available)</comments><journal-ref>Aerospace Science and Technology, Volume 115, August 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A novel approach to reduced-order modeling of high-dimensional time varying
systems is proposed. It leverages the formalism of the Dynamic Mode
Decomposition technique together with the concept of balanced realization. It
is assumed that the only information available on the system comes from input,
state, and output trajectories generated by numerical simulations or recorded
and estimated during experiments, thus the approach is fully data-driven. The
goal is to obtain an input-output low dimensional linear model which
approximates the system across its operating range. Since the dynamics of
aeroservoelastic systems markedly changes in operation (e.g. due to change in
flight speed or altitude), time-varying features are retained in the
constructed models. This is achieved by generating a Linear Parameter-Varying
representation made of a collection of state-consistent linear time-invariant
reduced-order models. The algorithm formulation hinges on the idea of replacing
the orthogonal projection onto the Proper Orthogonal Decomposition modes, used
in Dynamic Mode Decomposition-based approaches, with a balancing oblique
projection constructed entirely from data. As a consequence, the input-output
information captured in the lower-dimensional representation is increased
compared to other projections onto subspaces of same or lower size. Moreover, a
parameter-varying projection is possible while also achieving
state-consistency. The validity of the proposed approach is demonstrated on a
morphing wing for airborne wind energy applications by comparing the
performance against two algorithms recently proposed in the literature.
Comparisons cover both prediction accuracy and performance in model predictive
control applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03765</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03765</id><submitter>Theodore Moskovitz</submitter><version version="v1"><date>Sun, 7 Feb 2021 10:28:09 GMT</date><size>1940kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 9 Feb 2021 09:29:55 GMT</date><size>1940kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 20:42:19 GMT</date><size>9070kb</size><source_type>D</source_type></version><title>Tactical Optimism and Pessimism for Deep Reinforcement Learning</title><authors>Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel,
  Michael I. Jordan</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, deep off-policy actor-critic algorithms have become a
dominant approach to reinforcement learning for continuous control. One of the
primary drivers of this improved performance is the use of pessimistic value
updates to address function approximation errors, which previously led to
disappointing performance. However, a direct consequence of pessimism is
reduced exploration, running counter to theoretical support for the efficacy of
optimism in the face of uncertainty. So which approach is best? In this work,
we show that the most effective degree of optimism can vary both across tasks
and over the course of learning. Inspired by this insight, we introduce a novel
deep actor-critic framework, Tactical Optimistic and Pessimistic (TOP)
estimation, which switches between optimistic and pessimistic value learning
online. This is achieved by formulating the selection as a multi-arm bandit
problem. We show in a series of continuous control tasks that TOP outperforms
existing methods which rely on a fixed degree of optimism, setting a new state
of the art in challenging pixel-based environments. Since our changes are
simple to implement, we believe these insights can easily be incorporated into
a multitude of off-policy algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03888</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03888</id><submitter>Fengyang Sun</submitter><version version="v1"><date>Sun, 7 Feb 2021 19:12:09 GMT</date><size>4773kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 09:46:24 GMT</date><size>22219kb</size><source_type>D</source_type></version><title>OPT-GAN: Global Black-box Optimization by Learning Distribution of
  Optima</title><authors>Minfang Lu, Fengyang Sun, Lin Wang, Shuai Ning, Shuangrong Liu, Bo
  Yang</authors><categories>cs.LG cs.NE</categories><comments>12 pages, 6 figures, 2 tables, M. Lu and F. Sun contribute equally</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Black-box optimization (BBO) algorithms are concerned with finding the best
solutions for problems with missing analytical details. Most classical methods
for such problems are based on strong and fixed \emph{a priori} assumptions,
such as Gaussian distribution. However, the complex real-world problems,
especially when the global optimum is desired, could be very far from the
\emph{a priori} assumptions because of their diversities, bringing some
unexpected obstacles to these methods. In this paper, we present an optimizer
using generative adversarial nets (OPT-GAN) to adapt to diverse black-box
problems via estimating the distribution of optima. The method learns the
extensive distribution of the optimal region dominated by selective and
randomly moving candidates, balancing the exploration and exploitation.
Experiments demonstrate that on BBOB problems and several other benchmarks with
atypical distributions, OPT-GAN outperforms other classical BBO algorithms, in
particular the ones with Gaussian assumptions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03973</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03973</id><submitter>Xin Zhao</submitter><version version="v1"><date>Mon, 8 Feb 2021 02:51:34 GMT</date><size>30250kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 18 Mar 2021 11:29:46 GMT</date><size>32620kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 16:12:31 GMT</date><size>36550kb</size><source_type>D</source_type></version><title>Solid Texture Synthesis using Generative Adversarial Networks</title><authors>Xin Zhao, Jifeng Guo, Lin Wang, Fanqi Li, Junteng Zheng and Bo Yang</authors><categories>cs.CV cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solid texture synthesis, as an effective way to extend 2D exemplar to a
volumetric texture, exhibits advantages in numerous application domains.
However, existing methods generally suffer from synthesis distortion due to the
under-utilization of information. In this paper, we propose a novel approach
for the solid texture synthesis based on generative adversarial networks(GANs),
named STS-GAN, learning the distribution of 2D exemplars with volumetric
operation in a feature-free manner. The multi-scale discriminators evaluate the
similarities between patch exemplars and slices from generated volume,
promoting the generator to synthesize realistic solid texture. Experimental
results demonstrate that the proposed method can synthesize high-quality solid
texture with similar visual characteristics to the exemplar.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.04776</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.04776</id><submitter>Emilien Dupont</submitter><version version="v1"><date>Tue, 9 Feb 2021 11:47:55 GMT</date><size>8458kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 18:04:08 GMT</date><size>9700kb</size><source_type>D</source_type></version><title>Generative Models as Distributions of Functions</title><authors>Emilien Dupont, Yee Whye Teh, Arnaud Doucet</authors><categories>cs.LG cs.CV stat.ML</categories><comments>Added point clouds experiments, quantitative evaluations and link to
  github repo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models are typically trained on grid-like data such as images. As
a result, the size of these models usually scales directly with the underlying
grid resolution. In this paper, we abandon discretized grids and instead
parameterize individual data points by continuous functions. We then build
generative models by learning distributions over such functions. By treating
data points as functions, we can abstract away from the specific type of data
we train on and construct models that scale independently of signal resolution.
To train our model, we use an adversarial approach with a discriminator that
acts on continuous signals. Through experiments on both images and 3D shapes,
we demonstrate that our model can learn rich distributions of functions
independently of data type and resolution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05111</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05111</id><submitter>Miaomiao Wang</submitter><version version="v1"><date>Tue, 9 Feb 2021 20:22:11 GMT</date><size>2558kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 17:44:41 GMT</date><size>2578kb</size><source_type>D</source_type></version><title>Nonlinear Observers Design for Vision-Aided Inertial Navigation Systems</title><authors>Miaomiao Wang, Soulaimane Berkane, and Abdelhamid Tayebi</authors><categories>math.OC cs.RO cs.SY eess.SY</categories><comments>Accepted for publication in IEEE Transaction on Automatic Control. 16
  pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the simultaneous estimation of the attitude, position
and linear velocity for vision-aided inertial navigation systems. We propose a
nonlinear observer on $SO(3)\times \mathbb{R}^{15}$ relying on body-frame
acceleration, angular velocity and (stereo or monocular) bearing measurements
of some landmarks that are constant and known in the inertial frame. Unlike the
existing local Kalman-type observers, our proposed nonlinear observer
guarantees almost global asymptotic stability and local exponential stability.
A detailed uniform observability analysis has been conducted and sufficient
conditions are derived. Moreover, a hybrid version of the proposed observer is
provided to handle the intermittent nature of the measurements in practical
applications. Simulation and experimental results are provided to illustrate
the effectiveness of the proposed state observer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05152</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05152</id><submitter>Hao Yuan</submitter><version version="v1"><date>Tue, 9 Feb 2021 22:12:26 GMT</date><size>3189kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 21:16:36 GMT</date><size>3199kb</size><source_type>D</source_type></version><title>On Explainability of Graph Neural Networks via Subgraph Explorations</title><authors>Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, Shuiwang Ji</authors><categories>cs.LG cs.AI</categories><comments>Accepted by ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of explaining the predictions of graph neural
networks (GNNs), which otherwise are considered as black boxes. Existing
methods invariably focus on explaining the importance of graph nodes or edges
but ignore the substructures of graphs, which are more intuitive and
human-intelligible. In this work, we propose a novel method, known as
SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained
GNN model and an input graph, our SubgraphX explains its predictions by
efficiently exploring different subgraphs with Monte Carlo tree search. To make
the tree search more effective, we propose to use Shapley values as a measure
of subgraph importance, which can also capture the interactions among different
subgraphs. To expedite computations, we propose efficient approximation schemes
to compute Shapley values for graph data. Our work represents the first attempt
to explain GNNs via identifying subgraphs explicitly and directly. Experimental
results show that our SubgraphX achieves significantly improved explanations,
while keeping computations at a reasonable level.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05221</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05221</id><submitter>Matthieu Herrmann</submitter><version version="v1"><date>Wed, 10 Feb 2021 02:13:27 GMT</date><size>178kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 01:17:11 GMT</date><size>107kb</size><source_type>D</source_type></version><title>Early Abandoning and Pruning for Elastic Distances including Dynamic
  Time Warping</title><authors>Matthieu Herrmann and Geoffrey I. Webb</authors><categories>cs.LG</categories><comments>Updated taking reviewers' comments into account</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Nearest neighbor search under elastic distances is a key tool for time series
analysis, supporting many applications. However, straightforward
implementations of distances require $O(n^2)$ space and time complexities,
preventing these applications from scaling to long series. Much work has been
devoted to speeding up the NN search process, mostly with the development of
lower bounds, allowing to avoid costly distance computations when a given
threshold is exceeded. This threshold, provided by the similarity search
process, also allows to early abandon the computation of a distance itself.
Another approach, is to prune parts of the computation. All these techniques
are othogonal to each other. In this work, we develop a new generic strategy,
&quot;EAPruned&quot;, that tightly integrates pruning with early abandoning. We apply it
to six elastic distance measures: DTW, CDTW, WDTW, ERP, MSM and TWE, showing
substantial speedup in NN search applications. Pruning alone also shows
substantial speedup for some distances, benefiting applications beyond the
scope of NN search (e.g. requiring all pairwise distances), and hence where
early abandoning is not applicable. We~release our implementation as part of a
new C++ library for time series classification, along with easy to use
Python/Numpy bindings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05378</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05378</id><submitter>Fan Feng</submitter><version version="v1"><date>Wed, 10 Feb 2021 11:04:09 GMT</date><size>8941kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 13:46:14 GMT</date><size>5538kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 10:38:55 GMT</date><size>12920kb</size></version><title>Origami spring-inspired shape morphing for flexible robotics</title><authors>Qianying Chen, Fan Feng, Pengyu Lv, and Huiling Duan</authors><categories>cs.RO cond-mat.soft</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Flexible robotics are capable of achieving various functionalities by shape
morphing, benefiting from their compliant bodies and reconfigurable structures.
Here we construct and study a class of origami springs generalized from the
known interleaved origami spring, as promising candidates for shape morphing in
flexible robotics. These springs are found to exhibit nonlinear stretch-twist
coupling and linear/nonlinear mechanical response in the compression/tension
region, analyzed by the demonstrated continuum mechanics models, experiments,
and finite element simulations. To improve the mechanical performance such as
the damage resistance, we establish an origami rigidization method by adding
additional creases to the spring system. Guided by the theoretical framework,
we experimentally realize three types of flexible robotics -- origami spring
ejectors, crawlers, and transformers. These robots show the desired
functionality and outstanding mechanical performance. The proposed concept of
origami-aided design is expected to pave the way to facilitate the diverse
shape morphing of flexible robotics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05431</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05431</id><submitter>Thorsten Eisenhofer</submitter><version version="v1"><date>Wed, 10 Feb 2021 13:53:32 GMT</date><size>2761kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 12:46:05 GMT</date><size>4031kb</size><source_type>D</source_type></version><title>Dompteur: Taming Audio Adversarial Examples</title><authors>Thorsten Eisenhofer, Lea Sch\&quot;onherr, Joel Frank, Lars Speckemeier,
  Dorothea Kolossa, Thorsten Holz</authors><categories>cs.CR cs.LG cs.SD</categories><comments>Accepted at USENIX Security Symposium 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial examples seem to be inevitable. These specifically crafted inputs
allow attackers to arbitrarily manipulate machine learning systems. Even worse,
they often seem harmless to human observers. In our digital society, this poses
a significant threat. For example, Automatic Speech Recognition (ASR) systems,
which serve as hands-free interfaces to many kinds of systems, can be attacked
with inputs incomprehensible for human listeners. The research community has
unsuccessfully tried several approaches to tackle this problem. In this paper
we propose a different perspective: We accept the presence of adversarial
examples against ASR systems, but we require them to be perceivable by human
listeners. By applying the principles of psychoacoustics, we can remove
semantically irrelevant information from the ASR input and train a model that
resembles human perception more closely. We implement our idea in a tool named
DOMPTEUR and demonstrate that our augmented system, in contrast to an
unmodified baseline, successfully focuses on perceptible ranges of the input
signal. This change forces adversarial examples into the audible range, while
using minimal computational overhead and preserving benign performance. To
evaluate our approach, we construct an adaptive attacker that actively tries to
avoid our augmentations and demonstrate that adversarial examples from this
attacker remain clearly perceivable. Finally, we substantiate our claims by
performing a hearing test with crowd-sourced human listeners.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05441</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05441</id><submitter>Lei Liu</submitter><version version="v1"><date>Wed, 10 Feb 2021 14:02:21 GMT</date><size>637kb</size><source_type>D</source_type></version><title>Capacity Optimality of AMP in Coded Systems</title><authors>Lei Liu, Chulong Liang, Junjie Ma, and Li Ping</authors><categories>cs.IT math.IT</categories><comments>submitted to ISIT2021. arXiv admin note: text overlap with
  arXiv:1901.09559</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper studies a large random matrix system (LRMS) model involving an
arbitrary signal distribution and forward error control (FEC) coding. We
establish an area property based on the so-called Turbo approximate message
passing (Turbo-AMP) algorithm. Under the assumption that the state evolution
for AMP is correct for the coded system, the achievable rate of Turbo-AMP is
analyzed. We prove that Turbo-AMP achieves the constraint capacity of the LRMS
with an arbitrary signal distribution provided that a matching condition is
satisfied. We provide related numerical results of binary signaling using
irregular low-density parity-check (LDPC) codes. We show that optimized codes
demonstrate significantly better performance over un-matched ones under
Turbo-AMP. For quadrature phase shift keying (QPSK) modulation, bit error rate
(BER) performance within 1 dB from the constrained capacity limit is observed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05504</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05504</id><submitter>Joaquim Silva</submitter><version version="v1"><date>Wed, 10 Feb 2021 15:47:25 GMT</date><size>1952kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 10:05:40 GMT</date><size>5969kb</size><source_type>D</source_type></version><title>Energy-Aware Adaptive Offloading of Soft Real-Time Jobs in Mobile Edge
  Clouds</title><authors>Joaquim Silva, Eduardo R.B. Marques, Lu\'is M.B Lopes, Fernando Silva</authors><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present a model for measuring the impact of offloading soft real-time jobs
over multi-tier cloud infrastructures. The jobs originate in mobile devices and
offloading strategies may choose to execute them locally, in neighbouring
devices, in cloudlets or in infrastructure cloud servers. Within this
specification, we put forward several such offloading strategies characterised
by their differential use of the cloud tiers with the goal of optimizing
execution time and/or energy consumption. We implement an instance of the model
using Jay, a software framework for adaptive computation offloading in hybrid
edge clouds. The framework is modular and allows the model and the offloading
strategies to be seamlessly implemented while providing the tools to make
informed runtime offloading decisions based on system feedback, namely through
a built-in system profiler that gathers runtime information such as workload,
energy consumption and available bandwidth for every participating device or
server. The results show that offloading strategies sensitive to runtime
conditions can effectively and dynamically adjust their offloading decisions to
produce significant gains in terms of their target optimization functions,
namely, execution time, energy consumption and fulfillment of job deadlines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05591</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05591</id><submitter>Sotiris Tegos</submitter><version version="v1"><date>Wed, 10 Feb 2021 17:41:16 GMT</date><size>242kb</size></version><version version="v2"><date>Sat, 29 May 2021 11:38:17 GMT</date><size>250kb</size></version><title>On the Distribution of the Sum of Double-Nakagami-m Random Vectors and
  Application in Randomly Reconfigurable Surfaces</title><authors>Sotiris A. Tegos, Dimitrios Tyrovolas, Panagiotis D. Diamantoulakis,
  and George K. Karagiannidis</authors><categories>cs.IT eess.SP math.IT stat.AP</categories><comments>10 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reconfigurable intelligent surfaces (RISs) intend to improve significantly
the performance of future wireless networks, by controlling the wireless
propagation medium through elements that can shift the phase of the reflected
signals. Although ideally the signals reflected from a RIS are added coherently
at the receiver, this is very challenging in practice due to the requirement
for perfect channel state information (CSI) at the RIS and phase control. To
facilitate the performance analysis of more practical RIS-assisted systems,
first, we present novel closed-form expressions for the probability density
function, the cumulative distribution function, the moments, and the
characteristic function of the distribution of the sum of double-Nakagami-m
random vectors, whose amplitudes follow the double-Nakagami-m distribution,
i.e., the distribution of the product of two random variables following the
Nakagami-m distribution, and phases are circular uniformly distributed. We also
consider a special case of this distribution, namely the distribution of the
sum of Rayleigh-Nakagami-m random vectors. Then, we exploit these expressions
to investigate the performance of the RIS-assisted composite channel, assuming
that the two links undergo Nakagami-m fading and the equivalent phase follows
the uniform distribution, which corresponds to the case where CSI is not
available at the RIS and leads to a lower bound of the performance of a system
with CSI. Closed-form expressions for the outage probability, the average
received signal-to-noise ratio, the ergodic capacity, the bit error
probability, the amount of fading, and the channel quality estimation index are
provided to evaluate the performance of the considered system. These metrics
are also derived for the practical special case where one of the two links
undergoes Rayleigh fading.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05624</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05624</id><submitter>Kai Chen</submitter><version version="v1"><date>Wed, 10 Feb 2021 18:36:11 GMT</date><size>1534kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 07:47:53 GMT</date><size>0kb</size><source_type>I</source_type></version><title>NAST: Non-Autoregressive Spatial-Temporal Transformer for Time Series
  Forecasting</title><authors>Kai Chen, Guang Chen, Dan Xu, Lijun Zhang, Yuyao Huang, Alois Knoll</authors><categories>cs.LG stat.ML</categories><comments>Not a satisfying work and need to be reformed</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Although Transformer has made breakthrough success in widespread domains
especially in Natural Language Processing (NLP), applying it to time series
forecasting is still a great challenge. In time series forecasting, the
autoregressive decoding of canonical Transformer models could introduce huge
accumulative errors inevitably. Besides, utilizing Transformer to deal with
spatial-temporal dependencies in the problem still faces tough difficulties.~To
tackle these limitations, this work is the first attempt to propose a
Non-Autoregressive Transformer architecture for time series forecasting, aiming
at overcoming the time delay and accumulative error issues in the canonical
Transformer. Moreover, we present a novel spatial-temporal attention mechanism,
building a bridge by a learned temporal influence map to fill the gaps between
the spatial and temporal attention, so that spatial and temporal dependencies
can be processed integrally. Empirically, we evaluate our model on diversified
ego-centric future localization datasets and demonstrate state-of-the-art
performance on both real-time and accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06125</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06125</id><submitter>Dong Si</submitter><version version="v1"><date>Thu, 11 Feb 2021 17:06:20 GMT</date><size>775kb</size></version><version version="v2"><date>Wed, 24 Feb 2021 02:03:01 GMT</date><size>731kb</size></version><title>Artificial Intelligence Advances for De Novo Molecular Structure
  Modeling in Cryo-EM</title><authors>Dong Si, Andrew Nakamura, Runbang Tang, Haowen Guan, Jie Hou, Ammaar
  Firozi, Renzhi Cao, Kyle Hippe, Minglei Zhao</authors><categories>q-bio.BM cs.AI physics.bio-ph physics.comp-ph</categories><journal-ref>Wiley Interdisciplinary Reviews: Computational Molecular Science,
  e1542 (2021)</journal-ref><doi>10.1002/wcms.1542</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Cryo-electron microscopy (cryo-EM) has become a major experimental technique
to determine the structures of large protein complexes and molecular
assemblies, as evidenced by the 2017 Nobel Prize. Although cryo-EM has been
drastically improved to generate high-resolution three-dimensional (3D) maps
that contain detailed structural information about macromolecules, the
computational methods for using the data to automatically build structure
models are lagging far behind. The traditional cryo-EM model building approach
is template-based homology modeling. Manual de novo modeling is very
time-consuming when no template model is found in the database. In recent
years, de novo cryo-EM modeling using machine learning (ML) and deep learning
(DL) has ranked among the top-performing methods in macromolecular structure
modeling. Deep-learning-based de novo cryo-EM modeling is an important
application of artificial intelligence, with impressive results and great
potential for the next generation of molecular biomedicine. Accordingly, we
systematically review the representative ML/DL-based de novo cryo-EM modeling
methods. And their significances are discussed from both practical and
methodological viewpoints. We also briefly describe the background of cryo-EM
data processing workflow. Overall, this review provides an introductory guide
to modern research on artificial intelligence (AI) for de novo molecular
structure modeling and future directions in this emerging field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06322</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06322</id><submitter>Taishi Nakashima</submitter><version version="v1"><date>Fri, 12 Feb 2021 01:15:18 GMT</date><size>60kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:04:20 GMT</date><size>61kb</size><source_type>D</source_type></version><title>Joint Dereverberation and Separation with Iterative Source Steering</title><authors>Taishi Nakashima, Robin Scheibler, Masahito Togami, Nobutaka Ono</authors><categories>eess.AS cs.SD eess.SP</categories><comments>5 pages, 2 figures, accepted at ICASSP 2021</comments><doi>10.1109/ICASSP39728.2021.9413478</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm for joint dereverberation and blind source
separation (DR-BSS). Our work builds upon the IRLMA-T framework that applies a
unified filter combining dereverberation and separation. One drawback of this
framework is that it requires several matrix inversions, an operation
inherently costly and with potential stability issues. We leverage the recently
introduced iterative source steering (ISS) updates to propose two algorithms
mitigating this issue. Albeit derived from first principles, the first
algorithm turns out to be a natural combination of weighted prediction error
(WPE) dereverberation and ISS-based BSS, applied alternatingly. In this case,
we manage to reduce the number of matrix inversion to only one per iteration
and source. The second algorithm updates the ILRMA-T matrix using only
sequential ISS updates requiring no matrix inversion at all. Its implementation
is straightforward and memory efficient. Numerical experiments demonstrate that
both methods achieve the same final performance as ILRMA-T in terms of several
relevant objective metrics. In the important case of two sources, the number of
iterations required is also similar.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06361</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06361</id><submitter>Sandra Carrasco Limeros</submitter><version version="v1"><date>Fri, 12 Feb 2021 06:29:28 GMT</date><size>3764kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 14:00:58 GMT</date><size>7534kb</size><source_type>D</source_type></version><title>SCOUT: Socially-COnsistent and UndersTandable Graph Attention Network
  for Trajectory Prediction of Vehicles and VRUs</title><authors>Sandra Carrasco, David Fern\'andez Llorca, Miguel \'Angel Sotelo</authors><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous vehicles navigate in dynamically changing environments under a
wide variety of conditions, being continuously influenced by surrounding
objects. Modelling interactions among agents is essential for accurately
forecasting other agents' behaviour and achieving safe and comfortable motion
planning. In this work, we propose SCOUT, a novel Attention-based Graph Neural
Network that uses a flexible and generic representation of the scene as a graph
for modelling interactions, and predicts socially-consistent trajectories of
vehicles and Vulnerable Road Users (VRUs) under mixed traffic conditions. We
explore three different attention mechanisms and test our scheme with both
bird-eye-view and on-vehicle urban data, achieving superior performance than
existing state-of-the-art approaches on InD and ApolloScape Trajectory
benchmarks. Additionally, we evaluate our model's flexibility and
transferability by testing it under completely new scenarios on RounD dataset.
The importance and influence of each interaction in the final prediction is
explored by means of Integrated Gradients technique and the visualization of
the attention learned.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06559</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06559</id><submitter>Winnie Xu</submitter><version version="v1"><date>Fri, 12 Feb 2021 14:48:58 GMT</date><size>1498kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 21:03:32 GMT</date><size>1865kb</size><source_type>D</source_type></version><title>Infinitely Deep Bayesian Neural Networks with Stochastic Differential
  Equations</title><authors>Winnie Xu, Ricky T.Q. Chen, Xuechen Li, David Duvenaud</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform scalable approximate inference in a continuous-depth Bayesian
neural network family. In this model class, uncertainty about separate weights
in each layer gives hidden units that follow a stochastic differential
equation. We demonstrate gradient-based stochastic variational inference in
this infinite-parameter setting, producing arbitrarily-flexible approximate
posteriors. We also derive a novel gradient estimator that approaches zero
variance as the approximate posterior over weights approaches the true
posterior. This approach brings continuous-depth Bayesian neural nets to a
competitive comparison against discrete-depth alternatives, while inheriting
the memory-efficient training and tunable precision of Neural ODEs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06613</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06613</id><submitter>Mong-Jen Kao</submitter><version version="v1"><date>Fri, 12 Feb 2021 16:48:11 GMT</date><size>67kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 03:59:35 GMT</date><size>68kb</size><source_type>D</source_type></version><title>Improved LP-based Approximation Algorithms for Facility Location with
  Hard Capacities</title><authors>Mong-Jen Kao</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present LP-based approximation algorithms for the capacitated facility
location problem (CFL), a long-standing problem with intriguing unsettled
approximability in the literature dated back to the 90s. We present an elegant
iterative rounding scheme for the MFN relaxation that yields an approximation
guarantee of $\left(10+\sqrt{67}\right)/2 \approx 9.0927$, a significant
improvement upon the previous LP-based ratio of $288$ due to An et al. in~2014.
  For CFL with cardinality facility cost (CFL-CFC), we present an LP-based
$4$-approximation algorithm, which not only surpasses the long-standing ratio
of~$5$ due to Levi et al. that ages up for decades since 2004 but also unties
the long-time match to the best approximation for CFL that is obtained via
local search in 2012. Our result considerably deepens the current understanding
for the CFL problem and indicates that an LP-based ratio strictly better than
$5$ in polynomial time for the general problem may still be possible to pursue.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06648</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06648</id><submitter>Severi Rissanen</submitter><version version="v1"><date>Fri, 12 Feb 2021 17:43:18 GMT</date><size>3662kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 16 Mar 2021 13:48:46 GMT</date><size>3835kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 May 2021 19:27:45 GMT</date><size>4857kb</size><source_type>D</source_type></version><title>A Critical Look at the Consistency of Causal Estimation With Deep Latent
  Variable Models</title><authors>Severi Rissanen, Pekka Marttinen</authors><categories>cs.LG</categories><comments>9 pages for main text + 19 pages for references and supplementary. 18
  Figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Using deep latent variable models in causal inference has attracted
considerable interest recently, but an essential open question is their ability
to yield consistent causal estimates. While they have demonstrated promising
results and theory exists on some simple model formulations, we also know that
causal effects are not even identifiable in general with latent variables. We
investigate this gap between theory and empirical results with analytical
considerations and extensive experiments under multiple synthetic and
real-world data sets, using the causal effect variational autoencoder (CEVAE)
as a case study. While CEVAE seems to work reliably under some simple
scenarios, it does not estimate the causal effect correctly with a misspecified
latent variable or a complex data distribution, as opposed to its original
motivation. Hence, our results show that more attention should be paid to
ensuring the correctness of causal estimates with deep latent variable models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06674</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06674</id><submitter>Jesus Emeterio Navarro-Barrientos</submitter><version version="v1"><date>Fri, 12 Feb 2021 18:17:12 GMT</date><size>1087kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 22:14:29 GMT</date><size>1084kb</size><source_type>D</source_type></version><title>A model for traffic incident prediction using emergency braking data</title><authors>Alexander Reichenbach and J.-Emeterio Navarro-B</authors><categories>cs.LG cs.CY</categories><comments>6 pages, 7 figures, accepted for publication in the 32nd IEEE
  Intelligent Vehicles Symposium (https://2021.ieee-iv.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a model for traffic incident prediction. Specifically,
we address the fundamental problem of data scarcity in road traffic accident
prediction by training our model on emergency braking events instead of
accidents. Based on relevant risk factors for traffic accidents and
corresponding data categories, we evaluate different options for preprocessing
sparse data and different Machine Learning models. Furthermore, we present a
prototype implementing a traffic incident prediction model for Germany based on
emergency braking data from Mercedes-Benz vehicles as well as weather, traffic
and road data, respectively. After model evaluation and optimisation, we found
that a Random Forest model trained on artificially balanced (under-sampled)
data provided the highest classification accuracy of 85% on the original
imbalanced data. Finally, we present our conclusions and discuss further work;
from gathering more data over a longer period of time to build stronger
classification systems, to addition of internal factors such as the driver's
visual and cognitive attention.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06758</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06758</id><submitter>Jesus Emeterio Navarro-Barrientos</submitter><version version="v1"><date>Fri, 12 Feb 2021 20:22:38 GMT</date><size>6573kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 22:07:40 GMT</date><size>6573kb</size><source_type>D</source_type></version><title>Towards automatic extraction and validation of on-street parking spaces
  using park-out events data</title><authors>Martin Gebert and J.-Emeterio Navarro-B</authors><categories>cs.LG</categories><comments>6 pages, 8 figures, submitted to 32nd IEEE Intelligent Vehicles
  Symposium (https://2021.ieee-iv.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes two different approaches to automatically create a map
for valid on-street car parking spaces. For this, we use park-out events data
from car2go. The first one uses spatial aggregation and the second a machine
learning algorithm. For the former, we chose rasterization and road sectioning;
for the latter we chose decision trees. We compare the results of these
approaches and discuss their advantages and disadvantages. Furthermore, we show
our results for a neighborhood in the city of Berlin and report a
classification accuracy of 92% on the original imbalanced data. Finally, we
discuss further work; from gathering more data over a longer period of time to
fitting spatial Gaussian densities to the data and the usage of apps for manual
validation and annotation of parking spaces to improve ground truth data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06765</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06765</id><submitter>Karl Kurzer</submitter><version version="v1"><date>Fri, 12 Feb 2021 20:37:29 GMT</date><size>1275kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 11:48:00 GMT</date><size>1288kb</size><source_type>D</source_type></version><title>Generalizing Decision Making for Automated Driving with an Invariant
  Environment Representation using Deep Reinforcement Learning</title><authors>Karl Kurzer, Philip Sch\&quot;orner, Alexander Albers, Hauke Thomsen, Karam
  Daaboul, J. Marius Z\&quot;ollner</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data driven approaches for decision making applied to automated driving
require appropriate generalization strategies, to ensure applicability to the
world's variability. Current approaches either do not generalize well beyond
the training data or are not capable to consider a variable number of traffic
participants. Therefore we propose an invariant environment representation from
the perspective of the ego vehicle. The representation encodes all necessary
information for safe decision making. To assess the generalization capabilities
of the novel environment representation, we train our agents on a small subset
of scenarios and evaluate on the entire diverse set of scenarios. Here we show
that the agents are capable to generalize successfully to unseen scenarios, due
to the abstraction. In addition we present a simple occlusion model that
enables our agents to navigate intersections with occlusions without a
significant change in performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07507</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07507</id><submitter>Sijie Ji</submitter><version version="v1"><date>Mon, 15 Feb 2021 12:16:11 GMT</date><size>850kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 14:20:08 GMT</date><size>815kb</size><source_type>D</source_type></version><title>CLNet: Complex Input Lightweight Neural Network designed for Massive
  MIMO CSI Feedback</title><authors>Sijie Ji, Mo Li</authors><categories>cs.IT cs.AI eess.SP math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Massive Multiple Input Multiple Output (MIMO) system is a core technology
of the next generation communication. With the growing complexity of CSI, CSI
feedback in massive MIMO system has become a bottleneck problem, the
traditional compressive sensing based CSI feedback approaches have limited
performance. Recently, numerous deep learning based CSI feedback approaches
demonstrate their efficiency and potential. However, most existing methods
improve accuracy at the cost of computational complexity and the accuracy
decreases significantly as the CSI compression rate increases. This paper
presents a novel neural network CLNet tailored for CSI feedback problem based
on the intrinsic properties of CSI. The experiment result shows that CLNet
outperforms the state-of-the-art method by average accuracy improvement of
5.41% in both outdoor and indoor scenarios with average 24.1% less
computational overhead. Codes are available at GitHub.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07594</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07594</id><submitter>Ye Bai</submitter><version version="v1"><date>Mon, 15 Feb 2021 15:18:59 GMT</date><size>1185kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 16 Feb 2021 01:58:56 GMT</date><size>1185kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 20 Feb 2021 09:06:42 GMT</date><size>1185kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 06:27:22 GMT</date><size>1277kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 06:29:10 GMT</date><size>1185kb</size><source_type>D</source_type></version><title>Fast End-to-End Speech Recognition via Non-Autoregressive Models and
  Cross-Modal Knowledge Transferring from BERT</title><authors>Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian, Zhengqi Wen, Shuai
  Zhang</authors><categories>cs.CL cs.AI eess.AS</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention-based encoder-decoder (AED) models have achieved promising
performance in speech recognition. However, because the decoder predicts text
tokens (such as characters or words) in an autoregressive manner, it is
difficult for an AED model to predict all tokens in parallel. This makes the
inference speed relatively slow. We believe that because the encoder already
captures the whole speech utterance, which has the token-level relationship
implicitly, we can predict a token without explicitly autoregressive language
modeling. When the prediction of a token does not rely on other tokens, the
parallel prediction of all tokens in the sequence is realizable. Based on this
idea, we propose a non-autoregressive speech recognition model called LASO
(Listen Attentively, and Spell Once). The model consists of an encoder, a
decoder, and a position dependent summarizer (PDS). The three modules are based
on basic attention blocks. The encoder extracts high-level representations from
the speech. The PDS uses positional encodings corresponding to tokens to
convert the acoustic representations into token-level representations. The
decoder further captures token-level relationships with the self-attention
mechanism. At last, the probability distribution on the vocabulary is computed
for each token position. Therefore, speech recognition is re-formulated as a
position-wise classification problem. Further, we propose a cross-modal
transfer learning method to refine semantics from a large-scale pre-trained
language model BERT for improving the performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07766</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07766</id><submitter>Thi Kim Thoa Thieu</submitter><version version="v1"><date>Sun, 14 Feb 2021 01:47:03 GMT</date><size>1105kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:52:33 GMT</date><size>3991kb</size><source_type>D</source_type></version><title>Reflecting stochastic dynamics of active-passive populations with
  applications in operations research and neuroscience</title><authors>Thi Kim Thoa Thieu and Roderick Melnik</authors><categories>math.NA cs.NA</categories><comments>25 pages, 9 figures</comments><msc-class>91C05, 91F99, 82D99, 60J27, 60G99, 60G55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic dynamic models have been extensively used for the description of
processes with uncertainties arising in the operations research, behavioral
sciences, and many other application areas. A large class of the problems from
these domains is characterized by the necessity to deal with several distinct
groups of populations, which are usually labeled as &quot;active&quot; and &quot;passive&quot;.
Motivated by important applications of queueing networks and neuroscience, the
main focus of the present work is on the analysis of reflecting stochastic
dynamics of such mixed populations. We develop a general mathematical modeling
framework to describe the reflecting stochastic dynamics for active-passive
populations. The analysis of this model is carried out via a combination of
low- and high-delity results obtained from the solution of the underlying
coupled system of SDEs and from the simulations with a
statistical-mechanics-based lattice gas model, where we employ a kinetic Monte
Carlo procedure. We provide details of the queueing theory and neuronal models
and discuss a relationship between reflecting SDEs and a model of queueing
theory via a limit theorem. Furthermore, we present several representative
numerical examples, and discuss an intrinsic interconnection between active and
passive particles in the underlying stochastic process. Finally, possible
extensions of the proposed methodology have been highlighted.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07819</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07819</id><submitter>Alexander Wikner</submitter><version version="v1"><date>Mon, 15 Feb 2021 19:56:48 GMT</date><size>10831kb</size><source_type>D</source_type></version><title>Using Data Assimilation to Train a Hybrid Forecast System that Combines
  Machine-Learning and Knowledge-Based Components</title><authors>Alexander Wikner, Jaideep Pathak, Brian R. Hunt, Istvan Szunyogh,
  Michelle Girvan, and Edward Ott</authors><categories>cs.LG nlin.CD physics.ao-ph</categories><comments>28 pages, 9 figures</comments><doi>10.1063/5.0048050</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of data-assisted forecasting of chaotic dynamical
systems when the available data is in the form of noisy partial measurements of
the past and present state of the dynamical system. Recently there have been
several promising data-driven approaches to forecasting of chaotic dynamical
systems using machine learning. Particularly promising among these are hybrid
approaches that combine machine learning with a knowledge-based model, where a
machine-learning technique is used to correct the imperfections in the
knowledge-based model. Such imperfections may be due to incomplete
understanding and/or limited resolution of the physical processes in the
underlying dynamical system, e.g., the atmosphere or the ocean. Previously
proposed data-driven forecasting approaches tend to require, for training,
measurements of all the variables that are intended to be forecast. We describe
a way to relax this assumption by combining data assimilation with machine
learning. We demonstrate this technique using the Ensemble Transform Kalman
Filter (ETKF) to assimilate synthetic data for the 3-variable Lorenz system and
for the Kuramoto-Sivashinsky system, simulating model error in each case by a
misspecified parameter value. We show that by using partial measurements of the
state of the dynamical system, we can train a machine learning model to improve
predictions made by an imperfect knowledge-based model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07926</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07926</id><submitter>Ziyang Luo</submitter><version version="v1"><date>Tue, 16 Feb 2021 02:31:05 GMT</date><size>7322kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 02:10:17 GMT</date><size>7322kb</size><source_type>D</source_type></version><title>Have Attention Heads in BERT Learned Constituency Grammar?</title><authors>Ziyang Luo</authors><categories>cs.CL</categories><comments>Accept at the EACL 2021 Student Research Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the success of pre-trained language models in recent years, more and
more researchers focus on opening the &quot;black box&quot; of these models. Following
this interest, we carry out a qualitative and quantitative analysis of
constituency grammar in attention heads of BERT and RoBERTa. We employ the
syntactic distance method to extract implicit constituency grammar from the
attention weights of each head. Our results show that there exist heads that
can induce some grammar types much better than baselines, suggesting that some
heads act as a proxy for constituency grammar. We also analyze how attention
heads' constituency grammar inducing (CGI) ability changes after fine-tuning
with two kinds of tasks, including sentence meaning similarity (SMS) tasks and
natural language inference (NLI) tasks. Our results suggest that SMS tasks
decrease the average CGI ability of upper layers, while NLI tasks increase it.
Lastly, we investigate the connections between CGI ability and natural language
understanding ability on QQP and MNLI tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07944</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07944</id><submitter>Davis Gilton</submitter><version version="v1"><date>Tue, 16 Feb 2021 03:49:58 GMT</date><size>10602kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 03:14:53 GMT</date><size>14201kb</size><source_type>D</source_type></version><title>Deep Equilibrium Architectures for Inverse Problems in Imaging</title><authors>Davis Gilton, Gregory Ongie, Rebecca Willett</authors><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent efforts on solving inverse problems in imaging via deep neural
networks use architectures inspired by a fixed number of iterations of an
optimization method. The number of iterations is typically quite small due to
difficulties in training networks corresponding to more iterations; the
resulting solvers cannot be run for more iterations at test time without
incurring significant errors. This paper describes an alternative approach
corresponding to an infinite number of iterations, yielding a consistent
improvement in reconstruction accuracy above state-of-the-art alternatives and
where the computational budget can be selected at test time to optimize
context-dependent trade-offs between accuracy and computation. The proposed
approach leverages ideas from Deep Equilibrium Models, where the fixed-point
iteration is constructed to incorporate a known forward model and insights from
classical optimization-based reconstruction methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07958</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07958</id><submitter>HaiLong Liu</submitter><version version="v1"><date>Tue, 16 Feb 2021 04:58:35 GMT</date><size>1068kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 23 Feb 2021 02:31:25 GMT</date><size>713kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 5 Apr 2021 10:34:04 GMT</date><size>713kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 31 May 2021 00:50:42 GMT</date><size>815kb</size><source_type>D</source_type></version><title>Importance of Instruction for Pedestrian-Automated Driving Vehicle
  Interaction with an External Human Machine Interface: Effects on Pedestrians'
  Situation Awareness, Trust, Perceived Risks and Decision Making</title><authors>Hailong Liu, Takatsugu Hirayama, Masaya Watanabe</authors><categories>cs.HC</categories><comments>5 figures, Accepted by IEEE IV2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to a manual driving vehicle (MV), an automated driving vehicle lacks
a way to communicate with the pedestrian through the driver when it interacts
with the pedestrian because the driver usually does not participate in driving
tasks. Thus, an external human machine interface (eHMI) can be viewed as a
novel explicit communication method for providing driving intentions of an
automated driving vehicle (AV) to pedestrians when they need to negotiate in an
interaction, e.g., an encountering scene. However, the eHMI may not guarantee
that the pedestrians will fully recognize the intention of the AV. In this
paper, we propose that the instruction of the eHMI's rationale can help
pedestrians correctly understand the driving intentions and predict the
behavior of the AV, and thus their subjective feelings (i.e., dangerous
feeling, trust in the AV, and feeling of relief) and decision-making are also
improved. The results of an interaction experiment in a road-crossing scene
indicate that the participants were more difficult to be aware of the situation
when they encountered an AV w/o eHMI compared to when they encountered an MV;
further, the participants' subjective feelings and hesitation in
decision-making also deteriorated significantly. When the eHMI was used in the
AV, the situational awareness, subjective feelings and decision-making of the
participants regarding the AV w/ eHMI were improved. After the instruction, it
was easier for the participants to understand the driving intention and predict
driving behavior of the AV w/ eHMI. Further, the subjective feelings and the
hesitation related to decision-making were improved and reached the same
standards as that for the MV.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08019</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08019</id><submitter>Kevin Bello</submitter><version version="v1"><date>Tue, 16 Feb 2021 08:36:19 GMT</date><size>515kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:38:36 GMT</date><size>507kb</size><source_type>D</source_type></version><title>A Thorough View of Exact Inference in Graphs from the Degree-4
  Sum-of-Squares Hierarchy</title><authors>Kevin Bello, Chuyang Ke and Jean Honorio</authors><categories>cs.LG cs.AI stat.ML</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing inference in graphs is a common task within several machine
learning problems, e.g., image segmentation, community detection, among others.
For a given undirected connected graph, we tackle the statistical problem of
exactly recovering an unknown ground-truth binary labeling of the nodes from a
single corrupted observation of each edge. Such problem can be formulated as a
quadratic combinatorial optimization problem over the boolean hypercube, where
it has been shown before that one can (with high probability and in polynomial
time) exactly recover the ground-truth labeling of graphs that have an
isoperimetric number that grows with respect to the number of nodes (e.g.,
complete graphs, regular expanders). In this work, we apply a powerful
hierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the
combinatorial problem. Motivated by empirical evidence on the improvement in
exact recoverability, we center our attention on the degree-4 SoS relaxation
and set out to understand the origin of such improvement from a graph
theoretical perspective. We show that the solution of the dual of the relaxed
problem is related to finding edge weights of the Johnson and Kneser graphs,
where the weights fulfill the SoS constraints and intuitively allow the input
graph to increase its algebraic connectivity. Finally, as byproduct of our
analysis, we derive a novel Cheeger-type lower bound for the algebraic
connectivity of graphs with signed edge weights.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08127</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08127</id><submitter>Bruno Loureiro</submitter><version version="v1"><date>Tue, 16 Feb 2021 12:49:15 GMT</date><size>1581kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:19:46 GMT</date><size>1584kb</size><source_type>D</source_type></version><title>Learning curves of generic features maps for realistic datasets with a
  teacher-student model</title><authors>Bruno Loureiro, C\'edric Gerbelot, Hugo Cui, Sebastian Goldt, Florent
  Krzakala, Marc M\'ezard, Lenka Zdeborov\'a</authors><categories>stat.ML cond-mat.dis-nn cs.LG math.PR math.ST stat.TH</categories><comments>main: 11 pages, 5 figures; appendix: 50 pages, 4 figures; v2: revised
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teacher-student models provide a framework in which the typical-case
performance of high-dimensional supervised learning can be described in closed
form. The assumptions of Gaussian i.i.d. input data underlying the canonical
teacher-student model may, however, be perceived as too restrictive to capture
the behaviour of realistic data sets. In this paper, we introduce a Gaussian
covariate generalisation of the model where the teacher and student can act on
different spaces, generated with fixed, but generic feature maps. While still
solvable in a closed form, this generalization is able to capture the learning
curves for a broad range of realistic data sets, thus redeeming the potential
of the teacher-student framework. Our contribution is then two-fold: First, we
prove a rigorous formula for the asymptotic training loss and generalisation
error. Second, we present a number of situations where the learning curve of
the model captures the one of a realistic data set learned with kernel
regression and classification, with out-of-the-box feature maps such as random
projections or scattering transforms, or with pre-learned ones - such as the
features learned by training multi-layer neural networks. We discuss both the
power and the limitations of the framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08132</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08132</id><submitter>Chris Norval</submitter><version version="v1"><date>Tue, 16 Feb 2021 13:09:07 GMT</date><size>65kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 13:57:23 GMT</date><size>134kb</size><source_type>D</source_type></version><title>Towards an accountable Internet of Things: A call for reviewability</title><authors>Chris Norval, Jennifer Cobbe, Jatinder Singh</authors><categories>cs.CY</categories><comments>To appear in: Privacy by Design for the Internet of Things. Cite as:
  Norval C, Cobbe J, and Singh, J. Towards an accountable Internet of Things: A
  call for reviewability. In Privacy by Design for the Internet of Things:
  Building accountability and security; London, UK: IET; 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the IoT becomes increasingly ubiquitous, concerns are being raised about
how IoT systems are being built and deployed. Connected devices will generate
vast quantities of data, which drive algorithmic systems and result in
real-world consequences. Things will go wrong, and when they do, how do we
identify what happened, why they happened, and who is responsible? Given the
complexity of such systems, where do we even begin?
  This chapter outlines aspects of accountability as they relate to IoT, in the
context of the increasingly interconnected and data-driven nature of such
systems. Specifically, we argue the urgent need for mechanisms - legal,
technical, and organisational - that facilitate the review of IoT systems. Such
mechanisms work to support accountability, by enabling the relevant
stakeholders to better understand, assess, interrogate and challenge the
connected environments that increasingly pervade our world.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08369</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08369</id><submitter>Aditya Kunar</submitter><version version="v1"><date>Tue, 16 Feb 2021 18:53:57 GMT</date><size>13192kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 19:15:03 GMT</date><size>13673kb</size><source_type>D</source_type></version><title>CTAB-GAN: Effective Table Data Synthesizing</title><authors>Zilong Zhao, Aditya Kunar, Hiek Van der Scheer, Robert Birke and Lydia
  Y. Chen</authors><categories>cs.LG</categories><comments>This paper consists of 11 pages which contain 8 figures, 5 tables and
  an appendix with a user manual for our software application</comments><acm-class>I.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While data sharing is crucial for knowledge development, privacy concerns and
strict regulation (e.g., European General Data Protection Regulation (GDPR))
unfortunately limit its full effectiveness. Synthetic tabular data emerges as
an alternative to enable data sharing while fulfilling regulatory and privacy
constraints. The state-of-the-art tabular data synthesizers draw methodologies
from generative Adversarial Networks (GAN) and address two main data types in
the industry, i.e., continuous and categorical. In this paper, we develop
CTAB-GAN, a novel conditional table GAN architecture that can effectively model
diverse data types, including a mix of continuous and categorical variables.
Moreover, we address data imbalance and long-tail issues, i.e., certain
variables have drastic frequency differences across large values. To achieve
those aims, we first introduce the information loss and classification loss to
the conditional GAN. Secondly, we design a novel conditional vector, which
efficiently encodes the mixed data type and skewed distribution of data
variable. We extensively evaluate CTAB-GAN with the state of the art GANs that
generate synthetic tables, in terms of data similarity and analysis utility.
The results on five datasets show that the synthetic data of CTAB-GAN
remarkably resembles the real data for all three types of variables and results
into higher accuracy for five machine learning algorithms, by up to 17%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08431</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08431</id><submitter>Jonathan Lorraine</submitter><version version="v1"><date>Tue, 16 Feb 2021 19:55:27 GMT</date><size>3425kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 22:27:59 GMT</date><size>3546kb</size><source_type>D</source_type></version><title>Complex Momentum for Optimization in Games</title><authors>Jonathan Lorraine, David Acuna, Paul Vicol, David Duvenaud</authors><categories>cs.LG cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We generalize gradient descent with momentum for optimization in
differentiable games to have complex-valued momentum. We give theoretical
motivation for our method by proving convergence on bilinear zero-sum games for
simultaneous and alternating updates. Our method gives real-valued parameter
updates, making it a drop-in replacement for standard optimizers. We
empirically demonstrate that complex-valued momentum can improve convergence in
realistic adversarial games - like generative adversarial networks - by showing
we can find better solutions with an almost identical computational cost. We
also show a practical generalization to a complex-valued Adam variant, which we
use to train BigGAN to better inception scores on CIFAR-10.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08833</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08833</id><submitter>David Johnson</submitter><version version="v1"><date>Wed, 17 Feb 2021 15:41:38 GMT</date><size>717kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 19 Feb 2021 11:59:52 GMT</date><size>721kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 10:32:39 GMT</date><size>728kb</size><source_type>D</source_type></version><title>DESED-FL and URBAN-FL: Federated Learning Datasets for Sound Event
  Detection</title><authors>David S. Johnson, Wolfgang Lorenz, Michael Taenzer, Stylianos
  Mimilakis, Sascha Grollmisch, Jakob Abe{\ss}er, Hanna Lukashevich</authors><categories>cs.SD cs.DC cs.LG eess.AS</categories><comments>To be published in EUSIPCO 2021</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Research on sound event detection (SED) in environmental settings has seen
increased attention in recent years. The large amounts of (private) domestic or
urban audio data needed raise significant logistical and privacy concerns. The
inherently distributed nature of these tasks, make federated learning (FL) a
promising approach to take advantage of largescale data while mitigating
privacy issues. While FL has also seen increased attention recently, to the
best of our knowledge there is no research towards FL for SED. To address this
gap and foster further research in this field, we create and publish novel FL
datasets for SED in domestic and urban environments. Furthermore, we provide
baseline results on the datasets in a FL context for three deep neural network
architectures. The results indicate that FL is a promising approach for SED,
but faces challenges with divergent data distributions inherent to distributed
client edge devices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09041</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09041</id><submitter>Gilad Stern</submitter><version version="v1"><date>Wed, 17 Feb 2021 21:55:49 GMT</date><size>100kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 15:45:45 GMT</date><size>101kb</size></version><title>Reaching Consensus for Asynchronous Distributed Key Generation</title><authors>Ittai Abraham, Philipp Jovanovic, Mary Maller, Sarah Meiklejohn, Gilad
  Stern, Alin Tomescu</authors><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a protocol for Asynchronous Distributed Key Generation (A-DKG) that
is optimally resilient (can withstand $f&lt;\frac{n}{3}$ faulty parties), has a
constant expected number of rounds, has $\tilde{O}(n^3)$ expected communication
complexity, and assumes only the existence of a PKI. Prior to our work, the
best A-DKG protocols required $\Omega(n)$ expected number of rounds, and
$\Omega(n^4)$ expected communication.
  Our A-DKG protocol relies on several building blocks that are of independent
interest. We define and design a Proposal Election (PE) protocol that allows
parties to retrospectively agree on a valid proposal after enough proposals
have been sent from different parties. With constant probability the elected
proposal was proposed by a non-faulty party. In building our PE protocol, we
design a Verifiable Gather protocol which allows parties to communicate which
proposals they have and have not seen in a verifiable manner. The final
building block to our A-DKG is a Validated Asynchronous Byzantine Agreement
(VABA) protocol. We use our PE protocol to construct a VABA protocol that does
not require leaders or an asynchronous DKG setup. Our VABA protocol can be used
more generally when it is not possible to use threshold signatures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09194</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09194</id><submitter>Rafael Melo</submitter><version version="v1"><date>Thu, 18 Feb 2021 07:34:53 GMT</date><size>159kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 10:36:43 GMT</date><size>137kb</size><source_type>D</source_type></version><title>Maximum weighted induced forests and trees: New formulations and a
  computational comparative review</title><authors>Rafael A. Melo and Celso C. Ribeiro</authors><categories>math.OC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$ with a weight $w_v$ associated with each vertex $v\in
V$, the maximum weighted induced forest problem (MWIF) consists of encountering
a maximum weighted subset $V'\subseteq V$ of the vertices such that $V'$
induces a forest. This NP-hard problem is known to be equivalent to the minimum
weighted feedback vertex set problem, which has applicability in a variety of
domains. The closely related maximum weighted induced tree problem (MWIT), on
the other hand, requires that the subset $V'\subseteq V$ induces a tree. We
propose two new integer programming formulations with an exponential number of
constraints and branch-and-cut procedures for MWIF. Computational experiments
using benchmark instances are performed comparing several formulations,
including the newly proposed approaches and those available in the literature,
when solved by a standard commercial mixed integer programming solver. More
specifically, five formulations are compared, two compact (i.e., with a
polynomial number of variables and constraints) ones and the three others with
an exponential number of constraints. The experiments show that a new
formulation for the problem based on directed cutset inequalities for
eliminating cycles (DCUT) offers stronger linear relaxation bounds earlier in
the search process. The results also indicate that the other new formulation,
denoted tree with cycle elimination (TCYC), outperforms those available in the
literature when it comes to the average times for proving optimality for the
small instances, especially the more challenging ones. Additionally, this
formulation can achieve much lower average times for solving the larger random
instances that can be optimally solved. Furthermore, we show how the
formulations for MWIF can be easily extended for MWIT. Such extension allowed
us to compare the optimal solution values of the two problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09200</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09200</id><submitter>Harideep Nair</submitter><version version="v1"><date>Thu, 18 Feb 2021 07:47:43 GMT</date><size>793kb</size><source_type>D</source_type></version><title>Unsupervised Clustering of Time Series Signals using Neuromorphic
  Energy-Efficient Temporal Neural Networks</title><authors>Shreyas Chaudhari, Harideep Nair, Jos\'e M.F. Moura and John Paul Shen</authors><categories>cs.LG cs.AI cs.ET</categories><comments>Accepted for publication at ICASSP 2021</comments><doi>10.1109/ICASSP39728.2021.9414882</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised time series clustering is a challenging problem with diverse
industrial applications such as anomaly detection, bio-wearables, etc. These
applications typically involve small, low-power devices on the edge that
collect and process real-time sensory signals. State-of-the-art time-series
clustering methods perform some form of loss minimization that is extremely
computationally intensive from the perspective of edge devices. In this work,
we propose a neuromorphic approach to unsupervised time series clustering based
on Temporal Neural Networks that is capable of ultra low-power, continuous
online learning. We demonstrate its clustering performance on a subset of UCR
Time Series Archive datasets. Our results show that the proposed approach
either outperforms or performs similarly to most of the existing algorithms
while being far more amenable for efficient hardware implementation. Our
hardware assessment analysis shows that in 7 nm CMOS the proposed architecture,
on average, consumes only about 0.005 mm^2 die area and 22 uW power and can
process each signal with about 5 ns latency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09290</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09290</id><submitter>Franz Ru{\ss}wurm</submitter><version version="v1"><date>Thu, 18 Feb 2021 12:09:03 GMT</date><size>164kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 10:30:30 GMT</date><size>4084kb</size><source_type>D</source_type></version><title>On MPC without terminal conditions for dynamic non-holonomic robots</title><authors>Franz Ru{\ss}wurm, Willem Esterhuizen, Karl Worthmann, Stefan Streif</authors><categories>eess.SY cs.SY math.OC</categories><comments>6 pages, 2 figures, Submitted to NMPC for possible publication</comments><msc-class>34H15, 37M05, 65P99, 93B45, 93C15</msc-class><acm-class>G.1.6; G.1.10</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We consider an input-constrained differential-drive robot with actuator
dynamics. For this system, we establish asymptotic stability of the origin on
arbitrary compact, convex sets using Model Predictive Control (MPC) without
stabilizing terminal conditions despite the presence of state constraints and
actuator dynamics. We note that the problem without those two additional
ingredients was essentially solved beforehand, despite the fact that the
linearization is not stabilizable. We propose an approach successfully solving
the task at hand by combining the theory of barriers to characterize the
viability kernel and an MPC framework based on so-called cost controllability.
Moreover, we present a numerical case study to derive quantitative bounds on
the required length of the prediction horizon. To this end, we investigate the
boundary of the viability kernel and a neighbourhood of the origin, i.e. the
most interesting areas.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09397</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09397</id><submitter>Yi-Syuan Chen</submitter><version version="v1"><date>Thu, 18 Feb 2021 14:42:09 GMT</date><size>417kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 06:27:37 GMT</date><size>419kb</size><source_type>D</source_type></version><title>Meta-Transfer Learning for Low-Resource Abstractive Summarization</title><authors>Yi-Syuan Chen and Hong-Han Shuai</authors><categories>cs.CL cs.LG</categories><comments>Accepted by AAAI 2021; Camera Ready Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural abstractive summarization has been studied in many pieces of
literature and achieves great success with the aid of large corpora. However,
when encountering novel tasks, one may not always benefit from transfer
learning due to the domain shifting problem, and overfitting could happen
without adequate labeled examples. Furthermore, the annotations of abstractive
summarization are costly, which often demand domain knowledge to ensure the
ground-truth quality. Thus, there are growing appeals for Low-Resource
Abstractive Summarization, which aims to leverage past experience to improve
the performance with limited labeled examples of target corpus. In this paper,
we propose to utilize two knowledge-rich sources to tackle this problem, which
are large pre-trained models and diverse existing corpora. The former can
provide the primary ability to tackle summarization tasks; the latter can help
discover common syntactic or semantic information to improve the generalization
ability. We conduct extensive experiments on various summarization corpora with
different writing styles and forms. The results demonstrate that our approach
achieves the state-of-the-art on 6 corpora in low-resource scenarios, with only
0.7% of trainable parameters compared to previous work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09468</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09468</id><submitter>Guodong Zhang</submitter><version version="v1"><date>Thu, 18 Feb 2021 16:39:35 GMT</date><size>8257kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 17:00:50 GMT</date><size>5171kb</size><source_type>D</source_type></version><title>Near-optimal Local Convergence of Alternating Gradient Descent-Ascent
  for Minimax Optimization</title><authors>Guodong Zhang, Yuanhao Wang, Laurent Lessard, Roger Grosse</authors><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smooth minimax games often proceed by simultaneous or alternating gradient
updates. Although algorithms with alternating updates are commonly used in
practice for many applications (e.g., GAN training), the majority of existing
theoretical analyses focus on simultaneous algorithms for convenience of
analysis. In this paper, we study alternating gradient descent-ascent (Alt-GDA)
in minimax games and show that Alt-GDA is superior to its simultaneous
counterpart (Sim-GDA) in many settings. In particular, we prove that Alt-GDA
achieves a near-optimal local convergence rate for strongly convex-strongly
concave (SCSC) problems while Sim-GDA converges at a much slower rate. To our
knowledge, this is the \emph{first} result of any setting showing that Alt-GDA
converges faster than Sim-GDA by more than a constant. We further prove that
the acceleration effect of alternating updates remains when the minimax problem
has only strong concavity in the dual variables. Lastly, we adapt the theory of
integral quadratic constraints and show that Alt-GDA attains the same rate
\emph{globally} for a class of SCSC minimax problems. Numerical experiments on
quadratic minimax games validate our claims. Empirically, we demonstrate that
alternating updates speed up GAN training significantly and the use of optimism
only helps for simultaneous algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09703</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09703</id><submitter>Zhihan Xiong</submitter><version version="v1"><date>Fri, 19 Feb 2021 01:42:50 GMT</date><size>69kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 04:07:14 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Randomized Exploration is Near-Optimal for Tabular MDP</title><authors>Zhihan Xiong, Ruoqi Shen, Simon S. Du</authors><categories>cs.LG</categories><comments>The current version of this paper contains some technical issues. In
  particular, the Lemma 9 on page 20 does not hold in general because $s_h^k$
  is not generated by $\pi^*$. The Lemma 15 on page 27 does not hold in general
  because $\overline{V}$ and $\hat{P}$ are dependent</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study exploration using randomized value functions in Thompson Sampling
(TS)-like algorithms in reinforcement learning. This type of algorithms enjoys
appealing empirical performance. We show that when we use 1) a single random
seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a
worst-case $\widetilde{O}\left(H\sqrt{SAT}\right)$ regret bound for episodic
time-inhomogeneous Markov Decision Process where $S$ is the size of state
space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is
the number of interactions. This bound polynomially improves all existing
bounds for TS-like algorithms based on randomized value functions, and for the
first time, matches the $\Omega\left(H\sqrt{SAT}\right)$ lower bound up to
logarithmic factors. Our result highlights that randomized exploration can be
near-optimal, which was previously only achieved by optimistic algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.10078</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.10078</id><submitter>Yi Zhu</submitter><version version="v1"><date>Fri, 19 Feb 2021 18:25:24 GMT</date><size>7111kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 03:25:12 GMT</date><size>5892kb</size><source_type>D</source_type></version><title>Rapid Multi-Physics Simulation for Electro-Thermal Origami Systems</title><authors>Yi Zhu and Evgueni T. Filipov</authors><categories>cs.RO</categories><comments>This work has been submitted to the International Journal of
  Mechanical Science</comments><journal-ref>International Journal of Mechanical Sciences, 2021, 202-203,
  106537</journal-ref><doi>10.1016/j.ijmecsci.2021.106537</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Electro-thermally actuated origami provides a novel method for creating 3-D
systems with advanced morphing and functional capabilities. However, it is
currently difficult to simulate the multi-physical behavior of such systems
because the electro-thermal actuation and large folding deformations are highly
interdependent. In this work, we introduce a rapid multi-physics simulation
framework for electro-thermally actuated origami systems that can
simultaneously capture: thermo-mechancially coupled actuation, inter panel
contact, heat transfer, large deformation folding, and other complex loading
applied onto the origami. Comparisons with finite element models validate the
proposed framework for simulating origami heat transfer with different system
geometries, materials, and surrounding environments. Verification of the
simulated folding behaviors against physical electro-thermal micro-origami
further demonstrates the validity of the proposed model. Simulations of more
complex origami patterns and a case study for origami optimization are provided
as application examples to show the capability and efficiency of the model. The
framework provides a novel simulation tool for analysis, design, control, and
optimization of active origami systems, pushing the boundary for feasible shape
morphing and functional capability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.10395</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.10395</id><submitter>Yoav Wald</submitter><version version="v1"><date>Sat, 20 Feb 2021 17:24:34 GMT</date><size>436kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 12:55:11 GMT</date><size>1109kb</size><source_type>D</source_type></version><title>On Calibration and Out-of-domain Generalization</title><authors>Yoav Wald, Amir Feder, Daniel Greenfeld, Uri Shalit</authors><categories>cs.LG</categories><comments>24 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Out-of-domain (OOD) generalization is a significant challenge for machine
learning models. Many techniques have been proposed to overcome this challenge,
often focused on learning models with certain invariance properties. In this
work, we draw a link between OOD performance and model calibration, arguing
that calibration across multiple domains can be viewed as a special case of an
invariant representation leading to better OOD generalization. Specifically, we
show that under certain conditions, models which achieve \emph{multi-domain
calibration} are provably free of spurious correlations. This leads us to
propose multi-domain calibration as a measurable and trainable surrogate for
the OOD performance of a classifier. We therefore introduce methods that are
easy to apply and allow practitioners to improve multi-domain calibration by
training or modifying an existing model, leading to better performance on
unseen domains. Using five datasets from the recently proposed WILDS OOD
benchmark, as well as the Colored MNIST dataset, we demonstrate that training
or tuning models so they are calibrated across multiple domains leads to
significantly improved performance on unseen test domains. We believe this
intriguing connection between calibration and OOD generalization is promising
from both a practical and theoretical point of view.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.10652</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.10652</id><submitter>Francesco Ferrante</submitter><version version="v1"><date>Sun, 21 Feb 2021 17:38:19 GMT</date><size>157kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 09:32:58 GMT</date><size>114kb</size></version><title>Observer Design for Linear Aperiodic Sampled-Data Systems: A Hybrid
  Systems Approach</title><authors>Francesco Ferrante and Alexandre Seuret</authors><categories>eess.SY cs.SY</categories><comments>V1 is the original submission before peer review. V2 is the
  peer-reviewed version accepted for publication. V2 includes fixes to minor
  typos in the early access version of the paper and matches the final printed
  version to appear in the IEEE Control Systems Letters</comments><journal-ref>IEEE Control Systems Letters 2021</journal-ref><doi>10.1109/LCSYS.2021.3081345</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Observer design for linear systems with aperiodic sampled-data measurements
is addressed. To solve this problem, a novel hybrid observer is designed. The
main peculiarity of the proposed observer consists of the use two output
injection terms, one acting at the sampling instants and one providing an
intersample injection. The error dynamics are augmented with a timer variable
triggering the arrival of a new measurement and analyzed via hybrid system
tools. Using Lyapunov theory, sufficient conditions for the convergence of the
observer are provided. Relying on those conditions, an optimal LMI-based design
is proposed for the observer gains. The effectiveness of the approach is
illustrated in an example.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.11010</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.11010</id><submitter>Ginevra Carbone</submitter><version version="v1"><date>Mon, 22 Feb 2021 14:07:24 GMT</date><size>6351kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 08:10:17 GMT</date><size>2077kb</size><source_type>D</source_type></version><title>Resilience of Bayesian Layer-Wise Explanations under Adversarial Attacks</title><authors>Ginevra Carbone, Guido Sanguinetti, Luca Bortolussi</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of the stability of saliency-based explanations of
Neural Network predictions under adversarial attacks in a classification task.
Saliency interpretations of deterministic Neural Networks are remarkably
brittle even when the attacks fail, i.e. for attacks that do not change the
classification label. We empirically show that interpretations provided by
Bayesian Neural Networks are considerably more stable under adversarial
perturbations. By leveraging recent results, we also provide a theoretical
explanation of this result in terms of the geometry of adversarial attacks.
Additionally, we discuss the stability of the interpretations of high level
representations of the inputs in the internal layers of a Network. Our results
not only confirm that Bayesian Neural Networks are more robust to adversarial
attacks, but also demonstrate that Bayesian methods have the potential to
provide more stable and interpretable assessments of Neural Network
predictions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.11075</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.11075</id><submitter>Hannes Eriksson</submitter><version version="v1"><date>Mon, 22 Feb 2021 14:45:39 GMT</date><size>138kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 10:34:43 GMT</date><size>497kb</size><source_type>D</source_type></version><title>SENTINEL: Taming Uncertainty with Ensemble-based Distributional
  Reinforcement Learning</title><authors>Hannes Eriksson, Debabrota Basu, Mina Alibeigi, Christos Dimitrakakis</authors><categories>cs.LG</categories><comments>30 pages, 9 figures, 8 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we consider risk-sensitive sequential decision-making in
model-based Reinforcement Learning (RL). Our contributions are two-fold. First,
we introduce a novel and coherent quantification of risk, namely composite
risk, which quantifies joint effect of aleatory and epistemic risk during the
learning process. Existing works considered either aleatory or epistemic risk
individually, or an additive combination of the two. We prove that the additive
formulation is a particular case of the composite risk when the epistemic risk
measure is replaced with expectation. Thus, the composite risk provides an
estimate more sensitive to both aleatory and epistemic sources of uncertainties
than the individual and additive formulations. Following that, we propose to
use a bootstrapping method, SENTINEL-K, for performing distributional RL.
SENTINEL-K uses an ensemble of $K$ learners to estimate the return
distribution. We use the Follow The Regularised Leader (FTRL) to aggregate the
return distributions of $K$ learners and to estimate the composite risk. We
experimentally verify that SENTINEL-K estimates the return distribution better,
and while used with composite risk estimate, demonstrates better risk-sensitive
performance than state-of-the-art risk-sensitive and distributional RL
algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.11619</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.11619</id><submitter>Philipp Czerner</submitter><version version="v1"><date>Tue, 23 Feb 2021 10:55:11 GMT</date><size>46kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 18:22:27 GMT</date><size>46kb</size><source_type>D</source_type></version><title>Lower Bounds on the State Complexity of Population Protocols</title><authors>Philipp Czerner, Javier Esparza</authors><categories>cs.DC</categories><comments>Various minor revisions</comments><doi>10.1145/3465084.3467912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population protocols are a model of computation in which an arbitrary number
of indistinguishable finite-state agents interact in pairs. The goal of the
agents is to decide by stable consensus whether their initial global
configuration satisfies a given property, specified as a predicate on the set
of all initial configurations. The state complexity of a predicate is the
number of states of a smallest protocol that computes it. Previous work by
Blondin et al. has shown that the counting predicates $x \ge \eta$ have state
complexity $\mathcal{O}(\log \eta)$ for leaderless protocols and
$\mathcal{O}(\log \log \eta)$ for protocols with leaders. We obtain the first
non-trivial lower bounds: the state complexity of $x \geq \eta$ is
$\Omega(\log\log\log \eta)$ for leaderless protocols, and the inverse of a
non-elementary function for protocols with leaders.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.11630</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.11630</id><submitter>Philipp Czerner</submitter><version version="v1"><date>Tue, 23 Feb 2021 11:10:59 GMT</date><size>100kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 15:58:42 GMT</date><size>92kb</size><source_type>D</source_type></version><title>Decision Power of Weak Asynchronous Models of Distributed Computing</title><authors>Philipp Czerner, Roland Guttenberg, Martin Helfrich, Javier Esparza</authors><categories>cs.FL</categories><comments>Various minor revisions</comments><doi>10.1145/3465084.3467918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Esparza and Reiter have recently conducted a systematic comparative study of
models of distributed computing consisting of a network of identical
finite-state automata that cooperate to decide if the underlying graph of the
network satisfies a given property. The study classifies models according to
four criteria, and shows that twenty initially possible combinations collapse
into seven equivalence classes with respect to their decision power, i.e. the
properties that the automata of each class can decide. However, Esparza and
Reiter only show (proper) inclusions between the classes, and so do not
characterise their decision power. In this paper we do so for labelling
properties, i.e. properties that depend only on the labels of the nodes, but
not on the structure of the graph. In particular, majority (whether more nodes
carry label $a$ than $b$) is a labelling property. Our results show that only
one of the seven equivalence classes identified by Esparza and Reiter can
decide majority for arbitrary networks. We then study the expressive power of
the classes on bounded-degree networks, and show that three classes can. In
particular, we present an algorithm for majority that works for all
bounded-degree networks under adversarial schedulers, i.e. even if the
scheduler must only satisfy that every node makes a move infinitely often, and
prove that no such algorithm can work for arbitrary networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.11866</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.11866</id><submitter>Tengyu Xu</submitter><version version="v1"><date>Tue, 23 Feb 2021 18:56:13 GMT</date><size>208kb</size></version><version version="v2"><date>Sat, 27 Feb 2021 15:00:18 GMT</date><size>209kb</size></version><version version="v3"><date>Mon, 31 May 2021 04:49:32 GMT</date><size>221kb</size></version><title>Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality</title><authors>Tengyu Xu, Zhuoran Yang, Zhaoran Wang, Yingbin Liang</authors><categories>cs.LG stat.ML</categories><comments>Published in ICML 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Designing off-policy reinforcement learning algorithms is typically a very
challenging task, because a desirable iteration update often involves an
expectation over an on-policy distribution. Prior off-policy actor-critic (AC)
algorithms have introduced a new critic that uses the density ratio for
adjusting the distribution mismatch in order to stabilize the convergence, but
at the cost of potentially introducing high biases due to the estimation errors
of both the density ratio and value function. In this paper, we develop a
doubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take
advantage of learned nuisance functions to reduce estimation errors. Moreover,
DR-Off-PAC adopts a single timescale structure, in which both actor and critics
are updated simultaneously with constant stepsize, and is thus more sample
efficient than prior algorithms that adopt either two timescale or nested-loop
structure. We study the finite-time convergence rate and characterize the
sample complexity for DR-Off-PAC to attain an $\epsilon$-accurate optimal
policy. We also show that the overall convergence of DR-Off-PAC is doubly
robust to the approximation errors that depend only on the expressive power of
approximation functions. To the best of our knowledge, our study establishes
the first overall sample complexity analysis for a single time-scale off-policy
AC algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.12002</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.12002</id><submitter>Ecenaz Erdemir</submitter><version version="v1"><date>Wed, 24 Feb 2021 00:54:43 GMT</date><size>131kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 21:40:50 GMT</date><size>301kb</size><source_type>D</source_type></version><title>Adversarial Robustness with Non-uniform Perturbations</title><authors>Ecenaz Erdemir, Jeffrey Bickford, Luca Melis and Sergul Aydore</authors><categories>cs.LG cs.AI cs.CR stat.ML</categories><comments>21 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness of machine learning models is critical for security related
applications, where real-world adversaries are uniquely focused on evading
neural network based detectors. Prior work mainly focus on crafting adversarial
examples (AEs) with small uniform norm-bounded perturbations across features to
maintain the requirement of imperceptibility. However, uniform perturbations do
not result in realistic AEs in domains such as malware, finance, and social
networks. For these types of applications, features typically have some
semantically meaningful dependencies. The key idea of our proposed approach is
to enable non-uniform perturbations that can adequately represent these feature
dependencies during adversarial training. We propose using characteristics of
the empirical data distribution, both on correlations between the features and
the importance of the features themselves. Using experimental datasets for
malware classification, credit risk prediction, and spam detection, we show
that our approach is more robust to real-world attacks. Finally, we present
robustness certification utilizing non-uniform perturbation bounds, and show
that non-uniform bounds achieve better certification.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.12668</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.12668</id><submitter>Hiroyasu Tsukamoto</submitter><version version="v1"><date>Thu, 25 Feb 2021 03:47:15 GMT</date><size>1740kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 22:56:18 GMT</date><size>2717kb</size><source_type>D</source_type></version><title>Learning-based Robust Motion Planning with Guaranteed Stability: A
  Contraction Theory Approach</title><authors>Hiroyasu Tsukamoto and Soon-Jo Chung</authors><categories>cs.RO cs.AI cs.LG cs.SY eess.SY</categories><comments>IEEE Robotics and Automation Letters (RA-L), Accepted June 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Learning-based Autonomous Guidance with RObustness and
Stability guarantees (LAG-ROS), which provides machine learning-based nonlinear
motion planners with formal robustness and stability guarantees, by designing a
differential Lyapunov function using contraction theory. LAG-ROS utilizes a
neural network to model a robust tracking controller independently of a target
trajectory, for which we show that the Euclidean distance between the target
and controlled trajectories is exponentially bounded linearly in the learning
error, even under the existence of bounded external disturbances. We also
present a convex optimization approach that minimizes the steady-state bound of
the tracking error to construct the robust control law for neural network
training. In numerical simulations, it is demonstrated that the proposed method
indeed possesses superior properties of robustness and nonlinear stability
resulting from contraction theory, whilst retaining the computational
efficiency of existing learning-based motion planners.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.12827</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.12827</id><submitter>Maura Pintor</submitter><version version="v1"><date>Thu, 25 Feb 2021 12:56:26 GMT</date><size>3838kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 14:33:12 GMT</date><size>17571kb</size><source_type>D</source_type></version><title>Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints</title><authors>Maura Pintor, Fabio Roli, Wieland Brendel, Battista Biggio</authors><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating adversarial robustness amounts to finding the minimum perturbation
needed to have an input sample misclassified. The inherent complexity of the
underlying optimization requires current gradient-based attacks to be carefully
tuned, initialized, and possibly executed for many computationally-demanding
iterations, even if specialized to a given perturbation model. In this work, we
overcome these limitations by proposing a fast minimum-norm (FMN) attack that
works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$),
is robust to hyperparameter choices, does not require adversarial starting
points, and converges within few lightweight steps. It works by iteratively
finding the sample misclassified with maximum confidence within an
$\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to
minimize the distance of the current sample to the decision boundary. Extensive
experiments show that FMN significantly outperforms existing attacks in terms
of convergence speed and computation time, while reporting comparable or even
smaller perturbation sizes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.12959</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.12959</id><submitter>Xi Wang</submitter><version version="v1"><date>Wed, 24 Feb 2021 12:35:43 GMT</date><size>6174kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 14:03:06 GMT</date><size>9572kb</size><source_type>D</source_type></version><title>Undefined class-label detection vs out-of-distribution detection</title><authors>Xi Wang, Laurence Aitchison</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new problem, that of undefined class-label (UCL) detection.
For instance, if we try to classify an image of a radio as cat vs dog, there
will be no well-defined class label. In contrast, in out-of-distribution (OOD)
detection, we are interested in the related but different problem of
identifying regions of the input space with little training data, which might
result in poor classifier performance. This difference is critical: it is quite
possible for there to be a region of the input space where little training data
is available but where class-labels are well-defined. Likewise, there may be
regions with lots of training data, but without well-defined class-labels
(though in practice this would often be the result of a bug in the labelling
pipeline). We note that certain methods originally intended to detect OOD
inputs might actually be detecting UCL points and develop a method for training
on UCL points based on a generative model of data-curation originally used to
explain the cold posterior effect in Bayesian neural networks. This approach
gives superior performance to past methods originally intended for OOD
detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.13068</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.13068</id><submitter>Marios Papachristou</submitter><version version="v1"><date>Thu, 25 Feb 2021 18:34:45 GMT</date><size>1788kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 19:58:08 GMT</date><size>1813kb</size><source_type>D</source_type></version><title>Truncated Log-concave Sampling with Reflective Hamiltonian Monte Carlo</title><authors>Apostolos Chalkis, Vissarion Fisikopoulos, Marios Papachristou, Elias
  Tsigaridas</authors><categories>cs.LG cs.CG</categories><comments>Preprint version</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We introduce Reflective Hamiltonian Monte Carlo (ReHMC), an HMC-based
algorithm, to sample from a log-concave distribution restricted to a convex
body. We prove that, starting from a warm start, the walk mixes to a
log-concave target distribution $\pi(x) \propto e^{-f(x)}$, where $f$ is
$L$-smooth and $m$-strongly-convex, within accuracy $\varepsilon$ after
$\widetilde O(\kappa d^2 \ell^2 \log (1 / \varepsilon))$ steps for a
well-rounded convex body where $\kappa = L / m$ is the condition number of the
negative log-density, $d$ is the dimension, $\ell$ is an upper bound on the
number of reflections, and $\varepsilon$ is the accuracy parameter. We also
developed an efficient open source implementation of ReHMC and we performed an
experimental study on various high-dimensional data-sets. The experiments
suggest that ReHMC outperfroms Hit-and-Run and Coordinate-Hit-and-Run regarding
the time it needs to produce an independent sample and introduces practical
truncated sampling in thousands of dimensions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.13130</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.13130</id><submitter>Teodoro Alamo</submitter><version version="v1"><date>Thu, 25 Feb 2021 19:12:18 GMT</date><size>205kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 16:57:18 GMT</date><size>206kb</size><source_type>D</source_type></version><title>Data-Driven Methods for Present and Future Pandemics: Monitoring,
  Modelling and Managing</title><authors>Teodoro Alamo, Daniel G. Reina, Pablo Mill\'an Gata, Victor M.
  Preciado, Giulia Giordano</authors><categories>eess.SY cs.SY q-bio.PE</categories><comments>24 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2006.01731</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey analyses the role of data-driven methodologies for pandemic
modelling and control. We provide a roadmap from the access to epidemiological
data sources to the control of epidemic phenomena. We review the available
methodologies and discuss the challenges in the development of data-driven
strategies to combat the spreading of infectious diseases. Our aim is to bring
together several different disciplines required to provide a holistic approach
to epidemic analysis, such as data science, epidemiology, and
systems-and-control theory. A 3M-analysis is presented, whose three pillars
are: Monitoring, Modelling and Managing. The focus is on the potential of
data-driven schemes to address three different challenges raised by a pandemic:
(i) monitoring the epidemic evolution and assessing the effectiveness of the
adopted countermeasures; (ii) modelling and forecasting the spread of the
epidemic; (iii) making timely decisions to manage, mitigate and suppress the
contagion. For each step of this roadmap, we review consolidated theoretical
approaches (including data-driven methodologies that have been shown to be
successful in other contexts) and discuss their application to past or present
epidemics, such as Covid-19, as well as their potential application to future
epidemics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.13177</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.13177</id><submitter>Yixin Lin</submitter><version version="v1"><date>Thu, 25 Feb 2021 21:09:12 GMT</date><size>14327kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 00:30:56 GMT</date><size>5723kb</size><source_type>D</source_type></version><title>Efficient and Interpretable Robot Manipulation with Graph Neural
  Networks</title><authors>Yixin Lin, Austin S. Wang, Akshara Rai</authors><categories>cs.RO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many manipulation tasks can be naturally cast as a sequence of spatial
relationships and constraints between objects. We aim to discover and scale
these task-specific spatial relationships by representing manipulation tasks as
operations over graphs. To do this, we pose manipulating a large, variable
number of objects as a probabilistic classification problem over actions,
objects and goals, learned using graph neural networks (GNNs). Our formulation
first transforms the environment into a graph representation, then applies a
trained GNN policy to predict which object to manipulate towards which goal
state. Our GNN policies are trained using very few expert demonstrations on
simple tasks, and exhibit generalization over number and configurations of
objects in the environment and even to new, more complex tasks, while providing
interpretable explanations for their decision-making. We present experiments
which show that a single learned GNN policy can solve a variety of long-horizon
blockstacking and rearrangement tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.13235</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.13235</id><submitter>Ying-Cheng Lai</submitter><version version="v1"><date>Thu, 25 Feb 2021 23:53:51 GMT</date><size>2662kb</size><source_type>D</source_type></version><title>Adaptable Hamiltonian neural networks</title><authors>Chen-Di Han, Bryan Glaz, Mulugeta Haile, and Ying-Cheng Lai</authors><categories>cs.LG nlin.CD</categories><comments>12 pages, 9 figures</comments><journal-ref>Phys. Rev. Research 3, 023156 (2021)</journal-ref><doi>10.1103/PhysRevResearch.3.023156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of research in exploiting machine learning to predict
chaotic systems has revived a recent interest in Hamiltonian Neural Networks
(HNNs) with physical constraints defined by the Hamilton's equations of motion,
which represent a major class of physics-enhanced neural networks. We introduce
a class of HNNs capable of adaptable prediction of nonlinear physical systems:
by training the neural network based on time series from a small number of
bifurcation-parameter values of the target Hamiltonian system, the HNN can
predict the dynamical states at other parameter values, where the network has
not been exposed to any information about the system at these parameter values.
The architecture of the HNN differs from the previous ones in that we
incorporate an input parameter channel, rendering the HNN parameter--cognizant.
We demonstrate, using paradigmatic Hamiltonian systems, that training the HNN
using time series from as few as four parameter values bestows the neural
machine with the ability to predict the state of the target system in an entire
parameter interval. Utilizing the ensemble maximum Lyapunov exponent and the
alignment index as indicators, we show that our parameter-cognizant HNN can
successfully predict the route of transition to chaos. Physics-enhanced machine
learning is a forefront area of research, and our adaptable HNNs provide an
approach to understanding machine learning with broad applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.13400</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.13400</id><submitter>Kailun Yang</submitter><version version="v1"><date>Fri, 26 Feb 2021 11:22:40 GMT</date><size>1422kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 09:09:44 GMT</date><size>2108kb</size><source_type>D</source_type></version><title>Panoramic annular SLAM with loop closure and global optimization</title><authors>Hao Chen, Weijian Hu, Kailun Yang, Jian Bai, Kaiwei Wang</authors><categories>cs.RO cs.CV eess.IV</categories><comments>Accepted to Applied Optics. 12 pages, 11 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose panoramic annular simultaneous localization and
mapping (PA-SLAM), a visual SLAM system based on panoramic annular lens. A
hybrid point selection strategy is put forward in the tracking front-end, which
ensures repeatability of keypoints and enables loop closure detection based on
the bag-of-words approach. Every detected loop candidate is verified
geometrically and the $Sim(3)$ relative pose constraint is estimated to perform
pose graph optimization and global bundle adjustment in the back-end. A
comprehensive set of experiments on real-world datasets demonstrates that the
hybrid point selection strategy allows reliable loop closure detection, and the
accumulated error and scale drift have been significantly reduced via global
optimization, enabling PA-SLAM to reach state-of-the-art accuracy while
maintaining high robustness and efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.13640</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.13640</id><submitter>Jakob Heiss</submitter><version version="v1"><date>Fri, 26 Feb 2021 18:34:43 GMT</date><size>34818kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Mar 2021 16:53:19 GMT</date><size>4922kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 22:00:03 GMT</date><size>8449kb</size><source_type>D</source_type></version><title>NOMU: Neural Optimization-based Model Uncertainty</title><authors>Jakob Heiss, Jakob Weissteiner, Hanna Wutte, Sven Seuken, Josef
  Teichmann</authors><categories>cs.LG cs.AI stat.ML</categories><comments>9 pages + appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study methods for estimating model uncertainty for neural networks (NNs).
To isolate the effect of model uncertainty, we focus on a noiseless setting
with scarce training data. We introduce five important desiderata regarding
model uncertainty that any method should satisfy. However, we find that
established benchmarks often fail to reliably capture some of these desiderata,
even those that are required by Bayesian theory. To address this, we introduce
a new approach for capturing model uncertainty for NNs, which we call Neural
Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design
a network architecture consisting of two connected sub-NNs, one for model
prediction and one for model uncertainty, and to train it using a
carefully-designed loss function. Importantly, our design enforces that NOMU
satisfies our five desiderata. Due to its modular architecture, NOMU can
provide model uncertainty for any given (previously trained) NN if given access
to its training data. We first experimentally study noiseless regression with
scarce training data to highlight the deficiencies of the established
benchmarks. Finally, we study the important task of Bayesian optimization (BO)
with costly evaluations, where good model uncertainty estimates are essential.
Our results show that NOMU performs as well or better than state-of-the-art
benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00006</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00006</id><submitter>Yong-Yeon Jo Ph.D.</submitter><version version="v1"><date>Sun, 28 Feb 2021 09:23:17 GMT</date><size>953kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 00:57:21 GMT</date><size>3048kb</size><source_type>D</source_type></version><title>Electrocardiogram synthesis</title><authors>Yong-Yeon Jo and Joon-Myoung Kwon</authors><categories>eess.SP cs.LG</categories><comments>temporary version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The electrocardiogram (ECG) records electrical signals in a non-invasive way
to observe the condition of the heart, typically looking at the heart from 12
different directions. Several types of the cardiac disease are diagnosed by
using 12-lead ECGs Recently, various wearable devices have enabled immediate
access to the ECG without the use of wieldy equipment. However, they only
provide ECGs with a couple of leads. This results in an inaccurate diagnosis of
cardiac disease due to lacking of required leads. We propose a deep generative
model for ECG synthesis from two asynchronous leads to ten leads. It first
represents a heart condition referring to two leads, and then generates ten
leads based on the represented heart condition. Both the rhythm and amplitude
of leads generated resemble those of the original ones, while the technique
removes noise and the baseline wander appearing in the original leads. As a
data augmentation method, our model improves the classification performance of
models compared with models using ECGs with only one or two leads.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00222</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00222</id><submitter>Laurence Aitchison</submitter><version version="v1"><date>Sat, 27 Feb 2021 14:06:29 GMT</date><size>859kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 14:56:18 GMT</date><size>207kb</size><source_type>D</source_type></version><title>Variational Laplace for Bayesian neural networks</title><authors>Ali Unlu, Laurence Aitchison</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop variational Laplace for Bayesian neural networks (BNNs) which
exploits a local approximation of the curvature of the likelihood to estimate
the ELBO without the need for stochastic sampling of the neural-network
weights. The Variational Laplace objective is simple to evaluate, as it is (in
essence) the log-likelihood, plus weight-decay, plus a squared-gradient
regularizer. Variational Laplace gave better test performance and expected
calibration errors than maximum a-posteriori inference and standard
sampling-based variational inference, despite using the same variational
approximate posterior. Finally, we emphasise care needed in benchmarking
standard VI as there is a risk of stopping before the variance parameters have
converged. We show that early-stopping can be avoided by increasing the
learning rate for the variance parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00368</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00368</id><submitter>Yiling Jia</submitter><version version="v1"><date>Sun, 28 Feb 2021 01:16:55 GMT</date><size>3543kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Mar 2021 05:42:50 GMT</date><size>0kb</size><source_type>I</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 22:07:14 GMT</date><size>3543kb</size><source_type>D</source_type></version><title>PairRank: Online Pairwise Learning to Rank by Divide-and-Conquer</title><authors>Yiling Jia, Huazheng Wang, Stephen Guo, Hongning Wang</authors><categories>cs.LG cs.IR</categories><comments>The Web Conference 2021</comments><doi>10.1145/3442381.3449972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Learning to Rank (OL2R) eliminates the need of explicit relevance
annotation by directly optimizing the rankers from their interactions with
users. However, the required exploration drives it away from successful
practices in offline learning to rank, which limits OL2R's empirical
performance and practical applicability. In this work, we propose to estimate a
pairwise learning to rank model online. In each round, candidate documents are
partitioned and ranked according to the model's confidence on the estimated
pairwise rank order, and exploration is only performed on the uncertain pairs
of documents, i.e., \emph{divide-and-conquer}. Regret directly defined on the
number of mis-ordered pairs is proven, which connects the online solution's
theoretical convergence with its expected ranking performance. Comparisons
against an extensive list of OL2R baselines on two public learning to rank
benchmark datasets demonstrate the effectiveness of the proposed solution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00397</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00397</id><submitter>Tianlong Chen</submitter><version version="v1"><date>Sun, 28 Feb 2021 05:20:29 GMT</date><size>8617kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 04:57:31 GMT</date><size>9919kb</size><source_type>D</source_type></version><title>Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery
  Ticket Perspective</title><authors>Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, Zhangyang Wang</authors><categories>cs.LG cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training generative adversarial networks (GANs) with limited real image data
generally results in deteriorated performance and collapsed models. To conquer
this challenge, we are inspired by the latest observations, that one can
discover independently trainable and highly sparse subnetworks (a.k.a., lottery
tickets) from GANs. Treating this as an inductive prior, we suggest a brand-new
angle towards data-efficient GAN training: by first identifying the lottery
ticket from the original GAN using the small training set of real images; and
then focusing on training that sparse subnetwork by re-using the same set. Both
steps have lower complexity and are more data-efficient to train. We find our
coordinated framework to offer orthogonal gains to existing real image data
augmentation methods, and we additionally offer a new feature-level
augmentation that can be applied together with them. Comprehensive experiments
endorse the effectiveness of our proposed framework, across various GAN
architectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10,
CIFAR-100, Tiny-ImageNet, and ImageNet). Our training framework also displays
powerful few-shot generalization ability, i.e., generating high-fidelity images
by training from scratch with just 100 real images, without any pre-training.
Codes are available at:
https://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00497</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00497</id><submitter>Aryan Asadian</submitter><version version="v1"><date>Sun, 28 Feb 2021 12:52:52 GMT</date><size>176kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 13:20:57 GMT</date><size>164kb</size><source_type>D</source_type></version><title>Distilling Knowledge via Intermediate Classifiers</title><authors>Aryan Asadian, Amirali Salehi-Abari</authors><categories>cs.LG cs.AI cs.CV</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crux of knowledge distillation is to effectively train a resource-limited
student model with the guide of a pre-trained larger teacher model. However,
when there is a large difference between the model complexities of teacher and
student (i.e., capacity gap), knowledge distillation loses its strength in
transferring knowledge from the teacher to the student, thus training a weaker
student. To mitigate the impact of the capacity gap, we introduce knowledge
distillation via intermediate heads. By extending the intermediate layers of
the teacher (at various depths) with classifier heads, we cheaply acquire a
cohort of heterogeneous pre-trained teachers. The intermediate classifier heads
can all together be efficiently learned while freezing the backbone of the
pre-trained teacher. The cohort of teachers (including the original teacher)
co-teach the student simultaneously. Our experiments on various teacher-student
pairs and datasets have demonstrated that the proposed approach outperforms the
canonical knowledge distillation approach and its extensions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00737</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00737</id><submitter>Gwonsoo Che</submitter><version version="v1"><date>Mon, 1 Mar 2021 04:05:11 GMT</date><size>1154kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 15:57:24 GMT</date><size>4935kb</size><source_type>D</source_type></version><title>Meta-Learning an Inference Algorithm for Probabilistic Programs</title><authors>Gwonsoo Che and Hongseok Yang</authors><categories>cs.LG</categories><comments>(1) Improved related work; (2) improved the empirical evaluation with
  additional experiments and analyses for extrapolation and comparisons with
  HMC; and (3) explained the rationale for the choice of the probabilistic
  programming language</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a meta-algorithm for learning a posterior-inference algorithm for
restricted probabilistic programs. Our meta-algorithm takes a training set of
probabilistic programs that describe models with observations, and attempts to
learn an efficient method for inferring the posterior of a similar program. A
key feature of our approach is the use of what we call a white-box inference
algorithm that extracts information directly from model descriptions
themselves, given as programs. Concretely, our white-box inference algorithm is
equipped with multiple neural networks, one for each type of atomic command,
and computes an approximate posterior of a given probabilistic program by
analysing individual atomic commands in the program using these networks. The
parameters of these networks are then learnt from a training set by our
meta-algorithm. We empirically demonstrate that the learnt inference algorithm
generalises well to unseen programs in terms of both interpolation and
extrapolation, and report cases where our approach may be preferable to a
state-of-the-art inference algorithm such as HMC. The overall results show the
promise as well as remaining challenges of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00823</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00823</id><submitter>Junyang Lin</submitter><version version="v1"><date>Mon, 1 Mar 2021 07:46:27 GMT</date><size>14750kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 2 Mar 2021 06:03:16 GMT</date><size>14750kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 22 Apr 2021 04:14:00 GMT</date><size>15414kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 29 May 2021 09:16:05 GMT</date><size>15414kb</size><source_type>D</source_type></version><title>M6: A Chinese Multimodal Pretrainer</title><authors>Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang,
  Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou,
  Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin
  Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, Hongxia Yang</authors><categories>cs.CL</categories><comments>12 pages, technical report. Extension of paper &quot;M6&quot; accepted to KDD
  2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we construct the largest dataset for multimodal pretraining in
Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide
range of domains. We propose a cross-modal pretraining method called M6,
referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for
unified pretraining on the data of single modality and multiple modalities. We
scale the model size up to 10 billion and 100 billion parameters, and build the
largest pretrained model in Chinese. We apply the model to a series of
downstream applications, and demonstrate its outstanding performance in
comparison with strong baselines. Furthermore, we specifically design a
downstream task of text-guided image generation, and show that the finetuned M6
can create high-quality images with high resolution and abundant details.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.01924</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.01924</id><submitter>Naser Damer</submitter><version version="v1"><date>Tue, 2 Mar 2021 18:36:01 GMT</date><size>3147kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 16:41:01 GMT</date><size>3061kb</size><source_type>D</source_type></version><title>Masked Face Recognition: Human vs. Machine</title><authors>Naser Damer, Fadi Boutros, Marius S\&quot;u{\ss}milch, Meiling Fang,
  Florian Kirchbuchner, Arjan Kuijper</authors><categories>cs.CV cs.CY</categories><comments>Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent COVID-19 pandemic has increased the focus on hygienic and
contactless identity verification methods. However, the pandemic led to the
wide use of face masks, essential to keep the pandemic under control. The
effect of wearing a mask on face recognition in a collaborative environment is
currently sensitive yet understudied issue. Recent reports have tackled this by
evaluating the masked probe effect on the performance of automatic face
recognition solutions. However, such solutions can fail in certain processes,
leading to performing the verification task by a human expert. This work
provides a joint evaluation and in-depth analyses of the face verification
performance of human experts in comparison to state-of-the-art automatic face
recognition solutions. This involves an extensive evaluation with 12 human
experts and 4 automatic recognition solutions. The study concludes with a set
of take-home messages on different aspects of the correlation between the
verification behavior of human and machine.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.01948</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.01948</id><submitter>Robert Dadashi</submitter><version version="v1"><date>Tue, 2 Mar 2021 18:59:02 GMT</date><size>10163kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 08:33:42 GMT</date><size>10121kb</size><source_type>D</source_type></version><title>Offline Reinforcement Learning with Pseudometric Learning</title><authors>Robert Dadashi, Shideh Rezaeifar, Nino Vieillard, L\'eonard Hussenot,
  Olivier Pietquin, Matthieu Geist</authors><categories>cs.LG</categories><comments>ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offline Reinforcement Learning methods seek to learn a policy from logged
transitions of an environment, without any interaction. In the presence of
function approximation, and under the assumption of limited coverage of the
state-action space of the environment, it is necessary to enforce the policy to
visit state-action pairs close to the support of logged transitions. In this
work, we propose an iterative procedure to learn a pseudometric (closely
related to bisimulation metrics) from logged transitions, and use it to define
this notion of closeness. We show its convergence and extend it to the function
approximation setting. We then use this pseudometric to define a new lookup
based bonus in an actor-critic algorithm: PLOFF. This bonus encourages the
actor to stay close, in terms of the defined pseudometric, to the support of
logged transitions. Finally, we evaluate the method on hand manipulation and
locomotion tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.02136</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.02136</id><submitter>Margaret Chapman Dr.</submitter><version version="v1"><date>Wed, 3 Mar 2021 02:34:28 GMT</date><size>9794kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 19:23:27 GMT</date><size>595kb</size><source_type>D</source_type></version><title>Toward a Scalable Upper Bound for a CVaR-LQ Problem</title><authors>Margaret P. Chapman and Laurent Lessard</authors><categories>eess.SY cs.SY</categories><comments>accepted by IEEE Control Systems Letters, June 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a linear-quadratic, optimal control problem on a discrete, finite
time horizon with distributional ambiguity, in which the cost is assessed via
Conditional Value-at-Risk (CVaR). We take steps toward deriving a scalable
dynamic programming approach to upper-bound the optimal value function for this
problem. This dynamic program yields a novel, tunable risk-averse control
policy, which we compare to existing state-of-the-art methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.02630</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.02630</id><submitter>Rafael Poyiadzi</submitter><version version="v1"><date>Wed, 3 Mar 2021 19:03:06 GMT</date><size>785kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 21:59:18 GMT</date><size>966kb</size><source_type>D</source_type></version><title>Hypothesis Testing for Class-Conditional Label Noise</title><authors>Rafael Poyiadzi, Weisong Yang, Niall Twomey, Raul Santos-Rodriguez</authors><categories>cs.LG</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide machine learning practitioners with tools to answer
the question: is there class-conditional noise in my labels? In particular, we
present hypothesis tests to check whether a given dataset of instance-label
pairs has been corrupted with class-conditional label noise, as opposed to
uniform label noise, with the former biasing learning, while the latter --
under mild conditions -- does not. The outcome of these tests can then be used
in conjunction with other information to assess further steps. While previous
works explore the direct estimation of the noise rates, this is known to be
hard in practice and does not offer a real understanding of how trustworthy the
estimates are. These methods typically require anchor points -- examples whose
true posterior is either 0 or 1. Differently, in this paper we assume we have
access to a set of anchor points whose true posterior is approximately 1/2. The
proposed hypothesis tests are built upon the asymptotic properties of Maximum
Likelihood Estimators for Logistic Regression models. We establish the main
properties of the tests, including a theoretical and empirical analysis of the
dependence of the power on the test on the training sample size, the number of
anchor points, the difference of the noise rates and the use of relaxed
anchors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.02760</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.02760</id><submitter>Andrew Bradley</submitter><version version="v1"><date>Wed, 3 Mar 2021 23:49:02 GMT</date><size>1304kb</size></version><version version="v2"><date>Fri, 5 Mar 2021 20:41:10 GMT</date><size>1304kb</size></version><version version="v3"><date>Mon, 31 May 2021 23:22:55 GMT</date><size>1486kb</size></version><title>Worsening Perception: Real-time Degradation of Autonomous Vehicle
  Perception Performance for Simulation of Adverse Weather Conditions</title><authors>Ivan Fursa, Elias Fandi, Valentina Musat, Jacob Culley, Enric Gil,
  Izzedin Teeti, Louise Bilous, Isaac Vander Sluis, Alexander Rast and Andrew
  Bradley</authors><categories>cs.RO cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Autonomous vehicles rely heavily upon their perception subsystems to see the
environment in which they operate. Unfortunately, the effect of variable
weather conditions presents a significant challenge to object detection
algorithms, and thus it is imperative to test the vehicle extensively in all
conditions which it may experience. However, development of robust autonomous
vehicle subsystems requires repeatable, controlled testing - while real weather
is unpredictable and cannot be scheduled. Real-world testing in adverse
conditions is an expensive and time-consuming task, often requiring access to
specialist facilities. Simulation is commonly relied upon as a substitute, with
increasingly visually realistic representations of the real-world being
developed. In the context of the complete autonomous vehicle control pipeline,
subsystems downstream of perception need to be tested with accurate recreations
of the perception system output, rather than focusing on subjective visual
realism of the input - whether in simulation or the real world. This study
develops the untapped potential of a lightweight weather augmentation method in
an autonomous racing vehicle - focusing not on visual accuracy, but rather the
effect upon perception subsystem performance in real time. With minimal
adjustment, the prototype developed in this study can replicate the effects of
water droplets on the camera lens, and fading light conditions. This approach
introduces a latency of less than 8 ms using compute hardware well suited to
being carried in the vehicle - rendering it ideal for real-time implementation
that can be run during experiments in simulation, and augmented reality testing
in the real world.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.02847</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.02847</id><submitter>Bingzhe Li</submitter><version version="v1"><date>Thu, 4 Mar 2021 06:09:57 GMT</date><size>950kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 18:56:42 GMT</date><size>1058kb</size><source_type>D</source_type></version><title>IMG-DNA: Approximate DNA Storage for Images</title><authors>Bingzhe Li, Li Ou, David Du</authors><categories>cs.ET</categories><comments>11 pages, 12 figures</comments><doi>10.1145/3456727.3463771</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Deoxyribonucleic Acid (DNA) as a storage medium with high density and
long-term preservation properties can satisfy the requirement of archival
storage for rapidly increased digital volume. The read and write processes of
DNA storage are error-prone. Images widely used in social media have the
properties of fault tolerance which are well fitted to the DNA storage.
However, prior work simply investigated the feasibility of DNA storage storing
different types of data and simply store images in DNA storage, which did not
fully investigate the fault-tolerant potential of images in the DNA storage. In
this paper, we proposed a new image-based DNA system called IMG-DNA, which can
efficiently store images in DNA storage with improved DNA storage robustness.
First, a new DNA architecture is proposed to fit JPEG-based images and improve
the image's robustness in DNA storage. Moreover, barriers inserted in DNA
sequences efficiently prevent error propagation in images of DNA storage. The
experimental results indicate that the proposed IMG-DNA achieves much higher
fault-tolerant than prior work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.03000</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.03000</id><submitter>Paula Harder</submitter><version version="v1"><date>Thu, 4 Mar 2021 12:48:28 GMT</date><size>225kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 09:35:41 GMT</date><size>226kb</size><source_type>D</source_type></version><title>SpectralDefense: Detecting Adversarial Attacks on CNNs in the Fourier
  Domain</title><authors>Paula Harder, Franz-Josef Pfreundt, Margret Keuper, Janis Keuper</authors><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the success of convolutional neural networks (CNNs) in many computer
vision and image analysis tasks, they remain vulnerable against so-called
adversarial attacks: Small, crafted perturbations in the input images can lead
to false predictions. A possible defense is to detect adversarial examples. In
this work, we show how analysis in the Fourier domain of input images and
feature maps can be used to distinguish benign test samples from adversarial
images. We propose two novel detection methods: Our first method employs the
magnitude spectrum of the input images to detect an adversarial attack. This
simple and robust classifier can successfully detect adversarial perturbations
of three commonly used attack methods. The second method builds upon the first
and additionally extracts the phase of Fourier coefficients of feature-maps at
different layers of the network. With this extension, we are able to improve
adversarial detection rates compared to state-of-the-art detectors on five
different attack methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.03023</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.03023</id><submitter>Bi-Cheng Yan</submitter><version version="v1"><date>Thu, 4 Mar 2021 13:33:50 GMT</date><size>461kb</size></version><version version="v2"><date>Wed, 24 Mar 2021 12:38:42 GMT</date><size>461kb</size></version><version version="v3"><date>Mon, 29 Mar 2021 01:13:09 GMT</date><size>1199kb</size></version><version version="v4"><date>Tue, 1 Jun 2021 07:10:10 GMT</date><size>1361kb</size></version><title>End-to-End Mispronunciation Detection and Diagnosis From Raw Waveforms</title><authors>Bi-Cheng Yan and Berlin Chen</authors><categories>eess.AS cs.MM cs.SD</categories><comments>Preprint. Under review 5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mispronunciation detection and diagnosis (MDD) is designed to identify
pronunciation errors and provide instructive feedback to guide non-native
language learners, which is a core component in computer-assisted pronunciation
training (CAPT) systems. However, MDD often suffers from the data-sparsity
problem due to that collecting non-native data and the associated annotations
is time-consuming and labor-intensive. To address this issue, we explore a
fully end-to-end (E2E) neural model for MDD, which processes learners' speech
directly based on raw waveforms. Compared to conventional hand-crafted acoustic
features, raw waveforms retain more acoustic phenomena and potentially can help
neural networks discover better and more customized representations. To this
end, our MDD model adopts a co-called SincNet module to take input a raw
waveform and covert it to a suitable vector representation sequence. SincNet
employs the cardinal sine (sinc) function to implement learnable bandpass
filters, drawing inspiration from the convolutional neural network (CNN). By
comparison to CNN, SincNet has fewer parameters and is more amenable to human
interpretation. Extensive experiments are conducted on the L2-ARCTIC dataset,
which is a publicly-available non-native English speech corpus compiled for
research on CAPT. We find that the sinc filters of SincNet can be adapted
quickly for non-native language learners of different nationalities.
Furthermore, our model can achieve comparable mispronunciation detection
performance in relation to state-of-the-art E2E MDD models that take input the
standard handcrafted acoustic features. Besides that, our model also provides
considerable improvements on phone error rate (PER) and diagnosis accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.03027</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.03027</id><submitter>Praveen Tirupattur</submitter><version version="v1"><date>Thu, 4 Mar 2021 13:37:28 GMT</date><size>5830kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 5 Mar 2021 02:13:00 GMT</date><size>5829kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 16:19:41 GMT</date><size>5830kb</size><source_type>D</source_type></version><title>Modeling Multi-Label Action Dependencies for Temporal Action
  Localization</title><authors>Praveen Tirupattur, Kevin Duarte, Yogesh Rawat, Mubarak Shah</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Real-world videos contain many complex actions with inherent relationships
between action classes. In this work, we propose an attention-based
architecture that models these action relationships for the task of temporal
action localization in untrimmed videos. As opposed to previous works that
leverage video-level co-occurrence of actions, we distinguish the relationships
between actions that occur at the same time-step and actions that occur at
different time-steps (i.e. those which precede or follow each other). We define
these distinct relationships as action dependencies. We propose to improve
action localization performance by modeling these action dependencies in a
novel attention-based Multi-Label Action Dependency (MLAD)layer. The MLAD layer
consists of two branches: a Co-occurrence Dependency Branch and a Temporal
Dependency Branch to model co-occurrence action dependencies and temporal
action dependencies, respectively. We observe that existing metrics used for
multi-label classification do not explicitly measure how well action
dependencies are modeled, therefore, we propose novel metrics that consider
both co-occurrence and temporal dependencies between action classes. Through
empirical evaluation and extensive analysis, we show improved performance over
state-of-the-art methods on multi-label action localization
benchmarks(MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.03142</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.03142</id><submitter>Abhijeet Awasthi</submitter><version version="v1"><date>Thu, 4 Mar 2021 16:36:59 GMT</date><size>272kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 17:13:27 GMT</date><size>272kb</size><source_type>D</source_type></version><title>Error-driven Fixed-Budget ASR Personalization for Accented Speakers</title><authors>Abhijeet Awasthi, Aman Kansal, Sunita Sarawagi, Preethi Jyothi</authors><categories>cs.SD cs.CL eess.AS</categories><comments>In ICASSP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of personalizing ASR models while being constrained by a
fixed budget on recording speaker-specific utterances. Given a speaker and an
ASR model, we propose a method of identifying sentences for which the speaker's
utterances are likely to be harder for the given ASR model to recognize. We
assume a tiny amount of speaker-specific data to learn phoneme-level error
models which help us select such sentences. We show that speaker's utterances
on the sentences selected using our error model indeed have larger error rates
when compared to speaker's utterances on randomly selected sentences. We find
that fine-tuning the ASR model on the sentence utterances selected with the
help of error models yield higher WER improvements in comparison to fine-tuning
on an equal number of randomly selected sentence utterances. Thus, our method
provides an efficient way of collecting speaker utterances under budget
constraints for personalizing ASR models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.03288</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.03288</id><submitter>Vibhaalakshmi Sivaraman</submitter><version version="v1"><date>Thu, 4 Mar 2021 19:51:45 GMT</date><size>4286kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 20:52:38 GMT</date><size>4660kb</size><source_type>D</source_type></version><title>The Effect of Network Topology on Credit Network Throughput</title><authors>Vibhaalakshmi Sivaraman, Weizhao Tang, Shaileshh Bojja
  Venkatakrishnan, Giulia Fanti, Mohammad Alizadeh</authors><categories>cs.SI cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Credit networks rely on decentralized, pairwise trust relationships
(channels) to exchange money or goods. Credit networks arise naturally in many
financial systems, including the recent construct of payment channel networks
in blockchain systems. An important performance metric for these networks is
their transaction throughput. However, predicting the throughput of a credit
network is nontrivial. Unlike traditional communication channels, credit
channels can become imbalanced; they are unable to support more transactions in
a given direction once the credit limit has been reached. This potential for
imbalance creates a complex dependency between a network's throughput and its
topology, path choices, and the credit balances (state) on every channel. Even
worse, certain combinations of these factors can lead the credit network to
deadlocked states where no transactions can make progress. In this paper, we
study the relationship between the throughput of a credit network and its
topology and credit state. We show that the presence of deadlocks completely
characterizes a network's throughput sensitivity to different credit states.
Although we show that identifying deadlocks in an arbitrary topology is
NP-hard, we propose a peeling algorithm inspired by decoding algorithms for
erasure codes that upper bounds the severity of the deadlock. We use the
peeling algorithm as a tool to compare the performance of different topologies
as well as to aid in the synthesis of topologies robust to deadlocks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.03568</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.03568</id><submitter>Jiaye Teng</submitter><version version="v1"><date>Fri, 5 Mar 2021 09:53:10 GMT</date><size>261kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:36:34 GMT</date><size>253kb</size><source_type>D</source_type></version><title>Can Pretext-Based Self-Supervised Learning Be Boosted by Downstream
  Data? A Theoretical Analysis</title><authors>Jiaye Teng, Weiran Huang, Haowei He</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pretext-based self-supervised learning aims to learn the semantic
representation via a handcrafted pretext task over unlabeled data and then use
the learned representation for downstream prediction tasks. It is proved that
pretext-based self-supervised learning can effectively reduce the sample
complexity of downstream tasks under Conditional Independence (CI) between the
components of the pretext task conditional on the downstream label. However,
the downstream sample complexity will get much worse if the CI condition does
not hold. One interesting question is whether we can make the CI condition hold
by using downstream data to refine the unlabeled data to boost self-supervised
learning. At first glance, one might think that seeing downstream data in
advance would always boost the downstream performance. However, we show that it
is not intuitively true and point out that in some cases, it will hurt the
final performance instead. In particular, we prove both model-free and
model-dependent lower bounds of the number of downstream samples used for data
refinement. Moreover, we conduct several experiments on both synthetic and
real-world datasets to verify our theoretical results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04004</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04004</id><submitter>Sho Sakaino Prof.</submitter><version version="v1"><date>Sat, 6 Mar 2021 01:46:26 GMT</date><size>6850kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 9 Mar 2021 11:42:04 GMT</date><size>6852kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 09:51:59 GMT</date><size>5922kb</size><source_type>D</source_type></version><title>Bilateral Control-Based Imitation Learning for Velocity-Controlled Robot</title><authors>Sho Sakaino</authors><categories>cs.RO cs.SY eess.SY</categories><comments>6pages, 10 figures, submitted for ISIE2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning is now playing important role in robotic object
manipulation. In addition, force control is necessary for manipulating various
objects to achieve robustness against perturbations of configurations and
stiffness. The author's group revealed that fast and dynamic object
manipulation with force control can be obtained by bilateral control-based
imitation learning. However, the method is applicable only in robots that can
control torque, while it is not applicable in robots that can only follow
position or velocity commands like many commercially available robots. Then, in
this research, a way to implement bilateral control-based imitation learning to
velocity-controlled robots is proposed. The validity of the proposed method is
experimentally verified by a mopping task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04205</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04205</id><submitter>Shaddin Dughmi</submitter><version version="v1"><date>Sat, 6 Mar 2021 22:46:29 GMT</date><size>450kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 00:31:59 GMT</date><size>102kb</size></version><title>Matroid Secretary is Equivalent to Contention Resolution</title><authors>Shaddin Dughmi</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the matroid secretary problem is equivalent to correlated
contention resolution in the online random-order model. Specifically, the
matroid secretary conjecture is true if and only if every matroid admits an
online random-order contention resolution scheme which, given an arbitrary
(possibly correlated) prior distribution over subsets of the ground set,
matches the balance ratio of the best offline scheme for that distribution up
to a constant. We refer to such a scheme as universal. Our result indicates
that the core challenge of the matroid secretary problem lies in resolving
contention for positively correlated inputs, in particular when the positive
correlation is benign in as much as offline contention resolution is concerned.
  Our result builds on our previous work which establishes one direction of
this equivalence, namely that the secretary conjecture implies universal
random-order contention resolution, as well as a weak converse, which derives a
matroid secretary algorithm from a random-order contention resolution scheme
with only partial knowledge of the distribution. It is this weak converse that
we strengthen in this paper: We show that universal random-order contention
resolution for matroids, in the usual setting of a fully known prior
distribution, suffices to resolve the matroid secretary conjecture in the
affirmative.
  Our proof is the composition of three reductions. First, we use duality
arguments to reduce the matroid secretary problem to the matroid prophet
secretary problem with arbitrarily correlated distributions. Second, we
introduce a generalization of contention resolution we term labeled contention
resolution, to which we reduce the correlated matroid prophet secretary
problem. Finally, we combine duplication of elements with limiting arguments to
reduce labeled contention resolution to classical contention resolution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04258</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04258</id><submitter>Yunxiang Li</submitter><version version="v1"><date>Sun, 7 Mar 2021 04:28:09 GMT</date><size>3754kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 05:26:44 GMT</date><size>0kb</size><source_type>I</source_type></version><title>High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on
  Polynomial Curve Fitting with Landmark Detection</title><authors>Yunxiang Li, Yifan Zhang, Yaqi Wang, Shuai Wang, Ruizi Peng, Kai Tang,
  Qianni Zhang, Jun Wang, Qun Jin, Lingling Sun</authors><categories>cs.CV</categories><comments>There are some mistakes in the description of the maximum number of
  the shortest distances algorithm (MNSDA)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the most economical and routine auxiliary examination in the diagnosis of
root canal treatment, oral X-ray has been widely used by stomatologists. It is
still challenging to segment the tooth root with a blurry boundary for the
traditional image segmentation method. To this end, we propose a model for
high-resolution segmentation based on polynomial curve fitting with landmark
detection (HS-PCL). It is based on detecting multiple landmarks evenly
distributed on the edge of the tooth root to fit a smooth polynomial curve as
the segmentation of the tooth root, thereby solving the problem of fuzzy edge.
In our model, a maximum number of the shortest distances algorithm (MNSDA) is
proposed to automatically reduce the negative influence of the wrong landmarks
which are detected incorrectly and deviate from the tooth root on the fitting
result. Our numerical experiments demonstrate that the proposed approach not
only reduces Hausdorff95 (HD95) by 33.9% and Average Surface Distance (ASD) by
42.1% compared with the state-of-the-art method, but it also achieves excellent
results on the minute quantity of datasets, which greatly improves the
feasibility of automatic root canal therapy evaluation by medical image
computing.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04514</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04514</id><submitter>Cecilia Summers</submitter><version version="v1"><date>Mon, 8 Mar 2021 02:28:18 GMT</date><size>103kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 00:54:16 GMT</date><size>2268kb</size><source_type>D</source_type></version><title>Nondeterminism and Instability in Neural Network Optimization</title><authors>Cecilia Summers, Michael J. Dinneen</authors><categories>cs.LG</categories><comments>To be published in ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nondeterminism in neural network optimization produces uncertainty in
performance, making small improvements difficult to discern from run-to-run
variability. While uncertainty can be reduced by training multiple model
copies, doing so is time-consuming, costly, and harms reproducibility. In this
work, we establish an experimental protocol for understanding the effect of
optimization nondeterminism on model diversity, allowing us to isolate the
effects of a variety of sources of nondeterminism. Surprisingly, we find that
all sources of nondeterminism have similar effects on measures of model
diversity. To explain this intriguing fact, we identify the instability of
model training, taken as an end-to-end procedure, as the key determinant. We
show that even one-bit changes in initial parameters result in models
converging to vastly different values. Last, we propose two approaches for
reducing the effects of instability on run-to-run variability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04612</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04612</id><submitter>Bohao Li</submitter><version version="v1"><date>Mon, 8 Mar 2021 09:04:03 GMT</date><size>5581kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 10 Mar 2021 05:45:52 GMT</date><size>5756kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 04:55:57 GMT</date><size>2120kb</size><source_type>D</source_type></version><title>Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object
  Detection</title><authors>Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, Qixiang Ye</authors><categories>cs.CV</categories><comments>This paper has been modified by the author due to errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Few-shot object detection has made substantial progressby representing novel
class objects using the feature representation learned upon a set of base class
objects. However,an implicit contradiction between novel class classification
and representation is unfortunately ignored. On the one hand, to achieve
accurate novel class classification, the distributions of either two base
classes must be far away fromeach other (max-margin). On the other hand, to
precisely represent novel classes, the distributions of base classes should be
close to each other to reduce the intra-class distance of novel classes
(min-margin). In this paper, we propose a class margin equilibrium (CME)
approach, with the aim to optimize both feature space partition and novel class
reconstruction in a systematic way. CME first converts the few-shot detection
problem to the few-shot classification problem by using a fully connected layer
to decouple localization features. CME then reserves adequate margin space for
novel classes by introducing simple-yet-effective class margin loss during
feature learning. Finally, CME pursues margin equilibrium by disturbing the
features of novel class instances in an adversarial min-max fashion.
Experiments on Pascal VOC and MS-COCO datasets show that CME significantly
improves upon two baseline detectors (up to $3\sim 5\%$ in average), achieving
state-of-the-art performance. Code is available at
https://github.com/Bohao-Lee/CME .
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04656</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04656</id><submitter>Fabio Calefato</submitter><version version="v1"><date>Mon, 8 Mar 2021 10:36:00 GMT</date><size>7803kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 08:10:43 GMT</date><size>8904kb</size><source_type>D</source_type></version><title>Will You Come Back to Contribute? Investigating the Inactivity of OSS
  Core Developers in GitHub</title><authors>Fabio Calefato, Marco Aurelio Gerosa, Giuseppe Iaffaldano, Filippo
  Lanubile, Igor Steinmacher</authors><categories>cs.SE</categories><comments>Empirical Software Engineering, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several Open Source Software (OSS) projects depend on the continuity of their
development communities to remain sustainable. Understanding how developers
become inactive or why they take breaks can help communities prevent
abandonment and incentivize developers to come back. In this paper, we propose
a novel method to identify developers' inactive periods by analyzing the
individual rhythm of contributions to the projects. Using this method, we
quantitatively analyze the inactivity of core developers in 18 OSS
organizations hosted on GitHub. We also survey core developers to receive their
feedback about the identified breaks and transitions. Our results show that our
method was effective for identifying developers' breaks. About 94% of the
surveyed core developers agreed with our state model of inactivity; 71% and 79%
of them acknowledged their breaks and state transition, respectively. We also
show that all core developers take breaks (at least once) and about a half of
them (~45%}) have completely disengaged from a project for at least one year.
We also analyzed the probability of transitions to/from inactivity and found
that developers who pause their activity have a ~35-55\% chance to return to an
active state; yet, if the break lasts for a year or longer, then the
probability of resuming activities drops to ~21-26%, with a ~54% chance of
complete disengagement. These results may support the creation of policies and
mechanisms to make OSS community managers aware of breaks and potential project
abandonment.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04722</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04722</id><submitter>Zeyu Han</submitter><version version="v1"><date>Mon, 8 Mar 2021 12:55:54 GMT</date><size>219kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 12:44:13 GMT</date><size>428kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 08:45:47 GMT</date><size>434kb</size></version><title>Sparse Kronecker-Product Coding for Unsourced Multiple Access</title><authors>Zeyu Han, Xiaojun Yuan, Chongbin Xu, Shuchao Jiang, Xin Wang</authors><categories>cs.IT eess.SP math.IT</categories><comments>Submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a sparse Kronecker-product (SKP) coding scheme is proposed for
unsourced multiple access. Specifically, the data of each active user is
encoded as the Kronecker product of two component codewords with one being
sparse and the other being forward-error-correction (FEC) coded. At the
receiver, an iterative decoding algorithm is developed, consisting of matrix
factorization for the decomposition of the Kronecker product and soft-in
soft-out decoding for the component sparse code and the FEC code. The cyclic
redundancy check (CRC) aided interference cancellation technique is further
incorporated for performance improvement. Numerical results show that the
proposed scheme outperforms the state-of-the-art counterparts, and approaches
the random coding bound within a gap of only 0.1 dB at the code length of 30000
when the number of active users is less than 75, and the error rate can be made
very small even if the number of active users is relatively large.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.05405</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.05405</id><submitter>Kechun Xu</submitter><version version="v1"><date>Tue, 9 Mar 2021 13:03:33 GMT</date><size>5445kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 07:18:42 GMT</date><size>17107kb</size><source_type>D</source_type></version><title>Efficient learning of goal-oriented push-grasping synergy in clutter</title><authors>Kechun Xu, Hongxiang Yu, Qianen Lai, Yue Wang, Rong Xiong</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the task of goal-oriented grasping, in which a robot is supposed
to grasp a pre-assigned goal object in clutter and needs some pre-grasp actions
such as pushes to enable stable grasps. However, in this task, the robot gets
positive rewards from environment only when successfully grasping the goal
object. Besides, joint pushing and grasping elongates the action sequence,
compounding the problem of reward delay. Thus, sample inefficiency remains a
main challenge in this task. In this paper, a goal-conditioned hierarchical
reinforcement learning formulation with high sample efficiency is proposed to
learn a push-grasping policy for grasping a specific object in clutter. In our
work, sample efficiency is improved by two means. First, we use a
goal-conditioned mechanism by goal relabeling to enrich the replay buffer.
Second, the pushing and grasping policies are respectively regarded as a
generator and a discriminator and the pushing policy is trained with
supervision of the grasping discriminator, thus densifying pushing rewards. To
deal with the problem of distribution mismatch caused by different training
settings of two policies, an alternating training stage is added to learn
pushing and grasping in turn. A series of experiments carried out in simulation
and real world indicate that our method can quickly learn effective pushing and
grasping policies and outperforms existing methods in task completion rate and
goal grasp success rate by less times of motion. Furthermore, we validate that
our system can also adapt to goal-agnostic conditions with better performance.
Note that our system can be transferred to the real world without any
fine-tuning. Our code is available at https://github.com/xukechun/Efficient
goal-oriented push-grasping synergy
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.05588</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.05588</id><submitter>Marc Roth</submitter><version version="v1"><date>Tue, 9 Mar 2021 17:52:27 GMT</date><size>136kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 16:30:26 GMT</date><size>65kb</size></version><title>Exact and Approximate Pattern Counting in Degenerate Graphs: New
  Algorithms, Hardness Results, and Complexity Dichotomies</title><authors>Marco Bressan and Marc Roth</authors><categories>cs.CC cs.DS</categories><comments>44 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problems of counting the homomorphisms, counting the copies, and
counting the induced copies of a $k$-vertex graph $H$ in a $d$-degenerate
$n$-vertex graph $G$. Our main result establishes exhaustive and explicit
complexity classifications for counting subgraphs and induced subgraphs. We
show that the (not necessarily induced) copies of $H$ in $G$ can be counted in
time $f(k,d)\cdot n^{\max(\mathsf{imn}(H),1)}\cdot \log n$, where $f$ is some
computable function and $\mathsf{imn}(H)$ is the size of the largest induced
matching of $H$. Whenever the class of allowed patterns has unbounded induced
matching number, this algorithm is essentially optimal: Unless the Exponential
Time Hypothesis (ETH) fails, there is no algorithm running in time $f(k,d)\cdot
n^{o(\mathsf{imn}(H)/\log \mathsf{imn}(H))}$ for any function $f$. In case of
counting induced subgraphs, we obtain a similar classification along the
independence number $\alpha$: we can count the induced copies of $H$ in $G$ in
time $f(k,d)\cdot n^{\alpha(H)}\cdot \log n$, and if the class of allowed
patterns has unbounded independence number, an algorithm running in time
$f(k,d)\cdot n^{o(\alpha(H)/\log \alpha(H))}$ is impossible, unless ETH fails.
In the language of parameterized complexity, our results yield dichotomies in
fixed-parameter tractable and $\#\mathsf{W}[1]$-hard cases if we parameterize
by the size of the pattern and the degeneracy of the host graph. Our results
imply that several patterns cannot be counted in time $f(k,d)\cdot n^{o(k/\log
k)}$, including $k$-matchings, $k$-independent sets, (induced) $k$-paths,
(induced) $k$-cycles, and induced $(k,k)$-bicliques, unless ETH fails. Those
lower bounds for exact counting are complemented with new algorithms for
approximate counting of subgraphs and induced subgraphs in degenerate graphs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.05734</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.05734</id><submitter>Artemy Kolchinsky</submitter><version version="v1"><date>Tue, 9 Mar 2021 22:03:02 GMT</date><size>48kb</size></version><version version="v2"><date>Sat, 29 May 2021 00:55:05 GMT</date><size>5541kb</size><source_type>D</source_type></version><title>The dependence of integrated, instantaneous, and fluctuating entropy
  production on the initial state in quantum and classical processes</title><authors>Artemy Kolchinsky, David H. Wolpert</authors><categories>quant-ph cond-mat.stat-mech cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the additional entropy production (EP) incurred by a fixed
quantum or classical process on some initial state $\rho$, above the minimal EP
incurred by the same process on the least-dissipative initial state $\varphi$.
We show that this additional EP, which we term the &quot;mismatch cost of $\rho$&quot;,
has a universal information-theoretic form: it is given by the contraction of
the relative entropy between $\rho$ and $\varphi$ over time. We derive versions
of this result for integrated EP incurred over the course of a process, for
trajectory-level fluctuating EP, and for instantaneous EP rate. We also show
that mismatch cost for fluctuating EP obeys an integral fluctuation theorem.
Our results demonstrate a fundamental relationship between thermodynamic
irreversibility (generation of EP) and logical irreversibility (inability to
know the initial state corresponding to a given final state). We use this
relationship to derive quantitative bounds on the thermodynamics of quantum
error correction and to propose a thermodynamically-operationalized measure of
the logical irreversibility of a quantum channel. Our results hold for both
finite and infinite dimensional systems, and generalize beyond EP to many other
thermodynamic costs, including nonadiabatic EP, free energy loss, and entropy
gain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.06393</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.06393</id><submitter>Ilias Giannakopoulos</submitter><version version="v1"><date>Thu, 11 Mar 2021 00:20:27 GMT</date><size>748kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 17:25:51 GMT</date><size>757kb</size></version><title>Compression of volume-surface integral equation matrices via Tucker
  decomposition for magnetic resonance applications</title><authors>Ilias I. Giannakopoulos, Georgy D. Guryev, Jose E. C. Serralles,
  Ioannis P. Georgakis, Luca Daniel, Jacob K. White, Riccardo Lattanzi</authors><categories>cs.CE</categories><comments>13 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we propose a method for the compression of the coupling matrix
in volume\hyp surface integral equation (VSIE) formulations. VSIE methods are
used for electromagnetic analysis in magnetic resonance imaging (MRI)
applications, for which the coupling matrix models the interactions between the
coil and the body. We showed that these effects can be represented as
independent interactions between remote elements in 3D tensor formats, and
subsequently decomposed with the Tucker model. Our method can work in tandem
with the adaptive cross approximation technique to provide fast solutions of
VSIE problems. We demonstrated that our compression approaches can enable the
use of VSIE matrices of prohibitive memory requirements, by allowing the
effective use of modern graphical processing units (GPUs) to accelerate the
arising matrix\hyp vector products. This is critical to enable numerical MRI
simulations at clinical voxel resolutions in a feasible computation time. In
this paper, we demonstrate that the VSIE matrix\hyp vector products needed to
calculate the electromagnetic field produced by an MRI coil inside a numerical
body model with $1$ mm$^3$ voxel resolution, could be performed in $\sim 33$
seconds in a GPU, after compressing the associated coupling matrix from $\sim
80$ TB to $\sim 43$ MB.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.06518</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.06518</id><submitter>Hergys Rexha</submitter><version version="v1"><date>Thu, 11 Mar 2021 08:07:29 GMT</date><size>777kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 07:30:52 GMT</date><size>384kb</size><source_type>D</source_type></version><title>Data Collection and Utilization Framework for Edge AI Applications</title><authors>Hergys Rexha, Sebastien Lafond</authors><categories>cs.LG cs.IR cs.MA</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  As data being produced by IoT applications continues to explode, there is a
growing need to bring computing power closer to the source of the data to meet
the response time, power dissipation and cost goals of performance-critical
applications in various domains like the Industrial Internet of Things (IIoT),
Automated Driving, Medical Imaging or Surveillance among others. This paper
proposes a data collection and utilization framework that allows runtime
platform and application data to be sent to an edge and cloud system via data
collection agents running close to the platform. Agents are connected to a
cloud system able to train AI models to improve overall energy efficiency of an
AI application executed on an edge platform. In the implementation part, we
show the benefits of FPGA-based platform for the task of object detection.
Furthermore, we show that it is feasible to collect relevant data from an FPGA
platform, transmit the data to a cloud system for processing and receiving
feedback actions to execute an edge AI application energy efficiently. As
future work, we foresee the possibility to train, deploy and continuously
improve a base model able to efficiently adapt the execution of edge
applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.08330</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.08330</id><submitter>Johannes Bl\&quot;umlein</submitter><version version="v1"><date>Mon, 15 Mar 2021 12:20:31 GMT</date><size>43kb</size></version><title>Iterated integrals over letters induced by quadratic forms</title><authors>J. Ablinger, J. Bl\&quot;umlein and C. Schneider</authors><categories>hep-th cs.SC hep-ph math-ph math.MP</categories><comments>14 pages LATEX, 1 anc. file</comments><report-no>DESY 21--031, DO--TH 21/05 RISC-Linz Report Series No. 21-05,
  SAGEX-21-05</report-no><journal-ref>Phys. Rev. D 103, 096025 (2021)</journal-ref><doi>10.1103/PhysRevD.103.096025</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  An automated treatment of iterated integrals based on letters induced by
real-valued quadratic forms and Kummer--Poincar\'e letters is presented. These
quantities emerge in analytic single and multi--scale Feynman diagram
calculations. To compactify representations, one wishes to apply general
properties of these quantities in computer-algebraic implementations. We
provide the reduction to basis representations, expansions, analytic
continuation and numerical evaluation of these quantities.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.08394</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.08394</id><submitter>Vaclav Rozhon</submitter><version version="v1"><date>Mon, 15 Mar 2021 14:04:36 GMT</date><size>2050kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 13:27:43 GMT</date><size>5701kb</size><source_type>D</source_type></version><title>Local Problems on Grids from the Perspective of Distributed Algorithms,
  Finitary Factors, and Descriptive Combinatorics</title><authors>Jan Greb\'ik, V\'aclav Rozho\v{n}</authors><categories>math.CO cs.DS math.LO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an intimate connection among the following fields:
  (a) distributed local algorithms: coming from the area of computer science,
  (b) finitary factors of iid processes: coming from the area of analysis of
randomized processes,
  (c) descriptive combinatorics: coming from the area of combinatorics and
measure theory.
  In particular, we study locally checkable labellings in grid graphs from all
three perspectives. Most of our results are for the perspective (b) where we
prove time hierarchy theorems akin to those known in the field (a) [Chang,
Pettie FOCS 2017]. This approach that borrows techniques from the fields (a)
and (c) implies a number of results about possible complexities of finitary
factor solutions. Among others, it answers three open questions of [Holroyd et
al. Annals of Prob. 2017] or the more general question of [Brandt et al. PODC
2017] who asked for a formal connection between the fields (a) and (b). In
general, we hope that our treatment will help to view all three perspectives as
a part of a common theory of locality, in which we follow the insightful paper
of [Bernshteyn 2020+] .
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.08562</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.08562</id><submitter>Kai Packh\&quot;auser</submitter><version version="v1"><date>Mon, 15 Mar 2021 17:26:43 GMT</date><size>22732kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 17:22:04 GMT</date><size>8357kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 10:36:57 GMT</date><size>8357kb</size><source_type>D</source_type></version><title>Is Medical Chest X-ray Data Anonymous?</title><authors>Kai Packh\&quot;auser, Sebastian G\&quot;undel, Nicolas M\&quot;unster, Christopher
  Syben, Vincent Christlein, Andreas Maier</authors><categories>cs.CV cs.AI cs.LG eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the rise and ever-increasing potential of deep learning techniques in
recent years, publicly available medical datasets became a key factor to enable
reproducible development of diagnostic algorithms in the medical domain.
Medical data contains sensitive patient-related information and is therefore
usually anonymized by removing patient identifiers, e.g., patient names before
publication. To the best of our knowledge, we are the first to show that a
well-trained deep learning system is able to recover the patient identity from
chest X-ray data. We demonstrate this using the publicly available large-scale
ChestX-ray14 dataset, a collection of 112,120 frontal-view chest X-ray images
from 30,805 unique patients. Our verification system is able to identify
whether two frontal chest X-ray images are from the same person with an AUC of
0.9940 and a classification accuracy of 95.55%. We further highlight that the
proposed system is able to reveal the same person even ten and more years after
the initial scan. When pursuing a retrieval approach, we observe an mAP@R of
0.9748 and a precision@1 of 0.9963. Furthermore, we achieve an AUC of up to
0.9870 and a precision@1 of up to 0.9444 when evaluating our trained networks
on CheXpert and the COVID-19 Image Data Collection. Based on this high
identification rate, a potential attacker may leak patient-related information
and additionally cross-reference images to obtain more information. Thus, there
is a great risk of sensitive content falling into unauthorized hands or being
disseminated against the will of the concerned patients. Especially during the
COVID-19 pandemic, numerous chest X-ray datasets have been published to advance
research. Therefore, such data may be vulnerable to potential attacks by deep
learning-based re-identification algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.09118</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.09118</id><submitter>Joseph Robinson</submitter><version version="v1"><date>Tue, 16 Mar 2021 15:05:49 GMT</date><size>11086kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:21:18 GMT</date><size>15101kb</size><source_type>D</source_type></version><title>Balancing Biases and Preserving Privacy on Balanced Faces in the Wild</title><authors>Joseph P Robinson and Can Qin and Yann Henon and Samson Timoner and
  Yun Fu</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are demographic biases in current models used for facial recognition
(FR). Our Balanced Faces In the Wild (BFW) dataset serves as a proxy to measure
bias across ethnicity and gender subgroups, allowing one to characterize FR
performances per subgroup. We show performances are non-optimal when a single
score threshold is used to determine whether sample pairs are genuine or
imposter. Across subgroups, performance ratings vary from the reported across
the entire dataset. Thus, claims of specific error rates only hold true for
populations matching that of the validation data. We mitigate the imbalanced
performances using a novel domain adaptation learning scheme on the facial
features extracted using state-of-the-art. Not only does this technique balance
performance, but it also boosts the overall performance. A benefit of the
proposed is to preserve identity information in facial features while removing
demographic knowledge in the lower dimensional features. The removal of
demographic knowledge prevents future potential biases from being injected into
decision-making. This removal satisfies privacy concerns. We explore why this
works qualitatively; we also show quantitatively that subgroup classifiers can
no longer learn from the features mapped by the proposed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.09593</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.09593</id><submitter>Samson Tan</submitter><version version="v1"><date>Wed, 17 Mar 2021 12:20:53 GMT</date><size>20211kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 23 Apr 2021 09:30:27 GMT</date><size>20211kb</size><source_type>D</source_type></version><title>Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots</title><authors>Samson Tan, Shafiq Joty</authors><categories>cs.CL cs.AI cs.CY cs.LG cs.NE</categories><comments>To be presented at NAACL-HLT 2021. Abstract to be published in the
  Rising Stars Track of the Workshop on Computational Approaches to Linguistic
  Code-Switching (CALCS 2021)</comments><journal-ref>2021.naacl-main.282</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilingual models have demonstrated impressive cross-lingual transfer
performance. However, test sets like XNLI are monolingual at the example level.
In multilingual communities, it is common for polyglots to code-mix when
conversing with each other. Inspired by this phenomenon, we present two strong
black-box adversarial attacks (one word-level, one phrase-level) for
multilingual models that push their ability to handle code-mixed sentences to
the limit. The former uses bilingual dictionaries to propose perturbations and
translations of the clean example for sense disambiguation. The latter directly
aligns the clean example with its translations before extracting phrases as
perturbations. Our phrase-level attack has a success rate of 89.75% against
XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.
Finally, we propose an efficient adversarial training scheme that trains in the
same number of steps as the original model and show that it improves model
accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.09939</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.09939</id><submitter>Mathias Ibsen</submitter><version version="v1"><date>Wed, 17 Mar 2021 22:38:13 GMT</date><size>2096kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 27 Mar 2021 21:26:07 GMT</date><size>2046kb</size><source_type>D</source_type></version><title>Impact of Facial Tattoos and Paintings on Face Recognition Systems</title><authors>Mathias Ibsen, Christian Rathgeb, Thomas Fink, Pawel Drozdowski,
  Christoph Busch</authors><categories>cs.CV</categories><comments>Accepted to IET Biometrics</comments><doi>10.1049/bme2.12032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past years, face recognition technologies have shown impressive
recognition performance, mainly due to recent developments in deep
convolutional neural networks. Notwithstanding those improvements, several
challenges which affect the performance of face recognition systems remain. In
this work, we investigate the impact that facial tattoos and paintings have on
current face recognition systems. To this end, we first collected an
appropriate database containing image-pairs of individuals with and without
facial tattoos or paintings. The assembled database was used to evaluate how
facial tattoos and paintings affect the detection, quality estimation, as well
as the feature extraction and comparison modules of a face recognition system.
The impact on these modules was evaluated using state-of-the-art open-source
and commercial systems. The obtained results show that facial tattoos and
paintings affect all the tested modules, especially for images where a large
area of the face is covered with tattoos or paintings. Our work is an initial
case-study and indicates a need to design algorithms which are robust to the
visual changes caused by facial tattoos and paintings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10134</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10134</id><submitter>Kai Chen</submitter><version version="v1"><date>Thu, 18 Mar 2021 10:05:13 GMT</date><size>2424kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 15:37:33 GMT</date><size>2473kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 12:18:34 GMT</date><size>2473kb</size><source_type>D</source_type></version><title>Recent Advances in Data-Driven Wireless Communication Using Gaussian
  Processes: A Comprehensive Survey</title><authors>Kai Chen, Qinglei Kong, Yijue Dai, Yue Xu, Feng Yin, Lexi Xu, and
  Shuguang Cui</authors><categories>cs.LG cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data-driven paradigms are well-known and salient demands of future wireless
communication. Empowered by big data and machine learning, next-generation
data-driven communication systems will be intelligent with the characteristics
of expressiveness, scalability, interpretability, and especially uncertainty
modeling, which can confidently involve diversified latent demands and
personalized services in the foreseeable future. In this paper, we review a
promising family of nonparametric Bayesian machine learning methods, i.e.,
Gaussian processes (GPs), and their applications in wireless communication.
Since GPs achieve the expressive and interpretable learning ability with
uncertainty, it is particularly suitable for wireless communication. Moreover,
it provides a natural framework for collaborating data and empirical models
(DEM). Specifically, we first envision three-level motivations of data-driven
wireless communication using GPs. Then, we present the background of the GPs in
terms of covariance structure and model inference. The expressiveness of the GP
model using various interpretable kernel designs is surveyed, namely,
stationary, non-stationary, deep, and multi-task kernels. Furthermore, we
review the distributed GPs with promising scalability, which is suitable for
applications in wireless networks with a large number of distributed edge
devices. Finally, we list representative solutions and promising techniques
that adopt GPs in wireless communication systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10312</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10312</id><submitter>Isaac Gerg</submitter><version version="v1"><date>Thu, 18 Mar 2021 15:16:29 GMT</date><size>18490kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 26 May 2021 01:09:02 GMT</date><size>18490kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 15:19:25 GMT</date><size>18489kb</size><source_type>D</source_type></version><title>Real-Time, Deep Synthetic Aperture Sonar (SAS) Autofocus</title><authors>Isaac D. Gerg and Vishal Monga</authors><categories>cs.CV</categories><comments>Four pages. Accepted to IGARSS 2021. Fixed Eq 9</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Synthetic aperture sonar (SAS) requires precise time-of-flight measurements
of the transmitted/received waveform to produce well-focused imagery. It is not
uncommon for errors in these measurements to be present resulting in image
defocusing. To overcome this, an \emph{autofocus} algorithm is employed as a
post-processing step after image reconstruction to improve image focus. A
particular class of these algorithms can be framed as a sharpness/contrast
metric-based optimization. To improve convergence, a hand-crafted weighting
function to remove &quot;bad&quot; areas of the image is sometimes applied to the
image-under-test before the optimization procedure. Additionally, dozens of
iterations are necessary for convergence which is a large compute burden for
low size, weight, and power (SWaP) systems. We propose a deep learning
technique to overcome these limitations and implicitly learn the weighting
function in a data-driven manner. Our proposed method, which we call Deep
Autofocus, uses features from the single-look-complex (SLC) to estimate the
phase correction which is applied in $k$-space. Furthermore, we train our
algorithm on batches of training imagery so that during deployment, only a
single iteration of our method is sufficient to autofocus. We show results
demonstrating the robustness of our technique by comparing our results to four
commonly used image sharpness metrics. Our results demonstrate Deep Autofocus
can produce imagery perceptually better than common iterative techniques but at
a lower computational cost. We conclude that Deep Autofocus can provide a more
favorable cost-quality trade-off than alternatives with significant potential
of future research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10315</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10315</id><submitter>Biao Wu</submitter><version version="v1"><date>Thu, 18 Mar 2021 15:18:56 GMT</date><size>18kb</size></version><version version="v2"><date>Wed, 14 Apr 2021 05:40:29 GMT</date><size>18kb</size></version><version version="v3"><date>Thu, 3 Jun 2021 05:48:51 GMT</date><size>19kb</size></version><title>Lorentz Quantum Computer</title><authors>Wenhao He and Zhenduo Wang and Biao Wu</authors><categories>quant-ph cond-mat.other cs.CC</categories><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A theoretical model of computation is proposed based on Lorentz quantum
mechanics. Besides the standard qubits, this model has an additional bit, which
we call hyperbolic bit (or hybit in short). A set of basic logical gates are
constructed and their universality is proved. As an application, a search
algorithm is designed for this computer model and is found to be exponentially
faster than the Grover's search algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10498</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10498</id><submitter>Osvald Frisk</submitter><version version="v1"><date>Thu, 18 Mar 2021 19:53:00 GMT</date><size>159kb</size><source_type>D</source_type></version><title>Super-convergence and Differential Privacy: Training faster with better
  privacy guarantees</title><authors>Osvald Frisk, Friedrich D\&quot;ormann, Christian Marius Lillelund,
  Christian Fischer Pedersen</authors><categories>cs.LG cs.CR</categories><comments>(To be) Published and presented at the 55th Annual Conference on
  Information Sciences and Systems (CISS), 7 pages, 4 figures</comments><journal-ref>2021 55th Annual Conference on Information Sciences and Systems
  (CISS)</journal-ref><doi>10.1109/CISS50987.2021.9400274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combination of deep neural networks and Differential Privacy has been of
increasing interest in recent years, as it offers important data protection
guarantees to the individuals of the training datasets used. However, using
Differential Privacy in the training of neural networks comes with a set of
shortcomings, like a decrease in validation accuracy and a significant increase
in the use of resources and time in training. In this paper, we examine
super-convergence as a way of greatly increasing training speed of
differentially private neural networks, addressing the shortcoming of high
training time and resource use. Super-convergence allows for acceleration in
network training using very high learning rates, and has been shown to achieve
models with high utility in orders of magnitude less training iterations than
conventional ways. Experiments in this paper show that this order-of-magnitude
speedup can also be seen when combining it with Differential Privacy, allowing
for higher validation accuracies in much fewer training iterations compared to
non-private, non-super convergent baseline models. Furthermore,
super-convergence is shown to improve the privacy guarantees of private models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10548</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10548</id><submitter>Luigi Quaranta</submitter><version version="v1"><date>Thu, 18 Mar 2021 22:25:44 GMT</date><size>121kb</size></version><title>Towards Productizing AI/ML Models: An Industry Perspective from Data
  Scientists</title><authors>Filippo Lanubile, Fabio Calefato, Luigi Quaranta, Maddalena Amoruso,
  Fabio Fumarola, Michele Filannino</authors><categories>cs.SE cs.LG</categories><comments>4 pages</comments><journal-ref>Proc. of 2021 IEEE/ACM 1st Workshop on AI Engineering - Software
  Engineering for AI (WAIN), pp. 129-132</journal-ref><doi>10.1109/WAIN52551.2021.00027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transition from AI/ML models to production-ready AI-based systems is a
challenge for both data scientists and software engineers. In this paper, we
report the results of a workshop conducted in a consulting company to
understand how this transition is perceived by practitioners. Starting from the
need for making AI experiments reproducible, the main themes that emerged are
related to the use of the Jupyter Notebook as the primary prototyping tool, and
the lack of support for software engineering best practices as well as data
science specific functionalities.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10558</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10558</id><submitter>Luigi Quaranta</submitter><version version="v1"><date>Thu, 18 Mar 2021 22:57:01 GMT</date><size>144kb</size><source_type>D</source_type></version><title>KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle</title><authors>Luigi Quaranta, Fabio Calefato, Filippo Lanubile</authors><categories>cs.DB</categories><journal-ref>Proc. of 2021 IEEE/ACM 18th International Conference on Mining
  Software Repositories (MSR), pp. 550-554</journal-ref><doi>10.1109/MSR52588.2021.00072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational notebooks have become the tool of choice for many data
scientists and practitioners for performing analyses and disseminating results.
Despite their increasing popularity, the research community cannot yet count on
a large, curated dataset of computational notebooks. In this paper, we fill
this gap by introducing KGTorrent, a dataset of Python Jupyter notebooks with
rich metadata retrieved from Kaggle, a platform hosting data science
competitions for learners and practitioners with any levels of expertise. We
describe how we built KGTorrent, and provide instructions on how to use it and
refresh the collection to keep it up to date. Our vision is that the research
community will use KGTorrent to study how data scientists, especially
practitioners, use Jupyter Notebook in the wild and identify potential
shortcomings to inform the design of its future extensions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10710</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10710</id><submitter>William Wilkinson</submitter><version version="v1"><date>Fri, 19 Mar 2021 09:50:53 GMT</date><size>2343kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 08:09:19 GMT</date><size>2343kb</size><source_type>D</source_type></version><title>Sparse Algorithms for Markovian Gaussian Processes</title><authors>William J. Wilkinson, Arno Solin, Vincent Adam</authors><categories>stat.ML cs.LG</categories><comments>Appearing in the 24th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Bayesian inference methods that scale to very large datasets are
crucial in leveraging probabilistic models for real-world time series. Sparse
Markovian Gaussian processes combine the use of inducing variables with
efficient Kalman filter-like recursions, resulting in algorithms whose
computational and memory requirements scale linearly in the number of inducing
points, whilst also enabling parallel parameter updates and stochastic
optimisation. Under this paradigm, we derive a general site-based approach to
approximate inference, whereby we approximate the non-Gaussian likelihood with
local Gaussian terms, called sites. Our approach results in a suite of novel
sparse extensions to algorithms from both the machine learning and signal
processing literature, including variational inference, expectation
propagation, and the classical nonlinear Kalman smoothers. The derived methods
are suited to large time series, and we also demonstrate their applicability to
spatio-temporal data, where the model has separate inducing points in both time
and space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10997</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10997</id><submitter>Hamidreza Kasaei</submitter><version version="v1"><date>Fri, 19 Mar 2021 19:38:00 GMT</date><size>6573kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 13:56:58 GMT</date><size>6577kb</size><source_type>D</source_type></version><title>MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered
  Environments</title><authors>Hamidreza Kasaei, Mohammadreza Kasaei</authors><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Nowadays service robots are entering more and more in our daily life. In such
a dynamic environment, a robot frequently faces pile, packed, or isolated
objects. Therefore, it is necessary for the robot to know how to grasp and
manipulate various objects in different situations to help humans in everyday
tasks. Most state-of-the-art grasping approaches addressed four
degrees-of-freedom (DoF) object grasping, where the robot is forced to grasp
objects from above based on grasp synthesis of a given top-down scene. Although
such approaches showed a very good performance in predefined industrial
settings, they are not suitable for human-centric environments as the robot
will not able to grasp a range of household objects robustly, for example,
grasping a bottle from above is not stable. In this work, we propose a
multi-view deep learning approach to handle robust object grasping in
human-centric domains. In particular, our approach takes a partial point cloud
of a scene as an input, and then, generates multi-views of existing objects.
The obtained views of each object are used to estimate pixel-wise grasp
synthesis for each object. To evaluate the performance of the proposed
approach, we performed extensive experiments in both simulation and real-world
environments within the pile, packed, and isolated objects scenarios.
Experimental results showed that our approach can estimate appropriate grasp
configurations in only 22ms without the need for explicit collision checking.
Therefore, the proposed approach can be used in real-time robotic applications
that need closed-loop grasp planning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.11030</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.11030</id><submitter>Luca Vigan\`o</submitter><version version="v1"><date>Fri, 19 Mar 2021 21:27:27 GMT</date><size>1402kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 28 Mar 2021 18:54:51 GMT</date><size>1402kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 07:57:20 GMT</date><size>1402kb</size><source_type>D</source_type></version><title>Don't Tell Me The Cybersecurity Moon Is Shining... (Cybersecurity Show
  And Tell)</title><authors>Luca Vigan\`o</authors><categories>cs.CR cs.CY</categories><comments>28 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  &quot;Show, don't tell&quot; has become the literary commandment for any writer. It
applies to all forms of fiction, and to non-fiction, including scientific
writing, where it lies at the heart of many scientific communication and
storytelling approaches. In this paper, I discuss how &quot;show \emph{and} tell&quot; is
actually often the best approach when one wants to present, teach or explain
complicated ideas such as those underlying notions and results in mathematics
and science, and in particular in cybersecurity. I discuss how different kinds
of artworks can be used to explain cybersecurity and I illustrate how telling
(i.e., explaining notions in a formal, technical way) can be paired with
showing through visual storytelling or other forms of storytelling. I also
discuss four categories of artworks and the explanations they help provide.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.11233</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.11233</id><submitter>Vicky Kouni</submitter><version version="v1"><date>Sat, 20 Mar 2021 19:59:20 GMT</date><size>266kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 16:50:06 GMT</date><size>273kb</size><source_type>D</source_type></version><title>Spark Deficient Gabor Frame Provides a Novel Analysis Operator for
  Compressed Sensing</title><authors>Vasiliki Kouni, Holger Rauhut</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The analysis sparsity model is a very effective approach in modern Compressed
Sensing applications. Specifically, redundant analysis operators can lead to
fewer measurements needed for reconstruction when employing the analysis
$l_1$-minimization in Compressed Sensing. In this paper, we pick an eigenvector
of the Zauner unitary matrix and -- under certain assumptions on the ambient
dimension -- we build a spark deficient Gabor frame. The analysis operator
associated with such a spark deficient Gabor frame, is a new (highly) redundant
Gabor transform, which we use as a sparsifying transform in Compressed Sensing.
We conduct computational experiments -- on both synthetic and real-world data
-- solving the analysis $l_1$-minimization problem of Compressed Sensing, with
four different choices of analysis operators, including our Gabor analysis
operator. The results show that our proposed redundant Gabor transform
outperforms -- in all cases -- Gabor transforms generated by state-of-the-art
window vectors of time-frequency analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.11595</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.11595</id><submitter>Xin Hong</submitter><version version="v1"><date>Mon, 22 Mar 2021 05:47:41 GMT</date><size>139kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 08:10:54 GMT</date><size>136kb</size><source_type>D</source_type></version><title>Approximate Equivalence Checking of Noisy Quantum Circuits</title><authors>Xin Hong, Mingsheng Ying, Yuan Feng, Xiangzhen Zhou and Sanjiang Li</authors><categories>quant-ph cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental design automation problem of equivalence checking in
the NISQ (Noisy Intermediate-Scale Quantum) computing realm where quantum noise
is present inevitably. The notion of approximate equivalence of (possibly
noisy) quantum circuits is defined based on the Jamiolkowski fidelity which
measures the average distance between output states of two super-operators when
the input is chosen at random. By employing tensor network contraction, we
present two algorithms, aiming at different situations where the number of
noises varies, for computing the fidelity between an ideal quantum circuit and
its noisy implementation. The effectiveness of our algorithms is demonstrated
by experimenting on benchmarks of real NISQ circuits. When compared with the
state-of-the-art implementation incorporated in Qiskit, experimental results
show that the proposed algorithms outperform in both efficiency and
scalability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.11766</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.11766</id><submitter>Ruihan Wu</submitter><version version="v1"><date>Mon, 22 Mar 2021 12:29:10 GMT</date><size>5677kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 02:34:41 GMT</date><size>3010kb</size><source_type>D</source_type></version><title>Fixes That Fail: Self-Defeating Improvements in Machine-Learning Systems</title><authors>Ruihan Wu, Chuan Guo, Awni Hannun, Laurens van der Maaten</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine-learning systems such as self-driving cars or virtual assistants are
composed of a large number of machine-learning models that recognize image
content, transcribe speech, analyze natural language, infer preferences, rank
options, etc. Models in these systems are often developed and trained
independently, which raises an obvious concern: Can improving a
machine-learning model make the overall system worse? We answer this question
affirmatively by showing that improving a model can deteriorate the performance
of downstream models, even after those downstream models are retrained. Such
self-defeating improvements are the result of entanglement between the models
in the system. We perform an error decomposition of systems with multiple
machine-learning models, which sheds light on the types of errors that can lead
to self-defeating improvements. We also present the results of experiments
which show that self-defeating improvements emerge in a realistic stereo-based
detection system for cars and pedestrians.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.11807</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.11807</id><submitter>Kenji Suzuki</submitter><version version="v1"><date>Mon, 22 Mar 2021 13:08:46 GMT</date><size>921kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 08:23:25 GMT</date><size>960kb</size><source_type>D</source_type></version><title>Data Cleansing for Deep Neural Networks with Storage-efficient
  Approximation of Influence Functions</title><authors>Kenji Suzuki, Yoshiyuki Kobayashi, Takuya Narihira</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Identifying the influence of training data for data cleansing can improve the
accuracy of deep learning. An approach with stochastic gradient descent (SGD)
called SGD-influence to calculate the influence scores was proposed, but, the
calculation costs are expensive. It is necessary to temporally store the
parameters of the model during training phase for inference phase to calculate
influence sores. In close connection with the previous method, we propose a
method to reduce cache files to store the parameters in training phase for
calculating inference score. We only adopt the final parameters in last epoch
for influence functions calculation. In our experiments on classification, the
cache size of training using MNIST dataset with our approach is 1.236 MB. On
the other hand, the previous method used cache size of 1.932 GB in last epoch.
It means that cache size has been reduced to 1/1,563. We also observed the
accuracy improvement by data cleansing with removal of negatively influential
data using our approach as well as the previous method. Moreover, our simple
and general proposed method to calculate influence scores is available on our
auto ML tool without programing, Neural Network Console. The source code is
also available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.11933</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.11933</id><submitter>Hamid Bekamiri</submitter><version version="v1"><date>Mon, 22 Mar 2021 15:23:19 GMT</date><size>452kb</size></version><version version="v2"><date>Sat, 29 May 2021 15:06:36 GMT</date><size>1144kb</size></version><title>PatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and
  Classification using Augmented SBERT</title><authors>Hamid Bekamiri, Daniel S. Hain, Roman Jurowetzki</authors><categories>cs.LG econ.EM</categories><comments>18 pages, 7 figures and 4 Tables</comments><msc-class>H.0</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study provides an efficient approach for using text data to calculate
patent-to-patent (p2p) technological similarity, and presents a hybrid
framework for leveraging the resulting p2p similarity for applications such as
semantic search and automated patent classification. We create embeddings using
Sentence-BERT (SBERT) based on patent claims. To further increase the patent
embedding quality, we use transformer models based on SBERT and RoBERT, and
apply the augmented approach for fine-tuning SBERT by in-domain supervised
patent claims data. We leverage SBERTs efficiency in creating embedding
distance measures to map p2p similarity in large sets of patent data. We deploy
our framework for classification with a simple Nearest Neighbors (KNN) model
that predicts Cooperative Patent Classification (CPC) of a patent based on the
CPC assignment of the K patents with the highest p2p similarity. We thereby
validate that p2p similarity captures their technological features in terms of
CPC overlap, and at the same demonstrate the usefulness of this approach for
automatic patent classification based on text data. In the out-of-sample model
validation, we are able to perform a multi-label prediction of all assigned CPC
classes on the subclass (640) level on 163,269 patents with an accuracy of 54%
and F1 score &gt; 63%, which suggests that our model outperforms the current
state-of-the-art in text-based multi-label and multi-class patent
classification by a margin of &gt; 18% F1 score. We furthermore discuss the
applicability of the presented framework for semantic IP search, patent
landscaping, and technology intelligence. We finally point towards a future
research agenda for leveraging multi-source patent embeddings, their
appropriateness across applications, as well as to improve and validate patent
embeddings by creating domain-expert curated Semantic Textual Similarity (STS)
benchmark datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12212</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12212</id><submitter>Ange Lou</submitter><version version="v1"><date>Mon, 22 Mar 2021 22:39:30 GMT</date><size>987kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 19:45:09 GMT</date><size>987kb</size></version><title>CFPNet: Channel-wise Feature Pyramid for Real-Time Semantic Segmentation</title><authors>Ange Lou, Murray Loew</authors><categories>cs.CV</categories><comments>Accepted by ICIP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time semantic segmentation is playing a more important role in computer
vision, due to the growing demand for mobile devices and autonomous driving.
Therefore, it is very important to achieve a good trade-off among performance,
model size and inference speed. In this paper, we propose a Channel-wise
Feature Pyramid (CFP) module to balance those factors. Based on the CFP module,
we built CFPNet for real-time semantic segmentation which applied a series of
dilated convolution channels to extract effective features. Experiments on
Cityscapes and CamVid datasets show that the proposed CFPNet achieves an
effective combination of those factors. For the Cityscapes test dataset, CFPNet
achieves 70.1% class-wise mIoU with only 0.55 million parameters and 2.5 MB
memory. The inference speed can reach 30 FPS on a single RTX 2080Ti GPU with a
1024x2048-pixel image.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12347</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12347</id><submitter>Jihun Kang</submitter><version version="v1"><date>Tue, 23 Mar 2021 06:54:45 GMT</date><size>19946kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 12:06:29 GMT</date><size>19946kb</size><source_type>D</source_type></version><title>Shared Latent Space of Font Shapes and Impressions</title><authors>Jihun Kang, Daichi Haraguchi, Akisato Kimura, Seiichi Uchida</authors><categories>cs.CV</categories><comments>16 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We have specific impressions from the style of a typeface (font), suggesting
that there are correlations between font shape and its impressions. Based on
this hypothesis, we realize a shared latent space where a font shape image and
its impression words are embedded in a cross-modal manner. This latent space is
useful to understand the style-impression correlation and generate font images
by specifying several impression words. Experimental results with a large
style-impression dataset prove that it is possible to accurately realize the
shared latent space, especially for shape-relevant impression words, and then
use the space to generate font images with various impressions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12726</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12726</id><submitter>Hiroki Furuta</submitter><version version="v1"><date>Tue, 23 Mar 2021 17:49:50 GMT</date><size>1745kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 12:12:08 GMT</date><size>13565kb</size><source_type>D</source_type></version><title>Policy Information Capacity: Information-Theoretic Measure for Task
  Complexity in Deep Reinforcement Learning</title><authors>Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo,
  Sergey Levine, Ofir Nachum, Shixiang Shane Gu</authors><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted to ICML2021. The code is available at:
  https://github.com/frt03/pic</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Progress in deep reinforcement learning (RL) research is largely enabled by
benchmark task environments. However, analyzing the nature of those
environments is often overlooked. In particular, we still do not have agreeable
ways to measure the difficulty or solvability of a task, given that each has
fundamentally different actions, observations, dynamics, rewards, and can be
tackled with diverse RL algorithms. In this work, we propose policy information
capacity (PIC) -- the mutual information between policy parameters and episodic
return -- and policy-optimal information capacity (POIC) -- between policy
parameters and episodic optimality -- as two environment-agnostic,
algorithm-agnostic quantitative metrics for task difficulty. Evaluating our
metrics across toy environments as well as continuous control benchmark tasks
from OpenAI Gym and DeepMind Control Suite, we empirically demonstrate that
these information-theoretic metrics have higher correlations with normalized
task solvability scores than a variety of alternatives. Lastly, we show that
these metrics can also be used for fast and compute-efficient optimizations of
key design parameters such as reward shaping, policy architectures, and MDP
properties for better solvability by RL algorithms without ever running full RL
experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12803</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12803</id><submitter>Samy Wu Fung</submitter><version version="v1"><date>Tue, 23 Mar 2021 19:20:33 GMT</date><size>1462kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 01:27:46 GMT</date><size>1456kb</size><source_type>D</source_type></version><title>Fixed Point Networks: Implicit Depth Models with Jacobian-Free Backprop</title><authors>Samy Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley
  Osher, Wotao Yin</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A growing trend in deep learning replaces fixed depth models by
approximations of the limit as network depth approaches infinity. This approach
uses a portion of network weights to prescribe behavior by defining a limit
condition. This makes network depth implicit, varying based on the provided
data and an error tolerance. Moreover, existing implicit models can be
implemented and trained with fixed memory costs in exchange for additional
computational costs. In particular, backpropagation through implicit depth
models requires solving a Jacobian-based equation arising from the implicit
function theorem. We propose fixed point networks (FPNs), a simple setup for
implicit depth learning that guarantees convergence of forward propagation to a
unique limit defined by network weights and input data. Our key contribution is
to provide a new Jacobian-free backpropagation (JFB) scheme that circumvents
the need to solve Jacobian-based equations while maintaining fixed memory
costs. This makes FPNs much cheaper to train and easy to implement. Our
numerical examples yield state of the art classification results for implicit
depth models and outperform corresponding explicit models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12862</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12862</id><submitter>Melissa Antonelli</submitter><version version="v1"><date>Tue, 23 Mar 2021 21:49:16 GMT</date><size>84kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 13:15:50 GMT</date><size>88kb</size></version><title>On Counting Propositional Logic</title><authors>Melissa Antonelli, Ugo Dal Lago, Paolo Pistone</authors><categories>cs.LO cs.CC cs.PL</categories><acm-class>F.4.1; F.1.3; D.3.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study counting propositional logic as an extension of propositional logic
with counting quantifiers. We prove that the complexity of the underlying
decision problem perfectly matches the appropriate level of Wagner's counting
hierarchy, but also that the resulting logic admits a satisfactory
proof-theoretical treatment. From the latter, a type system for a probabilistic
lambda-calculus is derived in the spirit of the Curry-Howard correspondence,
showing the potential of counting propositional logic as a useful tool in
several fields of theoretical computer science.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12878</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12878</id><submitter>Renato Portugal</submitter><version version="v1"><date>Tue, 23 Mar 2021 22:57:07 GMT</date><size>13kb</size></version><version version="v2"><date>Wed, 31 Mar 2021 01:53:19 GMT</date><size>14kb</size></version><version version="v3"><date>Mon, 24 May 2021 13:01:54 GMT</date><size>129kb</size><source_type>D</source_type></version><title>Quantum walk-based search algorithms with multiple marked vertices</title><authors>G. A. Bezerra, P. H. G. Lug\~ao, and R. Portugal</authors><categories>quant-ph cs.CC cs.DS</categories><comments>12 pages, 1 table, 2 figs</comments><journal-ref>Phys. Rev. A 103, 062202, 2021</journal-ref><doi>10.1103/PhysRevA.103.062202</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The quantum walk is a powerful tool to develop quantum algorithms, which
usually are based on searching for a vertex in a graph with multiple marked
vertices, Ambainis's quantum algorithm for solving the element distinctness
problem being the most shining example. In this work, we address the problem of
calculating analytical expressions of the time complexity of finding a marked
vertex using quantum walk-based search algorithms with multiple marked vertices
on arbitrary graphs, extending previous analytical methods based on Szegedy's
quantum walk, which can be applied only to bipartite graphs. Two examples based
on the coined quantum walk on two-dimensional lattices and hypercubes show the
details of our method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12891</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12891</id><submitter>Shuonan Wu</submitter><version version="v1"><date>Tue, 23 Mar 2021 23:42:29 GMT</date><size>218kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 03:16:13 GMT</date><size>225kb</size><source_type>D</source_type></version><title>Robust BPX Preconditioner for Fractional Laplacians on Bounded Lipschitz
  Domains</title><authors>Juan Pablo Borthagaray, Ricardo H. Nochetto, Shuonan Wu, Jinchao Xu</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze a robust BPX preconditioner for the integral
fractional Laplacian on bounded Lipschitz domains. For either quasi-uniform
grids or graded bisection grids, we show that the condition numbers of the
resulting systems remain uniformly bounded with respect to both the number of
levels and the fractional power. The results apply also to the spectral and
censored fractional Laplacians.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12953</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12953</id><submitter>Dejiao Zhang</submitter><version version="v1"><date>Wed, 24 Mar 2021 03:05:17 GMT</date><size>1695kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:26:48 GMT</date><size>1695kb</size><source_type>D</source_type></version><title>Supporting Clustering with Contrastive Learning</title><authors>Dejiao Zhang, Feng Nan, Xiaokai Wei, Shangwen Li, Henghui Zhu,
  Kathleen McKeown, Ramesh Nallapati, Andrew Arnold, Bing Xiang</authors><categories>cs.LG cs.CL</categories><comments>NAACL 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Unsupervised clustering aims at discovering the semantic categories of data
according to some distance measured in the representation space. However,
different categories often overlap with each other in the representation space
at the beginning of the learning process, which poses a significant challenge
for distance-based clustering in achieving good separation between different
categories. To this end, we propose Supporting Clustering with Contrastive
Learning (SCCL) -- a novel framework to leverage contrastive learning to
promote better separation. We assess the performance of SCCL on short text
clustering and show that SCCL significantly advances the state-of-the-art
results on most benchmark datasets with 3%-11% improvement on Accuracy and
4%-15% improvement on Normalized Mutual Information. Furthermore, our
quantitative analysis demonstrates the effectiveness of SCCL in leveraging the
strengths of both bottom-up instance discrimination and top-down clustering to
achieve better intra-cluster and inter-cluster distances when evaluated with
the ground truth cluster labels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12983</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12983</id><submitter>Tri Minh Nguyen</submitter><version version="v1"><date>Wed, 24 Mar 2021 04:38:38 GMT</date><size>2173kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 01:28:49 GMT</date><size>3168kb</size><source_type>D</source_type></version><title>Counterfactual Explanation with Multi-Agent Reinforcement Learning for
  Drug Target Prediction</title><authors>Tri Minh Nguyen, Thomas P Quinn, Thin Nguyen, Truyen Tran</authors><categories>cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Motivation: Many high-performance DTA models have been proposed, but they are
mostly black-box and thus lack human interpretability. Explainable AI (XAI) can
make DTA models more trustworthy, and can also enable scientists to distill
biological knowledge from the models. Counterfactual explanation is one popular
approach to explaining the behaviour of a deep neural network, which works by
systematically answering the question &quot;How would the model output change if the
inputs were changed in this way?&quot;. Most counterfactual explanation methods only
operate on single input data. It remains an open problem how to extend
counterfactual-based XAI methods to DTA models, which have two inputs, one for
drug and one for target, that also happen to be discrete in nature.
  Methods: We propose a multi-agent reinforcement learning framework,
Multi-Agent Counterfactual Drug target binding Affinity (MACDA), to generate
counterfactual explanations for the drug-protein complex. Our proposed
framework provides human-interpretable counterfactual instances while
optimizing both the input drug and target for counterfactual generation at the
same time.
  Results: We benchmark the proposed MACDA framework using the Davis dataset
and find that our framework produces more parsimonious explanations with no
loss in explanation validity, as measured by encoding similarity and QED. We
then present a case study involving ABL1 and Nilotinib to demonstrate how MACDA
can explain the behaviour of a DTA model in the underlying substructure
interaction between inputs in its prediction, revealing mechanisms that align
with prior domain knowledge.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.13859</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.13859</id><submitter>Qing-Long Zhang</submitter><version version="v1"><date>Thu, 25 Mar 2021 14:16:02 GMT</date><size>13522kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 26 Mar 2021 08:56:42 GMT</date><size>13522kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 30 May 2021 14:51:34 GMT</date><size>13522kb</size><source_type>D</source_type></version><title>Group-CAM: Group Score-Weighted Visual Explanations for Deep
  Convolutional Networks</title><authors>Qinglong Zhang, Lu Rao, Yubin Yang</authors><categories>cs.CV cs.AI</categories><comments>Group-CAM is an efficient region-based saliency method, which can be
  used as an effective data augmentation trick</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose an efficient saliency map generation method, called
Group score-weighted Class Activation Mapping (Group-CAM), which adopts the
&quot;split-transform-merge&quot; strategy to generate saliency maps. Specifically, for
an input image, the class activations are firstly split into groups. In each
group, the sub-activations are summed and de-noised as an initial mask. After
that, the initial masks are transformed with meaningful perturbations and then
applied to preserve sub-pixels of the input (i.e., masked inputs), which are
then fed into the network to calculate the confidence scores. Finally, the
initial masks are weighted summed to form the final saliency map, where the
weights are confidence scores produced by the masked inputs. Group-CAM is
efficient yet effective, which only requires dozens of queries to the network
while producing target-related saliency maps. As a result, Group-CAM can be
served as an effective data augment trick for fine-tuning the networks. We
comprehensively evaluate the performance of Group-CAM on common-used
benchmarks, including deletion and insertion tests on ImageNet-1k, and pointing
game tests on COCO2017. Extensive experimental results demonstrate that
Group-CAM achieves better visual performance than the current state-of-the-art
explanation approaches. The code is available at
https://github.com/wofmanaf/Group-CAM.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.13956</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.13956</id><submitter>Guilherme D. da Fonseca</submitter><version version="v1"><date>Thu, 25 Mar 2021 16:24:49 GMT</date><size>568kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 17:21:46 GMT</date><size>605kb</size><source_type>D</source_type></version><title>Shadoks Approach to Low-Makespan Coordinated Motion Planning</title><authors>Lo\&quot;ic Crombez, Guilherme D. da Fonseca, Yan Gerard, Aldo
  Gonzalez-Lorenzo, Pascal Lafourcade, and Luc Libralesso</authors><categories>cs.CG cs.RO</categories><journal-ref>SoCG 2021, 63:1-9</journal-ref><doi>10.4230/LIPIcs.SoCG.2021.63</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper describes the heuristics used by the Shadoks team for the CG:SHOP
2021 challenge. This year's problem is to coordinate the motion of multiple
robots in order to reach their targets without collisions and minimizing the
makespan. Using the heuristics outlined in this paper, our team won first place
with the best solution to 202 out of 203 instances and optimal solutions to at
least 105 of them.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.14835</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.14835</id><submitter>Zhijie Deng</submitter><version version="v1"><date>Sat, 27 Mar 2021 07:48:58 GMT</date><size>731kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 06:42:21 GMT</date><size>732kb</size><source_type>D</source_type></version><title>LiBRe: A Practical Bayesian Approach to Adversarial Detection</title><authors>Zhijie Deng, Xiao Yang, Shizhen Xu, Hang Su, Jun Zhu</authors><categories>cs.LG cs.CR cs.CV</categories><comments>IEEE/ CVF International Conference on Computer Vision and Pattern
  Recognition (CVPR), 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite their appealing flexibility, deep neural networks (DNNs) are
vulnerable against adversarial examples. Various adversarial defense strategies
have been proposed to resolve this problem, but they typically demonstrate
restricted practicability owing to unsurmountable compromise on universality,
effectiveness, or efficiency. In this work, we propose a more practical
approach, Lightweight Bayesian Refinement (LiBRe), in the spirit of leveraging
Bayesian neural networks (BNNs) for adversarial detection. Empowered by the
task and attack agnostic modeling under Bayes principle, LiBRe can endow a
variety of pre-trained task-dependent DNNs with the ability of defending
heterogeneous adversarial attacks at a low cost. We develop and integrate
advanced learning techniques to make LiBRe appropriate for adversarial
detection. Concretely, we build the few-layer deep ensemble variational and
adopt the pre-training &amp; fine-tuning workflow to boost the effectiveness and
efficiency of LiBRe. We further provide a novel insight to realise adversarial
detection-oriented uncertainty quantification without inefficiently crafting
adversarial examples during training. Extensive empirical studies covering a
wide range of scenarios verify the practicability of LiBRe. We also conduct
thorough ablation studies to evidence the superiority of our modeling and
learning strategies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.14919</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.14919</id><submitter>Dongfang Li</submitter><version version="v1"><date>Sat, 27 Mar 2021 14:55:19 GMT</date><size>160kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 15:53:32 GMT</date><size>944kb</size><source_type>D</source_type></version><title>You Can Do Better! If You Elaborate the Reason When Making Prediction</title><authors>Dongfang Li, Jingcong Tao, Qingcai Chen, Baotian Hu</authors><categories>cs.CL cs.AI</categories><comments>14 pages, 1 figure, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural predictive models have achieved remarkable performance improvements in
various natural language processing tasks. However, most neural predictive
models suffer from the lack of explainability of predictions, limiting their
practical utility. This paper proposes a neural predictive approach to make a
prediction and generate its corresponding explanation simultaneously. It
leverages the knowledge entailed in explanations as an additional distillation
signal for more efficient learning. We conduct a preliminary study on Chinese
medical multiple-choice question answering, English natural language inference,
and commonsense question answering tasks. The experimental results show that
the proposed approach can generate reasonable explanations for its predictions
even with a small-scale training corpus. The proposed method also achieves
improved prediction accuracy on three datasets, which indicates that making
predictions can benefit from generating the explanation in the decision
process.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.14976</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.14976</id><submitter>Ana Lucic</submitter><version version="v1"><date>Sat, 27 Mar 2021 19:57:20 GMT</date><size>202kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 10:29:44 GMT</date><size>203kb</size></version><title>A Multistakeholder Approach Towards Evaluating AI Transparency
  Mechanisms</title><authors>Ana Lucic, Madhulika Srikumar, Umang Bhatt, Alice Xiang, Ankur Taly,
  Q. Vera Liao, Maarten de Rijke</authors><categories>cs.HC</categories><comments>Accepted to CHI 2021 Workshop on Operationalizing Human-Centered
  Perspectives in Explainable AI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given that there are a variety of stakeholders involved in, and affected by,
decisions from machine learning (ML) models, it is important to consider that
different stakeholders have different transparency needs. Previous work found
that the majority of deployed transparency mechanisms primarily serve technical
stakeholders. In our work, we want to investigate how well transparency
mechanisms might work in practice for a more diverse set of stakeholders by
conducting a large-scale, mixed-methods user study across a range of
organizations, within a particular industry such as health care, criminal
justice, or content moderation. In this paper, we outline the setup for our
study.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.15458</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.15458</id><submitter>Dimitris Tsakalidis</submitter><version version="v1"><date>Mon, 29 Mar 2021 09:44:52 GMT</date><size>1800kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 13:25:10 GMT</date><size>1880kb</size><source_type>D</source_type></version><title>A trustable and interoperable decentralized solution for citizen-centric
  and cross-border eGovernance: A conceptual approach</title><authors>George Domalis, Nikos Karacapilidis, Dimitris Tsakalidis, Anastasios
  Giannaros</authors><categories>cs.CY cs.NI</categories><comments>12 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Aiming to support a cross-sector and cross-border eGovernance paradigm for
sharing common public services, this paper introduces an AI-enhanced solution
that enables beneficiaries to participate in a decenntralized network for
effective big data exchange and service delivery that promotes the once-only
priority and is by design digital, efficient, cost-effective, interoperable and
secure. The solution comprises (i) a reliable and efficient decentralized
mechanism for data sharing, capable of addressing the complexity of the
processes and their high demand of resources; (ii) an ecosystem for delivering
mobile services tailored to the needs of stakeholders; (iii) a single sign-on
Wallet mechanism to manage the transactions with multiple services; and (iv) an
intercommunication layer, responsible for the secure exchange of information
among existing eGovernment systems with newly developed ones. An indicative
application scenario showcases the potential of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.15551</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.15551</id><submitter>A. M. Khalili</submitter><version version="v1"><date>Wed, 24 Mar 2021 14:04:03 GMT</date><size>559kb</size></version><version version="v2"><date>Mon, 5 Apr 2021 15:24:48 GMT</date><size>560kb</size></version><version version="v3"><date>Tue, 1 Jun 2021 20:02:36 GMT</date><size>561kb</size></version><title>Toward Building Science Discovery Machines</title><authors>Abdullah Khalili and Abdelhamid Bouchachia</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dream of building machines that can do science has inspired scientists
for decades. Remarkable advances have been made recently; however, we are still
far from achieving this goal. In this paper, we focus on the scientific
discovery process where a high level of reasoning and remarkable
problem-solving ability are required. We review different machine learning
techniques used in scientific discovery with their limitations. We survey and
discuss the main principles driving the scientific discovery process. These
principles are used in different fields and by different scientists to solve
problems and discover new knowledge. We provide many examples of the use of
these principles in different fields such as physics, mathematics, and biology.
We also review AI systems that attempt to implement some of these principles.
We argue that building science discovery machines should be guided by these
principles as an alternative to the dominant approach of current AI systems
that focuses on narrow objectives. Building machines that fully incorporate
these principles in an automated way might open the doors for many
advancements.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.15624</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.15624</id><submitter>Gabriel Kronberger</submitter><version version="v1"><date>Mon, 29 Mar 2021 14:04:18 GMT</date><size>1025kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 09:56:48 GMT</date><size>1025kb</size><source_type>D</source_type></version><title>Shape-constrained Symbolic Regression -- Improving Extrapolation with
  Prior Knowledge</title><authors>Gabriel Kronberger and Fabricio Olivetti de Fran\c{c}a and Bogdan
  Burlacu and Christian Haider and Michael Kommenda</authors><categories>cs.NE stat.ML</categories><doi>10.1162/evco_a_00294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the addition of constraints on the function image and its
derivatives for the incorporation of prior knowledge in symbolic regression.
The approach is called shape-constrained symbolic regression and allows us to
enforce e.g. monotonicity of the function over selected inputs. The aim is to
find models which conform to expected behaviour and which have improved
extrapolation capabilities. We demonstrate the feasibility of the idea and
propose and compare two evolutionary algorithms for shape-constrained symbolic
regression: i) an extension of tree-based genetic programming which discards
infeasible solutions in the selection step, and ii) a two population
evolutionary algorithm that separates the feasible from the infeasible
solutions. In both algorithms we use interval arithmetic to approximate bounds
for models and their partial derivatives. The algorithms are tested on a set of
19 synthetic and four real-world regression problems. Both algorithms are able
to identify models which conform to shape constraints which is not the case for
the unmodified symbolic regression algorithms. However, the predictive accuracy
of models with constraints is worse on the training set and the test set.
Shape-constrained polynomial regression produces the best results for the test
set but also significantly larger models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16336</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16336</id><submitter>Ranjan Maitra</submitter><version version="v1"><date>Tue, 30 Mar 2021 13:30:59 GMT</date><size>149kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 21:18:14 GMT</date><size>299kb</size><source_type>D</source_type></version><title>Model-based clustering of partial records</title><authors>Emily M. Goren and Ranjan Maitra</authors><categories>stat.ME astro-ph.HE cs.LG stat.CO stat.ML</categories><comments>14 pages, 4 figures, 1 table</comments><msc-class>62H30, 62G07, 62-04, 62P99</msc-class><acm-class>I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially recorded data are frequently encountered in many applications. In
practice, such datasets are usually clustered by removing incomplete cases or
features with missing values, or by imputing missing values, followed by
application of a clustering algorithm to the resulting altered data set. Here,
we develop clustering methodology through a model-based approach using the
marginal density for the observed values, assuming a finite mixture model of
multivariate $t$ distributions. We compare our algorithm to the corresponding
full expectation-maximization (EM) approach that considers the missing values
in the incomplete data set and makes a missing at random (MAR) assumption, as
well as case deletion and imputation methods. Since only the observed values
are utilized, our approach is computationally more efficient than imputation or
full EM. Simulation studies demonstrate that our approach has favorable
recovery of the true cluster partition compared to case deletion and imputation
under various missingness mechanisms, and is more robust to extreme MAR
violations than the full EM approach which we surmise is because it does not
use the observed values to inform those that are missing. Our methodology is
demonstrated on a problem of clustering gamma-ray bursts and is implemented at
\url{https://github.com/emilygoren/MixtClust}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16681</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16681</id><submitter>Jan Christoph Schlegel</submitter><version version="v1"><date>Tue, 30 Mar 2021 21:05:29 GMT</date><size>38kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 12 Apr 2021 17:23:59 GMT</date><size>38kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 May 2021 20:29:06 GMT</date><size>189kb</size><source_type>D</source_type></version><title>On-Chain Auctions with Deposits</title><authors>Jan Christoph Schlegel, Akaki Mamageishvili</authors><categories>cs.GT econ.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Second-price auctions with deposits are frequently used in blockchain
environments. An auction takes place on-chain: bidders deposit an amount that
fully covers their bid (but possibly exceeds it) in a smart contract. The
deposit is used as insurance against bidders not honoring their bid if they
win. The deposit, but not the bid, is publicly observed during the bidding
phase of the auction.
  The visibility of deposits can fundamentally change the strategic structure
of the auction if bidding happens sequentially: Bidding is costly since deposit
are costly to make. Thus, deposits can be used as a costly signal for a high
valuation. This is the source of multiple inefficiencies: To engage in costly
signalling, a bidder who bids first and has a high valuation will generally
over-deposit in equilibrium, i.e.~deposit more than he will bid. If high
valuations are likely there can, moreover, be entry deterrence through high
deposits: a bidder who bids first can deter subsequent bidders from entering
the auction. Pooling can happen in equilibrium, where bidders of different
valuations deposit the same amount. The auction fails to allocate the item to
the bidder with the highest valuation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16704</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16704</id><submitter>Hongjing Lu</submitter><version version="v1"><date>Tue, 30 Mar 2021 22:14:13 GMT</date><size>1406kb</size></version><version version="v2"><date>Sat, 29 May 2021 20:52:03 GMT</date><size>1760kb</size></version><title>Probabilistic Analogical Mapping with Semantic Relation Networks</title><authors>Hongjing Lu, Nicholas Ichien, Keith J. Holyoak</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The human ability to flexibly reason using analogies with domain-general
content depends on mechanisms for identifying relations between concepts, and
for mapping concepts and their relations across analogs. Building on a recent
model of how semantic relations can be learned from non-relational word
embeddings, we present a new computational model of mapping between two
analogs. The model adopts a Bayesian framework for probabilistic graph
matching, operating on semantic relation networks constructed from distributed
representations of individual concepts and of relations between concepts.
Through comparisons of model predictions with human performance in a novel
mapping task requiring integration of multiple relations, as well as in several
classic studies, we demonstrate that the model accounts for a broad range of
phenomena involving analogical mapping by both adults and children. We also
show the potential for extending the model to deal with analog retrieval. Our
approach demonstrates that human-like analogical mapping can emerge from
comparison mechanisms applied to rich semantic representations of individual
concepts and relations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16858</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16858</id><submitter>Helin Wang</submitter><version version="v1"><date>Wed, 31 Mar 2021 07:15:52 GMT</date><size>5052kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 09:07:17 GMT</date><size>2429kb</size><source_type>D</source_type></version><title>SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic
  Scene Classification</title><authors>Helin Wang, Yuexian Zou, Wenwu Wang</authors><categories>eess.AS cs.SD</categories><comments>Submitted to Interspeech 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present SpecAugment++, a novel data augmentation method for
deep neural networks based acoustic scene classification (ASC). Different from
other popular data augmentation methods such as SpecAugment and mixup that only
work on the input space, SpecAugment++ is applied to both the input space and
the hidden space of the deep neural networks to enhance the input and the
intermediate feature representations. For an intermediate hidden state, the
augmentation techniques consist of masking blocks of frequency channels and
masking blocks of time frames, which improve generalization by enabling a model
to attend not only to the most discriminative parts of the feature, but also
the entire parts. Apart from using zeros for masking, we also examine two
approaches for masking based on the use of other samples within the minibatch,
which helps introduce noises to the networks to make them more discriminative
for classification. The experimental results on the DCASE 2018 Task1 dataset
and DCASE 2019 Task1 dataset show that our proposed method can obtain 3.6% and
4.7% accuracy gains over a strong baseline without augmentation (i.e.
CP-ResNet) respectively, and outperforms other previous data augmentation
methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16898</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16898</id><submitter>Wojciech Ozga</submitter><version version="v1"><date>Wed, 31 Mar 2021 08:31:07 GMT</date><size>365kb</size><source_type>D</source_type></version><title>Perun: Secure Multi-Stakeholder Machine Learning Framework with GPU
  Support</title><authors>Wojciech Ozga, Do Le Quoc, Christof Fetzer</authors><categories>cs.LG cs.AI cs.CR</categories><journal-ref>The 35th Annual IFIP Conference on Data and Applications Security
  and Privacy (DBSec 2021)</journal-ref><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Confidential multi-stakeholder machine learning (ML) allows multiple parties
to perform collaborative data analytics while not revealing their intellectual
property, such as ML source code, model, or datasets. State-of-the-art
solutions based on homomorphic encryption incur a large performance overhead.
Hardware-based solutions, such as trusted execution environments (TEEs),
significantly improve the performance in inference computations but still
suffer from low performance in training computations, e.g., deep neural
networks model training, because of limited availability of protected memory
and lack of GPU support.
  To address this problem, we designed and implemented Perun, a framework for
confidential multi-stakeholder machine learning that allows users to make a
trade-off between security and performance. Perun executes ML training on
hardware accelerators (e.g., GPU) while providing security guarantees using
trusted computing technologies, such as trusted platform module and integrity
measurement architecture. Less compute-intensive workloads, such as inference,
execute only inside TEE, thus at a lower trusted computing base. The evaluation
shows that during the ML training on CIFAR-10 and real-world medical datasets,
Perun achieved a 161x to 1560x speedup compared to a pure TEE-based approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.17258</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.17258</id><submitter>Hiroki Furuta</submitter><version version="v1"><date>Wed, 31 Mar 2021 17:55:20 GMT</date><size>4753kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 21:27:30 GMT</date><size>6053kb</size><source_type>D</source_type></version><title>Co-Adaptation of Algorithmic and Implementational Innovations in
  Inference-based Deep Reinforcement Learning</title><authors>Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo,
  Shixiang Shane Gu</authors><categories>cs.LG cs.AI stat.ML</categories><comments>The implementation is available at:
  https://github.com/frt03/inference-based-rl</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently many algorithms were devised for reinforcement learning (RL) with
function approximation. While they have clear algorithmic distinctions, they
also have many implementation differences that are algorithm-independent and
sometimes under-emphasized. Such mixing of algorithmic novelty and
implementation craftsmanship makes rigorous analyses of the sources of
performance improvements across algorithms difficult. In this work, we focus on
a series of off-policy inference-based actor-critic algorithms -- MPO, AWR, and
SAC -- to decouple their algorithmic innovations and implementation decisions.
We present unified derivations through a single control-as-inference objective,
where we can categorize each algorithm as based on either
Expectation-Maximization (EM) or direct Kullback-Leibler (KL) divergence
minimization and treat the rest of specifications as implementation details. We
performed extensive ablation studies, and identified substantial performance
drops whenever implementation details are mismatched for algorithmic choices.
These results show which implementation details are co-adapted and co-evolved
with algorithms, and which are transferable across algorithms: as examples, we
identified that tanh Gaussian policy and network sizes are highly adapted to
algorithmic types, while layer normalization and ELU are critical for MPO's
performances but also transfer to noticeable gains in SAC. We hope our work can
inspire future work to further demystify sources of performance improvements
across multiple algorithms and allow researchers to build on one another's both
algorithmic and implementational innovations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.00099</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.00099</id><submitter>Hudson Bruno</submitter><version version="v1"><date>Wed, 31 Mar 2021 20:35:10 GMT</date><size>15250kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 13:44:22 GMT</date><size>16193kb</size><source_type>D</source_type></version><title>LIFT-SLAM: a deep-learning feature-based monocular visual SLAM method</title><authors>Hudson M. S. Bruno and Esther L. Colombini</authors><categories>cs.CV cs.AI cs.RO</categories><comments>30 pages, Published in Neurocomputing</comments><doi>10.1016/j.neucom.2021.05.027</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Simultaneous Localization and Mapping (SLAM) problem addresses the
possibility of a robot to localize itself in an unknown environment and
simultaneously build a consistent map of this environment. Recently, cameras
have been successfully used to get the environment's features to perform SLAM,
which is referred to as visual SLAM (VSLAM). However, classical VSLAM
algorithms can be easily induced to fail when either the motion of the robot or
the environment is too challenging. Although new approaches based on Deep
Neural Networks (DNNs) have achieved promising results in VSLAM, they still are
unable to outperform traditional methods. To leverage the robustness of deep
learning to enhance traditional VSLAM systems, we propose to combine the
potential of deep learning-based feature descriptors with the traditional
geometry-based VSLAM, building a new VSLAM system called LIFT-SLAM. Experiments
conducted on KITTI and Euroc datasets show that deep learning can be used to
improve the performance of traditional VSLAM systems, as the proposed approach
was able to achieve results comparable to the state-of-the-art while being
robust to sensorial noise. We enhance the proposed VSLAM pipeline by avoiding
parameter tuning for specific datasets with an adaptive approach while
evaluating how transfer learning can affect the quality of the features
extracted.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01378</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01378</id><submitter>Junbo Zhang</submitter><version version="v1"><date>Sat, 3 Apr 2021 11:31:59 GMT</date><size>1034kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 13:12:02 GMT</date><size>1029kb</size><source_type>D</source_type></version><title>speechocean762: An Open-Source Non-native English Speech Corpus For
  Pronunciation Assessment</title><authors>Junbo Zhang, Zhiwen Zhang, Yongqing Wang, Zhiyong Yan, Qiong Song,
  Yukai Huang, Ke Li, Daniel Povey and Yujun Wang</authors><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted in INTERSPEECH 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a new open-source speech corpus named &quot;speechocean762&quot;
designed for pronunciation assessment use, consisting of 5000 English
utterances from 250 non-native speakers, where half of the speakers are
children. Five experts annotated each of the utterances at sentence-level,
word-level and phoneme-level. A baseline system is released in open source to
illustrate the phoneme-level pronunciation assessment workflow on this corpus.
This corpus is allowed to be used freely for commercial and non-commercial
purposes. It is available for free download from OpenSLR, and the corresponding
baseline system is published in the Kaldi speech recognition toolkit.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01398</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01398</id><submitter>Tatsuya Chuman</submitter><version version="v1"><date>Sat, 3 Apr 2021 13:10:44 GMT</date><size>1206kb</size></version><version version="v2"><date>Sat, 29 May 2021 15:15:22 GMT</date><size>379kb</size></version><title>Block Scrambling Image Encryption Used in Combination with Data
  Augmentation for Privacy-Preserving DNNs</title><authors>Tatsuya Chuman, Hitoshi Kiya</authors><categories>cs.CR eess.IV</categories><comments>To be appeared in IEEE ICCE-TW 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel learnable image encryption method for
privacy-preserving deep neural networks (DNNs). The proposed method is carried
out on the basis of block scrambling used in combination with data augmentation
techniques such as random cropping, horizontal flip and grid mask. The use of
block scrambling enhances robustness against various attacks, and in contrast,
the combination with data augmentation enables us to maintain a high
classification accuracy even when using encrypted images. In an image
classification experiment, the proposed method is demonstrated to be effective
in privacy-preserving DNNs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01504</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01504</id><submitter>Elyes Balti</submitter><version version="v1"><date>Sun, 4 Apr 2021 00:10:44 GMT</date><size>130kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 10 Apr 2021 20:01:25 GMT</date><size>126kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 03:20:29 GMT</date><size>126kb</size><source_type>D</source_type></version><title>Adaptive Self-Interference Cancellation for Full-Duplex Wireless
  Communication Systems</title><authors>Elyes Balti, Brian L. Evans</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider single-cell, single-user systems wherein uplink
and downlink user equipment communicate with a full-duplex relay. Due to the
near-far problem, the self-interference (SI) can be 100-1000x the received
signal power. In this context, we consider the adaptive Least Mean Squares
(LMS) algorithm to estimate the SI channel and then subtract the SI from the
desired received signal before the analog-to-digital converter (ADC). We
measure the robustness of this technique in terms of bit error rate (BER) and
spectral efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01577</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01577</id><submitter>Sobirdzhon Bobiev</submitter><version version="v1"><date>Sun, 4 Apr 2021 10:02:45 GMT</date><size>156kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 6 Apr 2021 05:47:21 GMT</date><size>156kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 30 May 2021 13:16:51 GMT</date><size>154kb</size><source_type>D</source_type></version><title>Class-incremental Learning using a Sequence of Partial Implicitly
  Regularized Classifiers</title><authors>Sobirdzhon Bobiev, Adil Khan, Syed Muhammad Ahsan Raza Kazmi</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In class-incremental learning, the objective is to learn a number of classes
sequentially without having access to the whole training data. However, due to
a problem known as catastrophic forgetting, neural networks suffer substantial
performance drop in such settings. The problem is often approached by
experience replay, a method which stores a limited number of samples to be
replayed in future steps to reduce forgetting of the learned classes. When
using a pretrained network as a feature extractor, we show that instead of
training a single classifier incrementally, it is better to train a number of
specialized classifiers which do not interfere with each other yet can
cooperatively predict a single class. Our experiments on CIFAR100 dataset show
that the proposed method improves the performance over SOTA by a large margin.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01808</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01808</id><submitter>Ziyue Huang</submitter><version version="v1"><date>Mon, 5 Apr 2021 08:15:20 GMT</date><size>388kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 13:24:01 GMT</date><size>396kb</size><source_type>D</source_type></version><title>Frequency Estimation Under Multiparty Differential Privacy: One-shot and
  Streaming</title><authors>Ziyue Huang, Yuan Qiu, Ke Yi, Graham Cormode</authors><categories>cs.CR cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental problem of frequency estimation under both privacy
and communication constraints, where the data is distributed among $k$ parties.
We consider two application scenarios: (1) one-shot, where the data is static
and the aggregator conducts a one-time computation; and (2) streaming, where
each party receives a stream of items over time and the aggregator continuously
monitors the frequencies. We adopt the model of multiparty differential privacy
(MDP), which is more general than local differential privacy (LDP) and
(centralized) differential privacy. Our protocols achieve optimality (up to
logarithmic factors) permissible by the more stringent of the two constraints.
In particular, when specialized to the $\varepsilon$-LDP model, our protocol
achieves an error of $\sqrt{k}/(e^{\Theta(\varepsilon)}-1)$ using $O(k\max\{
\varepsilon, \frac{1}{\varepsilon} \})$ bits of communication and $O(k \log u)$
bits of public randomness, where $u$ is the size of the domain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.02095</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.02095</id><submitter>Aleksandr Beknazaryan</submitter><version version="v1"><date>Mon, 5 Apr 2021 18:02:04 GMT</date><size>15kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 13:01:53 GMT</date><size>13kb</size></version><title>Deep neural network approximation of analytic functions</title><authors>Aleksandr Beknazaryan</authors><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We provide an entropy bound for the spaces of neural networks with piecewise
linear activation functions, such as the ReLU and the absolute value functions.
This bound generalizes the known entropy bound for the space of linear
functions on $\mathbb{R}^d$ and it depends on the value at the point
$(1,1,...,1)$ of the networks obtained by taking the absolute values of all
parameters of original networks. Keeping this value together with the depth,
width and the parameters of the networks to have logarithmic dependence on
$1/\varepsilon$, we $\varepsilon$-approximate functions that are analytic on
certain regions of $\mathbb{C}^d$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.02141</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.02141</id><submitter>Yahao Chen</submitter><version version="v1"><date>Mon, 5 Apr 2021 20:32:31 GMT</date><size>505kb</size></version><version version="v2"><date>Sun, 30 May 2021 12:08:15 GMT</date><size>533kb</size></version><title>Feedback linearization of nonlinear differential-algebraic control
  systems</title><authors>Yahao Chen</authors><categories>math.OC cs.SY eess.SY</categories><comments>20 pages</comments><msc-class>34A09, 93B17, 93B27, 93C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study feedback linearization problems for nonlinear
differential-algebraic control systems (DACSs). We consider two kinds of
feedback equivalences, namely, the external feedback equivalence, which is
defined (locally) on the whole generalized state space, and the internal
feedback equivalence, which is defined on the locally maximal controlled
invariant submanifold (i.e., on the set where solutions exist). Necessary and
sufficient conditions are given for the locally internal and the locally
external feedback linearizability of DACSs with the help of a notion called the
explicitation with driving variables, which attaches a class of ordinary
differential equation control systems (ODECSs) to a given DACS. We show that
the feedback linearizability of a DACS is closely related to the involutivity
of the linearizability distributions of the explicitation systems. Finally, we
apply our results of feedback linearization of DACSs to an academical example
and a constrained mechanical system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.02739</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.02739</id><submitter>Albert Cheu</submitter><version version="v1"><date>Tue, 6 Apr 2021 18:24:57 GMT</date><size>199kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 3 May 2021 18:55:35 GMT</date><size>299kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 16:31:17 GMT</date><size>303kb</size><source_type>D</source_type></version><title>Differentially Private Histograms in the Shuffle Model from Fake Users</title><authors>Albert Cheu, Maxim Zhilyaev</authors><categories>cs.CR cs.DS</categories><comments>29 pages. May 3 updates: (1) experiments that compared results with
  prior work. (2) moved count-min section to the main body (3) expanded
  introduction May 29 updates: (1) further improved introduction (2) enhanced
  reduction in communication complexity (3) included reference to GKMP20
  protocol</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been much recent work in the shuffle model of differential privacy,
particularly for approximate $d$-bin histograms. While these protocols achieve
low error, the number of messages sent by each user -- the message complexity
-- has so far scaled with $d$ or the privacy parameters. The message complexity
is an informative predictor of a shuffle protocol's resource consumption. We
present a protocol whose message complexity is two when there are sufficiently
many users. The protocol essentially pairs each row in the dataset with a fake
row and performs a simple randomization on all rows. We show that the error
introduced by the protocol is small, using rigorous analysis as well as
experiments on real-world data. We also prove that corrupt users have a
relatively low impact on our protocol's estimates.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.02963</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.02963</id><submitter>Jinlai Zhang</submitter><version version="v1"><date>Wed, 7 Apr 2021 07:28:46 GMT</date><size>116kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 13:15:53 GMT</date><size>118kb</size><source_type>D</source_type></version><title>The art of defense: letting networks fool the attacker</title><authors>Jinlai Zhang, Binbin Liu, Lyvjie Chen, Bo Ouyang, Jihong Zhu, Minchi
  Kuang, Houqing Wang, Yanmei Meng</authors><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Some deep neural networks are invariant to some input transformations, such
as Pointnet is permutation invariant to the input point cloud. In this paper,
we demonstrated this property could be powerful in defense of gradient-based
attacks. Specifically, we apply random input transformation which is invariant
to the networks we want to defend. Extensive experiments demonstrate that the
proposed scheme defeats various gradient-based attackers in the targeted attack
setting, and breaking the attack accuracy into nearly zero. Our code is
available at: {\footnotesize{\url{https://github.com/cuge1995/IT-Defense}}}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.03224</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.03224</id><submitter>Michael Kaufmann</submitter><version version="v1"><date>Wed, 7 Apr 2021 16:23:19 GMT</date><size>749kb</size></version><version version="v2"><date>Mon, 31 May 2021 14:43:37 GMT</date><size>732kb</size></version><title>Efficient and Accurate In-Database Machine Learning with SQL Code
  Generation in Python</title><authors>Michael Kaufmann, Gabriel Stechschulte, Anna Huber</authors><categories>cs.DB cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Following an analysis of the advantages of SQL-based Machine Learning (ML)
and a short literature survey of the field, we describe a novel method for
In-Database Machine Learning (IDBML). We contribute a process for SQL-code
generation in Python using template macros in Jinja2 as well as the prototype
implementation of the process. We describe our implementation of the process to
compute multidimensional histogram (MDH) probability estimation in SQL. For
this, we contribute and implement a novel discretization method called equal
quantized rank binning (EQRB) and equal-width binning (EWB). Based on this, we
provide data gathered in a benchmarking experiment for the quantitative
empirical evaluation of our method and system using the Covertype dataset. We
measured accuracy and computation time and compared it to Scikit Learn state of
the art classification algorithms. Using EWB, our multidimensional probability
estimation was the fastest of all tested algorithms, while being only 1-2% less
accurate than the best state of the art methods found (decision trees and
random forests). Our method was significantly more accurate than Naive Bayes,
which assumes independent one-dimensional probabilities and/or densities. Also,
our method was significantly more accurate and faster than logistic regression.
This motivates for further research in accuracy improvement and in IDBML with
SQL code generation for big data and larger-than-memory datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04132</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04132</id><submitter>Tyler Hayes</submitter><version version="v1"><date>Thu, 1 Apr 2021 15:19:08 GMT</date><size>343kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 21:01:25 GMT</date><size>347kb</size><source_type>D</source_type></version><title>Replay in Deep Learning: Current Approaches and Missing Biological
  Elements</title><authors>Tyler L. Hayes, Giri P. Krishnan, Maxim Bazhenov, Hava T. Siegelmann,
  Terrence J. Sejnowski, Christopher Kanan</authors><categories>q-bio.NC cs.AI cs.LG</categories><comments>Accepted for publication in the MIT Press journal of Neural
  Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replay is the reactivation of one or more neural patterns, which are similar
to the activation patterns experienced during past waking experiences. Replay
was first observed in biological neural networks during sleep, and it is now
thought to play a critical role in memory formation, retrieval, and
consolidation. Replay-like mechanisms have been incorporated into deep
artificial neural networks that learn over time to avoid catastrophic
forgetting of previous knowledge. Replay algorithms have been successfully used
in a wide range of deep learning methods within supervised, unsupervised, and
reinforcement learning paradigms. In this paper, we provide the first
comprehensive comparison between replay in the mammalian brain and replay in
artificial neural networks. We identify multiple aspects of biological replay
that are missing in deep learning systems and hypothesize how they could be
utilized to improve artificial neural networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04547</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04547</id><submitter>Garrett A. Stevenson</submitter><version version="v1"><date>Fri, 9 Apr 2021 18:18:26 GMT</date><size>1082kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 06:33:10 GMT</date><size>1771kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 01:13:21 GMT</date><size>1762kb</size><source_type>D</source_type></version><title>High-Throughput Virtual Screening of Small Molecule Inhibitors for
  SARS-CoV-2 Protein Targets with Deep Fusion Models</title><authors>Garrett A. Stevenson, Derek Jones, Hyojin Kim, W. F. Drew Bennett,
  Brian J. Bennion, Monica Borucki, Feliza Bourguet, Aidan Epstein, Magdalena
  Franco, Brooke Harmon, Stewart He, Max P. Katz, Daniel Kirshner, Victoria
  Lao, Edmond Y. Lau, Jacky Lo, Kevin McLoughlin, Richard Mosesso, Deepa K.
  Murugesh, Oscar A. Negrete, Edwin A. Saada, Brent Segelke, Maxwell Stefan,
  Marisa W. Torres, Dina Weilhammer, Sergio Wong, Yue Yang, Adam Zemla, Xiaohua
  Zhang, Fangqiang Zhu, Felice C. Lightstone, Jonathan E. Allen</authors><categories>cs.LG q-bio.BM</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Structure-based Deep Fusion models were recently shown to outperform several
physics- and machine learning-based protein-ligand binding affinity prediction
methods. As part of a multi-institutional COVID-19 pandemic response, over 500
million small molecules were computationally screened against four protein
structures from the novel coronavirus (SARS-CoV-2), which causes COVID-19.
Three enhancements to Deep Fusion were made in order to evaluate more than 5
billion docked poses on SARS-CoV-2 protein targets. First, the Deep Fusion
concept was refined by formulating the architecture as one, coherently
backpropagated model (Coherent Fusion) to improve binding-affinity prediction
accuracy. Secondly, the model was trained using a distributed, genetic
hyper-parameter optimization. Finally, a scalable, high-throughput screening
capability was developed to maximize the number of ligands evaluated and
expedite the path to experimental evaluation. In this work, we present both the
methods developed for machine learning-based high-throughput screening and
results from using our computational pipeline to find SARS-CoV-2 inhibitors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04637</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04637</id><submitter>Abdelhaliem Babiker</submitter><version version="v1"><date>Fri, 9 Apr 2021 23:15:23 GMT</date><size>263kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 17:35:07 GMT</date><size>196kb</size></version><version version="v3"><date>Wed, 2 Jun 2021 13:33:47 GMT</date><size>196kb</size></version><title>A Novel Provably Secure Key Agreement Protocol Based On Binary Matrices</title><authors>Abdelhaliem Babiker</authors><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, a new key agreement protocol is presented. The protocol uses
exponentiation of matrices over GF(2) to establish the key agreement. Security
analysis of the protocol shows that the shared secret key is indistinguishable
from the random under Decisional Diffie-Hellman (DDH) assumption for subgroup
of matrices over GF(2) with prime order, and furthermore, the analysis shows
that, unlike many other exponentiation based protocols, security of the
protocol goes beyond the level of security provided by (DDH) assumption and
intractability of Discrete Logarithm Problem (DLP). Actually, security of the
protocol completely transcends the reliance on the DLP in the sense that
breaking the DLP does not mean breaking the protocol. Complexity of brute force
attack on the protocol is equivalent to exhaustive search for the secret key.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04692</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04692</id><submitter>Hongqiu Wu</submitter><version version="v1"><date>Sat, 10 Apr 2021 06:24:52 GMT</date><size>30kb</size></version><version version="v2"><date>Sat, 29 May 2021 12:13:12 GMT</date><size>578kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 03:09:39 GMT</date><size>570kb</size><source_type>D</source_type></version><title>Not All Attention Is All You Need</title><authors>Hongqiu Wu and Hai Zhao and Min Zhang</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beyond the success story of pre-trained language models (PrLMs) in recent
natural language processing, they are susceptible to over-fitting due to
unusual large model size. To this end, dropout serves as a therapy. However,
existing methods like random-based, knowledge-based and search-based dropout
are more general but less effective onto self-attention based models, which are
broadly chosen as the fundamental architecture of PrLMs. In this paper, we
propose a novel dropout method named AttendOut to let self-attention empowered
PrLMs capable of more robust task-specific tuning. We demonstrate that
state-of-the-art models with elaborate training design may achieve much
stronger results. We verify the universality of our approach on extensive
natural language processing tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04940</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04940</id><submitter>Gerardo Lauro Maldonado Mart\'inez</submitter><version version="v1"><date>Sun, 11 Apr 2021 07:21:57 GMT</date><size>61kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 18:19:51 GMT</date><size>177kb</size><source_type>D</source_type></version><title>Dissecting the square into seven congruent parts</title><authors>Gerardo L. Maldonado and Edgardo Rold\'an-Pensado</authors><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a computer-based proof of the following fact: If a square is tiled by
seven convex tiles which are congruent among themselves, then the tiles are
rectangles. This confirms a new case of a conjecture posed by Yuen, Zamfirescu
and Zamfirescu.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.05003</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.05003</id><submitter>Chengjin Xu</submitter><version version="v1"><date>Sun, 11 Apr 2021 12:26:50 GMT</date><size>3152kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 08:51:14 GMT</date><size>3153kb</size><source_type>D</source_type></version><title>Multiple Run Ensemble Learning with Low-Dimensional Knowledge Graph
  Embeddings</title><authors>Chengjin Xu, Mojtaba Nayyeri, Sahar Vahdati, and Jens Lehmann</authors><categories>cs.AI</categories><comments>Accepted by the 2021 International Joint Conference on Neural
  Networks (IJCNN 2021)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Among the top approaches of recent years, link prediction using knowledge
graph embedding (KGE) models has gained significant attention for knowledge
graph completion. Various embedding models have been proposed so far, among
which, some recent KGE models obtain state-of-the-art performance on link
prediction tasks by using embeddings with a high dimension (e.g. 1000) which
accelerate the costs of training and evaluation considering the large scale of
KGs. In this paper, we propose a simple but effective performance boosting
strategy for KGE models by using multiple low dimensions in different
repetition rounds of the same model. For example, instead of training a model
one time with a large embedding size of 1200, we repeat the training of the
model 6 times in parallel with an embedding size of 200 and then combine the 6
separate models for testing while the overall numbers of adjustable parameters
are same (6*200=1200) and the total memory footprint remains the same. We show
that our approach enables different models to better cope with their
expressiveness issues on modeling various graph patterns such as symmetric,
1-n, n-1 and n-n. In order to justify our findings, we conduct experiments on
various KGE models. Experimental results on standard benchmark datasets, namely
FB15K, FB15K-237 and WN18RR, show that multiple low-dimensional models of the
same kind outperform the corresponding single high-dimensional models on link
prediction in a certain range and have advantages in training efficiency by
using parallel training while the overall numbers of adjustable parameters are
same.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.05571</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.05571</id><submitter>Junwhan Kim</submitter><version version="v1"><date>Mon, 12 Apr 2021 15:40:43 GMT</date><size>1091kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 23:55:51 GMT</date><size>995kb</size></version><title>Using a Neural Network to Detect Anomalies given an N-gram Profile</title><authors>Byunggu Yu, Junwhan Kim</authors><categories>cs.CR cs.LG</categories><comments>17 pages, 7 figures, 5th International Symposium on Cyber Security
  Cryptology and Machine Learning (CSCML 2021)</comments><acm-class>D.4.6; I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to detect unknown intrusions and runtime errors of computer
programs, the cyber-security community has developed various detection
techniques. Anomaly detection is an approach that is designed to profile the
normal runtime behavior of computer programs in order to detect intrusions and
errors as anomalous deviations from the observed normal. However, normal but
unobserved behavior can trigger false positives. This limitation has
significantly decreased the practical viability of anomaly detection
techniques. Reported approaches to this limitation span a simple alert
threshold definition to distribution models for approximating all normal
behavior based on the limited observation. However, each assumption or
approximation poses the potential for even greater false positive rates. This
paper presents our study on how to explain the presence of anomalies using a
neural network, particularly Long Short-Term Memory, independent of actual data
distributions. We present and compare three anomaly detection models, and
report on our experience running different types of attacks on an Apache
Hypertext Transfer Protocol server. We performed a comparative study, focusing
on each model's ability to detect the onset of each attack while avoiding false
positives resulting from unknown normal behavior. Our best-performing model
detected the true onset of every attack with zero false positives.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.06131</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.06131</id><submitter>Shuping Dang</submitter><version version="v1"><date>Tue, 13 Apr 2021 12:08:28 GMT</date><size>2968kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 06:46:46 GMT</date><size>5939kb</size><source_type>D</source_type></version><title>Big Communications: Connect the Unconnected</title><authors>Shuping Dang, Chuanting Zhang, Basem Shihada, Mohamed-Slim Alouini</authors><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present the analysis of the digital divide to illustrate
the unfair access to the benefits brought by information and communications
technology (ICT) over the globe and provide our solution termed big
communications (BigCom) to close the digital divide and democratize the
benefits of ICT. To facilitate the implementation of BigCom, we give a complete
framework of BigCom considering both technological and non-technological
factors as well as a set of suggestions for content providers, mobile network
operators, and governments. By implementing BigCom, we aim to connect the last
four billion unconnected people living in far-flung and underdeveloped areas
and achieve the goal of global and ubiquitous connectivity for everyone in the
6G era.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.06135</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.06135</id><submitter>Nis Meinert</submitter><version version="v1"><date>Tue, 13 Apr 2021 12:20:18 GMT</date><size>789kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 15 Apr 2021 12:47:38 GMT</date><size>790kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 07:41:18 GMT</date><size>4402kb</size><source_type>D</source_type></version><title>Multivariate Deep Evidential Regression</title><authors>Nis Meinert and Alexander Lavin</authors><categories>cs.LG cs.NE stat.ML</categories><comments>20 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  There is significant need for principled uncertainty reasoning in machine
learning systems as they are increasingly deployed in safety-critical domains.
A new approach with uncertainty-aware neural networks (NNs), based on learning
evidential distributions for aleatoric and epistemic uncertainties, shows
promise over traditional deterministic methods and typical Bayesian NNs, yet
several important gaps in the theory and implementation of these networks
remain. We discuss three issues with a proposed solution to extract aleatoric
and epistemic uncertainties from regression-based neural networks. The approach
derives a technique by placing evidential priors over the original Gaussian
likelihood function and training the NN to infer the hyperparameters of the
evidential distribution. Doing so allows for the simultaneous extraction of
both uncertainties without sampling or utilization of out-of-distribution data
for univariate regression tasks. We describe the outstanding issues in detail,
provide a possible solution, and generalize the deep evidential regression
technique for multivariate cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.06601</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.06601</id><submitter>Ye Zheng</submitter><version version="v1"><date>Wed, 14 Apr 2021 03:02:48 GMT</date><size>3591kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 03:05:23 GMT</date><size>8211kb</size><source_type>D</source_type></version><title>Zero-Shot Instance Segmentation</title><authors>Ye Zheng, Jiahong Wu, Yongqiang Qin, Faen Zhang, Li Cui</authors><categories>cs.CV</categories><comments>8 pages, 6 figures</comments><journal-ref>CVPR2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has significantly improved the precision of instance
segmentation with abundant labeled data. However, in many areas like medical
and manufacturing, collecting sufficient data is extremely hard and labeling
this data requires high professional skills. We follow this motivation and
propose a new task set named zero-shot instance segmentation (ZSI). In the
training phase of ZSI, the model is trained with seen data, while in the
testing phase, it is used to segment all seen and unseen instances. We first
formulate the ZSI task and propose a method to tackle the challenge, which
consists of Zero-shot Detector, Semantic Mask Head, Background Aware RPN and
Synchronized Background Strategy. We present a new benchmark for zero-shot
instance segmentation based on the MS-COCO dataset. The extensive empirical
results in this benchmark show that our method not only surpasses the
state-of-the-art results in zero-shot object detection task but also achieves
promising performance on ZSI. Our approach will serve as a solid baseline and
facilitate future research in zero-shot instance segmentation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.06624</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.06624</id><submitter>Jiangchao Yao</submitter><version version="v1"><date>Wed, 14 Apr 2021 05:06:59 GMT</date><size>6624kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 16:07:24 GMT</date><size>6628kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 01:57:03 GMT</date><size>6628kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 2 Jun 2021 03:43:18 GMT</date><size>6628kb</size><source_type>D</source_type></version><title>Device-Cloud Collaborative Learning for Recommendation</title><authors>Jiangchao Yao and Feng Wang and KunYang Jia and Bo Han and Jingren
  Zhou and Hongxia Yang</authors><categories>cs.LG cs.AI</categories><comments>KDD 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  With the rapid development of storage and computing power on mobile devices,
it becomes critical and popular to deploy models on devices to save onerous
communication latencies and to capture real-time features. While quite a lot of
works have explored to facilitate on-device learning and inference, most of
them focus on dealing with response delay or privacy protection. Little has
been done to model the collaboration between the device and the cloud modeling
and benefit both sides jointly. To bridge this gap, we are among the first
attempts to study the Device-Cloud Collaborative Learning (DCCL) framework.
Specifically, we propose a novel MetaPatch learning approach on the device side
to efficiently achieve &quot;thousands of people with thousands of models&quot; given a
centralized cloud model. Then, with billions of updated personalized device
models, we propose a &quot;model-over-models&quot; distillation algorithm, namely
MoMoDistill, to update the centralized cloud model. Our extensive experiments
over a range of datasets with different settings demonstrate the effectiveness
of such collaboration on both cloud and devices, especially its superiority to
model long-tailed users.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.06993</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.06993</id><submitter>Faris B Mismar</submitter><version version="v1"><date>Wed, 14 Apr 2021 17:17:09 GMT</date><size>215kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 16:10:17 GMT</date><size>215kb</size><source_type>D</source_type></version><title>Unsupervised Learning in Next-Generation Networks: Real-Time Performance
  Self-Diagnosis</title><authors>Faris B. Mismar and Jakob Hoydis</authors><categories>cs.NI</categories><comments>5 pages, 5 figures. Submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter demonstrates the use of unsupervised machine learning to enable
performance self-diagnosis of next-generation cellular networks. We propose two
simplified applications of unsupervised learning that can enable real-time
performance self-diagnosis on edge nodes such as the radio access network
intelligent controller (RIC). The first application detects anomalous
performance and finds its root cause of faults, configuration, or network
procedure failures. The second application uses clustering to learn the
relationship between two performance measures. Our proposed applications run in
near-constant time complexity, making them, combined with subject-matter
expertise validation, suitable real-time RIC applications for network
diagnosis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.07070</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.07070</id><submitter>Vladan Stojni\'c</submitter><version version="v1"><date>Wed, 14 Apr 2021 18:25:43 GMT</date><size>304kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 17:59:23 GMT</date><size>304kb</size><source_type>D</source_type></version><title>Self-Supervised Learning of Remote Sensing Scene Representations Using
  Contrastive Multiview Coding</title><authors>Vladan Stojni\'c (1), Vladimir Risojevi\'c (1) ((1) Faculty of
  Electrical Engineering, University of Banja Luka, Bosnia and Herzegovina)</authors><categories>cs.CV</categories><comments>EarthVision 2021 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years self-supervised learning has emerged as a promising candidate
for unsupervised representation learning. In the visual domain its applications
are mostly studied in the context of images of natural scenes. However, its
applicability is especially interesting in specific areas, like remote sensing
and medicine, where it is hard to obtain huge amounts of labeled data. In this
work, we conduct an extensive analysis of the applicability of self-supervised
learning in remote sensing image classification. We analyze the influence of
the number and domain of images used for self-supervised pre-training on the
performance on downstream tasks. We show that, for the downstream task of
remote sensing image classification, using self-supervised pre-training on
remote sensing images can give better results than using supervised
pre-training on images of natural scenes. Besides, we also show that
self-supervised pre-training can be easily extended to multispectral images
producing even better results on our downstream tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.07279</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.07279</id><submitter>Mohammad Reza Feizi Derakhshi</submitter><version version="v1"><date>Thu, 15 Apr 2021 07:12:58 GMT</date><size>1423kb</size></version><version version="v2"><date>Sun, 30 May 2021 16:17:19 GMT</date><size>418kb</size></version><title>Deep learning for COVID-19 diagnosis based feature selection using
  binary differential evolution algorithm</title><authors>Mohammad Saber Iraji, Mohammad-Reza Feizi-Derakhshi, Jafar Tanha</authors><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new Coronavirus is spreading rapidly and it has taken the lives of many
people so far. The virus has destructive effects on the human lung and early
detection is very important. Deep Convolution neural networks are a powerful
tool in classifying images. Therefore, in this paper a hybrid approach based on
a deep network is presented. Feature vectors were extracted by applying a deep
convolution neural network on the images and effective features were selected
by the binary differential meta-heuristic algorithm. These optimized features
were given to the SVM classifier. A database consisting of three categories of
images as COVID-19, pneumonia, and healthy included 1092 X-ray samples was
considered. The proposed method achieved an accuracy of 99.43%, a sensitivity
of 99.16%, and a specificity of 99.57%. Our results demonstrate the suggested
approach is better than recent studies on COVID-19 detection with X-ray images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.07463</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.07463</id><submitter>Tuukka Korhonen</submitter><version version="v1"><date>Thu, 15 Apr 2021 13:50:05 GMT</date><size>12kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 15:01:02 GMT</date><size>18kb</size><source_type>D</source_type></version><title>A Single-Exponential Time 2-Approximation Algorithm for Treewidth</title><authors>Tuukka Korhonen</authors><categories>cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm, that given an $n$-vertex graph $G$ and an integer $k$,
in time $2^{O(k)} n$ either outputs a tree decomposition of $G$ of width at
most $2k + 1$ or determines that the treewidth of $G$ is larger than $k$. This
is the first 2-approximation algorithm for treewidth that is faster than the
known exact algorithms. In particular, our algorithm improves upon both the
previous best approximation ratio of 5 in time $2^{O(k)} n$ and the previous
best approximation ratio of 3 in time $2^{O(k)} n^{O(1)}$, both given by
Bodlaender et al. [FOCS 2013, SICOMP 2016]. Our algorithm is based on a local
improvement method adapted from a proof of Bellenbaum and Diestel [Comb.
Probab. Comput. 2002].
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08044</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08044</id><submitter>Peilun Wu</submitter><version version="v1"><date>Fri, 16 Apr 2021 11:42:10 GMT</date><size>1194kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 25 Apr 2021 09:23:18 GMT</date><size>1194kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 7 May 2021 06:47:56 GMT</date><size>573kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 12 May 2021 07:24:49 GMT</date><size>572kb</size><source_type>D</source_type></version><version version="v5"><date>Wed, 19 May 2021 06:51:54 GMT</date><size>1239kb</size><source_type>D</source_type></version><version version="v6"><date>Sat, 29 May 2021 13:35:32 GMT</date><size>677kb</size><source_type>D</source_type></version><title>Holmes: An Efficient and Lightweight Semantic Based Anomalous Email
  Detector</title><authors>Peilun Wu, Fan Yan, Hui Guo</authors><categories>cs.CR cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Email threat is a serious issue for enterprise security, which consists of
various malicious scenarios, such as phishing, fraud, blackmail and
malvertisement. Traditional anti-spam gateway commonly requires to maintain a
greylist to filter out unexpected emails based on suspicious vocabularies
existed in the mail subject and content. However, the signature-based approach
cannot effectively discover novel and unknown suspicious emails that utilize
various hot topics at present, such as COVID-19 and US election. To address the
problem, in this paper, we present Holmes, an efficient and lightweight
semantic based engine for anomalous email detection. Holmes can convert each
event log of email to a sentence through word embedding then extract
interesting items among them by novelty detection. Based on our observations,
we claim that, in an enterprise environment, there is a stable relation between
senders and receivers, but suspicious emails are commonly from unusual sources,
which can be detected through the rareness selection. We evaluate the
performance of Holmes in a real-world enterprise environment, in which it sends
and receives around 5,000 emails each day. As a result, Holmes can achieve a
high detection rate (output around 200 suspicious emails per day) and maintain
a low false alarm rate for anomaly detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08135</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08135</id><submitter>Yue Ren</submitter><version version="v1"><date>Fri, 16 Apr 2021 14:33:21 GMT</date><size>86kb</size><source_type>D</source_type></version><title>Sharp bounds for the number of regions of maxout networks and vertices
  of Minkowski sums</title><authors>Guido Mont\'ufar and Yue Ren and Leon Zhang</authors><categories>math.CO cs.DM cs.LG</categories><comments>25 pages, 5 figures</comments><msc-class>68T07, 52B05, 14T15, 06A07</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present results on the number of linear regions of the functions that can
be represented by artificial feedforward neural networks with maxout units. A
rank-k maxout unit is a function computing the maximum of $k$ linear functions.
For networks with a single layer of maxout units, the linear regions correspond
to the upper vertices of a Minkowski sum of polytopes. We obtain face counting
formulas in terms of the intersection posets of tropical hypersurfaces or the
number of upper faces of partial Minkowski sums, along with explicit sharp
upper bounds for the number of regions for any input dimension, any number of
units, and any ranks, in the cases with and without biases. Based on these
results we also obtain asymptotically sharp upper bounds for networks with
multiple layers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08267</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08267</id><submitter>Ugo Rosolia</submitter><version version="v1"><date>Fri, 16 Apr 2021 17:52:37 GMT</date><size>7649kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 01:27:55 GMT</date><size>6769kb</size><source_type>D</source_type></version><title>Iterative Model Predictive Control for Piecewise Systems</title><authors>Ugo Rosolia and Aaron D. Ames</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present an iterative Model Predictive Control (MPC) design
for piecewise nonlinear systems. We consider finite time control tasks where
the goal of the controller is to steer the system from a starting configuration
to a goal state while minimizing a cost function. First, we present an
algorithm that leverages a feasible trajectory that completes the task to
construct a control policy which guarantees that state and input constraints
are recursively satisfied and that the closed-loop system reaches the goal
state in finite time. Utilizing this construction, we present a policy
iteration scheme that iteratively generates safe trajectories which have
non-decreasing performance. Finally, we test the proposed strategy on a
discretized Spring Loaded Inverted Pendulum (SLIP) model with massless legs. We
show that our methodology is robust to changes in initial conditions and
disturbances acting on the system. Furthermore, we demonstrate the
effectiveness of our policy iteration algorithm in a minimum time control task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08427</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08427</id><submitter>Thomas Fork</submitter><version version="v1"><date>Sat, 17 Apr 2021 03:09:42 GMT</date><size>405kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 9 May 2021 15:26:41 GMT</date><size>1313kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 18:30:02 GMT</date><size>1315kb</size><source_type>D</source_type></version><title>Models and Predictive Control for Nonplanar Vehicle Navigation</title><authors>Thomas Fork, H. Eric Tseng, and Francesco Borrelli</authors><categories>eess.SY cs.SY</categories><comments>Minor changes to Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simplified model of a vehicle driving on a nonplanar road. A
parametric surface is used to describe the nonplanar road which can describe
any combination of curvature, bank and slope. We show that the proposed
modeling approach generalizes planar vehicle models that reference a
centerline, such as the Frenet model.
  We use the proposed approach for vehicle path planning and following using
model predictive control. We also model and control vehicle contact with the
road surface. We demonstrate that the proposed controller improves speed and
lane following on complex roads compared to planar vehicle controllers, and
mitigates loss of control on complex road surfaces including off-camber turns.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08452</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08452</id><submitter>Simon Le Cleac'h</submitter><version version="v1"><date>Sat, 17 Apr 2021 05:17:01 GMT</date><size>4824kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 05:30:43 GMT</date><size>5890kb</size><source_type>D</source_type></version><title>ALGAMES: A Fast Augmented Lagrangian Solver for Constrained Dynamic
  Games</title><authors>Simon Le Cleac'h, Mac Schwager, Zachary Manchester</authors><categories>cs.RO</categories><comments>Submitted to Autonomous Robots (under review) as an extension of a
  paper accepted to RSS 2020. arXiv admin note: text overlap with
  arXiv:1910.09713</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic games are an effective paradigm for dealing with the control of
multiple interacting actors. This paper introduces ALGAMES (Augmented
Lagrangian GAME-theoretic Solver), a solver that handles
trajectory-optimization problems with multiple actors and general nonlinear
state and input constraints. Its novelty resides in satisfying the first-order
optimality conditions with a quasi-Newton root-finding algorithm and rigorously
enforcing constraints using an augmented Lagrangian method. We evaluate our
solver in the context of autonomous driving on scenarios with a strong level of
interactions between the vehicles. We assess the robustness of the solver using
Monte Carlo simulations. It is able to reliably solve complex problems like
ramp merging with three vehicles three times faster than a state-of-the-art
DDP-based approach. A model-predictive control (MPC) implementation of the
algorithm, running at more than 60 Hz, demonstrates ALGAMES' ability to
mitigate the &quot;frozen robot&quot; problem on complex autonomous driving scenarios
like merging onto a crowded highway.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08453</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08453</id><submitter>Lei Xu</submitter><version version="v1"><date>Sat, 17 Apr 2021 05:21:35 GMT</date><size>2194kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 11:21:51 GMT</date><size>5630kb</size><source_type>D</source_type></version><title>Attacking Text Classifiers via Sentence Rewriting Sampler</title><authors>Lei Xu, Kalyan Veeramachaneni</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most adversarial attack methods on text classification can change the
classifier's prediction by synonym substitution. We propose the adversarial
sentence rewriting sampler (ASRS), which rewrites the whole sentence to
generate more similar and higher-quality adversarial examples. Our method
achieves a better attack success rate on 4 out of 7 datasets, as well as
significantly better sentence quality on all 7 datasets. ASRS is an
indispensable supplement to the existing attack methods, because classifiers
cannot resist the attack from ASRS unless they are trained on adversarial
examples found by ASRS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08570</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08570</id><submitter>Evgeniia Razumovskaia</submitter><version version="v1"><date>Sat, 17 Apr 2021 15:19:56 GMT</date><size>7442kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 12:57:54 GMT</date><size>8153kb</size><source_type>D</source_type></version><title>Crossing the Conversational Chasm: A Primer on Natural Language
  Processing for Multilingual Task-Oriented Dialogue Systems</title><authors>Evgeniia Razumovskaia, Goran Glava\v{s}, Olga Majewska, Edoardo M.
  Ponti, Anna Korhonen, Ivan Vuli\'c</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In task-oriented dialogue (ToD), a user holds a conversation with an
artificial agent to complete a concrete task. Although this technology
represents one of the central objectives of AI and has been the focus of ever
more intense research and development efforts, it is currently limited to a few
narrow domains (e.g., food ordering, ticket booking) and a handful of languages
(e.g., English, Chinese). This work provides an extensive overview of existing
methods and resources in multilingual ToD as an entry point to this exciting
and emerging field. We find that the most critical factor preventing the
creation of truly multilingual ToD systems is the lack of datasets in most
languages for both training and evaluation. In fact, acquiring annotations or
human feedback for each component of modular systems or for data-hungry
end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches
to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer
from resource-rich languages (almost exclusively English), either by means of
machine translation or multilingual representations. These approaches are
currently viable only for typologically similar languages and languages with
parallel / monolingual corpora available. On the other hand, their
effectiveness beyond these boundaries is doubtful or hard to assess due to the
lack of linguistically diverse benchmarks (especially for natural language
generation and end-to-end evaluation). To overcome this limitation, we draw
parallels between components of the ToD pipeline and other NLP tasks, which can
inspire solutions for learning in low-resource scenarios. Finally, we list
additional challenges that multilinguality poses for related areas (such as
speech and human-centred evaluation), and indicate future directions that hold
promise to further expand language coverage and dialogue capabilities of
current ToD systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08700</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08700</id><submitter>Yuxin Zhang</submitter><version version="v1"><date>Sun, 18 Apr 2021 03:50:28 GMT</date><size>544kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 06:21:53 GMT</date><size>111kb</size><source_type>D</source_type></version><title>Lottery Jackpots Exist in Pre-trained Models</title><authors>Yuxin Zhang, Mingbao Lin, Fei Chao, Yan Wang, Yongjian Wu, Feiyue
  Huang, Mingliang Xu, Yonghong Tian, Rongrong Ji</authors><categories>cs.CV</categories><comments>10 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Network pruning is an effective approach to reduce network complexity without
performance compromise. Existing studies achieve the sparsity of neural
networks via time-consuming weight tuning or complex search on networks with
expanded width, which greatly limits the applications of network pruning. In
this paper, we show that high-performing and sparse sub-networks without the
involvement of weight tuning, termed &quot;lottery jackpots&quot;, exist in pre-trained
models with unexpanded width. For example, we obtain a lottery jackpot that has
only 10% parameters and still reaches the performance of the original dense
VGGNet-19 without any modifications on the pre-trained weights. Furthermore, we
observe that the sparse masks derived from many existing pruning criteria have
a high overlap with the searched mask of our lottery jackpot, among which, the
magnitude-based pruning results in the most similar mask with ours. Based on
this insight, we initialize our sparse mask using the magnitude pruning,
resulting in at least 3x cost reduction on the lottery jackpot search while
achieves comparable or even better performance. Specifically, our
magnitude-based lottery jackpot removes 90% weights in the ResNet-50, while
easily obtains more than 70% top-1 accuracy using only 10 searching epochs on
ImageNet.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08736</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08736</id><submitter>Qi Qi</submitter><version version="v1"><date>Sun, 18 Apr 2021 06:22:21 GMT</date><size>945kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 05:06:40 GMT</date><size>1715kb</size><source_type>D</source_type></version><title>Stochastic Optimization of Areas Under Precision-Recall Curves with
  Provable Convergence</title><authors>Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, Tianbao Yang</authors><categories>cs.LG cs.AI cs.CV math.OC</categories><comments>25 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common
metrics for evaluating classification performance for imbalanced problems.
Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced
datasets. While stochastic optimization of AUROC has been studied extensively,
principled stochastic optimization of AUPRC has been rarely explored. In this
work, we propose a principled technical method to optimize AUPRC for deep
learning. Our approach is based on maximizing the averaged precision (AP),
which is an unbiased point estimator of AUPRC. We cast the objective into a sum
of {\it dependent compositional functions} with inner functions dependent on
random variables of the outer level. We propose efficient adaptive and
non-adaptive stochastic algorithms with {\it provable convergence guarantee
under mild conditions} by leveraging recent advances in stochastic
compositional optimization. Extensive experimental results on image and graph
datasets demonstrate that our proposed method outperforms prior methods on
imbalanced problems in terms of AUPRC. To the best of our knowledge, our work
represents the first attempt to optimize AUPRC with provable convergence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08755</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08755</id><submitter>Zhaohao Zeng</submitter><version version="v1"><date>Sun, 18 Apr 2021 07:35:15 GMT</date><size>4669kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 15:32:47 GMT</date><size>4669kb</size><source_type>D</source_type></version><title>DCH-2: A Parallel Customer-Helpdesk Dialogue Corpus with Distributions
  of Annotators' Labels</title><authors>Zhaohao Zeng and Tetsuya Sakai</authors><categories>cs.CL cs.AI cs.IR</categories><comments>6 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We introduce a data set called DCH-2, which contains 4,390 real
customer-helpdesk dialogues in Chinese and their English translations. DCH-2
also contains dialogue-level annotations and turn-level annotations obtained
independently from either 19 or 20 annotators. The data set was built through
our effort as organisers of the NTCIR-14 Short Text Conversation and NTCIR-15
Dialogue Evaluation tasks, to help researchers understand what constitutes an
effective customer-helpdesk dialogue, and thereby build efficient and helpful
helpdesk systems that are available to customers at all times. In addition,
DCH-2 may be utilised for other purposes, for example, as a repository for
retrieval-based dialogue systems, or as a parallel corpus for machine
translation in the helpdesk domain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.09696</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.09696</id><submitter>Meryem M'hamdi</submitter><version version="v1"><date>Tue, 20 Apr 2021 00:13:35 GMT</date><size>8670kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 18:00:09 GMT</date><size>8677kb</size><source_type>D</source_type></version><title>X-METRA-ADA: Cross-lingual Meta-Transfer Learning Adaptation to Natural
  Language Understanding and Question Answering</title><authors>Meryem M'hamdi, Doo Soon Kim, Franck Dernoncourt, Trung Bui, Xiang
  Ren, and Jonathan May</authors><categories>cs.CL</categories><comments>Accepted at NAACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multilingual models, such as M-BERT and XLM-R, have gained increasing
popularity, due to their zero-shot cross-lingual transfer learning
capabilities. However, their generalization ability is still inconsistent for
typologically diverse languages and across different benchmarks. Recently,
meta-learning has garnered attention as a promising technique for enhancing
transfer learning under low-resource scenarios: particularly for cross-lingual
transfer in Natural Language Understanding (NLU). In this work, we propose
X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for
NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to
learn to adapt to new languages. We extensively evaluate our framework on two
challenging cross-lingual NLU tasks: multilingual task-oriented dialog and
typologically diverse question answering. We show that our approach outperforms
naive fine-tuning, reaching competitive performance on both tasks for most
languages. Our analysis reveals that X-METRA-ADA can leverage limited data for
faster adaptation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.09907</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.09907</id><submitter>Kaustubh Kulkarni</submitter><version version="v1"><date>Tue, 20 Apr 2021 11:32:43 GMT</date><size>720kb</size></version><version version="v2"><date>Mon, 31 May 2021 18:59:57 GMT</date><size>718kb</size></version><title>Table Tennis Stroke Recognition Using Two-Dimensional Human Pose
  Estimation</title><authors>Kaustubh Milind Kulkarni and Sucheth Shenoy</authors><categories>cs.CV cs.LG</categories><comments>Accepted at CVPR Sports Workshop 2021 (7th International Workshop on
  Computer Vision in Sports) (CVSports)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We introduce a novel method for collecting table tennis video data and
perform stroke detection and classification. A diverse dataset containing video
data of 11 basic strokes obtained from 14 professional table tennis players,
summing up to a total of 22111 videos has been collected using the proposed
setup. The temporal convolutional neural network model developed using 2D pose
estimation performs multiclass classification of these 11 table tennis strokes
with a validation accuracy of 99.37%. Moreover, the neural network generalizes
well over the data of a player excluded from the training and validation
dataset, classifying the fresh strokes with an overall best accuracy of 98.72%.
Various model architectures using machine learning and deep learning based
approaches have been trained for stroke recognition and their performances have
been compared and benchmarked. Inferences such as performance monitoring and
stroke comparison of the players using the model have been discussed.
Therefore, we are contributing to the development of a computer vision based
sports analytics system for the sport of table tennis that focuses on the
previously unexploited aspect of the sport i.e., a player's strokes, which is
extremely insightful for performance improvement.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.09959</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.09959</id><submitter>Ekaterina Tolstaya</submitter><version version="v1"><date>Tue, 20 Apr 2021 13:43:28 GMT</date><size>11838kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 18:25:53 GMT</date><size>11838kb</size><source_type>D</source_type></version><title>Identifying Driver Interactions via Conditional Behavior Prediction</title><authors>Ekaterina Tolstaya, Reza Mahjourian, Carlton Downey, Balakrishnan
  Varadarajan, Benjamin Sapp, Dragomir Anguelov</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive driving scenarios, such as lane changes, merges and unprotected
turns, are some of the most challenging situations for autonomous driving.
Planning in interactive scenarios requires accurately modeling the reactions of
other agents to different future actions of the ego agent. We develop
end-to-end models for conditional behavior prediction (CBP) that take as an
input a query future trajectory for an ego-agent, and predict distributions
over future trajectories for other agents conditioned on the query. Leveraging
such a model, we develop a general-purpose agent interactivity score derived
from probabilistic first principles. The interactivity score allows us to find
interesting interactive scenarios for training and evaluating behavior
prediction models. We further demonstrate that the proposed score is effective
for agent prioritization under computational budget constraints.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.09976</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.09976</id><submitter>Dibyayan Chakraborty</submitter><version version="v1"><date>Tue, 20 Apr 2021 14:04:48 GMT</date><size>63kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 03:34:11 GMT</date><size>1113kb</size><source_type>D</source_type></version><title>Finding Geometric Representations of Apex Graphs is NP-Hard</title><authors>Dibyayan Chakraborty, Kshitij Gajjar</authors><categories>cs.CG cs.DM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Planar graphs can be represented as intersection graphs of different types of
geometric objects in the plane, e.g., circles (Koebe, 1936), line segments
(Chalopin \&amp; Gon{\c{c}}alves, 2009), \textsc{L}-shapes (Gon{\c{c}}alves et al,
2018). For general graphs, however, even deciding whether such representations
exist is often $NP$-hard. We consider apex graphs, i.e., graphs that can be
made planar by removing one vertex from them. We show, somewhat surprisingly,
that deciding whether geometric representations exist for apex graphs is
$NP$-hard.
  More precisely, we show that for every positive integer $k$, recognizing
every graph class $\mathcal{G}$ which satisfies $\textsc{PURE-2-DIR} \subseteq
\mathcal{G} \subseteq \textsc{1-STRING}$ is $NP$-hard, even when the input
graphs are apex graphs of girth at least $k$. Here, $PURE-2-DIR$ is the class
of intersection graphs of axis-parallel line segments (where intersections are
allowed only between horizontal and vertical segments) and \textsc{1-STRING} is
the class of intersection graphs of simple curves (where two curves share at
most one point) in the plane. This partially answers an open question raised by
Kratochv{\'\i}l \&amp; Pergel (2007).
  Most known $NP$-hardness reductions for these problems are from variants of
3-SAT. We reduce from the \textsc{PLANAR HAMILTONIAN PATH COMPLETION} problem,
which uses the more intuitive notion of planarity. As a result, our proof is
much simpler and encapsulates several classes of geometric graphs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10083</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10083</id><submitter>Tao Qi</submitter><version version="v1"><date>Tue, 20 Apr 2021 16:05:16 GMT</date><size>1125kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 04:12:58 GMT</date><size>2215kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 17:13:48 GMT</date><size>2215kb</size><source_type>D</source_type></version><title>Personalized News Recommendation with Knowledge-aware Interactive
  Matching</title><authors>Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang</authors><categories>cs.IR</categories><comments>SIGIR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important task in personalized news recommendation is accurate
matching between candidate news and user interest. Most of existing news
recommendation methods model candidate news from its textual content and user
interest from their clicked news in an independent way. However, a news article
may cover multiple aspects and entities, and a user usually has different kinds
of interest. Independent modeling of candidate news and user interest may lead
to inferior matching between news and users. In this paper, we propose a
knowledge-aware interactive matching method for news recommendation. Our method
interactively models candidate news and user interest to facilitate their
accurate matching. We design a knowledge-aware news co-encoder to interactively
learn representations for both clicked news and candidate news by capturing
their relatedness in both semantic and entities with the help of knowledge
graphs. We also design a user-news co-encoder to learn candidate news-aware
user interest representation and user-aware candidate news representation for
better interest matching. Experiments on two real-world datasets validate that
our method can effectively improve the performance of news recommendation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10210</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10210</id><submitter>Richard A. Blythe</submitter><version version="v1"><date>Tue, 20 Apr 2021 19:02:49 GMT</date><size>189kb</size><source_type>D</source_type></version><title>How individuals change language</title><authors>Richard A Blythe and William Croft</authors><categories>cs.CL physics.soc-ph q-bio.PE</categories><comments>50 pages, 11 figures</comments><journal-ref>PLoS ONE 16(6): e0252582 (2021)</journal-ref><doi>10.1371/journal.pone.0252582</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Languages emerge and change over time at the population level though
interactions between individual speakers. It is, however, hard to directly
observe how a single speaker's linguistic innovation precipitates a
population-wide change in the language, and many theoretical proposals exist.
We introduce a very general mathematical model that encompasses a wide variety
of individual-level linguistic behaviours and provides statistical predictions
for the population-level changes that result from them. This model allows us to
compare the likelihood of empirically-attested changes in definite and
indefinite articles in multiple languages under different assumptions on the
way in which individuals learn and use language. We find that accounts of
language change that appeal primarily to errors in childhood language
acquisition are very weakly supported by the historical data, whereas those
that allow speakers to change incrementally across the lifespan are more
plausible, particularly when combined with social network effects.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10283</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10283</id><submitter>Weixin Liang</submitter><version version="v1"><date>Tue, 20 Apr 2021 23:54:41 GMT</date><size>3799kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 05:29:00 GMT</date><size>3799kb</size><source_type>D</source_type></version><title>GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual
  Question Answering</title><authors>Weixin Liang, Yanhao Jiang and Zixuan Liu</authors><categories>cs.CL cs.CV</categories><comments>NAACL 2021 MAI-Workshop. Code available at
  https://github.com/codexxxl/GraphVQA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Images are more than a collection of objects or attributes -- they represent
a web of relationships among interconnected objects. Scene Graph has emerged as
a new modality for a structured graphical representation of images. Scene Graph
encodes objects as nodes connected via pairwise relations as edges. To support
question answering on scene graphs, we propose GraphVQA, a language-guided
graph neural network framework that translates and executes a natural language
question as multiple iterations of message passing among graph nodes. We
explore the design space of GraphVQA framework, and discuss the trade-off of
different design choices. Our experiments on GQA dataset show that GraphVQA
outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10593</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10593</id><submitter>Daniel Paulusma</submitter><version version="v1"><date>Wed, 21 Apr 2021 15:45:07 GMT</date><size>36kb</size></version><version version="v2"><date>Mon, 10 May 2021 20:16:03 GMT</date><size>41kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 13:43:03 GMT</date><size>38kb</size></version><title>Acyclic, Star, and Injective Colouring: Bounding the Diameter</title><authors>Christoph Brause and Petr Golovach and Barnaby Martin and Pascal Ochem
  and Dani\&quot;el Paulusma and Siani Smith</authors><categories>cs.DS cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the effect of bounding the diameter for well-studied variants of
the Colouring problem. A colouring is acyclic, star, or injective if any two
colour classes induce a forest, star forest or disjoint union of vertices and
edges, respectively. The corresponding decision problems are Acyclic Colouring,
Star Colouring and Injective Colouring. The last problem is also known as
$L(1,1)$-Labelling and we also consider the framework of $L(a,b)$-Labelling. We
prove a number of (almost-)complete complexity classifications. In particular,
we show that for graphs of diameter at most $d$, Acyclic $3$-Colouring is
polynomial-time solvable if $d\leq 2$ but NP-complete if $d\geq 4$, and Star
$3$-Colouring is polynomial-time solvable if $d\leq 3$ but NP-complete for
$d\geq 8$. As far as we are aware, Star $3$-Colouring is the first problem that
exhibits a complexity jump for some $d\geq 3$. Our third main result is that
$L(1,2)$-Labelling is NP-complete for graphs of diameter $2$; we relate the
latter problem to a special case of Hamiltonian Path.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10649</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10649</id><submitter>Ruiqing Yan</submitter><version version="v1"><date>Thu, 25 Mar 2021 06:14:18 GMT</date><size>8641kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 10:08:32 GMT</date><size>9072kb</size><source_type>D</source_type></version><title>K-XLNet: A General Method for Combining Explicit Knowledge with Language
  Model Pretraining</title><authors>Ruiqing Yan, Lanchang Sun, Fang Wang, Xiaoming Zhang</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though pre-trained language models such as Bert and XLNet, have rapidly
advanced the state-of-the-art on many NLP tasks, they implicit semantics only
relying on surface information between words in corpus. Intuitively, background
knowledge influences the efficacy of understanding. Inspired by this common
sense, we focus on improving model pretraining by leveraging explicit
knowledge. Different from recent research that optimize pretraining model by
knowledge masking strategies, we propose a simple but general method to combine
explicit knowledge with pretraining. To be specific, we first match knowledge
facts from knowledge graph (KG) and then add a knowledge injunction layer to
transformer directly without changing its architecture. The present study seeks
to find the direct impact of explicit knowledge on transformer per-training. We
conduct experiments on various datasets for different downstream tasks. The
experimental results show that solely by adding external knowledge to
transformer can improve the learning performance on many NLP tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10747</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10747</id><submitter>Arthur Hinsvark</submitter><version version="v1"><date>Wed, 21 Apr 2021 20:21:06 GMT</date><size>20kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 14:58:18 GMT</date><size>20kb</size></version><title>Accented Speech Recognition: A Survey</title><authors>Arthur Hinsvark (1), Natalie Delworth (1), Miguel Del Rio (1), Quinten
  McNamara (1), Joshua Dong (1), Ryan Westerman (1), Michelle Huang (1), Joseph
  Palakapilly (1), Jennifer Drexler (1), Ilya Pirkin (1), Nishchal Bhandari
  (1), Miguel Jette (1) ((1) Rev.com)</authors><categories>cs.CL cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic Speech Recognition (ASR) systems generalize poorly on accented
speech. The phonetic and linguistic variability of accents present hard
challenges for ASR systems today in both data collection and modeling
strategies. The resulting bias in ASR performance across accents comes at a
cost to both users and providers of ASR.
  We present a survey of current promising approaches to accented speech
recognition and highlight the key challenges in the space. Approaches mostly
focus on single model generalization and accent feature engineering. Among the
challenges, lack of a standard benchmark makes research and comparison
especially difficult.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10762</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10762</id><submitter>Robert Murphy</submitter><version version="v1"><date>Fri, 9 Apr 2021 07:09:54 GMT</date><size>456kb</size></version><version version="v2"><date>Wed, 19 May 2021 21:22:25 GMT</date><size>451kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 24 May 2021 02:33:01 GMT</date><size>453kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 30 May 2021 21:50:26 GMT</date><size>496kb</size><source_type>D</source_type></version><title>Image Distribution Estimation with Random Field and Random Cluster
  Theories</title><authors>Robert A. Murphy</authors><categories>eess.IV cs.CV</categories><msc-class>60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random field and random cluster theory is used to prove certain mathematical
results concerning the probability distribution of images characterized as
generic $2D$ integer arrays during simultaneous learning. Example models in
image classification and object segmentation illustrate the mathematical
results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10831</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10831</id><submitter>Tixiao Shan</submitter><version version="v1"><date>Thu, 22 Apr 2021 02:33:15 GMT</date><size>4790kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 12:32:16 GMT</date><size>4894kb</size><source_type>D</source_type></version><title>LVI-SAM: Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing
  and Mapping</title><authors>Tixiao Shan and Brendan Englot and Carlo Ratti and Daniela Rus</authors><categories>cs.RO</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We propose a framework for tightly-coupled lidar-visual-inertial odometry via
smoothing and mapping, LVI-SAM, that achieves real-time state estimation and
map-building with high accuracy and robustness. LVI-SAM is built atop a factor
graph and is composed of two sub-systems: a visual-inertial system (VIS) and a
lidar-inertial system (LIS). The two sub-systems are designed in a
tightly-coupled manner, in which the VIS leverages LIS estimation to facilitate
initialization. The accuracy of the VIS is improved by extracting depth
information for visual features using lidar measurements. In turn, the LIS
utilizes VIS estimation for initial guesses to support scan-matching. Loop
closures are first identified by the VIS and further refined by the LIS.
LVI-SAM can also function when one of the two sub-systems fails, which
increases its robustness in both texture-less and feature-less environments.
LVI-SAM is extensively evaluated on datasets gathered from several platforms
over a variety of scales and environments. Our implementation is available at
https://git.io/lvi-sam
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10903</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10903</id><submitter>Jay Kumar</submitter><version version="v1"><date>Thu, 22 Apr 2021 07:32:04 GMT</date><size>2324kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 06:04:39 GMT</date><size>4070kb</size><source_type>D</source_type></version><title>Blockchain based Privacy-Preserved Federated Learning for Medical
  Images: A Case Study of COVID-19 CT Scans</title><authors>Rajesh Kumar, WenYong Wang, Cheng Yuan, Jay Kumar, Zakria, He Qing,
  Ting Yang, Abdullah Aman Khan</authors><categories>cs.CR cs.LG eess.IV</categories><comments>15 Pages, 5 Tables, 11 Figures, Journal Paper, Elsevier format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical health care centers are envisioned as a promising paradigm to handle
the massive volume of data of COVID-19 patients using artificial intelligence
(AI). Traditionally, AI techniques often require centralized data collection
and training the model in a single organization, which is most common weakness
due to the privacy and security of raw data communication. To solve this
challenging task, we propose a blockchain-based federated learning framework
that provides collaborative data training solutions by coordinating multiple
hospitals to train and share encrypted federated models without leakage of data
privacy. The blockchain ledger technology provides the decentralization of
federated learning model without any central server. The proposed homomorphic
encryption scheme encrypts and decrypts the gradients of model to preserve the
privacy. More precisely, the proposed framework: i) train the local model by a
novel capsule network to segmentation and classify COVID-19 images, ii) then
use the homomorphic encryption scheme to secure the local model that encrypts
and decrypts the gradients, and finally the model is shared over a
decentralized platform through proposed blockchain-based federated learning
algorithm. The integration of blockchain and federated learning leads to a new
paradigm for medical image data sharing in the decentralized network. The
conducted experimental resultsdemonstrate the performance of the proposed
scheme.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.11016</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.11016</id><submitter>Miloslav Znojil</submitter><version version="v1"><date>Thu, 22 Apr 2021 12:27:09 GMT</date><size>29kb</size></version><title>Exceptional points and domains of unitarity for a class of strongly
  non-Hermitian real-matrix Hamiltonians</title><authors>Miloslav Znojil</authors><categories>math-ph cs.NA cs.SC math.MP math.NA quant-ph</categories><comments>27 pp</comments><journal-ref>J. Math. Phys. 62 (2021) 052103</journal-ref><doi>10.1063/5.0041185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A phenomenological Hamiltonian of a closed (i.e., unitary) quantum system is
assumed to have an $N$ by $N$ real-matrix form composed of a unperturbed
diagonal-matrix part $H^{(N)}_0$ and of a tridiagonal-matrix perturbation
$\lambda\,W^{(N)}(\lambda)$. The requirement of the unitarity of the evolution
of the system (i.e., of the diagonalizability and of the reality of the
spectrum) restricts, naturally, the variability of the matrix elements to a
&quot;physical&quot; domain ${\cal D}^{[N]} \subset \mathbb{R}^d$. We fix the unperturbed
matrix (simulating a non-equidistant, square-well-type unperturbed spectrum)
and we only admit the maximally non-Hermitian antisymmetric-matrix
perturbations. This yields the hiddenly Hermitian model with the measure of
perturbation $\lambda$ and with the $d=N$ matrix elements which are, inside
${\cal D}^{[N]}$, freely variable. Our aim is to describe the quantum
phase-transition boundary $\partial {\cal D}^{[N]}$ (alias exceptional-point
boundary) at which the unitarity of the system is lost. Our main attention is
paid to the strong-coupling extremes of stability, i.e., to the Kato's
exceptional points of order $N$ (EPN) and to the (sharply spiked) shape of the
boundary $\partial {\cal D}^{[N]}$ in their vicinity. The feasibility of our
constructions is based on the use of the high-precision arithmetics in
combination with the computer-assisted symbolic manipulations (including, in
particular, the Gr\&quot;{o}bner basis elimination technique).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.11601</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.11601</id><submitter>Amin Honarmandi Shandiz</submitter><version version="v1"><date>Fri, 23 Apr 2021 13:48:21 GMT</date><size>282kb</size></version><title>Improving Neural Silent Speech Interface Models by Adversarial Training</title><authors>Amin Honarmandi Shandiz, L\'aszl\'o T\'oth, G\'abor Gosztolya,
  Alexandra Mark\'o, Tam\'as G\'abor Csap\'o</authors><categories>cs.SD eess.AS</categories><comments>11 pages, 3 tables, 2 figures</comments><doi>10.1007/978-3-030-76346-6_39</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Besides the well-known classification task, these days neural networks are
frequently being applied to generate or transform data, such as images and
audio signals. In such tasks, the conventional loss functions like the mean
squared error (MSE) may not give satisfactory results. To improve the
perceptual quality of the generated signals, one possibility is to increase
their similarity to real signals, where the similarity is evaluated via a
discriminator network. The combination of the generator and discriminator nets
is called a Generative Adversarial Network (GAN). Here, we evaluate this
adversarial training framework in the articulatory-to-acoustic mapping task,
where the goal is to reconstruct the speech signal from a recording of the
movement of articulatory organs. As the generator, we apply a 3D convolutional
network that gave us good results in an earlier study. To turn it into a GAN,
we extend the conventional MSE training loss with an adversarial loss component
provided by a discriminator network. As for the evaluation, we report various
objective speech quality metrics such as the Perceptual Evaluation of Speech
Quality (PESQ), and the Mel-Cepstral Distortion (MCD). Our results indicate
that the application of the adversarial training loss brings about a slight,
but consistent improvement in all these metrics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.12146</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.12146</id><submitter>Jinlai Zhang</submitter><version version="v1"><date>Sun, 25 Apr 2021 13:01:41 GMT</date><size>3502kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 13:09:19 GMT</date><size>4316kb</size><source_type>D</source_type></version><title>3D Adversarial Attacks Beyond Point Cloud</title><authors>Jinlai Zhang, Lyujie Chen, Binbin Liu, Bo Ouyang, Qizhi Xie, Jihong
  Zhu, Weiming Li, Yanmei Meng</authors><categories>cs.CV</categories><comments>9 pages, 5 figs</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, 3D deep learning models have been shown to be susceptible to
adversarial attacks like their 2D counterparts. Most of the state-of-the-art
(SOTA) 3D adversarial attacks perform perturbation to 3D point clouds. To
reproduce these attacks in pseudo physical scenario, a generated adversarial 3D
point cloud need to be reconstructed to mesh, which leads to a significant drop
in its adversarial effect. In this paper, we propose a strong 3D adversarial
attack named Mesh Attack to address this problem by directly performing
perturbation on mesh of a 3D object. Specifically, in each iteration of our
method, the mesh is first sampled to point cloud by a differentiable sample
module. Then a point cloud classifier is used to back-propagate a combined loss
to update the mesh vertices. The combined loss includes an adversarial loss to
mislead the point cloud classifier and three mesh losses to regularize the mesh
to be smooth. Extensive experiments demonstrate that the proposed scheme
outperforms SOTA 3D attacks by a significant margin in the pseudo physical
scenario. We also achieved SOTA performance under various defenses. Moreover,
to the best of our knowledge, our Mesh Attack is the first attempt of
adversarial attack on mesh classifier. Our code is available at:
{\footnotesize{\url{https://github.com/cuge1995/Mesh-Attack}}}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.12174</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.12174</id><submitter>Ivan Y. Tyukin</submitter><version version="v1"><date>Sun, 25 Apr 2021 14:47:05 GMT</date><size>118kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 12:25:59 GMT</date><size>117kb</size><source_type>D</source_type></version><title>Demystification of Few-shot and One-shot Learning</title><authors>Ivan Y. Tyukin, Alexander N. Gorban, Muhammad H. Alkhudaydi, Qinghua
  Zhou</authors><categories>cs.LG cs.AI math.ST stat.TH</categories><comments>IEEE International Joint Conference on Neural Networks, IJCNN 2021</comments><msc-class>68T05, 68T07</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Few-shot and one-shot learning have been the subject of active and intensive
research in recent years, with mounting evidence pointing to successful
implementation and exploitation of few-shot learning algorithms in practice.
Classical statistical learning theories do not fully explain why few- or
one-shot learning is at all possible since traditional generalisation bounds
normally require large training and testing samples to be meaningful. This
sharply contrasts with numerous examples of successful one- and few-shot
learning systems and applications.
  In this work we present mathematical foundations for a theory of one-shot and
few-shot learning and reveal conditions specifying when such learning schemes
are likely to succeed. Our theory is based on intrinsic properties of
high-dimensional spaces. We show that if the ambient or latent decision space
of a learning machine is sufficiently high-dimensional than a large class of
objects in this space can indeed be easily learned from few examples provided
that certain data non-concentration conditions are met.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.12335</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.12335</id><submitter>Yingchen Yu</submitter><version version="v1"><date>Mon, 26 Apr 2021 03:52:27 GMT</date><size>7524kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 30 Apr 2021 10:54:10 GMT</date><size>0kb</size><source_type>I</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 04:10:48 GMT</date><size>11529kb</size><source_type>D</source_type></version><title>Diverse Image Inpainting with Bidirectional and Autoregressive
  Transformers</title><authors>Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui,
  Shijian Lu, Feiying Ma, Xuansong Xie, Chunyan Miao</authors><categories>cs.CV</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image inpainting is an underdetermined inverse problem, which naturally
allows diverse contents to fill up the missing or corrupted regions
realistically. Prevalent approaches using convolutional neural networks (CNNs)
can synthesize visually pleasant contents, but CNNs suffer from limited
perception fields for capturing global features. With image-level attention,
transformers enable to model long-range dependencies and generate diverse
contents with autoregressive modeling of pixel-sequence distributions. However,
the unidirectional attention in autoregressive transformers is suboptimal as
corrupted image regions may have arbitrary shapes with contexts from any
direction. We propose BAT-Fill, an innovative image inpainting framework that
introduces a novel bidirectional autoregressive transformer (BAT) for image
inpainting. BAT utilizes the transformers to learn autoregressive
distributions, which naturally allows the diverse generation of missing
contents. In addition, it incorporates the masked language model like BERT,
which enables bidirectionally modeling of contextual information of missing
regions for better image completion. Extensive experiments over multiple
datasets show that BAT-Fill achieves superior diversity and fidelity in image
inpainting qualitatively and quantitatively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.12653</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.12653</id><submitter>Kerstin Bongard-Blanchy Dr</submitter><version version="v1"><date>Mon, 26 Apr 2021 15:30:22 GMT</date><size>13970kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 06:31:05 GMT</date><size>28158kb</size><source_type>D</source_type></version><title>I am Definitely Manipulated, Even When I am Aware of it. It s
  Ridiculous! -- Dark Patterns from the End-User Perspective</title><authors>Kerstin Bongard-Blanchy, Arianna Rossi, Salvador Rivas, Sophie
  Doublet, Vincent Koenig, Gabriele Lenzini</authors><categories>cs.HC</categories><comments>ACM DIS Conference on Designing interactive systems, June 28 - July
  2, 2021, online. ACM, New York, NY, USA</comments><doi>10.1145/3461778.3462086</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Online services pervasively employ manipulative designs (i.e., dark patterns)
to influence users to purchase goods and subscriptions, spend more time
on-site, or mindlessly accept the harvesting of their personal data. To protect
users from the lure of such designs, we asked: are users aware of the presence
of dark patterns? If so, are they able to resist them? By surveying 406
individuals, we found that they are generally aware of the influence that
manipulative designs can exert on their online behaviour. However, being aware
does not equip users with the ability to oppose such influence. We further find
that respondents, especially younger ones, often recognise the &quot;darkness&quot; of
certain designs, but remain unsure of the actual harm they may suffer. Finally,
we discuss a set of interventions (e.g., bright patterns, design frictions,
training games, applications to expedite legal enforcement) in the light of our
findings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.12828</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.12828</id><submitter>Syed Eqbal Alam</submitter><version version="v1"><date>Mon, 26 Apr 2021 19:10:00 GMT</date><size>9761kb</size></version><version version="v2"><date>Mon, 24 May 2021 14:44:00 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Multi-resource allocation for federated settings: A non-homogeneous
  Markov chain model</title><authors>Syed Eqbal Alam and Fabian Wirth and Jia Yuan Yu</authors><categories>math.OC cs.AI cs.SY eess.SY</categories><comments>The paper was published without the co-authors' notice, and it is
  withdrawn due to their objection and due to authorship conflicts</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In a federated setting, agents coordinate with a central agent or a server to
solve an optimization problem in which agents do not share their information
with each other. Wirth and his co-authors, in a recent paper, describe how the
basic additive-increase multiplicative-decrease (AIMD) algorithm can be
modified in a straightforward manner to solve a class of optimization problems
for federated settings for a single shared resource with no inter-agent
communication. The AIMD algorithm is one of the most successful distributed
resource allocation algorithms currently deployed in practice. It is best known
as the backbone of the Internet and is also widely explored in other
application areas. We extend the single-resource algorithm to multiple
heterogeneous shared resources that emerge in smart cities, sharing economy,
and many other applications. Our main results show the convergence of the
average allocations to the optimal values. We model the system as a
non-homogeneous Markov chain with place-dependent probabilities. Furthermore,
simulation results are presented to demonstrate the efficacy of the algorithms
and to highlight the main features of our analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13173</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13173</id><submitter>Xinmeng Li</submitter><version version="v1"><date>Tue, 27 Apr 2021 13:32:41 GMT</date><size>1352kb</size><source_type>D</source_type></version><title>Question-Aware Memory Network for Multi-hop Question Answering in
  Human-Robot Interaction</title><authors>Xinmeng Li, Mamoun Alazab, Qian Li, Keping Yu, Quanjun Yin</authors><categories>cs.CL</categories><comments>25 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge graph question answering is an important technology in intelligent
human-robot interaction, which aims at automatically giving answer to human
natural language question with the given knowledge graph. For the
multi-relation question with higher variety and complexity, the tokens of the
question have different priority for the triples selection in the reasoning
steps. Most existing models take the question as a whole and ignore the
priority information in it. To solve this problem, we propose question-aware
memory network for multi-hop question answering, named QA2MN, to update the
attention on question timely in the reasoning process. In addition, we
incorporate graph context information into knowledge graph embedding model to
increase the ability to represent entities and relations. We use it to
initialize the QA2MN model and fine-tune it in the training process. We
evaluate QA2MN on PathQuestion and WorldCup2014, two representative datasets
for complex multi-hop question answering. The result demonstrates that QA2MN
achieves state-of-the-art Hits@1 accuracy on the two datasets, which validates
the effectiveness of our model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13254</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13254</id><submitter>John Emanuello Ph.D.</submitter><version version="v1"><date>Tue, 27 Apr 2021 17:35:31 GMT</date><size>1kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 15:01:28 GMT</date><size>1kb</size></version><title>Proceedings - AI/ML for Cybersecurity: Challenges, Solutions, and Novel
  Ideas at SIAM Data Mining 2021</title><authors>John Emanuello, Kimberly Ferguson-Walter, Erik Hemberg, Una-May O
  Reilly, Ahmad Ridley, Dennis Ross, Diane Staheli, William Streilein</authors><categories>cs.CR cs.AI</categories><proxy>Diane Staheli</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malicious cyber activity is ubiquitous and its harmful effects have dramatic
and often irreversible impacts on society. Given the shortage of cybersecurity
professionals, the ever-evolving adversary, the massive amounts of data which
could contain evidence of an attack, and the speed at which defensive actions
must be taken, innovations which enable autonomy in cybersecurity must continue
to expand, in order to move away from a reactive defense posture and towards a
more proactive one.
  The challenges in this space are quite different from those associated with
applying AI in other domains such as computer vision. The environment suffers
from an incredibly high degree of uncertainty, stemming from the intractability
of ingesting all the available data, as well as the possibility that malicious
actors are manipulating the data. Another unique challenge in this space is the
dynamism of the adversary causes the indicators of compromise to change
frequently and without warning.
  In spite of these challenges, machine learning has been applied to this
domain and has achieved some success in the realm of detection. While this
aspect of the problem is far from solved, a growing part of the commercial
sector is providing ML-enhanced capabilities as a service. Many of these
entities also provide platforms which facilitate the deployment of these
automated solutions. Academic research in this space is growing and continues
to influence current solutions, as well as strengthen foundational knowledge
which will make autonomous agents in this space a possibility.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13582</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13582</id><submitter>Sunnie S. Y. Kim</submitter><version version="v1"><date>Wed, 28 Apr 2021 06:21:28 GMT</date><size>17312kb</size><source_type>D</source_type></version><title>[Re] Don't Judge an Object by Its Context: Learning to Overcome
  Contextual Bias</title><authors>Sunnie S. Y. Kim, Sharon Zhang, Nicole Meister, Olga Russakovsky</authors><categories>cs.CV</categories><comments>ML Reproducibility Challenge 2020. Accepted for publication in the
  ReScience C journal</comments><doi>10.5281/zenodo.4834352</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Singh et al. (2020) point out the dangers of contextual bias in visual
recognition datasets. They propose two methods, CAM-based and feature-split,
that better recognize an object or attribute in the absence of its typical
context while maintaining competitive within-context accuracy. To verify their
performance, we attempted to reproduce all 12 tables in the original paper,
including those in the appendix. We also conducted additional experiments to
better understand the proposed methods, including increasing the regularization
in CAM-based and removing the weighted loss in feature-split. As the original
code was not made available, we implemented the entire pipeline from scratch in
PyTorch 1.7.0. Our implementation is based on the paper and email exchanges
with the authors. We found that both proposed methods in the original paper
help mitigate contextual bias, although for some methods, we could not
completely replicate the quantitative results in the paper even after
completing an extensive hyperparameter search. For example, on COCO-Stuff,
DeepFashion, and UnRel, our feature-split model achieved an increase in
accuracy on out-of-context images over the standard baseline, whereas on AwA,
we saw a drop in performance. For the proposed CAM-based method, we were able
to reproduce the original paper's results to within 0.5$\%$ mAP. Our
implementation can be found at
https://github.com/princetonvisualai/ContextualBias.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13872</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13872</id><submitter>Ukyo Honda</submitter><version version="v1"><date>Wed, 28 Apr 2021 16:36:52 GMT</date><size>8539kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 07:04:37 GMT</date><size>8540kb</size><source_type>D</source_type></version><title>Removing Word-Level Spurious Alignment between Images and
  Pseudo-Captions in Unsupervised Image Captioning</title><authors>Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, Yuji
  Matsumoto</authors><categories>cs.CL cs.CV</categories><comments>EACL 2021 (11 pages, 3 figures; added references)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised image captioning is a challenging task that aims at generating
captions without the supervision of image-sentence pairs, but only with images
and sentences drawn from different sources and object labels detected from the
images. In previous work, pseudo-captions, i.e., sentences that contain the
detected object labels, were assigned to a given image. The focus of the
previous work was on the alignment of input images and pseudo-captions at the
sentence level. However, pseudo-captions contain many words that are irrelevant
to a given image. In this work, we investigate the effect of removing
mismatched words from image-sentence alignment to determine how they make this
task difficult. We propose a simple gating mechanism that is trained to align
image features with only the most reliable words in pseudo-captions: the
detected object labels. The experimental results show that our proposed method
outperforms the previous methods without introducing complex sentence-level
learning objectives. Combined with the sentence-level alignment method of
previous work, our method further improves its performance. These results
confirm the importance of careful alignment in word-level details.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13950</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13950</id><submitter>Zafiirah Hosenie</submitter><version version="v1"><date>Wed, 28 Apr 2021 18:12:51 GMT</date><size>2877kb</size><source_type>D</source_type></version><title>MeerCRAB: MeerLICHT Classification of Real and Bogus Transients using
  Deep Learning</title><authors>Zafiirah Hosenie, Steven Bloemen, Paul Groot, Robert Lyon, Bart
  Scheers, Benjamin Stappers, Fiorenzo Stoppa, Paul Vreeswijk, Simon De Wet,
  Marc Klein Wolt, Elmar K\&quot;ording, Vanessa McBride, Rudolf Le Poole, Kerry
  Paterson, Dani\&quot;elle L. A. Pieterse and Patrick Woudt</authors><categories>astro-ph.IM astro-ph.GA cs.AI cs.CV cs.LG</categories><comments>15 pages, 13 figures, Accepted for publication in Experimental
  Astronomy and appeared in the 3rd Workshop on Machine Learning and the
  Physical Sciences, NeurIPS 2020</comments><journal-ref>Exp Astron (2021)</journal-ref><doi>10.1007/s10686-021-09757-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomers require efficient automated detection and classification
pipelines when conducting large-scale surveys of the (optical) sky for variable
and transient sources. Such pipelines are fundamentally important, as they
permit rapid follow-up and analysis of those detections most likely to be of
scientific value. We therefore present a deep learning pipeline based on the
convolutional neural network architecture called $\texttt{MeerCRAB}$. It is
designed to filter out the so called 'bogus' detections from true astrophysical
sources in the transient detection pipeline of the MeerLICHT telescope. Optical
candidates are described using a variety of 2D images and numerical features
extracted from those images. The relationship between the input images and the
target classes is unclear, since the ground truth is poorly defined and often
the subject of debate. This makes it difficult to determine which source of
information should be used to train a classification algorithm. We therefore
used two methods for labelling our data (i) thresholding and (ii) latent class
model approaches. We deployed variants of $\texttt{MeerCRAB}$ that employed
different network architectures trained using different combinations of input
images and training set choices, based on classification labels provided by
volunteers. The deepest network worked best with an accuracy of 99.5$\%$ and
Matthews correlation coefficient (MCC) value of 0.989. The best model was
integrated to the MeerLICHT transient vetting pipeline, enabling the accurate
and efficient classification of detected transients that allows researchers to
select the most promising candidates for their research goals.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14060</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14060</id><submitter>Yunxiang Zhao</submitter><version version="v1"><date>Thu, 29 Apr 2021 00:50:06 GMT</date><size>810kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:17:20 GMT</date><size>1622kb</size><source_type>D</source_type></version><title>WGCN: Graph Convolutional Networks with Weighted Structural Features</title><authors>Yunxiang Zhao and Jianzhong Qi and Qingwei Liu and Rui Zhang</authors><categories>cs.LG cs.AI</categories><doi>10.1145/3404835.3462834</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph structural information such as topologies or connectivities provides
valuable guidance for graph convolutional networks (GCNs) to learn nodes'
representations. Existing GCN models that capture nodes' structural information
weight in- and out-neighbors equally or differentiate in- and out-neighbors
globally without considering nodes' local topologies. We observe that in- and
out-neighbors contribute differently for nodes with different local topologies.
To explore the directional structural information for different nodes, we
propose a GCN model with weighted structural features, named WGCN. WGCN first
captures nodes' structural fingerprints via a direction and degree aware Random
Walk with Restart algorithm, where the walk is guided by both edge direction
and nodes' in- and out-degrees. Then, the interactions between nodes'
structural fingerprints are used as the weighted node structural features. To
further capture nodes' high-order dependencies and graph geometry, WGCN embeds
graphs into a latent space to obtain nodes' latent neighbors and geometrical
relationships. Based on nodes' geometrical relationships in the latent space,
WGCN differentiates latent, in-, and out-neighbors with an attention-based
geometrical aggregation. Experiments on transductive node classification tasks
show that WGCN outperforms the baseline models consistently by up to 17.07% in
terms of accuracy on five benchmark datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14074</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14074</id><submitter>Kelly Zhang</submitter><version version="v1"><date>Thu, 29 Apr 2021 01:56:44 GMT</date><size>4587kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:11:42 GMT</date><size>1414kb</size><source_type>D</source_type></version><title>Statistical Inference with M-Estimators on Adaptively Collected Data</title><authors>Kelly W. Zhang, Lucas Janson, and Susan A. Murphy</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms are increasingly used in real-world sequential
decision-making problems. Associated with this is an increased desire to be
able to use the resulting datasets to answer scientific questions like: Did one
type of ad lead to more purchases? In which contexts is a mobile health
intervention effective? However, classical statistical approaches fail to
provide valid confidence intervals when used with data collected with bandit
algorithms. Alternative methods have recently been developed for simple models
(e.g., comparison of means). Yet there is a lack of general methods for
conducting statistical inference using more complex models on data collected
with (contextual) bandit algorithms; for example, current methods cannot be
used for valid inference on parameters in a logistic regression model for a
binary reward. In this work, we develop theory justifying the use of
M-estimators -- which includes estimators based on empirical risk minimization
as well as maximum likelihood -- on data collected with adaptive algorithms,
including (contextual) bandit algorithms. Specifically, we show that
M-estimators, modified with particular adaptive weights, can be used to
construct asymptotically valid confidence regions for a variety of inferential
targets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14118</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14118</id><submitter>Hanbo Zhang</submitter><version version="v1"><date>Thu, 29 Apr 2021 05:31:21 GMT</date><size>7047kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 06:46:10 GMT</date><size>5035kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 14:00:32 GMT</date><size>5578kb</size><source_type>D</source_type></version><title>REGRAD: A Large-Scale Relational Grasp Dataset for Safe and
  Object-Specific Robotic Grasping in Clutter</title><authors>Hanbo Zhang, Deyu Yang, Han Wang, Binglei Zhao, Xuguang Lan, Jishiyu
  Ding, Nanning Zheng</authors><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the impressive progress achieved in robust grasp detection, robots
are not skilled in sophisticated grasping tasks (e.g. search and grasp a
specific object in clutter). Such tasks involve not only grasping, but
comprehensive perception of the visual world (e.g. the relationship between
objects). Recently, the advanced deep learning techniques provide a promising
way for understanding the high-level visual concepts. It encourages robotic
researchers to explore solutions for such hard and complicated fields. However,
deep learning usually means data-hungry. The lack of data severely limits the
performance of deep-learning-based algorithms. In this paper, we present a new
dataset named \regrad to sustain the modeling of relationships among objects
and grasps. We collect the annotations of object poses, segmentations, grasps,
and relationships in each image for comprehensive perception of grasping. Our
dataset is collected in both forms of 2D images and 3D point clouds. Moreover,
since all the data are generated automatically, users are free to import their
own object models for the generation of as many data as they want. We have
released our dataset and codes. A video that demonstrates the process of data
generation is also available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14392</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14392</id><submitter>Shreshth Tuli</submitter><version version="v1"><date>Thu, 29 Apr 2021 15:09:44 GMT</date><size>7847kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 17:32:14 GMT</date><size>7847kb</size><source_type>D</source_type></version><title>COSCO: Container Orchestration using Co-Simulation and Gradient Based
  Optimization for Fog Computing Environments</title><authors>Shreshth Tuli, Shivananda Poojara, Satish N. Srirama, Giuliano Casale,
  Nicholas R. Jennings</authors><categories>cs.DC cs.PF</categories><comments>Accepted in IEEE Transactions on Parallel and Distributed Systems,
  2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Intelligent task placement and management of tasks in large-scale fog
platforms is challenging due to the highly volatile nature of modern workload
applications and sensitive user requirements of low energy consumption and
response time. Container orchestration platforms have emerged to alleviate this
problem with prior art either using heuristics to quickly reach scheduling
decisions or AI driven methods like reinforcement learning and evolutionary
approaches to adapt to dynamic scenarios. The former often fail to quickly
adapt in highly dynamic environments, whereas the latter have run-times that
are slow enough to negatively impact response time. Therefore, there is a need
for scheduling policies that are both reactive to work efficiently in volatile
environments and have low scheduling overheads. To achieve this, we propose a
Gradient Based Optimization Strategy using Back-propagation of gradients with
respect to Input (GOBI). Further, we leverage the accuracy of predictive
digital-twin models and simulation capabilities by developing a Coupled
Simulation and Container Orchestration Framework (COSCO). Using this, we create
a hybrid simulation driven decision approach, GOBI*, to optimize Quality of
Service (QoS) parameters. Co-simulation and the back-propagation approaches
allow these methods to adapt quickly in volatile environments. Experiments
conducted using real-world data on fog applications using the GOBI and GOBI*
methods, show a significant improvement in terms of energy consumption,
response time, Service Level Objective and scheduling time by up to 15, 40, 4,
and 82 percent respectively when compared to the state-of-the-art algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14654</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14654</id><submitter>Yang Chen</submitter><version version="v1"><date>Thu, 29 Apr 2021 21:03:49 GMT</date><size>325kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 22:37:43 GMT</date><size>351kb</size><source_type>D</source_type></version><title>Agent-Level Maximum Entropy Inverse Reinforcement Learning for Mean
  Field Games</title><authors>Yang Chen, Jiamou Liu and Bakhadyr Khoussainov</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean field games (MFG) facilitate the application of reinforcement learning
(RL) in large-scale multi-agent systems, through reducing interplays among
agents to those between an individual agent and the average effect from the
population. However, RL agents are notoriously prone to unexpected behaviours
due to the reward mis-specification. Although inverse RL (IRL) holds promise
for automatically acquiring suitable rewards from demonstrations, its extension
to MFG is challenging due to the complicated notion of mean-field-type
equilibria and the coupling between agent-level and population-level dynamics.
To this end, we propose a novel IRL framework for MFG, called Mean Field IRL
(MFIRL), where we build upon a new equilibrium concept and the maximum entropy
IRL framework. Crucially, MFIRL is brought forward as the first IRL method that
can recover the agent-level (ground-truth) reward functions for MFG.
Experiments show the superior performance of MFIRL on sample efficiency, reward
recovery and robustness against varying environment dynamics, compared to the
state-of-the-art method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14840</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14840</id><submitter>Zhishuai Guo</submitter><version version="v1"><date>Fri, 30 Apr 2021 08:50:24 GMT</date><size>264kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 01:24:31 GMT</date><size>277kb</size></version><title>On Stochastic Moving-Average Estimators for Non-Convex Optimization</title><authors>Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, Tianbao Yang</authors><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we demonstrate the power of a widely used stochastic estimator
based on moving average (SEMA) on a range of stochastic non-convex optimization
problems, which only requires {\bf a general unbiased stochastic oracle}. We
analyze various stochastic methods (existing or newly proposed) based on the
{\bf variance recursion property} of SEMA for three families of non-convex
optimization, namely standard stochastic non-convex minimization, stochastic
non-convex strongly-concave min-max optimization, and stochastic bilevel
optimization. Our contributions include: (i) for standard stochastic non-convex
minimization, we present a simple and intuitive proof of convergence for a
family Adam-style methods (including Adam) with an increasing or large
&quot;momentum&quot; parameter for the first-order moment, which gives an alternative yet
more natural way to guarantee Adam converge; (ii) for stochastic non-convex
strongly-concave min-max optimization, we present a single-loop stochastic
gradient descent ascent method based on the moving average estimators and
establish its oracle complexity of $O(1/\epsilon^4)$ without using a large
mini-batch size, addressing a gap in the literature; (iii) for stochastic
bilevel optimization, we present a single-loop stochastic method based on the
moving average estimators and establish its oracle complexity of $\widetilde
O(1/\epsilon^4)$ without computing the inverse or SVD of the Hessian matrix,
improving state-of-the-art results. For all these problems, we also establish a
variance diminishing result for the used stochastic gradient estimators.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14842</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14842</id><submitter>Likun Ren</submitter><version version="v1"><date>Fri, 30 Apr 2021 08:55:17 GMT</date><size>1921kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 03:27:06 GMT</date><size>1921kb</size><source_type>D</source_type></version><title>A Thermodynamic based and Data Driven Hybrid Network for Gas Turbine
  Modeling</title><authors>Likun Ren</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The on-wing engine performance is difficult to track for thermodynamic models
because of its inaccurate component maps, and also difficult for data driven
methods for their over-fitting to measurement errors. So, we propose a
thermodynamic based and data driven hybrid network for gas turbine modeling.
Different from thermodynamic models, our network reconstructs the component
characteristics in a data-driven way to take component degeneration and
individual difference into consideration. Moreover, different from data driven
methods, in the training phase, physical based equations and the analytical
mathematical description are used to ensure that the optimization converges to
the gas turbine's dynamics. A huge number of relaxed quasi steady state flight
data to 26970 is used to train and test our hybrid network. The result shows
that the accuracy of our hybrid network can reach about 7% measured by max T6
relative error, 5% better than map fitting based thermodynamic model and 8%
better than pure data driven method with similar model volume.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14937</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14937</id><submitter>Zheng Wang</submitter><version version="v1"><date>Fri, 30 Apr 2021 12:02:03 GMT</date><size>288kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 11:23:17 GMT</date><size>289kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 15:28:09 GMT</date><size>1713kb</size><source_type>D</source_type></version><title>Federated Learning with Fair Averaging</title><authors>Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang,
  Rongshan Yu</authors><categories>cs.LG</categories><comments>to be published in IJCAI2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fairness has emerged as a critical problem in federated learning (FL). In
this work, we identify a cause of unfairness in FL -- \emph{conflicting}
gradients with large differences in the magnitudes. To address this issue, we
propose the federated fair averaging (FedFV) algorithm to mitigate potential
conflicts among clients before averaging their gradients. We first use the
cosine similarity to detect gradient conflicts, and then iteratively eliminate
such conflicts by modifying both the direction and the magnitude of the
gradients. We further show the theoretical foundation of FedFV to mitigate the
issue conflicting gradients and converge to Pareto stationary solutions.
Extensive experiments on a suite of federated datasets confirm that FedFV
compares favorably against state-of-the-art methods in terms of fairness,
accuracy and efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14963</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14963</id><submitter>Georg W\&quot;olflein</submitter><version version="v1"><date>Fri, 30 Apr 2021 13:02:13 GMT</date><size>16737kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 14:27:50 GMT</date><size>16859kb</size><source_type>D</source_type></version><title>Determining Chess Game State From an Image</title><authors>Georg W\&quot;olflein and Ognjen Arandjelovi\'c</authors><categories>cs.CV cs.LG</categories><comments>https://github.com/georgw777/chesscog</comments><journal-ref>J. Imaging 2021, 7(6), 94</journal-ref><doi>10.3390/jimaging7060094</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Identifying the configuration of chess pieces from an image of a chessboard
is a problem in computer vision that has not yet been solved accurately.
However, it is important for helping amateur chess players improve their games
by facilitating automatic computer analysis without the overhead of manually
entering the pieces. Current approaches are limited by the lack of large
datasets and are not designed to adapt to unseen chess sets. This paper puts
forth a new dataset synthesised from a 3D model that is an order of magnitude
larger than existing ones. Trained on this dataset, a novel end-to-end chess
recognition system is presented that combines traditional computer vision
techniques with deep learning. It localises the chessboard using a RANSAC-based
algorithm that computes a projective transformation of the board onto a regular
grid. Using two convolutional neural networks, it then predicts an occupancy
mask for the squares in the warped image and finally classifies the pieces. The
described system achieves an error rate of 0.23% per square on the test set, 28
times better than the current state of the art. Further, a few-shot transfer
learning approach is developed that is able to adapt the inference system to a
previously unseen chess set using just two photos of the starting position,
obtaining a per-square accuracy of 99.83% on images of that new chess set. The
code, dataset, and trained models are made available online.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00065</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00065</id><submitter>Michael Frank</submitter><version version="v1"><date>Fri, 30 Apr 2021 19:53:47 GMT</date><size>4081kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 5 May 2021 02:34:54 GMT</date><size>4284kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 27 May 2021 18:57:49 GMT</date><size>4369kb</size><source_type>D</source_type></version><title>Quantum Foundations of Classical Reversible Computing</title><authors>Michael P. Frank and Karpur Shukla</authors><categories>quant-ph cond-mat.stat-mech cs.ET</categories><comments>73 pages, 16 figures, accepted by Entropy</comments><journal-ref>Entropy 2021, 23 (6), 701</journal-ref><doi>10.3390/e23060701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reversible computation paradigm aims to provide a new foundation for
general classical digital computing that is capable of circumventing the
thermodynamic limits to the energy efficiency of the conventional,
non-reversible digital paradigm. However, to date, the essential rationale for
and analysis of classical reversible computing (RC) has not yet been expressed
in terms that leverage the modern formal methods of non-equilibrium quantum
thermodynamics (NEQT). In this paper, we begin developing an NEQT-based
foundation for the physics of reversible computing. We use the framework of
Gorini-Kossakowski-Sudarshan-Lindblad dynamics (a.k.a. Lindbladians) with
multiple asymptotic states, incorporating recent results from resource theory,
full counting statistics, and stochastic thermodynamics. Important conclusions
include that, as expected: (1) Landauer's Principle indeed sets a strict lower
bound on entropy generation in traditional non-reversible architectures for
deterministic computing machines when we account for the loss of correlations;
and (2) implementations of the alternative reversible computation paradigm can
potentially avoid such losses, and thereby circumvent the Landauer limit,
potentially allowing the efficiency of future digital computing technologies to
continue improving indefinitely. We also outline a research plan for
identifying the fundamental minimum energy dissipation of reversible computing
machines as a function of speed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00113</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00113</id><submitter>Yisroel Mirsky Dr.</submitter><version version="v1"><date>Fri, 30 Apr 2021 22:34:32 GMT</date><size>8049kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 15:21:53 GMT</date><size>11191kb</size><source_type>D</source_type></version><title>IPatch: A Remote Adversarial Patch</title><authors>Yisroel Mirsky</authors><categories>cs.CV cs.AI cs.CR cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Applications such as autonomous vehicles and medical screening use deep
learning models to localize and identify hundreds of objects in a single frame.
In the past, it has been shown how an attacker can fool these models by placing
an adversarial patch within a scene. However, these patches must be placed in
the target location and do not explicitly alter the semantics elsewhere in the
image.
  In this paper, we introduce a new type of adversarial patch which alters a
model's perception of an image's semantics. These patches can be placed
anywhere within an image to change the classification or semantics of locations
far from the patch. We call this new class of adversarial examples `remote
adversarial patches' (RAP).
  We implement our own RAP called IPatch and perform an in-depth analysis on
image segmentation RAP attacks using five state-of-the-art architectures with
eight different encoders on the CamVid street view dataset. Moreover, we
demonstrate that the attack can be extended to object recognition models with
preliminary results on the popular YOLOv3 model. We found that the patch can
change the classification of a remote target region with a success rate of up
to 93% on average.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00132</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00132</id><submitter>Nikolay Ivanov</submitter><version version="v1"><date>Sat, 1 May 2021 00:39:59 GMT</date><size>908kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 23:58:08 GMT</date><size>908kb</size><source_type>D</source_type></version><title>Targeting the Weakest Link: Social Engineering Attacks in Ethereum Smart
  Contracts</title><authors>Nikolay Ivanov, Jianzhi Lou, Ting Chen, Jin Li, Qiben Yan</authors><categories>cs.CR</categories><comments>ACM ASIA Conference on Computer and Communications Security 2021, 15
  pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ethereum holds multiple billions of U.S. dollars in the form of Ether
cryptocurrency and ERC-20 tokens, with millions of deployed smart contracts
algorithmically operating these funds. Unsurprisingly, the security of Ethereum
smart contracts has been under rigorous scrutiny. In recent years, numerous
defense tools have been developed to detect different types of smart contract
code vulnerabilities. When opportunities for exploiting code vulnerabilities
diminish, the attackers start resorting to social engineering attacks, which
aim to influence humans -- often the weakest link in the system. The only known
class of social engineering attacks in Ethereum are honeypots, which plant
hidden traps for attackers attempting to exploit existing vulnerabilities,
thereby targeting only a small population of potential victims.
  In this work, we explore the possibility and existence of new social
engineering attacks beyond smart contract honeypots. We present two novel
classes of Ethereum social engineering attacks - Address Manipulation and
Homograph - and develop six zero-day social engineering attacks. To show how
the attacks can be used in popular programming patterns, we conduct a case
study of five popular smart contracts with combined market capitalization
exceeding $29 billion, and integrate our attack patterns in their source codes
without altering their existing functionality. Moreover, we show that these
attacks remain dormant during the test phase but activate their malicious logic
only at the final production deployment. We further analyze 85,656 open-source
smart contracts, and discover that 1,027 of them can be used for the proposed
social engineering attacks. We conduct a professional opinion survey with
experts from seven smart contract auditing firms, corroborating that the
exposed social engineering attacks bring a major threat to the smart contract
systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00149</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00149</id><submitter>Zhaoxin Fan</submitter><version version="v1"><date>Sat, 1 May 2021 02:23:49 GMT</date><size>1716kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 08:19:06 GMT</date><size>1861kb</size><source_type>D</source_type></version><title>SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale
  Place Recognition</title><authors>Zhaoxin Fan, Zhenbo Song, Hongyan Liu, Zhiwu Lu, Jun He and Xiaoyong
  Du</authors><categories>cs.CV</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point cloud-based large scale place recognition is fundamental for many
applications like Simultaneous Localization and Mapping (SLAM). Although many
models have been proposed and have achieved good performance by learning
short-range local features, long-range contextual properties have often been
neglected. Moreover, the model size has also become a bottleneck for their wide
applications. To overcome these challenges, we propose a super light-weight
network model termed SVT-Net for large scale place recognition. Specifically,
on top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based
Sparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer
(CSVT) are proposed to learn both short-range local features and long-range
contextual features in this model. Consisting of ASVT and CSVT, SVT-Net can
achieve state-of-the-art on benchmark datasets in terms of both accuracy and
speed with a super-light model size (0.9M). Meanwhile, two simplified versions
of SVT-Net are introduced, which also achieve state-of-the-art and further
reduce the model size to 0.8M and 0.4M respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00174</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00174</id><submitter>Hao Wang</submitter><version version="v1"><date>Sat, 1 May 2021 06:04:27 GMT</date><size>6188kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 06:52:05 GMT</date><size>6188kb</size><source_type>D</source_type></version><title>Blockchain-Based Decentralized Energy Management Platform for
  Residential Distributed Energy Resources in A Virtual Power Plant</title><authors>Qing Yang, Hao Wang, Taotao Wang, Shengli Zhang, Xiaoxiao Wu, Hui Wang</authors><categories>eess.SY cs.CR cs.DC cs.SY</categories><journal-ref>Applied Energy, 2021</journal-ref><doi>10.1016/j.apenergy.2021.117026</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The advent of distributed energy resources (DERs), such as distributed
renewables, energy storage, electric vehicles, and controllable loads,
\rv{brings} a significantly disruptive and transformational impact on the
centralized power system. It is widely accepted that a paradigm shift to a
decentralized power system with bidirectional power flow is necessary to the
integration of DERs. The virtual power plant (VPP) emerges as a promising
paradigm for managing DERs to participate in the power system. In this paper,
we develop a blockchain-based VPP energy management platform to facilitate a
rich set of transactive energy activities among residential users with
renewables, energy storage, and flexible loads in a VPP. Specifically, users
can interact with each other to trade energy for mutual benefits and provide
network services, such as feed-in energy, reserve, and demand response, through
the VPP. To respect the users' independence and preserve their privacy, we
design a decentralized optimization algorithm to optimize the users' energy
scheduling, energy trading, and network services. Then we develop a prototype
blockchain network for VPP energy management and implement the proposed
algorithm on the blockchain network. By experiments using real-world
data-trace, we validated the feasibility and effectiveness of our algorithm and
the blockchain system. The simulation results demonstrate that our
blockchain-based VPP energy management platform reduces the users' cost by up
to 38.6% and reduces the overall system cost by 11.2%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00175</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00175</id><submitter>Hao Wang</submitter><version version="v1"><date>Sat, 1 May 2021 06:25:43 GMT</date><size>8240kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 06:55:11 GMT</date><size>8240kb</size><source_type>D</source_type></version><title>Distributed Energy Trading Management for Renewable Prosumers with HVAC
  and Energy Storage</title><authors>Qing Yang, Hao Wang</authors><categories>eess.SY cs.SY</categories><journal-ref>Energy Reports, 2021</journal-ref><doi>10.1016/j.egyr.2021.03.038</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Heating, ventilating, and air-conditioning (HVAC) systems consume a large
amount of energy in residential houses and buildings. Effective energy
management of HVAC is a cost-effective way to improve energy efficiency and
reduce the energy cost of residential users. This work develops a novel
distributed method for the residential transactive energy system that enables
multiple users to interactively optimize their energy management of HVAC
systems and behind-the-meter batteries. Specifically, this method effectively
reduces the cost of smart homes by employing energy trading among users to
leverage their power usage flexibility without compromising the users' privacy.
To achieve this goal, we design a distributed optimization algorithm based on
the alternating direction method of multipliers (ADMM) to automatically operate
the HVAC system and batteries, which minimizes the energy costs of users.
Specifically, we decouple the optimization problem into a primal subproblem and
a dual subproblem. The primal subproblem is solved by the users, and the dual
subproblem is solved by the grid operator. Unlike the existing centralized
method, our approach only uses the users' private information locally for
solving the primal subproblem hence preserves the users' privacy. Using
real-world data, we validate our proposed algorithm through extensive
simulations in Matlab. The results demonstrate that our method effectively
incentivizes the energy trading among the users to reduce users' peak load and
reduce the overall energy cost of the system by 23% on average.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00368</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00368</id><submitter>Jhacson Meza</submitter><version version="v1"><date>Sun, 2 May 2021 01:09:13 GMT</date><size>13442kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 22:41:12 GMT</date><size>7408kb</size><source_type>D</source_type></version><title>MarkerPose: Robust Real-time Planar Target Tracking for Accurate Stereo
  Pose Estimation</title><authors>Jhacson Meza, Lenny A. Romero, Andres G. Marrugo</authors><categories>cs.CV</categories><comments>Accepted at CVPR 2021 LXCV Workshop</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the attention marker-less pose estimation has attracted in recent
years, marker-based approaches still provide unbeatable accuracy under
controlled environmental conditions. Thus, they are used in many fields such as
robotics or biomedical applications but are primarily implemented through
classical approaches, which require lots of heuristics and parameter tuning for
reliable performance under different environments. In this work, we propose
MarkerPose, a robust, real-time pose estimation system based on a planar target
of three circles and a stereo vision system. MarkerPose is meant for
high-accuracy pose estimation applications. Our method consists of two deep
neural networks for marker point detection. A SuperPoint-like network for
pixel-level accuracy keypoint localization and classification, and we introduce
EllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level
accuracy keypoint detection. The marker's pose is estimated through stereo
triangulation. The target point detection is robust to low lighting and motion
blur conditions. We compared MarkerPose with a detection method based on
classical computer vision techniques using a robotic arm for validation. The
results show our method provides better accuracy than the classical technique.
Finally, we demonstrate the suitability of MarkerPose in a 3D freehand
ultrasound system, which is an application where highly accurate pose
estimation is required. Code is available in Python and C++ at
https://github.com/jhacsonmeza/MarkerPose.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00385</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00385</id><submitter>Zachary Pardos</submitter><version version="v1"><date>Sun, 2 May 2021 03:08:53 GMT</date><size>1373kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 04:20:30 GMT</date><size>1374kb</size><source_type>D</source_type></version><title>pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models</title><authors>Anirudhan Badrinath, Frederic Wang, Zachary Pardos</authors><categories>cs.MS cs.AI cs.CY cs.LG</categories><comments>Accepted to the 2021 Conference on Educational Data Mining (EDM '21)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Knowledge Tracing, a model used for cognitive mastery estimation,
has been a hallmark of adaptive learning research and an integral component of
deployed intelligent tutoring systems (ITS). In this paper, we provide a brief
history of knowledge tracing model research and introduce pyBKT, an accessible
and computationally efficient library of model extensions from the literature.
The library provides data generation, fitting, prediction, and cross-validation
routines, as well as a simple to use data helper interface to ingest typical
tutor log dataset formats. We evaluate the runtime with various dataset sizes
and compare to past implementations. Additionally, we conduct sanity checks of
the model using experiments with simulated data to evaluate the accuracy of its
EM parameter learning and use real-world data to validate its predictions,
comparing pyBKT's supported model variants with results from the papers in
which they were originally introduced. The library is open source and open
license for the purpose of making knowledge tracing more accessible to
communities of research and practice and to facilitate progress in the field
through easier replication of past approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00795</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00795</id><submitter>Hankook Lee</submitter><version version="v1"><date>Mon, 3 May 2021 12:47:57 GMT</date><size>1042kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 16:03:58 GMT</date><size>1043kb</size><source_type>D</source_type></version><title>RetCL: A Selection-based Approach for Retrosynthesis via Contrastive
  Learning</title><authors>Hankook Lee, Sungsoo Ahn, Seung-Woo Seo, You Young Song, Eunho Yang,
  Sung-Ju Hwang, Jinwoo Shin</authors><categories>cs.LG</categories><comments>Accepted to IJCAI 2021. Short version was accepted to Machine
  Learning for Molecules Workshop at NeurIPS 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrosynthesis, of which the goal is to find a set of reactants for
synthesizing a target product, is an emerging research area of deep learning.
While the existing approaches have shown promising results, they currently lack
the ability to consider availability (e.g., stability or purchasability) of the
reactants or generalize to unseen reaction templates (i.e., chemical reaction
rules). In this paper, we propose a new approach that mitigates the issues by
reformulating retrosynthesis into a selection problem of reactants from a
candidate set of commercially available molecules. To this end, we design an
efficient reactant selection framework, named RetCL (retrosynthesis via
contrastive learning), for enumerating all of the candidate molecules based on
selection scores computed by graph neural networks. For learning the score
functions, we also propose a novel contrastive training scheme with hard
negative mining. Extensive experiments demonstrate the benefits of the proposed
selection-based approach. For example, when all 671k reactants in the USPTO
{database} are given as candidates, our RetCL achieves top-1 exact match
accuracy of $71.3\%$ for the USPTO-50k benchmark, while a recent
transformer-based approach achieves $59.6\%$. We also demonstrate that RetCL
generalizes well to unseen templates in various settings in contrast to
template-based approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00846</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00846</id><submitter>Alexander Robertson</submitter><version version="v1"><date>Mon, 3 May 2021 13:35:10 GMT</date><size>16105kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 4 May 2021 08:28:06 GMT</date><size>5640kb</size><source_type>D</source_type></version><title>Semantic Journeys: Quantifying Change in Emoji Meaning from 2012-2018</title><authors>Alexander Robertson, Farhana Ferdousi Liza, Dong Nguyen, Barbara
  McGillivray, Scott A. Hale</authors><categories>cs.CL</categories><journal-ref>4th International Workshop on Emoji Understanding and Applications
  in Social Media 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The semantics of emoji has, to date, been considered from a static
perspective. We offer the first longitudinal study of how emoji semantics
changes over time, applying techniques from computational linguistics to six
years of Twitter data. We identify five patterns in emoji semantic development
and find evidence that the less abstract an emoji is, the more likely it is to
undergo semantic change. In addition, we analyse select emoji in more detail,
examining the effect of seasonality and world events on emoji semantics. To aid
future work on emoji and semantics, we make our data publicly available along
with a web-based interface that anyone can use to explore semantic change in
emoji.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.00930</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.00930</id><submitter>Arnab Karmakar</submitter><version version="v1"><date>Sun, 11 Apr 2021 15:47:03 GMT</date><size>5119kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 10:01:51 GMT</date><size>5914kb</size><source_type>D</source_type></version><title>Pose Invariant Person Re-Identification using Robust Pose-transformation
  GAN</title><authors>Arnab Karmakar and Deepak Mishra</authors><categories>cs.CV cs.AI cs.LG</categories><comments>Undergraduate thesis at Indian Institute of Space Science and
  Technology, Under review in IEEE Systems, Man and Cybernetics (SMCA)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The objective of person re-identification (re-ID) is to retrieve a person's
images from an image gallery, given a single instance of the person of
interest. Despite several advancements, learning discriminative
identity-sensitive and viewpoint invariant features for robust Person
Re-identification is a major challenge owing to the large pose variation of
humans. This paper proposes a re-ID pipeline that utilizes the image generation
capability of Generative Adversarial Networks combined with pose clustering and
feature fusion to achieve pose invariant feature learning. The objective is to
model a given person under different viewpoints and large pose changes and
extract the most discriminative features from all the appearances. The pose
transformational GAN (pt-GAN) module is trained to generate a person's image in
any given pose. In order to identify the most significant poses for
discriminative feature extraction, a Pose Clustering module is proposed. The
given instance of the person is modelled in varying poses and these features
are effectively combined through the Feature Fusion Network. The final re-ID
model consisting of these 3 sub-blocks, alleviates the pose dependence in
person re-ID. Also, The proposed model is robust to occlusion, scale, rotation
and illumination, providing a framework for viewpoint invariant feature
learning. The proposed method outperforms the state-of-the-art GAN based models
in 4 benchmark datasets. It also surpasses the state-of-the-art models that
report higher re-ID accuracy in terms of improvement over baseline.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01275</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01275</id><submitter>Yunxiang Zhao</submitter><version version="v1"><date>Tue, 4 May 2021 03:50:21 GMT</date><size>714kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:08:24 GMT</date><size>714kb</size><source_type>D</source_type></version><title>Graph Pooling via Coarsened Graph Infomax</title><authors>Yunsheng Pang, Yunxiang Zhao, Dongsheng Li</authors><categories>cs.LG cs.AI</categories><doi>10.1145/3404835.3463074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph pooling that summaries the information in a large graph into a compact
form is essential in hierarchical graph representation learning. Existing graph
pooling methods either suffer from high computational complexity or cannot
capture the global dependencies between graphs before and after pooling. To
address the problems of existing graph pooling methods, we propose Coarsened
Graph Infomax Pooling (CGIPool) that maximizes the mutual information between
the input and the coarsened graph of each pooling layer to preserve graph-level
dependencies. To achieve mutual information neural maximization, we apply
contrastive learning and propose a self-attention-based algorithm for learning
positive and negative samples. Extensive experimental results on seven datasets
illustrate the superiority of CGIPool comparing to the state-of-the-art
methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01648</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01648</id><submitter>Robert Tjarko Lange</submitter><version version="v1"><date>Tue, 4 May 2021 17:47:39 GMT</date><size>10712kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 04:24:37 GMT</date><size>10889kb</size><source_type>D</source_type></version><title>On Lottery Tickets and Minimal Task Representations in Deep
  Reinforcement Learning</title><authors>Marc Aurel Vischer, Robert Tjarko Lange, Henning Sprekeler</authors><categories>cs.LG cs.AI</categories><comments>15 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The lottery ticket hypothesis questions the role of overparameterization in
supervised deep learning. But how is the performance of winning lottery tickets
affected by the distributional shift inherent to reinforcement learning
problems? In this work, we address this question by comparing sparse agents who
have to address the non-stationarity of the exploration-exploitation problem
with supervised agents trained to imitate an expert. We show that feed-forward
networks trained via reinforcement learning and imitation learning can be
pruned to the same level of sparsity, suggesting that the distributional shift
has a limited impact on the size of winning tickets. Using a set of carefully
designed baseline conditions, we find that the majority of the lottery ticket
effect in both learning paradigms can be attributed to the identified mask
rather than the weight initialization. The input layer mask selectively prunes
entire input dimensions that turn out to be irrelevant for the task at hand. At
a moderate level of sparsity the mask identified by iterative magnitude pruning
yields minimal task-relevant representations, i.e., an interpretable inductive
bias. Finally, we propose a simple initialization rescaling which promotes the
robust identification of sparse task representations in low-dimensional control
tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02135</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02135</id><submitter>Denis Belomestny</submitter><version version="v1"><date>Wed, 5 May 2021 15:38:36 GMT</date><size>492kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 14:47:30 GMT</date><size>432kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 17:47:26 GMT</date><size>432kb</size><source_type>D</source_type></version><title>UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms</title><authors>D. Belomestny, I. Levin, E. Moulines, A. Naumov, S. Samsonov, V.
  Zorina</authors><categories>cs.LG math.OC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Policy evaluation is an important instrument for the comparison of different
algorithms in Reinforcement Learning (RL). Yet even a precise knowledge of the
value function $V^{\pi}$ corresponding to a policy $\pi$ does not provide
reliable information on how far is the policy $\pi$ from the optimal one. We
present a novel model-free upper value iteration procedure $({\sf UVIP})$ that
allows us to estimate the suboptimality gap $V^{\star}(x) - V^{\pi}(x)$ from
above and to construct confidence intervals for $V^\star$. Our approach relies
on upper bounds to the solution of the Bellman optimality equation via
martingale approach. We provide theoretical guarantees for ${\sf UVIP}$ under
general assumptions and illustrate its performance on a number of benchmark RL
problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02195</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02195</id><submitter>Dan Xu</submitter><version version="v1"><date>Wed, 5 May 2021 17:08:10 GMT</date><size>5694kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 06:49:35 GMT</date><size>11394kb</size><source_type>D</source_type></version><title>Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes</title><authors>Dan Xu, Andrea Vedaldi, Joao F. Henriques</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to train deep networks to decompose videos into 3D
geometry (camera and depth), moving objects, and their motions, with no
supervision. We build on the idea of view synthesis, which uses classical
camera geometry to re-render a source image from a different point-of-view,
specified by a predicted relative pose and depth map. By minimizing the error
between the synthetic image and the corresponding real image in a video, the
deep network that predicts pose and depth can be trained completely
unsupervised. However, the view synthesis equations rely on a strong
assumption: that objects do not move. This rigid-world assumption limits the
predictive power, and rules out learning about objects automatically. We
propose a simple solution: minimize the error on small regions of the image
instead. While the scene as a whole may be non-rigid, it is always possible to
find small regions that are approximately rigid, such as inside a moving
object. Our network can then predict different poses for each region, in a
sliding window from a learned dense pose map. This represents a significantly
richer model, including 6D object motions, with little additional complexity.
We achieve very competitive performance on unsupervised odometry and depth
prediction on KITTI. We also demonstrate new capabilities on EPIC-Kitchens, a
challenging dataset of indoor videos, where there is no ground truth
information for depth, odometry, object segmentation or motion. Yet all are
recovered automatically by our method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02358</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02358</id><submitter>Meng-Hao Guo</submitter><version version="v1"><date>Wed, 5 May 2021 22:29:52 GMT</date><size>13269kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 14:49:59 GMT</date><size>12943kb</size><source_type>D</source_type></version><title>Beyond Self-attention: External Attention using Two Linear Layers for
  Visual Tasks</title><authors>Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, Shi-Min Hu</authors><categories>cs.CV</categories><comments>11 pages, 6 figures. external attention and EAMLP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention mechanisms, especially self-attention, have played an increasingly
important role in deep feature representation for visual tasks. Self-attention
updates the feature at each position by computing a weighted sum of features
using pair-wise affinities across all positions to capture the long-range
dependency within a single sample. However, self-attention has quadratic
complexity and ignores potential correlation between different samples. This
paper proposes a novel attention mechanism which we call external attention,
based on two external, small, learnable, shared memories, which can be
implemented easily by simply using two cascaded linear layers and two
normalization layers; it conveniently replaces self-attention in existing
popular architectures. External attention has linear complexity and implicitly
considers the correlations between all data samples. We further incorporate the
multi-head mechanism into external attention to provide an all-MLP
architecture, external attention MLP (EAMLP), for image classification.
Extensive experiments on image classification, object detection, semantic
segmentation, instance segmentation, image generation, and point cloud analysis
reveal that our method provides results comparable or superior to the
self-attention mechanism and some of its variants, with much lower
computational and memory costs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02446</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02446</id><submitter>Jinglin Liu</submitter><version version="v1"><date>Thu, 6 May 2021 05:21:42 GMT</date><size>3168kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 13:24:58 GMT</date><size>3147kb</size><source_type>D</source_type></version><title>DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism</title><authors>Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Peng Liu, Zhou Zhao</authors><categories>eess.AS cs.LG cs.SD</categories><comments>acoustic model, singing voice synthesis, text to speech, diffusion
  model, shallow diffusion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singing voice synthesis (SVS) system is built to synthesize high-quality and
expressive singing voice, in which the acoustic model generates the acoustic
features (e.g., mel-spectrogram) given a music score. Previous singing acoustic
models adopt simple loss (e.g., L1 and L2) or generative adversarial network
(GAN) to reconstruct the acoustic features, while they suffer from
over-smoothing and unstable training issues respectively, which hinder the
naturalness of synthesized singing. In this work, we propose DiffSinger, an
acoustic model for SVS based on the diffusion probabilistic model. DiffSinger
is a parameterized Markov chain which iteratively converts the noise into
mel-spectrogram conditioned on the music score. By implicitly optimizing
variational bound, DiffSinger can be stably trained and generates realistic
outputs. To further improve the voice quality and speed up inference, we
introduce a shallow diffusion mechanism to make better use of the prior
knowledge learned by the simple loss. Specifically, DiffSinger starts
generation at a shallow step smaller than the total number of diffusion steps,
according to the intersection of the diffusion trajectories of the ground-truth
mel-spectrogram and the one predicted by a simple mel-spectrogram decoder.
Besides, we train a boundary prediction network to locate the intersection and
determine the shallow step adaptively. The evaluations conducted on the Chinese
singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS
work. Our extensional experiments also prove the generalization of DiffSinger
on text-to-speech task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02590</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02590</id><submitter>Samson Tan</submitter><version version="v1"><date>Thu, 6 May 2021 11:24:58 GMT</date><size>331kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 04:17:44 GMT</date><size>330kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 03:55:40 GMT</date><size>304kb</size><source_type>D</source_type></version><title>Reliability Testing for Natural Language Processing Systems</title><authors>Samson Tan, Shafiq Joty, Kathy Baxter, Araz Taeihagh, Gregory A.
  Bennett, Min-Yen Kan</authors><categories>cs.LG cs.AI cs.CL cs.CY cs.NE</categories><comments>Accepted to ACL-IJCNLP 2021 (main conference). Camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Questions of fairness, robustness, and transparency are paramount to address
before deploying NLP systems. Central to these concerns is the question of
reliability: Can NLP systems reliably treat different demographics fairly and
function correctly in diverse and noisy environments? To address this, we argue
for the need for reliability testing and contextualize it among existing work
on improving accountability. We show how adversarial attacks can be reframed
for this goal, via a framework for developing reliability tests. We argue that
reliability testing -- with an emphasis on interdisciplinary collaboration --
will enable rigorous and targeted testing, and aid in the enactment and
enforcement of industry standards.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02732</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02732</id><submitter>Joseph Viviano</submitter><version version="v1"><date>Thu, 6 May 2021 14:49:43 GMT</date><size>181kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 7 May 2021 17:28:38 GMT</date><size>181kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 20:39:54 GMT</date><size>185kb</size><source_type>D</source_type></version><title>What's in the Box? A Preliminary Analysis of Undesirable Content in the
  Common Crawl Corpus</title><authors>Alexandra Sasha Luccioni, Joseph D. Viviano</authors><categories>cs.CL</categories><comments>5 pages, 1 figure, 3 tables. Published as a main conference paper at
  ACL-IJCNLP 2021, submission #87. Code available at
  https://github.com/josephdviviano/whatsinthebox</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whereas much of the success of the current generation of neural language
models has been driven by increasingly large training corpora, relatively
little research has been dedicated to analyzing these massive sources of
textual data. In this exploratory analysis, we delve deeper into the Common
Crawl, a colossal web corpus that is extensively used for training language
models. We find that it contains a significant amount of undesirable content,
including hate speech and sexually explicit content, even after filtering
procedures. We discuss the potential impacts of this content on language models
and conclude with future research directions and a more mindful approach to
corpus collection and analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02866</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02866</id><submitter>Umang Gupta</submitter><version version="v1"><date>Thu, 6 May 2021 17:51:06 GMT</date><size>890kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 08:02:00 GMT</date><size>890kb</size><source_type>D</source_type></version><title>Membership Inference Attacks on Deep Regression Models for Neuroimaging</title><authors>Umang Gupta, Dimitris Stripelis, Pradeep K. Lam, Paul M. Thompson,
  Jos\'e Luis Ambite, Greg Ver Steeg</authors><categories>q-bio.QM cs.CR cs.LG eess.IV</categories><comments>To appear at Medical Imaging with Deep Learning 2021 (MIDL 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring the privacy of research participants is vital, even more so in
healthcare environments. Deep learning approaches to neuroimaging require large
datasets, and this often necessitates sharing data between multiple sites,
which is antithetical to the privacy objectives. Federated learning is a
commonly proposed solution to this problem. It circumvents the need for data
sharing by sharing parameters during the training process. However, we
demonstrate that allowing access to parameters may leak private information
even if data is never directly shared. In particular, we show that it is
possible to infer if a sample was used to train the model given only access to
the model prediction (black-box) or access to the model itself (white-box) and
some leaked samples from the training data distribution. Such attacks are
commonly referred to as Membership Inference attacks. We show realistic
Membership Inference attacks on deep learning models trained for 3D
neuroimaging tasks in a centralized as well as decentralized setup. We
demonstrate feasible attacks on brain age prediction models (deep learning
models that predict a person's age from their brain MRI scan). We correctly
identified whether an MRI scan was used in model training with a 60% to over
80% success rate depending on model complexity and security assumptions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03023</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03023</id><submitter>Alisa Liu</submitter><version version="v1"><date>Fri, 7 May 2021 01:19:38 GMT</date><size>6467kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 05:26:11 GMT</date><size>5963kb</size><source_type>D</source_type></version><title>DExperts: Decoding-Time Controlled Text Generation with Experts and
  Anti-Experts</title><authors>Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra
  Bhagavatula, Noah A. Smith, Yejin Choi</authors><categories>cs.CL</categories><comments>ACL 2021 camera-ready</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite recent advances in natural language generation, it remains
challenging to control attributes of generated text. We propose DExperts:
Decoding-time Experts, a decoding-time method for controlled text generation
that combines a pretrained language model with &quot;expert&quot; LMs and/or
&quot;anti-expert&quot; LMs in a product of experts. Intuitively, under the ensemble,
tokens only get high probability if they are considered likely by the experts,
and unlikely by the anti-experts. We apply DExperts to language detoxification
and sentiment-controlled generation, where we outperform existing controllable
generation methods on both automatic and human evaluations. Moreover, because
DExperts operates only on the output of the pretrained LM, it is effective with
(anti-)experts of smaller size, including when operating on GPT-3. Our work
highlights the promise of tuning small LMs on text with (un)desirable
attributes for efficient decoding-time steering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03033</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03033</id><submitter>Yilin Kang</submitter><version version="v1"><date>Fri, 7 May 2021 02:20:23 GMT</date><size>18kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 06:28:25 GMT</date><size>22kb</size></version><title>Towards Sharper Utility Bounds for Differentially Private Pairwise
  Learning</title><authors>Yilin Kang, Yong Liu, Jian Li, Weiping Wang</authors><categories>cs.LG</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairwise learning focuses on learning tasks with pairwise loss functions,
depends on pairs of training instances, and naturally fits for modeling
relationships between pairs of samples. In this paper, we focus on the privacy
of pairwise learning and propose a new differential privacy paradigm for
pairwise learning, based on gradient perturbation. Except for the privacy
guarantees, we also analyze the excess population risk and give corresponding
bounds under both expectation and high probability conditions. We use the
\textit{on-average stability} and the \textit{pairwise locally elastic
stability} theories to analyze the expectation bound and the high probability
bound, respectively. Moreover, our analyzed utility bounds do not require
convex pairwise loss functions, which means that our method is general to both
convex and non-convex conditions. Under these circumstances, the utility bounds
are similar to (or better than) previous bounds under convexity or strongly
convexity assumption, which are attractive results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03070</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03070</id><submitter>Yi-Chen Chen</submitter><version version="v1"><date>Fri, 7 May 2021 05:31:34 GMT</date><size>8744kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 11:05:43 GMT</date><size>481kb</size><source_type>D</source_type></version><title>SpeechNet: A Universal Modularized Model for Speech Processing Tasks</title><authors>Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin,
  Sung-Feng Huang, Da-Rong Liu, Chi-Liang Liu, Cheng-Kuang Lee, Hung-yi Lee</authors><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  There is a wide variety of speech processing tasks ranging from extracting
content information from speech signals to generating speech signals. For
different tasks, model networks are usually designed and tuned separately. If a
universal model can perform multiple speech processing tasks, some tasks might
be improved with the related abilities learned from other tasks. The multi-task
learning of a wide variety of speech processing tasks with a universal model
has not been studied. This paper proposes a universal modularized model,
SpeechNet, which treats all speech processing tasks into a speech/text input
and speech/text output format. We select five essential speech processing tasks
for multi-task learning experiments with SpeechNet. We show that SpeechNet
learns all of the above tasks, and we further analyze which tasks can be
improved by other tasks. SpeechNet is modularized and flexible for
incorporating more modules, tasks, or training approaches in the future. We
release the code and experimental settings to facilitate the research of
modularized universal models and multi-task learning of speech processing
tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03074</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03074</id><submitter>Ivan Tjuawinata</submitter><version version="v1"><date>Fri, 7 May 2021 05:59:22 GMT</date><size>34kb</size></version><version version="v2"><date>Sun, 30 May 2021 02:03:26 GMT</date><size>34kb</size></version><title>Leakage-Resilient Secret Sharing with Constant Share Size</title><authors>Ivan Tjuawinata and Chaoping Xing</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the leakage resilience of AG code-based ramp secret sharing
schemes extending the leakage resilience of linear threshold secret sharing
schemes over prime fields done by Benhamouda et al. Since there is not any
explicit efficient construction of AG codes over prime fields, we consider
constructions over prime fields with the help of concatenation method and those
over field extensions. Extending the Fourier analysis done by Benhamouda et
al., concatenated algebraic geometric codes over prime fields do produce some
nice leakage-resilient secret sharing schemes. One natural and curious question
is whether AG codes over extension fields produce better leakage-resilient
secret sharing schemes than the construction based on concatenated AG codes.
Such construction provides several advantages compared to the construction over
prime fields using concatenation method. First, AG codes over extension fields
give secret sharing schemes with smaller reconstruction for a fixed privacy
parameter t. Second, concatenated AG codes do not enjoy strong multiplicity and
hence they are not applicable to secure MPC schemes. It is also confirmed that
indeed AG codes over extension fields have stronger leakage-resilience under
some reasonable assumptions. These three advantages strongly motivate the study
of secret sharing schemes from AG codes over extension fields. The current
paper has two main contributions: 1, we obtain leakage-resilient secret sharing
schemes with constant share sizes and unbounded numbers of players. Like Shamir
secret scheme, our schemes enjoy multiplicity and hence can be applied to MPC.
2, via a sophisticated Fourier Analysis, we analyze the leakage-resilience of
secret sharing schemes from codes over extension fields. This is of its own
theoretical interest independent of its application to secret sharing schemes
from algebraic geometric codes over extension fields.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03075</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03075</id><submitter>Steven Y. Feng</submitter><version version="v1"><date>Fri, 7 May 2021 06:03:45 GMT</date><size>7707kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 20 May 2021 23:39:31 GMT</date><size>7712kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 21:57:45 GMT</date><size>7845kb</size><source_type>D</source_type></version><title>A Survey of Data Augmentation Approaches for NLP</title><authors>Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush
  Vosoughi, Teruko Mitamura, Eduard Hovy</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to ACL 2021 Findings. GitHub repo with paper list at
  https://github.com/styfeng/DataAug4NLP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data augmentation has recently seen increased interest in NLP due to more
work in low-resource domains, new tasks, and the popularity of large-scale
neural networks that require large amounts of training data. Despite this
recent upsurge, this area is still relatively underexplored, perhaps due to the
challenges posed by the discrete nature of language data. In this paper, we
present a comprehensive and unifying survey of data augmentation for NLP by
summarizing the literature in a structured manner. We first introduce and
motivate data augmentation for NLP, and then discuss major methodologically
representative approaches. Next, we highlight techniques that are used for
popular NLP applications and tasks. We conclude by outlining current challenges
and directions for future research. Overall, our paper aims to clarify the
landscape of existing literature in data augmentation for NLP and motivate
additional work in this area. We also present a GitHub repository with a paper
list that will be continuously updated at
https://github.com/styfeng/DataAug4NLP
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03081</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03081</id><submitter>Ryohei Oura</submitter><version version="v1"><date>Fri, 7 May 2021 06:31:35 GMT</date><size>1538kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 14:53:35 GMT</date><size>1538kb</size><source_type>D</source_type></version><title>Bounded Synthesis and Reinforcement Learning of Supervisors for
  Stochastic Discrete Event Systems with LTL Specifications</title><authors>Ryohei Oura, Toshimitsu Ushio, and Ami Sakakibara</authors><categories>eess.SY cs.FL cs.SY</categories><comments>15 pages, 4 figures, 2 tables, submitted to a journal</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we consider supervisory control of stochastic discrete event
systems (SDESs) under linear temporal logic specifications. Applying the
bounded synthesis, we reduce the supervisor synthesis into a problem of
satisfying a safety condition. First, we consider a synthesis problem of a
directed controller using the safety condition. We assign a negative reward to
the unsafe states and introduce an expected return with a state-dependent
discount factor. We compute a winning region and a directed controller with the
maximum satisfaction probability using a dynamic programming method, where the
expected return is used as a value function. Next, we construct a permissive
supervisor via the optimal value function. We show that the supervisor
accomplishes the maximum satisfaction probability and maximizes the reachable
set within the winning region. Finally, for an unknown SDES, we propose a
two-stage model-free reinforcement learning method for efficient learning of
the winning region and the directed controllers with the maximum satisfaction
probability. We also demonstrate the effectiveness of the proposed method by
simulation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03095</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03095</id><submitter>Chi Han</submitter><version version="v1"><date>Fri, 7 May 2021 07:49:56 GMT</date><size>2702kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 16:57:33 GMT</date><size>4963kb</size><source_type>D</source_type></version><title>Learning Shared Semantic Space for Speech-to-Text Translation</title><authors>Chi Han, Mingxuan Wang, Heng Ji, Lei Li</authors><categories>cs.CL</categories><comments>9 pages, 5 figures, Accepted by Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Having numerous potential applications and great impact, end-to-end speech
translation (ST) has long been treated as an independent task, failing to fully
draw strength from the rapid advances of its sibling - text machine translation
(MT). With text and audio inputs represented differently, the modality gap has
rendered MT data and its end-to-end models incompatible with their ST
counterparts. In observation of this obstacle, we propose to bridge this
representation gap with Chimera. By projecting audio and text features to a
common semantic representation, Chimera unifies MT and ST tasks and boosts the
performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new
state-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,
improving the SOTA by a +1.9 BLEU margin. Further experimental analyses
demonstrate that the shared semantic space indeed conveys common knowledge
between these two tasks and thus paves a new way for augmenting training
resources across modalities. Code, data, and resources are available at
https://github.com/Glaciohound/Chimera-ST.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03160</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03160</id><submitter>Alexander Robertson</submitter><version version="v1"><date>Fri, 7 May 2021 10:56:19 GMT</date><size>1119kb</size><source_type>D</source_type></version><title>Identity Signals in Emoji Do not Influence Perception of Factual Truth
  on Twitter</title><authors>Alexander Robertson, Walid Magdy, Sharon Goldwater</authors><categories>cs.CL</categories><journal-ref>International Workshop on Emoji Understanding and Applications in
  Social Media 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Prior work has shown that Twitter users use skin-toned emoji as an act of
self-representation to express their racial/ethnic identity. We test whether
this signal of identity can influence readers' perceptions about the content of
a post containing that signal. In a large scale (n=944) pre-registered
controlled experiment, we manipulate the presence of skin-toned emoji and
profile photos in a task where readers rate obscure trivia facts (presented as
tweets) as true or false. Using a Bayesian statistical analysis, we find that
neither emoji nor profile photo has an effect on how readers rate these facts.
This result will be of some comfort to anyone concerned about the manipulation
of online users through the crafting of fake profiles.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03229</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03229</id><submitter>Anthony Ferritto</submitter><version version="v1"><date>Fri, 7 May 2021 13:03:43 GMT</date><size>2025kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:28:26 GMT</date><size>4057kb</size><source_type>D</source_type></version><title>VAULT: VAriable Unified Long Text Representation for Machine Reading
  Comprehension</title><authors>Haoyang Wen, Anthony Ferritto, Heng Ji, Radu Florian, Avirup Sil</authors><categories>cs.CL</categories><comments>Accepted at ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing models on Machine Reading Comprehension (MRC) require complex model
architecture for effectively modeling long texts with paragraph representation
and classification, thereby making inference computationally inefficient for
production use. In this work, we propose VAULT: a light-weight and
parallel-efficient paragraph representation for MRC based on contextualized
representation from long document input, trained using a new Gaussian
distribution-based objective that pays close attention to the partially correct
instances that are close to the ground-truth. We validate our VAULT
architecture showing experimental results on two benchmark MRC datasets that
require long context modeling; one Wikipedia-based (Natural Questions (NQ)) and
the other on TechNotes (TechQA). VAULT can achieve comparable performance on NQ
with a state-of-the-art (SOTA) complex document modeling approach while being
16 times faster, demonstrating the efficiency of our proposed model. We also
demonstrate that our model can also be effectively adapted to a completely
different domain -- TechQA -- with large improvement over a model fine-tuned on
a previously published large PLM.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03482</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03482</id><submitter>Patrick Fernandes</submitter><version version="v1"><date>Fri, 7 May 2021 19:55:35 GMT</date><size>5292kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 11:07:39 GMT</date><size>5293kb</size><source_type>D</source_type></version><title>Measuring and Increasing Context Usage in Context-Aware Machine
  Translation</title><authors>Patrick Fernandes, Kayo Yin, Graham Neubig, Andr\'e F. T. Martins</authors><categories>cs.CL</categories><comments>ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in neural machine translation has demonstrated both the necessity
and feasibility of using inter-sentential context -- context from sentences
other than those currently being translated. However, while many current
methods present model architectures that theoretically can use this extra
context, it is often not clear how much they do actually utilize it at
translation time. In this paper, we introduce a new metric, conditional
cross-mutual information, to quantify the usage of context by these models.
Using this metric, we measure how much document-level machine translation
systems use particular varieties of context. We find that target context is
referenced more than source context, and that conditioning on a longer context
has a diminishing effect on results. We then introduce a new, simple training
method, context-aware word dropout, to increase the usage of context by
context-aware models. Experiments show that our method increases context usage
and that this reflects on the translation quality according to metrics such as
BLEU and COMET, as well as performance on anaphoric pronoun resolution and
lexical cohesion contrastive datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03571</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03571</id><submitter>Puhai Yang</submitter><version version="v1"><date>Sat, 8 May 2021 03:18:13 GMT</date><size>1730kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 03:55:48 GMT</date><size>1754kb</size><source_type>D</source_type></version><title>Comprehensive Study: How the Context Information of Different
  Granularity Affects Dialogue State Tracking?</title><authors>Puhai Yang and Heyan Huang and Xian-Ling Mao</authors><categories>cs.CL</categories><comments>Accepted as long paper at main conference of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dialogue state tracking (DST) plays a key role in task-oriented dialogue
systems to monitor the user's goal. In general, there are two strategies to
track a dialogue state: predicting it from scratch and updating it from
previous state. The scratch-based strategy obtains each slot value by inquiring
all the dialogue history, and the previous-based strategy relies on the current
turn dialogue to update the previous dialogue state. However, it is hard for
the scratch-based strategy to correctly track short-dependency dialogue state
because of noise; meanwhile, the previous-based strategy is not very useful for
long-dependency dialogue state tracking. Obviously, it plays different roles
for the context information of different granularity to track different kinds
of dialogue states. Thus, in this paper, we will study and discuss how the
context information of different granularity affects dialogue state tracking.
First, we explore how greatly different granularities affect dialogue state
tracking. Then, we further discuss how to combine multiple granularities for
dialogue state tracking. Finally, we apply the findings about context
granularity to few-shot learning scenario. Besides, we have publicly released
all codes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03603</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03603</id><submitter>Periyapattana Narayana Prasad Karthik</submitter><version version="v1"><date>Sat, 8 May 2021 05:53:12 GMT</date><size>123kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 08:17:21 GMT</date><size>125kb</size></version><title>Learning to Detect an Odd Restless Markov Arm with a Trembling Hand</title><authors>P. N. Karthik and Rajesh Sundaresan</authors><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>49 pages. A shorter version of this manuscript has been accepted for
  presentation at the 2021 IEEE International Symposium on Information Theory.
  This manuscript contains the proofs of all the main results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper studies the problem of finding an anomalous arm in a multi-armed
bandit when (a) each arm is a finite-state Markov process, and (b) the arms are
restless. Here, anomaly means that the transition probability matrix (TPM) of
one of the arms (the odd arm) is different from the common TPM of each of the
non-odd arms. The TPMs are unknown to a decision entity that wishes to find the
index of the odd arm as quickly as possible, subject to an upper bound on the
error probability. We derive a problem instance-specific asymptotic lower bound
on the expected time required to find the odd arm index, where the asymptotics
is as the error probability vanishes. Further, we devise a policy based on the
principle of certainty equivalence, and demonstrate that under a continuous
selection assumption and a certain regularity assumption on the TPMs, the
policy achieves the lower bound arbitrarily closely. Thus, while the lower
bound is shown for all problem instances, the upper bound is shown only for
those problem instances satisfying the continuous selection and the regularity
assumptions. Our achievability analysis is based on resolving the
identifiability problem in the context of a certain lifted countable-state
controlled Markov process.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03620</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03620</id><submitter>Chunhua Shen</submitter><version version="v1"><date>Sat, 8 May 2021 07:46:55 GMT</date><size>8924kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 03:25:50 GMT</date><size>8921kb</size><source_type>D</source_type></version><title>ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text
  Spotting</title><authors>Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu
  Liu, Hao Chen</authors><categories>cs.CV</categories><comments>Table 7 updated. Code is at: https://git.io/AdelaiDet. Journal
  extension of arXiv:2002.10200</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  End-to-end text-spotting, which aims to integrate detection and recognition
in a unified framework, has attracted increasing attention due to its
simplicity of the two complimentary tasks. It remains an open problem
especially when processing arbitrarily-shaped text instances. Previous methods
can be roughly categorized into two groups: character-based and
segmentation-based, which often require character-level annotations and/or
complex post-processing due to the unstructured output. Here, we tackle
end-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet
v2). Our main contributions are four-fold: 1) For the first time, we adaptively
fit arbitrarily-shaped text by a parameterized Bezier curve, which, compared
with segmentation-based methods, can not only provide structured output but
also controllable representation. 2) We design a novel BezierAlign layer for
extracting accurate convolution features of a text instance of arbitrary
shapes, significantly improving the precision of recognition over previous
methods. 3) Different from previous methods, which often suffer from complex
post-processing and sensitive hyper-parameters, our ABCNet v2 maintains a
simple pipeline with the only post-processing non-maximum suppression (NMS). 4)
As the performance of text recognition closely depends on feature alignment,
ABCNet v2 further adopts a simple yet effective coordinate convolution to
encode the position of the convolutional filters, which leads to a considerable
improvement with negligible computation overhead. Comprehensive experiments
conducted on various bilingual (English and Chinese) benchmark datasets
demonstrate that ABCNet v2 can achieve state-of-the-art performance while
maintaining very high efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03654</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03654</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Sat, 8 May 2021 09:45:21 GMT</date><size>348kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 02:08:10 GMT</date><size>1017kb</size><source_type>D</source_type></version><title>Improving Named Entity Recognition by External Context Retrieving and
  Cooperative Learning</title><authors>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
  Huang, Kewei Tu</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to ACL 2021, 12 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent advances in Named Entity Recognition (NER) show that document-level
contexts can significantly improve model performance. In many application
scenarios, however, such contexts are not available. In this paper, we propose
to find external contexts of a sentence by retrieving and selecting a set of
semantically relevant texts through a search engine, with the original sentence
as the query. We find empirically that the contextual representations computed
on the retrieval-based input view, constructed through the concatenation of a
sentence and its external contexts, can achieve significantly improved
performance compared to the original input view based only on the sentence.
Furthermore, we can improve the model performance of both input views by
Cooperative Learning, a training method that encourages the two input views to
produce similar contextual representations or output label distributions.
Experiments show that our approach can achieve new state-of-the-art performance
on 8 NER data sets across 5 domains.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03801</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03801</id><submitter>Potsawee Manakul</submitter><version version="v1"><date>Sat, 8 May 2021 23:53:03 GMT</date><size>5542kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 11:23:29 GMT</date><size>5890kb</size><source_type>D</source_type></version><title>Long-Span Summarization via Local Attention and Content Selection</title><authors>Potsawee Manakul and Mark J. F. Gales</authors><categories>cs.CL</categories><comments>ACL 2021 (camera-ready)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformer-based models have achieved state-of-the-art results in a wide
range of natural language processing (NLP) tasks including document
summarization. Typically these systems are trained by fine-tuning a large
pre-trained model to the target task. One issue with these transformer-based
models is that they do not scale well in terms of memory and compute
requirements as the input length grows. Thus, for long document summarization,
it can be challenging to train or fine-tune these models. In this work, we
exploit large pre-trained transformer-based models and address long-span
dependencies in abstractive summarization using two methods: local
self-attention; and explicit content selection. These approaches are compared
on a range of network configurations. Experiments are carried out on standard
long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed
datasets. We demonstrate that by combining these methods, we can achieve
state-of-the-art results on all three tasks in the ROUGE scores. Moreover,
without a large-scale GPU card, our approach can achieve comparable or better
results than existing approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03842</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03842</id><submitter>Yichong Leng</submitter><version version="v1"><date>Sun, 9 May 2021 05:35:36 GMT</date><size>410kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 20 May 2021 13:20:13 GMT</date><size>410kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 02:11:12 GMT</date><size>499kb</size><source_type>D</source_type></version><title>FastCorrect: Fast Error Correction with Edit Alignment for Automatic
  Speech Recognition</title><authors>Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu,
  Tao Qin, Xiang-Yang Li, Ed Lin, Tie-Yan Liu</authors><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Error correction techniques have been used to refine the output sentences
from automatic speech recognition (ASR) models and achieve a lower word error
rate (WER) than original ASR outputs. Previous works usually use a
sequence-to-sequence model to correct an ASR output sentence autoregressively,
which causes large latency and cannot be deployed in online ASR services. A
straightforward solution to reduce latency, inspired by non-autoregressive
(NAR) neural machine translation, is to use an NAR sequence generation model
for ASR error correction, which, however, comes at the cost of significantly
increased ASR error rate. In this paper, observing distinctive error patterns
and correction operations (i.e., insertion, deletion, and substitution) in ASR,
we propose FastCorrect, a novel NAR error correction model based on edit
alignment. In training, FastCorrect aligns each source token from an ASR output
sentence to the target tokens from the corresponding ground-truth sentence
based on the edit distance between the source and target sentences, and
extracts the number of target tokens corresponding to each source token during
edition/correction, which is then used to train a length predictor and to
adjust the source tokens to match the length of the target sentence for
parallel generation. In inference, the token number predicted by the length
predictor is used to adjust the source tokens for target sequence generation.
Experiments on the public AISHELL-1 dataset and an internal industrial-scale
ASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)
it speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER
reduction) compared with the autoregressive correction model; and 2) it
outperforms the popular NAR models adopted in neural machine translation and
text edition by a large margin.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04037</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04037</id><submitter>Liheng Ma</submitter><version version="v1"><date>Sun, 9 May 2021 22:13:46 GMT</date><size>232kb</size></version><version version="v2"><date>Sat, 29 May 2021 03:38:16 GMT</date><size>47kb</size></version><title>Graph Attention Networks with Positional Embeddings</title><authors>Liheng Ma, Reihaneh Rabbany, Adriana Romero-Soriano</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Neural Networks (GNNs) are deep learning methods which provide the
current state of the art performance in node classification tasks. GNNs often
assume homophily -- neighboring nodes having similar features and labels--, and
therefore may not be at their full potential when dealing with non-homophilic
graphs. In this work, we focus on addressing this limitation and enable Graph
Attention Networks (GAT), a commonly used variant of GNNs, to explore the
structural information within each graph locality. Inspired by the positional
encoding in the Transformers, we propose a framework, termed Graph Attentional
Networks with Positional Embeddings (GAT-POS), to enhance GATs with positional
embeddings which capture structural and positional information of the nodes in
the graph. In this framework, the positional embeddings are learned by a model
predictive of the graph context, plugged into an enhanced GAT architecture,
which is able to leverage both the positional and content information of each
node. The model is trained jointly to optimize for the task of node
classification as well as the task of predicting graph context. Experimental
results show that GAT-POS reaches remarkable improvement compared to strong GNN
baselines and recent structural embedding enhanced GNNs on non-homophilic
graphs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04054</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04054</id><submitter>Emily Sheng</submitter><version version="v1"><date>Mon, 10 May 2021 00:17:33 GMT</date><size>5256kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 18:54:32 GMT</date><size>5261kb</size><source_type>D</source_type></version><title>Societal Biases in Language Generation: Progress and Challenges</title><authors>Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng</authors><categories>cs.CL</categories><comments>ACL 2021 camera-ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology for language generation has advanced rapidly, spurred by
advancements in pre-training large models on massive amounts of data and the
need for intelligent agents to communicate in a natural manner. While
techniques can effectively generate fluent text, they can also produce
undesirable societal biases that can have a disproportionately negative impact
on marginalized populations. Language generation presents unique challenges for
biases in terms of direct user interaction and the structure of decoding
techniques. To better understand these challenges, we present a survey on
societal biases in language generation, focusing on how data and techniques
contribute to biases and progress towards reducing biases. Motivated by a lack
of studies on biases from decoding techniques, we also conduct experiments to
quantify the effects of these techniques. By further discussing general trends
and open challenges, we call to attention promising directions for research and
the importance of fairness and inclusivity considerations for language
generation applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04062</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04062</id><submitter>Francois Meyer</submitter><version version="v1"><date>Mon, 10 May 2021 01:13:25 GMT</date><size>1035kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 00:48:10 GMT</date><size>1035kb</size><source_type>D</source_type></version><title>Approximate Fr\'echet Mean for Data Sets of Sparse Graphs</title><authors>Daniel Ferguson and Fran\c{c}ois G. Meyer</authors><categories>cs.SI physics.data-an stat.ML</categories><comments>28 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To characterize the location (mean, median) of a set of graphs, one needs a
notion of centrality that is adapted to metric spaces, since graph sets are not
Euclidean spaces. A standard approach is to consider the Fr\'echet mean. In
this work, we equip a set of graph with the pseudometric defined by the
$\ell_2$ norm between the eigenvalues of their respective adjacency matrix .
Unlike the edit distance, this pseudometric reveals structural changes at
multiple scales, and is well adapted to studying various statistical problems
on sets of graphs. We describe an algorithm to compute an approximation to the
Fr\'echet mean of a set of undirected unweighted graphs with a fixed size.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04075</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04075</id><submitter>Ange Lou</submitter><version version="v1"><date>Mon, 10 May 2021 02:29:11 GMT</date><size>1134kb</size></version><version version="v2"><date>Sun, 30 May 2021 15:07:16 GMT</date><size>1120kb</size></version><title>CFPNet-M: A Light-Weight Encoder-Decoder Based Network for Multimodal
  Biomedical Image Real-Time Segmentation</title><authors>Ange Lou, Shuyue Guan and Murray Loew</authors><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, developments of deep learning techniques are providing
instrumental to identify, classify, and quantify patterns in medical images.
Segmentation is one of the important applications in medical image analysis. In
this regard, U-Net is the predominant approach to medical image segmentation
tasks. However, we found that those U-Net based models have limitations in
several aspects, for example, millions of parameters in the U-Net consuming
considerable computation resource and memory, lack of global information, and
missing some tough objects. Therefore, we applied two modifications to improve
the U-Net model: 1) designed and added the dilated channel-wise CNN module, 2)
simplified the U shape network. Based on these two modifications, we proposed a
novel light-weight architecture -- Channel-wise Feature Pyramid Network for
Medicine (CFPNet-M). To evaluate our method, we selected five datasets with
different modalities: thermography, electron microscopy, endoscopy, dermoscopy,
and digital retinal images. And we compared its performance with several models
having different parameter scales. This paper also involves our previous
studies of DC-UNet and some commonly used light-weight neural networks. We
applied the Tanimoto similarity instead of the Jaccard index for gray-level
image measurements. By comparison, CFPNet-M achieves comparable segmentation
results on all five medical datasets with only 0.65 million parameters, which
is about 2% of U-Net, and 8.8 MB memory. Meanwhile, the inference speed can
reach 80 FPS on a single RTX 2070Ti GPU with the 256 by 192 pixels input size.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04117</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04117</id><submitter>KayYen Wong</submitter><version version="v1"><date>Mon, 10 May 2021 05:07:03 GMT</date><size>1338kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 11:57:14 GMT</date><size>1338kb</size><source_type>D</source_type></version><title>Wiki-Reliability: A Large Scale Dataset for Content Reliability on
  Wikipedia</title><authors>KayYen Wong, Miriam Redi, Diego Saez-Trumper</authors><categories>cs.IR cs.CL cs.LG</categories><comments>Proceedings of the 44th International ACM SIGIR Conference on
  Research and Development in Information Retrieval (SIGIR '21), 2021</comments><doi>10.1145/3404835.3463253</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Wikipedia is the largest online encyclopedia, used by algorithms and web
users as a central hub of reliable information on the web. The quality and
reliability of Wikipedia content is maintained by a community of volunteer
editors. Machine learning and information retrieval algorithms could help scale
up editors' manual efforts around Wikipedia content reliability. However, there
is a lack of large-scale data to support the development of such research. To
fill this gap, in this paper, we propose Wiki-Reliability, the first dataset of
English Wikipedia articles annotated with a wide set of content reliability
issues. To build this dataset, we rely on Wikipedia &quot;templates&quot;. Templates are
tags used by expert Wikipedia editors to indicate content issues, such as the
presence of &quot;non-neutral point of view&quot; or &quot;contradictory articles&quot;, and serve
as a strong signal for detecting reliability issues in a revision. We select
the 10 most popular reliability-related templates on Wikipedia, and propose an
effective method to label almost 1M samples of Wikipedia article revisions as
positive or negative with respect to each template. Each positive/negative
example in the dataset comes with the full article text and 20 features from
the revision's metadata. We provide an overview of the possible downstream
tasks enabled by such data, and show that Wiki-Reliability can be used to train
large-scale models for content reliability prediction. We release all data and
code for public use.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04165</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04165</id><submitter>Pan Lu</submitter><version version="v1"><date>Mon, 10 May 2021 07:46:55 GMT</date><size>7695kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:28:02 GMT</date><size>5980kb</size><source_type>D</source_type></version><title>Inter-GPS: Interpretable Geometry Problem Solving with Formal Language
  and Symbolic Reasoning</title><authors>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan
  Liang, Song-Chun Zhu</authors><categories>cs.CL cs.AI cs.CV cs.FL</categories><comments>Accepted to ACL 2021, 13 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Geometry problem solving has attracted much attention in the NLP community
recently. The task is challenging as it requires abstract problem understanding
and symbolic reasoning with axiomatic knowledge. However, current datasets are
either small in scale or not publicly available. Thus, we construct a new
large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with
dense annotation in formal language. We further propose a novel geometry
solving approach with formal language and symbolic reasoning, called
Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the
problem text and diagram into formal language automatically via rule-based text
parsing and neural object detecting, respectively. Unlike implicit learning in
existing methods, Inter-GPS incorporates theorem knowledge as conditional rules
and performs symbolic reasoning step by step. Also, a theorem predictor is
designed to infer the theorem application sequence fed to the symbolic solver
for the more efficient and reasonable searching path. Extensive experiments on
the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves
significant improvements over existing methods. The project with code and data
is available at https://lupantech.github.io/inter-gps.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04297</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04297</id><submitter>Shuxin Zheng</submitter><version version="v1"><date>Mon, 10 May 2021 12:21:42 GMT</date><size>255kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 08:44:44 GMT</date><size>305kb</size><source_type>D</source_type></version><title>How could Neural Networks understand Programs?</title><authors>Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, Tie-Yan Liu</authors><categories>cs.PL cs.LG cs.SE</categories><journal-ref>ICML 2021</journal-ref><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Semantic understanding of programs is a fundamental problem for programming
language processing (PLP). Recent works that learn representations of code
based on pre-training techniques in NLP have pushed the frontiers in this
direction. However, the semantics of PL and NL have essential differences.
These being ignored, we believe it is difficult to build a model to better
understand programs, by either directly applying off-the-shelf NLP pre-training
techniques to the source code, or adding features to the model by the
heuristic. In fact, the semantics of a program can be rigorously defined by
formal semantics in PL theory. For example, the operational semantics,
describes the meaning of a valid program as updating the environment (i.e., the
memory address-value function) through fundamental operations, such as memory
I/O and conditional branching. Inspired by this, we propose a novel program
semantics learning paradigm, that the model should learn from information
composed of (1) the representations which align well with the fundamental
operations in operational semantics, and (2) the information of environment
transition, which is indispensable for program understanding. To validate our
proposal, we present a hierarchical Transformer-based pre-training model called
OSCAR to better facilitate the understanding of programs. OSCAR learns from
intermediate representation (IR) and an encoded representation derived from
static analysis, which are used for representing the fundamental operations and
approximating the environment transitions respectively. OSCAR empirically shows
the outstanding capability of program semantics understanding on many practical
software engineering tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04350</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04350</id><submitter>Liangzhen Zheng</submitter><version version="v1"><date>Mon, 10 May 2021 13:32:12 GMT</date><size>3054kb</size></version><version version="v2"><date>Thu, 20 May 2021 11:36:31 GMT</date><size>3423kb</size></version><version version="v3"><date>Sat, 22 May 2021 15:26:43 GMT</date><size>2907kb</size></version><version version="v4"><date>Sun, 30 May 2021 08:53:23 GMT</date><size>1641kb</size></version><title>tFold-TR: Combining Deep Learning Enhanced Hybrid Potential Energy for
  Template-Based Modeling Structure Refinement</title><authors>Liangzhen Zheng, Haidong Lan, Tao Shen, Jiaxiang Wu, Sheng Wang, Wei
  Liu, Junzhou Huang</authors><categories>physics.bio-ph cs.LG</categories><comments>28 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Protein structure prediction has been a grand challenge for over 50 years,
owing to its broad scientific and application interests. There are two primary
types of modeling algorithms, template-free modeling and template-based
modeling. The latter one is suitable for easy prediction tasks and is widely
adopted in computer-aided drug discoveries for drug design and screening.
Although it has been several decades since its first edition, the current
template-based modeling approach suffers from two critical problems: 1) there
are many missing regions in the template-query sequence alignment, and 2) the
accuracy of the distance pairs from different regions of the template varies,
and this information is not well introduced into the modeling. To solve these
two problems, we propose a structural optimization process based on template
modeling, introducing two neural network models to predict the distance
information of the missing regions and the accuracy of the distance pairs of
different regions in the template modeling structure. The predicted distances
and residue pairwise-specific deviations are incorporated into the potential
energy function for structural optimization, which significantly improves the
qualities of the original template modeling decoys.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04387</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04387</id><submitter>Jinjie Ni</submitter><version version="v1"><date>Mon, 10 May 2021 14:07:49 GMT</date><size>2469kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 13:45:12 GMT</date><size>2473kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 04:23:43 GMT</date><size>2473kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 06:12:48 GMT</date><size>2478kb</size><source_type>D</source_type></version><title>Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
  Survey</title><authors>Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, Vinay Adiga, Erik
  Cambria</authors><categories>cs.CL cs.AI cs.IR</categories><comments>76 pages, 19 figures</comments><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dialogue systems are a popular Natural Language Processing (NLP) task as it
is promising in real-life applications. It is also a complicated task since
many NLP tasks deserving study are involved. As a result, a multitude of novel
works on this task are carried out, and most of them are deep learning-based
due to the outstanding performance. In this survey, we mainly focus on the deep
learning-based dialogue systems. We comprehensively review state-of-the-art
research outcomes in dialogue systems and analyze them from two angles: model
type and system type. Specifically, from the angle of model type, we discuss
the principles, characteristics, and applications of different models that are
widely used in dialogue systems. This will help researchers acquaint these
models and see how they are applied in state-of-the-art frameworks, which is
rather helpful when designing a new dialogue system. From the angle of system
type, we discuss task-oriented and open-domain dialogue systems as two streams
of research, providing insight into the hot topics related. Furthermore, we
comprehensively review the evaluation methods and datasets for dialogue systems
to pave the way for future research. Finally, some possible research trends are
identified based on the recent research outcomes. To the best of our knowledge,
this survey is the most comprehensive and up-to-date one at present in the area
of dialogue systems and dialogue-related tasks, extensively covering the
popular frameworks, topics, and datasets.
  Keywords: Dialogue Systems, Chatbots, Conversational AI, Task-oriented, Open
Domain, Chit-chat, Question Answering, Artificial Intelligence, Natural
Language Processing, Information Retrieval, Deep Learning, Neural Networks,
CNN, RNN, Hierarchical Recurrent Encoder-Decoder, Memory Networks, Attention,
Transformer, Pointer Net, CopyNet, Reinforcement Learning, GANs, Knowledge
Graph, Survey, Review
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04447</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04447</id><submitter>Bing Li</submitter><version version="v1"><date>Mon, 10 May 2021 15:16:14 GMT</date><size>6945kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 23:42:06 GMT</date><size>6945kb</size><source_type>D</source_type></version><title>SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation</title><authors>Bing Li, Cheng Zheng, Silvio Giancola, Bernard Ghanem</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel scene flow estimation approach to capture and infer 3D
motions from point clouds. Estimating 3D motions for point clouds is
challenging, since a point cloud is unordered and its density is significantly
non-uniform. Such unstructured data poses difficulties in matching
corresponding points between point clouds, leading to inaccurate flow
estimation. We propose a novel architecture named Sparse
Convolution-Transformer Network (SCTN) that equips the sparse convolution with
the transformer. Specifically, by leveraging the sparse convolution, SCTN
transfers irregular point cloud into locally consistent flow features for
estimating continuous and consistent motions within an object/local object
part. We further propose to explicitly learn point relations using a point
transformer module, different from exiting methods. We show that the learned
relation-based contextual information is rich and helpful for matching
corresponding points, benefiting scene flow estimation. In addition, a novel
loss function is proposed to adaptively encourage flow consistency according to
feature similarity. Extensive experiments demonstrate that our proposed
approach achieves a new state of the art in scene flow estimation. Our approach
achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene
Flow respectively, which significantly outperforms previous methods by large
margins.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04607</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04607</id><submitter>Shadi Endrawis</submitter><version version="v1"><date>Mon, 10 May 2021 18:42:58 GMT</date><size>5864kb</size><source_type>D</source_type></version><title>Efficient Self-Supervised Data Collection for Offline Robot Learning</title><authors>Shadi Endrawis, Gal Leibovich, Guy Jacob, Gal Novik and Aviv Tamar</authors><categories>cs.RO cs.AI cs.LG</categories><comments>Accepted in ICRA 2021</comments><acm-class>I.2.9; I.2.6; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A practical approach to robot reinforcement learning is to first collect a
large batch of real or simulated robot interaction data, using some data
collection policy, and then learn from this data to perform various tasks,
using offline learning algorithms. Previous work focused on manually designing
the data collection policy, and on tasks where suitable policies can easily be
designed, such as random picking policies for collecting data about object
grasping. For more complex tasks, however, it may be difficult to find a data
collection policy that explores the environment effectively, and produces data
that is diverse enough for the downstream task. In this work, we propose that
data collection policies should actively explore the environment to collect
diverse data. In particular, we develop a simple-yet-effective goal-conditioned
reinforcement-learning method that actively focuses data collection on novel
observations, thereby collecting a diverse data-set. We evaluate our method on
simulated robot manipulation tasks with visual inputs and show that the
improved diversity of active data collection leads to significant improvements
in the downstream learning tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04771</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04771</id><submitter>Jiaxiang Wu</submitter><version version="v1"><date>Tue, 11 May 2021 03:40:29 GMT</date><size>7028kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 16:32:58 GMT</date><size>8001kb</size></version><title>EBM-Fold: Fully-Differentiable Protein Folding Powered by Energy-based
  Models</title><authors>Jiaxiang Wu, Shitong Luo, Tao Shen, Haidong Lan, Sheng Wang, Junzhou
  Huang</authors><categories>cs.LG</categories><comments>18 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate protein structure prediction from amino-acid sequences is critical
to better understanding the protein function. Recent advances in this area
largely benefit from more precise inter-residue distance and orientation
predictions, powered by deep neural networks. However, the structure
optimization procedure is still dominated by traditional tools, e.g. Rosetta,
where the structure is solved via minimizing a pre-defined statistical energy
function (with optional prediction-based restraints). Such energy function may
not be optimal in formulating the whole conformation space of proteins. In this
paper, we propose a fully-differentiable approach for protein structure
optimization, guided by a data-driven generative network. This network is
trained in a denoising manner, attempting to predict the correction signal from
corrupted distance matrices between Ca atoms. Once the network is well trained,
Langevin dynamics based sampling is adopted to gradually optimize structures
from random initialization. Extensive experiments demonstrate that our EBM-Fold
approach can efficiently produce high-quality decoys, compared against
traditional Rosetta-based structure optimization routines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04776</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04776</id><submitter>Xiaobin Liu</submitter><version version="v1"><date>Tue, 11 May 2021 04:09:49 GMT</date><size>438kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 05:57:52 GMT</date><size>288kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 14 May 2021 13:28:39 GMT</date><size>288kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 19 May 2021 02:11:13 GMT</date><size>289kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 31 May 2021 01:02:48 GMT</date><size>289kb</size><source_type>D</source_type></version><title>Graph Consistency Based Mean-Teaching for Unsupervised Domain Adaptive
  Person Re-Identification</title><authors>Xiaobin Liu, Shiliang Zhang</authors><categories>cs.CV cs.AI</categories><comments>IJCAI 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent works show that mean-teaching is an effective framework for
unsupervised domain adaptive person re-identification. However, existing
methods perform contrastive learning on selected samples between teacher and
student networks, which is sensitive to noises in pseudo labels and neglects
the relationship among most samples. Moreover, these methods are not effective
in cooperation of different teacher networks. To handle these issues, this
paper proposes a Graph Consistency based Mean-Teaching (GCMT) method with
constructing the Graph Consistency Constraint (GCC) between teacher and student
networks. Specifically, given unlabeled training images, we apply teacher
networks to extract corresponding features and further construct a teacher
graph for each teacher network to describe the similarity relationships among
training images. To boost the representation learning, different teacher graphs
are fused to provide the supervise signal for optimizing student networks. GCMT
fuses similarity relationships predicted by different teacher networks as
supervision and effectively optimizes student networks with more sample
relationships involved. Experiments on three datasets, i.e., Market-1501,
DukeMTMCreID, and MSMT17, show that proposed GCMT outperforms state-of-the-art
methods by clear margin. Specially, GCMT even outperforms the previous method
that uses a deeper backbone. Experimental results also show that GCMT can
effectively boost the performance with multiple teacher and student networks.
Our code is available at https://github.com/liu-xb/GCMT .
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04854</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04854</id><submitter>Ryan Henderson</submitter><version version="v1"><date>Tue, 11 May 2021 08:13:34 GMT</date><size>2393kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 12:07:30 GMT</date><size>2395kb</size><source_type>D</source_type></version><title>Improving Molecular Graph Neural Network Explainability with
  Orthonormalization and Induced Sparsity</title><authors>Ryan Henderson, Djork-Arn\'e Clevert, Floriane Montanari</authors><categories>stat.ML cs.LG</categories><comments>Accepted to ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rationalizing which parts of a molecule drive the predictions of a molecular
graph convolutional neural network (GCNN) can be difficult. To help, we propose
two simple regularization techniques to apply during the training of GCNNs:
Batch Representation Orthonormalization (BRO) and Gini regularization. BRO,
inspired by molecular orbital theory, encourages graph convolution operations
to generate orthonormal node embeddings. Gini regularization is applied to the
weights of the output layer and constrains the number of dimensions the model
can use to make predictions. We show that Gini and BRO regularization can
improve the accuracy of state-of-the-art GCNN attribution methods on artificial
benchmark datasets. In a real-world setting, we demonstrate that medicinal
chemists significantly prefer explanations extracted from regularized models.
While we only study these regularizers in the context of GCNNs, both can be
applied to other types of neural networks
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04944</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04944</id><submitter>Susana Nunes</submitter><version version="v1"><date>Tue, 11 May 2021 11:20:38 GMT</date><size>263kb</size></version><version version="v2"><date>Mon, 31 May 2021 10:44:07 GMT</date><size>263kb</size></version><title>Predicting Gene-Disease Associations with Knowledge Graph Embeddings
  over Multiple Ontologies</title><authors>Susana Nunes, Rita T. Sousa, Catia Pesquita</authors><categories>cs.LG</categories><comments>4 pages, 1 figure, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ontology-based approaches for predicting gene-disease associations include
the more classical semantic similarity methods and more recently knowledge
graph embeddings. While semantic similarity is typically restricted to
hierarchical relations within the ontology, knowledge graph embeddings consider
their full breadth. However, embeddings are produced over a single graph and
complex tasks such as gene-disease association may require additional
ontologies. We investigate the impact of employing richer semantic
representations that are based on more than one ontology, able to represent
both genes and diseases and consider multiple kinds of relations within the
ontologies. Our experiments demonstrate the value of employing knowledge graph
embeddings based on random-walks and highlight the need for a closer
integration of different ontologies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04949</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04949</id><submitter>Asahi Ushio</submitter><version version="v1"><date>Tue, 11 May 2021 11:38:49 GMT</date><size>5853kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 26 May 2021 10:10:38 GMT</date><size>5853kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 16:39:21 GMT</date><size>5853kb</size><source_type>D</source_type></version><title>BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models
  Identify Analogies?</title><authors>Asahi Ushio and Luis Espinosa-Anke and Steven Schockaert and Jose
  Camacho-Collados</authors><categories>cs.CL cs.LG</categories><comments>Accepted by ACL 2021 main conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Analogies play a central role in human commonsense reasoning. The ability to
recognize analogies such as &quot;eye is to seeing what ear is to hearing&quot;,
sometimes referred to as analogical proportions, shape how we structure
knowledge and understand language. Surprisingly, however, the task of
identifying such analogies has not yet received much attention in the language
model era. In this paper, we analyze the capabilities of transformer-based
language models on this unsupervised task, using benchmarks obtained from
educational settings, as well as more commonly used datasets. We find that
off-the-shelf language models can identify analogies to a certain extent, but
struggle with abstract and complex relations, and results are highly sensitive
to model architecture and hyperparameters. Overall the best results were
obtained with GPT-2 and RoBERTa, while configurations using BERT were not able
to outperform word embedding models. Our results raise important questions for
future work about how, and to what extent, pre-trained language models capture
knowledge about abstract semantic relations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05172</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05172</id><submitter>Hayato Takahashi</submitter><version version="v1"><date>Tue, 11 May 2021 16:27:48 GMT</date><size>242kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 27 May 2021 12:23:34 GMT</date><size>175kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 12:11:46 GMT</date><size>175kb</size><source_type>D</source_type></version><title>The explicit formula of the distributions of the nonoverlapping words
  and its applications to statistical tests for random numbers</title><authors>Hayato Takahashi</authors><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bassino et al. 2010 and Regnier et al. 1998 showed the generating functions
of the distributions of the number of the occurrences of words (distributions
of words for short) in finite string in the form of rational functions. However
the coefficients of the expansion of the rational functions are complicated and
we do not have a simple formula of the exact distributions of words from
rational functions. In this paper we study the finite dimensional generating
functions of the distribution of nonoverlapping words for each fixed sample
size and show the explicit formula of the distributions of words for Bernoulli
model. We demonstrate that 1) the tests based on the distributions of words
reject the random number generator in BSD Library with p-value almost zero and
2) computation of the distributions of words in the human DNA size strings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05233</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05233</id><submitter>Prafulla Dhariwal</submitter><version version="v1"><date>Tue, 11 May 2021 17:50:24 GMT</date><size>30977kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 12 May 2021 17:57:59 GMT</date><size>30978kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 13 May 2021 17:57:08 GMT</date><size>30980kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 1 Jun 2021 17:49:49 GMT</date><size>44152kb</size><source_type>D</source_type></version><title>Diffusion Models Beat GANs on Image Synthesis</title><authors>Prafulla Dhariwal, Alex Nichol</authors><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Added compute requirements, ImageNet 256$\times$256 upsampling FID
  and samples, DDIM guided sampler, fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that diffusion models can achieve image sample quality superior to
the current state-of-the-art generative models. We achieve this on
unconditional image synthesis by finding a better architecture through a series
of ablations. For conditional image synthesis, we further improve sample
quality with classifier guidance: a simple, compute-efficient method for
trading off diversity for fidelity using gradients from a classifier. We
achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet
256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep
even with as few as 25 forward passes per sample, all while maintaining better
coverage of the distribution. Finally, we find that classifier guidance
combines well with upsampling diffusion models, further improving FID to 3.94
on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our
code at https://github.com/openai/guided-diffusion
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05418</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05418</id><submitter>Aman Madaan</submitter><version version="v1"><date>Wed, 12 May 2021 04:04:10 GMT</date><size>1299kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 03:46:46 GMT</date><size>1587kb</size><source_type>D</source_type></version><title>Could you give me a hint? Generating inference graphs for defeasible
  reasoning</title><authors>Aman Madaan, Dheeraj Rajagopal, Niket Tandon, Yiming Yang, Eduard Hovy</authors><categories>cs.CL cs.AI</categories><comments>Findings of the Association for Computational Linguistics: ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defeasible reasoning is the mode of reasoning where conclusions can be
overturned by taking into account new evidence. A commonly used method in
cognitive science and logic literature is to handcraft argumentation supporting
inference graphs. While humans find inference graphs very useful for reasoning,
constructing them at scale is difficult. In this paper, we automatically
generate such inference graphs through transfer learning from another NLP task
that shares the kind of reasoning that inference graphs support. Through
automated metrics and human evaluation, we find that our method generates
meaningful graphs for the defeasible inference task. Human accuracy on this
task improves by 20% by consulting the generated graphs. Our findings open up
exciting new research avenues for cases where machine reasoning can help human
reasoning. (A dataset of 230,000 influence graphs for each defeasible query is
located at: https://tinyurl.com/defeasiblegraphs.)
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05482</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05482</id><submitter>Wagner Gon\c{c}alves Pinto</submitter><version version="v1"><date>Wed, 12 May 2021 07:39:30 GMT</date><size>2631kb</size><source_type>D</source_type></version><title>On the reproducibility of fully convolutional neural networks for
  modeling time-space evolving physical systems</title><authors>Wagner Gon\c{c}alves Pinto, Antonio Alguacil and Micha\&quot;el Bauerheim</authors><categories>cs.LG physics.flu-dyn</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Reproducibility of a deep-learning fully convolutional neural network is
evaluated by training several times the same network on identical conditions
(database, hyperparameters, hardware) with non-deterministic Graphics
Processings Unit (GPU) operations. The propagation of two-dimensional acoustic
waves, typical of time-space evolving physical systems, is studied on both
recursive and non-recursive tasks. Significant changes in models properties
(weights, featured fields) are observed. When tested on various propagation
benchmarks, these models systematically returned estimations with a high level
of deviation, especially for the recurrent analysis which strongly amplifies
variability due to the non-determinism. Trainings performed with double
floating-point precision provide slightly better estimations and a significant
reduction of the variability of both the network parameters and its testing
error range.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05490</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05490</id><submitter>Muhammad Ashfaq</submitter><version version="v1"><date>Wed, 12 May 2021 07:54:43 GMT</date><size>2533kb</size><source_type>D</source_type></version><title>SWFC-ART: A Cost-effective Approach for Fixed-Size-Candidate-Set
  Adaptive Random Testing through Small World Graphs</title><authors>Muhammad Ashfaq, Rubing Huang, Dave Towey, Michael Omari, Dmitry
  Yashunin, Patrick Kwaku Kudjo, Tao Zhang</authors><categories>cs.SE</categories><comments>26 Pages</comments><acm-class>D.2.5</acm-class><doi>10.1016/j.jss.2021.111008</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Adaptive random testing (ART) improves the failure-detection effectiveness of
random testing by leveraging properties of the clustering of failure-causing
inputs of most faulty programs: ART uses a sampling mechanism that evenly
spreads test cases within a software's input domain. The widely-used
Fixed-Sized-Candidate-Set ART (FSCS-ART) sampling strategy faces a quadratic
time cost, which worsens as the dimensionality of the software input domain
increases. In this paper, we propose an approach based on small world graphs
that can enhance the computational efficiency of FSCS-ART: SWFC-ART. To
efficiently perform nearest neighbor queries for candidate test cases, SWFC-ART
incrementally constructs a hierarchical navigable small world graph for
previously executed, non-failure-causing test cases. Moreover, SWFC-ART has
shown consistency in programs with high dimensional input domains. Our
simulation and empirical studies show that SWFC-ART reduces the computational
overhead of FSCS-ART from quadratic to log-linear order while maintaining the
failure-detection effectiveness of FSCS-ART, and remaining consistent in high
dimensional input domains. We recommend using SWFC-ART in practical software
testing scenarios, where real-life programs often have high dimensional input
domains and low failure rates.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05498</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05498</id><submitter>Gyubok Lee</submitter><version version="v1"><date>Wed, 12 May 2021 08:11:33 GMT</date><size>344kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 10:59:03 GMT</date><size>5532kb</size><source_type>D</source_type></version><title>Improving Lexically Constrained Neural Machine Translation with
  Source-Conditioned Masked Span Prediction</title><authors>Gyubok Lee, Seongjun Yang, Edward Choi</authors><categories>cs.CL cs.AI cs.LG</categories><comments>To appear in ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate terminology translation is crucial for ensuring the practicality and
reliability of neural machine translation (NMT) systems. To address this,
lexically constrained NMT explores various methods to ensure pre-specified
words and phrases appear in the translation output. However, in many cases,
those methods are studied on general domain corpora, where the terms are mostly
uni- and bi-grams (&gt;98%). In this paper, we instead tackle a more challenging
setup consisting of domain-specific corpora with much longer n-gram and highly
specialized terms. Inspired by the recent success of masked span prediction
models, we propose a simple and effective training strategy that achieves
consistent improvements on both terminology and sentence-level translation for
three domain-specific corpora in two language pairs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05545</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05545</id><submitter>Matthieu Dolbeault</submitter><version version="v1"><date>Wed, 12 May 2021 09:52:49 GMT</date><size>13kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 09:22:51 GMT</date><size>13kb</size></version><title>Optimal pointwise sampling for $L^2$ approximation</title><authors>Albert Cohen and Matthieu Dolbeault</authors><categories>math.NA cs.NA</categories><comments>17 pages</comments><msc-class>41A65 (Primary) 41A81, 93E24, 62E17, 94A20 (Secondary)</msc-class><acm-class>G.1.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given a function $u\in L^2=L^2(D,\mu)$, where $D\subset \mathbb R^d$ and
$\mu$ is a measure on $D$, and a linear subspace $V_n\subset L^2$ of dimension
$n$, we show that near-best approximation of $u$ in $V_n$ can be computed from
a near-optimal budget of $Cn$ pointwise evaluations of $u$, with $C&gt;1$ a
universal constant. The sampling points are drawn according to some random
distribution, the approximation is computed by a weighted least-squares method,
and the error is assessed in expected $L^2$ norm. This result improves on the
results in [6,8] which require a sampling budget that is sub-optimal by a
logarithmic factor, thanks to a sparsification strategy introduced in [17,18].
As a consequence, we obtain for any compact class $\mathcal K\subset L^2$ that
the sampling number $\rho_{Cn}^{\rm rand}(\mathcal K)_{L^2}$ in the randomized
setting is dominated by the Kolmogorov $n$-width $d_n(\mathcal K)_{L^2}$. While
our result shows the existence of a randomized sampling with such near-optimal
properties, we discuss remaining issues concerning its generation by a
computationally efficient algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05553</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05553</id><submitter>Guy Hacohen</submitter><version version="v1"><date>Wed, 12 May 2021 10:08:42 GMT</date><size>5010kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 07:41:49 GMT</date><size>6779kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 08:07:59 GMT</date><size>6779kb</size><source_type>D</source_type></version><title>Principal Components Bias in Deep Neural Networks</title><authors>Guy Hacohen and Daphna Weinshall</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent work suggests that convolutional neural networks of different
architectures learn to classify images in the same order. To understand this
phenomenon, we revisit the over-parametrized deep linear network model. Our
asymptotic analysis, assuming that the hidden layers are wide enough, reveals
that the convergence rate of this model's parameters is exponentially faster
along directions corresponding to the larger principal components of the data,
at a rate governed by the singular values. We term this convergence pattern the
Principal Components bias (PC-bias). We show how the PC-bias streamlines the
order of learning of both linear and non-linear networks, more prominently at
earlier stages of learning. We then compare our results to the spectral bias,
showing that both biases can be seen independently, and affect the order of
learning in different ways. Finally, we discuss how the PC-bias may explain
some benefits of early stopping and its connection to PCA, and why deep
networks converge more slowly when given random labels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05624</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05624</id><submitter>James Cusick</submitter><version version="v1"><date>Tue, 11 May 2021 00:22:13 GMT</date><size>490kb</size></version><version version="v2"><date>Sat, 29 May 2021 17:27:02 GMT</date><size>493kb</size></version><title>Turning the Tables: The View from Offshore During 60 Days in JST</title><authors>James J. Cusick</authors><categories>cs.SE</categories><comments>10 pages, 5 figures, 8 references</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A report and examination of a Remote Work experience during the Covid-19
pandemic encompassing a 14-hour time difference from the primary work location.
Advantages and disadvantages of a globally distributed work experience as
compared to an aligned time zone are explored. Logistical aspects of the
arrangement are provided as well as the management support, peer reaction, and
relative productivity. Recommendations are also provided on how to improve
future geographically diverse team arrangements.
  [Keywords: Global Software Development, Offshore Development, Software
Engineering, IT Management, Remote Work, Remote Office Design, Distributed
Communications, Collaboration Technology, Team Management, Research,
Development, Productivity]
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05887</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05887</id><submitter>Alexander Robertson</submitter><version version="v1"><date>Wed, 12 May 2021 18:23:51 GMT</date><size>2421kb</size><source_type>D</source_type></version><title>Black or White but never neutral: How readers perceive identity from
  yellow or skin-toned emoji</title><authors>Alexander Robertson, Walid Magdy, Sharon Goldwater</authors><categories>cs.CL</categories><journal-ref>ACM Conference On Computer-supported Cooperative Work And Social
  Computing 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Research in sociology and linguistics shows that people use language not only
to express their own identity but to understand the identity of others. Recent
work established a connection between expression of identity and emoji usage on
social media, through use of emoji skin tone modifiers. Motivated by that
finding, this work asks if, as with language, readers are sensitive to such
acts of self-expression and use them to understand the identity of authors. In
behavioral experiments (n=488), where text and emoji content of social media
posts were carefully controlled before being presented to participants, we find
in the affirmative -- emoji are a salient signal of author identity. That
signal is distinct from, and complementary to, the one encoded in language.
Participant groups (based on self-identified ethnicity) showed no differences
in how they perceive this signal, except in the case of the default yellow
emoji. While both groups associate this with a White identity, the effect was
stronger in White participants. Our finding that emoji can index social
variables will have experimental applications for researchers but also
implications for designers: supposedly ``neutral`` defaults may be more
representative of some users than others.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06041</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06041</id><submitter>Silin Gao</submitter><version version="v1"><date>Thu, 13 May 2021 01:58:39 GMT</date><size>6814kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:14:17 GMT</date><size>6815kb</size><source_type>D</source_type></version><title>HyKnow: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge
  Management</title><authors>Silin Gao, Ryuichi Takanobu, Wei Peng, Qun Liu, Minlie Huang</authors><categories>cs.CL</categories><comments>Findings of ACL-IJCNLP 2021, long paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task-oriented dialog (TOD) systems typically manage structured knowledge
(e.g. ontologies and databases) to guide the goal-oriented conversations.
However, they fall short of handling dialog turns grounded on unstructured
knowledge (e.g. reviews and documents). In this paper, we formulate a task of
modeling TOD grounded on both structured and unstructured knowledge. To address
this task, we propose a TOD system with hybrid knowledge management, HyKnow. It
extends the belief state to manage both structured and unstructured knowledge,
and is the first end-to-end model that jointly optimizes dialog modeling
grounded on these two kinds of knowledge. We conduct experiments on the
modified version of MultiWOZ 2.1 dataset, where dialogs are grounded on hybrid
knowledge. Experimental results show that HyKnow has strong end-to-end
performance compared to existing TOD systems. It also outperforms the pipeline
knowledge management schemes, with higher unstructured knowledge retrieval
accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06202</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06202</id><submitter>Agi Kurucz</submitter><version version="v1"><date>Thu, 13 May 2021 11:45:07 GMT</date><size>48kb</size></version><version version="v2"><date>Fri, 21 May 2021 09:43:39 GMT</date><size>34kb</size></version><version version="v3"><date>Tue, 25 May 2021 10:38:14 GMT</date><size>54kb</size></version><version version="v4"><date>Thu, 3 Jun 2021 12:56:27 GMT</date><size>34kb</size></version><title>Deciding FO-definability of Regular Languages</title><authors>Agi Kurucz, Vladislav Ryzhikov, Yury Savateev, and Michael
  Zakharyaschev</authors><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that, similarly to known PSpace-completeness of recognising
FO(&lt;)-definability of the language L(A) of a DFA A, deciding both FO(&lt;,C)- and
FO(&lt;,MOD)-definability are PSpace-complete. (Here, FO(&lt;,C) extends the
first-order logic FO(&lt;) with the standard congruence modulo n relation, and
FO(&lt;,MOD) with the quantifiers checking whether the number of positions
satisfying a given formula is divisible by a given n&gt;1. These FO-languages are
known to define regular languages that are decidable in AC0 and ACC0,
respectively.) We obtain these results by first showing that known algebraic
characterisations of FO-definability of L(A) can be captured by `localisable'
properties of the transition monoid of A. Using our criterion, we then
generalise the known proof of PSpace-hardness of FO(&lt;)-definability, and
establish the upper bounds not only for arbitrary DFAs but also for two-way
NFAs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06451</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06451</id><submitter>Rami Ezzine</submitter><version version="v1"><date>Thu, 13 May 2021 17:48:08 GMT</date><size>666kb</size></version><version version="v2"><date>Mon, 17 May 2021 19:57:34 GMT</date><size>660kb</size></version><version version="v3"><date>Wed, 2 Jun 2021 14:19:27 GMT</date><size>660kb</size></version><title>Outage Common Randomness Capacity Characterization of Multiple-Antenna
  Slow Fading Channels</title><authors>Rami Ezzine and Moritz Wiese and Christian Deppe and Holger Boche</authors><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:2102.01197</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of common randomness (CR) generation from discrete
correlated sources aided by one-way communication over single-user
multiple-input multiple-output (MIMO) slow fading channels with additive white
Gaussian noise (AWGN) and arbitrary state distribution. MIMO slow fading
channels are indispensable in many scenarios in modern wireless communication.
We completely solve the MIMO slow fading case by providing first an outage
formulation of its channel capacity that holds for arbitrary state
distribution. For this purpose, we provide an achievable rate for a specific
MIMO compound Gaussian channel. Second, we establish the outage CR capacity
over the MIMO slow fading channel using our result on its outage transmission
capacity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06597</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06597</id><submitter>Yizhe Zhang</submitter><version version="v1"><date>Fri, 14 May 2021 00:11:38 GMT</date><size>3859kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 02:24:28 GMT</date><size>3917kb</size><source_type>D</source_type></version><title>Joint Retrieval and Generation Training for Grounded Text Generation</title><authors>Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel
  Galley, Jianfeng Gao, Bill Dolan</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in large-scale pre-training such as GPT-3 allow seemingly
high quality text to be generated from a given prompt. However, such generation
systems often suffer from problems of hallucinated facts, and are not
inherently designed to incorporate useful external information. Grounded
generation models appear to offer remedies, but their training typically relies
on rarely-available parallel data where corresponding information-relevant
documents are provided for context. We propose a framework that alleviates this
data constraint by jointly training a grounded generator and document retriever
on the language model signal. The model learns to reward retrieval of the
documents with the highest utility in generation, and attentively combines them
using a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We
demonstrate that both generator and retriever can take advantage of this joint
training and work synergistically to produce more informative and relevant text
in both prose and dialogue generation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06625</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06625</id><submitter>Joshua Engelsma</submitter><version version="v1"><date>Fri, 14 May 2021 03:07:25 GMT</date><size>13950kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 16:02:59 GMT</date><size>13962kb</size><source_type>D</source_type></version><title>Biometrics: Trust, but Verify</title><authors>Anil K. Jain, Debayan Deb and Joshua J. Engelsma</authors><categories>cs.CV</categories><comments>20 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past two decades, biometric recognition has exploded into a plethora
of different applications around the globe. This proliferation can be
attributed to the high levels of authentication accuracy and user convenience
that biometric recognition systems afford end-users. However, in-spite of the
success of biometric recognition systems, there are a number of outstanding
problems and concerns pertaining to the various sub-modules of biometric
recognition systems that create an element of mistrust in their use - both by
the scientific community and also the public at large. Some of these problems
include: i) questions related to system recognition performance, ii) security
(spoof attacks, adversarial attacks, template reconstruction attacks and
demographic information leakage), iii) uncertainty over the bias and fairness
of the systems to all users, iv) explainability of the seemingly black-box
decisions made by most recognition systems, and v) concerns over data
centralization and user privacy. In this paper, we provide an overview of each
of the aforementioned open-ended challenges. We survey work that has been
conducted to address each of these concerns and highlight the issues requiring
further attention. Finally, we provide insights into how the biometric
community can address core biometric recognition systems design issues to
better instill trust, fairness, and security for all.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06709</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06709</id><submitter>Guofeng Lv</submitter><version version="v1"><date>Fri, 14 May 2021 08:42:55 GMT</date><size>2054kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 20 May 2021 02:16:37 GMT</date><size>1938kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 04:27:33 GMT</date><size>99kb</size><source_type>D</source_type></version><title>Learning Unknown from Correlations: Graph Neural Network for
  Inter-novel-protein Interaction Prediction</title><authors>Guofeng Lv, Zhiqiang Hu, Yanguang Bi, Shaoting Zhang</authors><categories>cs.LG cs.CE</categories><comments>10 pages(3 pages appendix), 2 figures, Accepted by Conference
  IJCAI2021, which is its extended version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The study of multi-type Protein-Protein Interaction (PPI) is fundamental for
understanding biological processes from a systematic perspective and revealing
disease mechanisms. Existing methods suffer from significant performance
degradation when tested in unseen dataset. In this paper, we investigate the
problem and find that it is mainly attributed to the poor performance for
inter-novel-protein interaction prediction. However, current evaluations
overlook the inter-novel-protein interactions, and thus fail to give an
instructive assessment. As a result, we propose to address the problem from
both the evaluation and the methodology. Firstly, we design a new evaluation
framework that fully respects the inter-novel-protein interactions and gives
consistent assessment across datasets. Secondly, we argue that correlations
between proteins must provide useful information for analysis of novel
proteins, and based on this, we propose a graph neural network based method
(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental
results on real-world datasets of different scales demonstrate that GNN-PPI
significantly outperforms state-of-the-art PPI prediction methods, especially
for the inter-novel-protein interaction prediction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06762</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06762</id><submitter>Yulong Chen</submitter><version version="v1"><date>Fri, 14 May 2021 11:12:40 GMT</date><size>5576kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 26 May 2021 08:32:00 GMT</date><size>5508kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 03:46:14 GMT</date><size>5502kb</size><source_type>D</source_type></version><title>DialogSum: A Real-Life Scenario Dialogue Summarization Dataset</title><authors>Yulong Chen, Yang Liu, Liang Chen and Yue Zhang</authors><categories>cs.CL cs.AI</categories><comments>ACL findings</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Proposal of large-scale datasets has facilitated research on deep neural
models for news summarization. Deep learning can also be potentially useful for
spoken dialogue summarization, which can benefit a range of real-life scenarios
including customer service management and medication tracking. To this end, we
propose DialogSum, a large-scale labeled dialogue summarization dataset. We
conduct empirical analysis on DialogSum using state-of-the-art neural
summarizers. Experimental results show unique challenges in dialogue
summarization, such as spoken terms, special discourse structures, coreferences
and ellipsis, pragmatics and social common sense, which require specific
representation learning technologies to better deal with.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06813</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06813</id><submitter>Guilherme Moraes Rosa</submitter><version version="v1"><date>Fri, 14 May 2021 13:21:12 GMT</date><size>92kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 14:09:54 GMT</date><size>108kb</size><source_type>D</source_type></version><title>A cost-benefit analysis of cross-lingual transfer methods</title><authors>Guilherme Moraes Rosa, Luiz Henrique Bonifacio, Leandro Rodrigues de
  Souza, Roberto Lotufo and Rodrigo Nogueira</authors><categories>cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An effective method for cross-lingual transfer is to fine-tune a bilingual or
multilingual model on a supervised dataset in one language and evaluating it on
another language in a zero-shot manner. Translating examples at training time
or inference time are also viable alternatives. However, there are costs
associated with these methods that are rarely addressed in the literature. In
this work, we analyze cross-lingual methods in terms of their effectiveness
(e.g., accuracy), development and deployment costs, as well as their latencies
at inference time. Our experiments on three tasks indicate that the best
cross-lingual method is highly task-dependent. Finally, by combining zero-shot
and translation methods, we achieve the state-of-the-art in two of the three
datasets used in this work. Based on these results, we question the need for
manually labeled training data in a target language. Code, models and
translated datasets are available at
https://github.com/unicamp-dl/cross-lingual-analysis
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06990</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06990</id><submitter>Saurabh Kulshreshtha</submitter><version version="v1"><date>Fri, 14 May 2021 17:54:28 GMT</date><size>3140kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 18:09:50 GMT</date><size>3221kb</size><source_type>D</source_type></version><title>BERT Busters: Outlier Dimensions that Disrupt Transformers</title><authors>Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers and Anna Rumshisky</authors><categories>cs.CL</categories><comments>Accepted as long paper at Findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multiple studies have shown that Transformers are remarkably robust to
pruning. Contrary to this received wisdom, we demonstrate that pre-trained
Transformer encoders are surprisingly fragile to the removal of a very small
number of features in the layer outputs (&lt;0.0001% of model weights). In case of
BERT and other pre-trained encoder Transformers, the affected component is the
scaling factors and biases in the LayerNorm. The outliers are high-magnitude
normalization parameters that emerge early in pre-training and show up
consistently in the same dimensional position throughout the model. We show
that disabling them significantly degrades both the MLM loss and the downstream
task performance. This effect is observed across several BERT-family models and
other popular pre-trained Transformer architectures, including BART, XLNet and
ELECTRA; we also show a similar effect in GPT-2.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07144</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07144</id><submitter>Jason Wei</submitter><version version="v1"><date>Sat, 15 May 2021 05:37:42 GMT</date><size>5389kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 20:44:20 GMT</date><size>5499kb</size><source_type>D</source_type></version><title>A Cognitive Regularizer for Language Modeling</title><authors>Jason Wei, Clara Meister, and Ryan Cotterell</authors><categories>cs.CL</categories><comments>ACL 2021 Camera-ready</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The uniform information density (UID) hypothesis, which posits that speakers
behaving optimally tend to distribute information uniformly across a linguistic
signal, has gained traction in psycholinguistics as an explanation for certain
syntactic, morphological, and prosodic choices. In this work, we explore
whether the UID hypothesis can be operationalized as an inductive bias for
statistical language modeling. Specifically, we augment the canonical MLE
objective for training language models with a regularizer that encodes UID. In
experiments on ten languages spanning five language families, we find that
using UID regularization consistently improves perplexity in language models,
having a larger effect when training data is limited. Moreover, via an analysis
of generated sequences, we find that UID-regularized language models have other
desirable properties, e.g., they generate text that is more lexically diverse.
Our results not only suggest that UID is a reasonable inductive bias for
language modeling, but also provide an alternative validation of the UID
hypothesis using modern-day NLP tools.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07246</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07246</id><submitter>Minkai Xu</submitter><version version="v1"><date>Sat, 15 May 2021 15:22:29 GMT</date><size>10862kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 13:01:35 GMT</date><size>10866kb</size><source_type>D</source_type></version><title>An End-to-End Framework for Molecular Conformation Generation via
  Bilevel Programming</title><authors>Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael
  Gomez-Bombarelli, Jian Tang</authors><categories>cs.LG q-bio.BM</categories><comments>Accepted by ICML 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Predicting molecular conformations (or 3D structures) from molecular graphs
is a fundamental problem in many applications. Most existing approaches are
usually divided into two steps by first predicting the distances between atoms
and then generating a 3D structure through optimizing a distance geometry
problem. However, the distances predicted with such two-stage approaches may
not be able to consistently preserve the geometry of local atomic
neighborhoods, making the generated structures unsatisfying. In this paper, we
propose an end-to-end solution for molecular conformation prediction called
ConfVAE based on the conditional variational autoencoder framework.
Specifically, the molecular graph is first encoded in a latent space, and then
the 3D structures are generated by solving a principled bilevel optimization
program. Extensive experiments on several benchmark data sets prove the
effectiveness of our proposed approach over existing state-of-the-art
approaches. Code is available at
\url{https://github.com/MinkaiXu/ConfVAE-ICML21}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07283</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07283</id><submitter>Dirk Tasche</submitter><version version="v1"><date>Sat, 15 May 2021 19:48:28 GMT</date><size>37kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 18:36:00 GMT</date><size>37kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 29 May 2021 13:02:08 GMT</date><size>43kb</size><source_type>D</source_type></version><title>Calibrating sufficiently</title><authors>Dirk Tasche</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>19 pages, 2 figures, appendices</comments><msc-class>62B05, 62P99</msc-class><acm-class>G.3; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When probabilistic classifiers are trained and calibrated, the so-called
grouping loss component of the calibration loss can easily be overlooked.
Grouping loss refers to the gap between observable information and information
actually exploited in the calibration exercise. We investigate the relation
between grouping loss and the concept of sufficiency, identifying
comonotonicity as a useful criterion for sufficiency. We revisit the probing
reduction approach of Langford &amp; Zadrozny (2005) and find that it produces an
estimator of probabilistic classifiers that reduces grouping loss. Finally, we
discuss Brier curves as tools to support training and 'sufficient' calibration
of probabilistic classifiers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07400</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07400</id><submitter>Samia Touileb</submitter><version version="v1"><date>Sun, 16 May 2021 10:22:21 GMT</date><size>7166kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 26 May 2021 08:57:31 GMT</date><size>7166kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 14:45:06 GMT</date><size>7166kb</size><source_type>D</source_type></version><title>The interplay between language similarity and script on a novel
  multi-layer Algerian dialect corpus</title><authors>Samia Touileb and Jeremy Barnes</authors><categories>cs.CL</categories><comments>Accepted at Findings of ACL: ACL2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent years have seen a rise in interest for cross-lingual transfer between
languages with similar typology, and between languages of various scripts.
However, the interplay between language similarity and difference in script on
cross-lingual transfer is a less studied problem. We explore this interplay on
cross-lingual transfer for two supervised tasks, namely part-of-speech tagging
and sentiment analysis. We introduce a newly annotated corpus of Algerian
user-generated comments comprising parallel annotations of Algerian written in
Latin, Arabic, and code-switched scripts, as well as annotations for sentiment
and topic categories. We perform baseline experiments by fine-tuning
multi-lingual language models. We further explore the effect of script vs.
language similarity in cross-lingual transfer by fine-tuning multi-lingual
models on languages which are a) typologically distinct, but use the same
script, b) typologically similar, but use a distinct script, or c) are
typologically similar and use the same script. We find there is a delicate
relationship between script and typology for part-of-speech, while sentiment
analysis is less sensitive.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07428</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07428</id><submitter>Xianjun Jiao</submitter><version version="v1"><date>Sun, 16 May 2021 13:00:00 GMT</date><size>1103kb</size></version><version version="v2"><date>Sat, 29 May 2021 21:30:42 GMT</date><size>1814kb</size><source_type>D</source_type></version><title>Openwifi CSI fuzzer for authorized sensing and covert channels</title><authors>Xianjun Jiao, Michael Mehari, Wei Liu, Muhammad Aslam, Ingrid Moerman</authors><categories>cs.CR cs.AR cs.NI</categories><comments>Accepted by ACM WiSec 2021</comments><doi>10.1145/3448300.3468255</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  CSI (Channel State Information) of WiFi systems contains the environment
channel response between the transmitter and the receiver, so the
people/objects and their movement in between can be sensed. To get CSI, the
receiver performs channel estimation based on the pre-known training field of
the transmitted WiFi signal. CSI related technology is useful in many cases,
but it also brings concerns on privacy and security. In this paper, we open
sourced a CSI fuzzer to enhance the privacy and security of WiFi CSI
applications. It is built and embedded into the transmitter of openwifi, which
is an open source full-stack WiFi chip design, to prevent unauthorized sensing
without sacrificing the WiFi link performance. The CSI fuzzer imposes an
artificial channel response to the signal before it is transmitted, so the CSI
seen by the receiver will indicate the actual channel response combined with
the artificial response. Only the authorized receiver, that knows the
artificial response, can calculate the actual channel response and perform the
CSI sensing. Another potential application of the CSI fuzzer is covert channels
based on a set of pre-defined artificial response patterns. Our work resolves
the pain point of implementing the anti-sensing idea based on the commercial
off-the-shelf WiFi devices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07464</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07464</id><submitter>Ning Ding</submitter><version version="v1"><date>Sun, 16 May 2021 15:53:17 GMT</date><size>555kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 19 May 2021 08:50:14 GMT</date><size>556kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 06:56:03 GMT</date><size>5981kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 2 Jun 2021 07:23:06 GMT</date><size>5981kb</size><source_type>D</source_type></version><title>Few-NERD: A Few-Shot Named Entity Recognition Dataset</title><authors>Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie,
  Hai-Tao Zheng, Zhiyuan Liu</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted by ACL-IJCNLP 2021 (long paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, considerable literature has grown up around the theme of few-shot
named entity recognition (NER), but little published benchmark data
specifically focused on the practical and challenging task. Current approaches
collect existing supervised NER datasets and re-organize them to the few-shot
setting for empirical study. These strategies conventionally aim to recognize
coarse-grained entity types with few examples, while in practice, most unseen
entity types are fine-grained. In this paper, we present Few-NERD, a
large-scale human-annotated few-shot NER dataset with a hierarchy of 8
coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238
sentences from Wikipedia, 4,601,160 words are included and each is annotated as
context or a part of a two-level entity type. To the best of our knowledge,
this is the first few-shot NER dataset and the largest human-crafted NER
dataset. We construct benchmark tasks with different emphases to
comprehensively assess the generalization capability of models. Extensive
empirical results and analysis show that Few-NERD is challenging and the
problem requires further research. We make Few-NERD public at
https://ningding97.github.io/fewnerd/.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07535</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07535</id><submitter>Michail Mylonakis</submitter><version version="v1"><date>Sun, 16 May 2021 22:34:01 GMT</date><size>134kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 14:28:55 GMT</date><size>160kb</size><source_type>D</source_type></version><title>Adaptive Interference Coordination over Channels with Unknown State at
  the Encoder and the Decoder</title><authors>Michail Mylonakis, Photios A. Stavrou, Mikael Skoglund</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We generalize the problem of controlling the interference created to an
external observer while communicating over a discrete memoryless channel (DMC)
which was studied in \cite{serrano:2014}. In particular, we consider the
scenario where the transmission is established over a compound DMC channel with
unknown state at both the encoder and the decoder. Depending on the exact state
$s$ of the channel, we ask for a different level of average precision
$\Delta_s$ on the establishment of the interference coordination with the
external observer. For this set-up, we fully characterize the capacity region.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07566</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07566</id><submitter>Hao Xue</submitter><version version="v1"><date>Mon, 17 May 2021 01:27:20 GMT</date><size>543kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 06:25:29 GMT</date><size>542kb</size><source_type>D</source_type></version><title>Exploring Self-Supervised Representation Ensembles for COVID-19 Cough
  Classification</title><authors>Hao Xue and Flora D. Salim</authors><categories>cs.SD cs.LG eess.AS</categories><comments>Accepted to ACM KDD 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usage of smartphone-collected respiratory sound, trained with deep
learning models, for detecting and classifying COVID-19 becomes popular
recently. It removes the need for in-person testing procedures especially for
rural regions where related medical supplies, experienced workers, and
equipment are limited. However, existing sound-based diagnostic approaches are
trained in a fully supervised manner, which requires large scale well-labelled
data. It is critical to discover new methods to leverage unlabelled respiratory
data, which can be obtained more easily. In this paper, we propose a novel
self-supervised learning enabled framework for COVID-19 cough classification. A
contrastive pre-training phase is introduced to train a Transformer-based
feature encoder with unlabelled data. Specifically, we design a random masking
mechanism to learn robust representations of respiratory sounds. The
pre-trained feature encoder is then fine-tuned in the downstream phase to
perform cough classification. In addition, different ensembles with varied
random masking rates are also explored in the downstream phase. Through
extensive evaluations, we demonstrate that the proposed contrastive
pre-training, the random masking mechanism, and the ensemble architecture
contribute to improving cough classification performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07574</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07574</id><submitter>Jianzhi Lou</submitter><version version="v1"><date>Mon, 17 May 2021 01:49:02 GMT</date><size>5638kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 18:55:17 GMT</date><size>5638kb</size><source_type>D</source_type></version><title>SoundFence: Securing Ultrasonic Sensors in Vehicles Using Physical-Layer
  Defense</title><authors>Jianzhi Lou, Qiben Yan, Qing Hui, Huacheng Zeng</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous vehicles (AVs), equipped with numerous sensors such as camera,
LiDAR, radar, and ultrasonic sensor, are revolutionizing the transportation
industry. These sensors are expected to sense reliable information from a
physical environment, facilitating the critical decision-making process of the
AVs. Ultrasonic sensors, which detect obstacles in a short distance, play an
important role in assisted parking and blind spot detection events. However,
due to their weak security level, ultrasonic sensors are particularly
vulnerable to signal injection attacks, when the attackers inject malicious
acoustic signals to create fake obstacles and intentionally mislead the
vehicles to make wrong decisions with disastrous aftermath. In this paper, we
systematically analyze the attack model of signal injection attacks toward
moving vehicles. By considering the potential threats, we propose SoundFence, a
physical-layer defense system which leverages the sensors' signal processing
capability without requiring any additional equipment. SoundFence verifies the
benign measurement results and detects signal injection attacks by analyzing
sensor readings and the physical-layer signatures of ultrasonic signals. Our
experiment with commercial sensors shows that SoundFence detects most (more
than 95%) of the abnormal sensor readings with very few false alarms, and it
can also accurately distinguish the real echo from injected signals to identify
injection attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07585</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07585</id><submitter>Yujuan Ding</submitter><version version="v1"><date>Mon, 17 May 2021 03:02:04 GMT</date><size>512kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 01:05:22 GMT</date><size>512kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 30 May 2021 02:08:48 GMT</date><size>512kb</size><source_type>D</source_type></version><title>Leveraging Two Types of Global Graph for Sequential Fashion
  Recommendation</title><authors>Yujuan Ding, Yunshan Ma, Wai Keung Wong, Tat-Seng Chua</authors><categories>cs.IR cs.MM</categories><doi>10.1145/3460426.3463638</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Sequential fashion recommendation is of great significance in online fashion
shopping, which accounts for an increasing portion of either fashion retailing
or online e-commerce. The key to building an effective sequential fashion
recommendation model lies in capturing two types of patterns: the personal
fashion preference of users and the transitional relationships between adjacent
items. The two types of patterns are usually related to user-item interaction
and item-item transition modeling respectively. However, due to the large sets
of users and items as well as the sparse historical interactions, it is
difficult to train an effective and efficient sequential fashion recommendation
model. To tackle these problems, we propose to leverage two types of global
graph, i.e., the user-item interaction graph and item-item transition graph, to
obtain enhanced user and item representations by incorporating higher-order
connections over the graphs. In addition, we adopt the graph kernel of LightGCN
for the information propagation in both graphs and propose a new design for
item-item transition graph. Extensive experiments on two established sequential
fashion recommendation datasets validate the effectiveness and efficiency of
our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07592</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07592</id><submitter>Yutong Li</submitter><version version="v1"><date>Mon, 17 May 2021 03:50:51 GMT</date><size>6751kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 04:42:41 GMT</date><size>6751kb</size><source_type>D</source_type></version><title>Dermoscopic Image Classification with Neural Style Transfer</title><authors>Yutong Li, Ruoqing Zhu, Annie Qu and Mike Yeh</authors><categories>eess.IV cs.CV cs.LG</categories><comments>32 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Skin cancer, the most commonly found human malignancy, is primarily diagnosed
visually via dermoscopic analysis, biopsy, and histopathological examination.
However, unlike other types of cancer, automated image classification of skin
lesions is deemed more challenging due to the irregularity and variability in
the lesions' appearances. In this work, we propose an adaptation of the Neural
Style Transfer (NST) as a novel image pre-processing step for skin lesion
classification problems. We represent each dermoscopic image as the style image
and transfer the style of the lesion onto a homogeneous content image. This
transfers the main variability of each lesion onto the same localized region,
which allows us to integrate the generated images together and extract latent,
low-rank style features via tensor decomposition. We train and cross-validate
our model on a dermoscopic data set collected and preprocessed from the
International Skin Imaging Collaboration (ISIC) database. We show that the
classification performance based on the extracted tensor features using the
style-transferred images significantly outperforms that of the raw images by
more than 10%, and is also competitive with well-studied, pre-trained CNN
models through transfer learning. Additionally, the tensor decomposition
further identifies latent style clusters, which may provide clinical
interpretation and insights.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07624</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07624</id><submitter>Fengbin Zhu</submitter><version version="v1"><date>Mon, 17 May 2021 06:12:06 GMT</date><size>7275kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 05:38:50 GMT</date><size>7275kb</size><source_type>D</source_type></version><title>TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and
  Textual Content in Finance</title><authors>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang,
  Jiancheng Lv, Fuli Feng and Tat-Seng Chua</authors><categories>cs.CL cs.AI</categories><comments>Accepted by ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hybrid data combining both tabular and textual content (e.g., financial
reports) are quite pervasive in the real world. However, Question Answering
(QA) over such hybrid data is largely neglected in existing research. In this
work, we extract samples from real financial reports to build a new large-scale
QA dataset containing both Tabular And Textual data, named TAT-QA, where
numerical reasoning is usually required to infer the answer, such as addition,
subtraction, multiplication, division, counting, comparison/sorting, and the
compositions. We further propose a novel QA model termed TAGOP, which is
capable of reasoning over both tables and text. It adopts sequence tagging to
extract relevant cells from the table along with relevant spans from the text
to infer their semantics, and then applies symbolic reasoning over them with a
set of aggregation operators to arrive at the final answer. TAGOPachieves 58.0%
inF1, which is an 11.1% absolute increase over the previous best baseline
model, according to our experiments on TAT-QA. But this result still lags far
behind performance of expert human, i.e.90.8% in F1. It is demonstrated that
our TAT-QA is very challenging and can serve as a benchmark for training and
testing powerful QA models that address hybrid form data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07660</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07660</id><submitter>Etienne David</submitter><version version="v1"><date>Mon, 17 May 2021 08:18:30 GMT</date><size>963kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 15:32:53 GMT</date><size>948kb</size></version><title>Global Wheat Head Dataset 2021: more diversity to improve the
  benchmarking of wheat head localization methods</title><authors>Etienne David, Mario Serouart, Daniel Smith, Simon Madec, Kaaviya
  Velumani, Shouyang Liu, Xu Wang, Francisco Pinto Espinosa, Shahameh Shafiee,
  Izzat S. A. Tahir, Hisashi Tsujimoto, Shuhei Nasuda, Bangyou Zheng, Norbert
  Kichgessner, Helge Aasen, Andreas Hund, Pouria Sadhegi-Tehran, Koichi
  Nagasawa, Goro Ishikawa, S\'ebastien Dandrifosse, Alexis Carlier, Benoit
  Mercatoris, Ken Kuroki, Haozhou Wang, Masanori Ishii, Minhajul A. Badhon,
  Curtis Pozniak, David Shaner LeBauer, Morten Lilimo, Jesse Poland, Scott
  Chapman, Benoit de Solan, Fr\'ed\'eric Baret, Ian Stavness, Wei Guo</authors><categories>cs.CV</categories><comments>8 pages, 2 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Global Wheat Head Detection (GWHD) dataset was created in 2020 and has
assembled 193,634 labelled wheat heads from 4,700 RGB images acquired from
various acquisition platforms and 7 countries/institutions. With an associated
competition hosted in Kaggle, GWHD has successfully attracted attention from
both the computer vision and agricultural science communities. From this first
experience in 2020, a few avenues for improvements have been identified,
especially from the perspective of data size, head diversity and label
reliability. To address these issues, the 2020 dataset has been reexamined,
relabeled, and augmented by adding 1,722 images from 5 additional countries,
allowing for 81,553 additional wheat heads to be added. We now release a new
version of the Global Wheat Head Detection (GWHD) dataset in 2021, which is
bigger, more diverse, and less noisy than the 2020 version. The GWHD 2021 is
now publicly available at http://www.global-wheat.com/ and a new data challenge
has been organized on AIcrowd to make use of this updated dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07739</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07739</id><submitter>Akash Kumar</submitter><version version="v1"><date>Mon, 17 May 2021 11:20:14 GMT</date><size>2187kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 12:34:23 GMT</date><size>2260kb</size><source_type>D</source_type></version><title>Toward the Design of Fault-Tolerance- and Peak- Power-Aware Multi-Core
  Mixed-Criticality Systems</title><authors>Behnaz Ranjbar, Ali Hosseinghorban, Mohammad Salehi, Alireza Ejlali
  and Akash Kumar</authors><categories>cs.DC</categories><comments>This is an extended version of the paper accepted to be published in
  IEEE TCAD 2021 which includes Appendices A-E</comments><doi>10.1109/TCAD.2021.3082495</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mixed-Criticality (MC) systems have recently been devised to address the
requirements of real-time systems in industrial applications, where the system
runs tasks with different criticality levels on a single platform. In some
workloads, a high-critically task might overrun and overload the system, or a
fault can occur during the execution. However, these systems must be
fault-tolerant and guarantee the correct execution of all high-criticality
tasks by their deadlines to avoid catastrophic consequences, in any situation.
Furthermore, in these MC systems, the peak power consumption of the system may
increase, especially in an overload situation and exceed the processor Thermal
Design Power (TDP) constraint. This may cause generating heat beyond the
cooling capacity, resulting the system stop to avoid excessive heat and halting
the processor. In this paper, we propose a technique for dependent
dual-criticality tasks in fault-tolerant multi-core MC systems to manage peak
power consumption and temperature. The technique develops a tree of possible
task mapping and scheduling at design-time to cover all possible scenarios and
reduce the low-criticality task drop rate in the high-criticality mode. At
run-time, the system exploits the tree to select a proper schedule according to
fault occurrences and criticality mode changes. Experimental results show that
the average task schedulability is 74.14% on average for the proposed method,
while the peak power consumption and maximum temperature are improved by 16.65%
and 14.9 C on average, respectively, compared to a recent work. In addition,
for a real-life application, our method reduces the peak power and maximum
temperature by up to 20.06% and 5 C, respectively, compared to a
state-of-the-art approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07856</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07856</id><submitter>Edward Simmons</submitter><version version="v1"><date>Fri, 7 May 2021 10:08:47 GMT</date><size>136kb</size></version><title>Correlations Between Learning Environments and Dropout Intention</title><authors>Edward Simmons</authors><categories>cs.CY math.ST stat.TH</categories><doi>10.13140/RG.2.2.28550.50245</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This research is comparing learning environments to students dropout
intentions. While using statistics I looked at data and the correlations
between two articles to see how the two studies looked side to side. Learning
environments and dropout intentions can both have vary effects on students.
They can both determine if a student does well, or bad in school especially
math.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07889</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07889</id><submitter>Jiayi Chen</submitter><version version="v1"><date>Mon, 17 May 2021 14:22:58 GMT</date><size>4194kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 18:45:33 GMT</date><size>4279kb</size><source_type>D</source_type></version><title>Simultaneous Meta-Learning with Diverse Task Spaces</title><authors>Jiayi Chen, Aidong Zhang</authors><categories>cs.AI</categories><comments>Submitted to CIKM 2021. The original title and abstract has been
  changed due to the double-blind policy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of how to learn a model-agnostic
meta-learner that simultaneously learning from different feature spaces. The
reason that most of model-agnostic meta-learner methods cannot handle multiple
task spaces is due to less common knowledge for the task instances. The
reduction of shared knowledge is because different tasks with different
example-level manifolds cannot entirely share the same model architecture.
Actually, various tasks only share partial meta-parameters. For example, for
two multi-feature tasks whose example-level manifolds contain a same subspace
but their remaining subspaces are not the same, one can imagine that the common
knowledge can be the feature extractor for that common subspace, but other
subspaces' feature extractors cannot be used between the two tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08021</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08021</id><submitter>Qingyun Wang</submitter><version version="v1"><date>Mon, 17 May 2021 17:15:29 GMT</date><size>133kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 22:47:33 GMT</date><size>131kb</size><source_type>D</source_type></version><title>Stage-wise Fine-tuning for Graph-to-Text Generation</title><authors>Qingyun Wang, Semih Yavuz, Victoria Lin, Heng Ji, Nazneen Rajani</authors><categories>cs.CL cs.AI</categories><comments>10 pages, Accepted by Proceedings of ACL-IJCNLP 2021 Student Research
  Workshop, Code and Resources at
  https://github.com/EagleW/Stage-wise-Fine-tuning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-to-text generation has benefited from pre-trained language models
(PLMs) in achieving better performance than structured graph encoders. However,
they fail to fully utilize the structure information of the input graph. In
this paper, we aim to further improve the performance of the pre-trained
language model by proposing a structured graph-to-text model with a two-step
fine-tuning mechanism which first fine-tunes the model on Wikipedia before
adapting to the graph-to-text generation. In addition to using the traditional
token and position embeddings to encode the knowledge graph (KG), we propose a
novel tree-level embedding method to capture the inter-dependency structures of
the input graph. This new approach has significantly improved the performance
of all text generation metrics for the English WebNLG 2017 dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08039</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08039</id><submitter>Nadir Durrani Dr</submitter><version version="v1"><date>Mon, 17 May 2021 17:43:36 GMT</date><size>30kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 09:14:48 GMT</date><size>30kb</size><source_type>D</source_type></version><title>Fine-grained Interpretation and Causation Analysis in Deep NLP Models</title><authors>Hassan Sajjad, Narine Kokhlikyan, Fahim Dalvi, Nadir Durrani</authors><categories>cs.CL</categories><comments>Accepted at NAACL Tutorial</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a write-up for the tutorial on &quot;Fine-grained Interpretation and
Causation Analysis in Deep NLP Models&quot; that we are presenting at NAACL 2021. We
present and discuss the research work on interpreting fine-grained components
of a model from two perspectives, i) fine-grained interpretation, ii) causation
analysis. The former introduces methods to analyze individual neurons and a
group of neurons with respect to a language property or a task. The latter
studies the role of neurons and input features in explaining decisions made by
the model. We also discuss application of neuron analysis such as network
manipulation and domain adaptation. Moreover, we present two toolkits namely
NeuroX and Captum, that support functionalities discussed in this tutorial.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08050</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08050</id><submitter>Hanxiao Liu</submitter><version version="v1"><date>Mon, 17 May 2021 17:55:04 GMT</date><size>642kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 20:24:06 GMT</date><size>582kb</size><source_type>D</source_type></version><title>Pay Attention to MLPs</title><authors>Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le</authors><categories>cs.LG cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformers have become one of the most important architectural innovations
in deep learning and have enabled many breakthroughs over the past few years.
Here we propose a simple network architecture, gMLP, based on MLPs with gating,
and show that it can perform as well as Transformers in key language and vision
applications. Our comparisons show that self-attention is not critical for
Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model
achieves parity with Transformers on pretraining perplexity and is better on
some downstream NLP tasks. On finetuning tasks where gMLP performs worse,
making the gMLP model substantially larger can close the gap with Transformers.
In general, our experiments show that gMLP can scale as well as Transformers
over increased data and compute.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08095</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08095</id><submitter>Amin Nikanjam</submitter><version version="v1"><date>Mon, 17 May 2021 18:06:11 GMT</date><size>941kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 02:17:12 GMT</date><size>1925kb</size><source_type>D</source_type></version><title>Automatic Fault Detection for Deep Learning Programs Using Graph
  Transformations</title><authors>Amin Nikanjam, Houssem Ben Braiek, Mohammad Mehdi Morovati, Foutse
  Khomh</authors><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, we are witnessing an increasing demand in both corporates and
academia for exploiting Deep Learning (DL) to solve complex real-world
problems. A DL program encodes the network structure of a desirable DL model
and the process by which the model learns from the training dataset. Like any
software, a DL program can be faulty, which implies substantial challenges of
software quality assurance, especially in safety-critical domains. It is
therefore crucial to equip DL development teams with efficient fault detection
techniques and tools. In this paper, we propose NeuraLint, a model-based fault
detection approach for DL programs, using meta-modelling and graph
transformations. First, we design a meta-model for DL programs that includes
their base skeleton and fundamental properties. Then, we construct a
graph-based verification process that covers 23 rules defined on top of the
meta-model and implemented as graph transformations to detect faults and design
inefficiencies in the generated models (i.e., instances of the meta-model).
First, the proposed approach is evaluated by finding faults and design
inefficiencies in 28 synthesized examples built from common problems reported
in the literature. Then NeuraLint successfully finds 64 faults and design
inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts
and GitHub repositories. The results show that NeuraLint effectively detects
faults and design issues in both synthesized and real-world examples with a
recall of 70.5 % and a precision of 100 %. Although the proposed meta-model is
designed for feedforward neural networks, it can be extended to support other
neural network architectures such as recurrent neural networks. Researchers can
also expand our set of verification rules to cover more types of issues in DL
programs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08304</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08304</id><submitter>Jesus Cerquides</submitter><version version="v1"><date>Tue, 18 May 2021 06:45:05 GMT</date><size>516kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 20:11:27 GMT</date><size>516kb</size><source_type>D</source_type></version><title>Parametrization invariant interpretation of priors and posteriors</title><authors>Jesus Cerquides</authors><categories>math.ST cs.AI cs.LG stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we leverage on probability over Riemannian manifolds to rethink
the interpretation of priors and posteriors in Bayesian inference. The main
mindshift is to move away from the idea that &quot;a prior distribution establishes
a probability distribution over the parameters of our model&quot; to the idea that
&quot;a prior distribution establishes a probability distribution over probability
distributions&quot;. To do that we assume that our probabilistic model is a
Riemannian manifold with the Fisher metric. Under this mindset, any
distribution over probability distributions should be &quot;intrinsic&quot;, that is,
invariant to the specific parametrization which is selected for the manifold.
We exemplify our ideas through a simple analysis of distributions over the
manifold of Bernoulli distributions. One of the major shortcomings of maximum a
posteriori estimates is that they depend on the parametrization. Based on the
understanding developed here, we can define the maximum a posteriori estimate
which is independent of the parametrization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08339</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08339</id><submitter>Ran Ben Basat</submitter><version version="v1"><date>Tue, 18 May 2021 08:03:39 GMT</date><size>1527kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 16:10:02 GMT</date><size>2730kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 17:12:21 GMT</date><size>2892kb</size><source_type>D</source_type></version><title>DRIVE: One-bit Distributed Mean Estimation</title><authors>Shay Vargaftik, Ran Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv
  Ben-Itzhak, Michael Mitzenmacher</authors><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem where $n$ clients transmit $d$-dimensional
real-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the
receiver to approximately reconstruct their mean. Such compression problems
naturally arise in distributed and federated learning. We provide novel
mathematical results and derive computationally efficient algorithms that are
more accurate than previous compression techniques. We evaluate our methods on
a collection of distributed and federated learning tasks, using a variety of
datasets, and show a consistent improvement over the state of the art.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08620</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08620</id><submitter>Yao Li</submitter><version version="v1"><date>Tue, 18 May 2021 15:51:24 GMT</date><size>832kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:04:48 GMT</date><size>1421kb</size><source_type>D</source_type></version><title>Detecting Adversarial Examples with Bayesian Neural Network</title><authors>Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee</authors><categories>stat.ML cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a new framework to detect adversarial examples
motivated by the observations that random components can improve the smoothness
of predictors and make it easier to simulate output distribution of deep neural
network. With these observations, we propose a novel Bayesian adversarial
example detector, short for BATer, to improve the performance of adversarial
example detection. In specific, we study the distributional difference of
hidden layer output between natural and adversarial examples, and propose to
use the randomness of Bayesian neural network (BNN) to simulate hidden layer
output distribution and leverage the distribution dispersion to detect
adversarial examples. The advantage of BNN is that the output is stochastic
while neural networks without random components do not have such
characteristics. Empirical results on several benchmark datasets against
popular attacks show that the proposed BATer outperforms the state-of-the-art
detectors in adversarial example detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08675</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08675</id><submitter>Christoph Hertrich</submitter><version version="v1"><date>Tue, 18 May 2021 17:05:26 GMT</date><size>21kb</size></version><version version="v2"><date>Mon, 31 May 2021 08:32:12 GMT</date><size>22kb</size></version><title>The Computational Complexity of ReLU Network Training Parameterized by
  Data Dimensionality</title><authors>Vincent Froese, Christoph Hertrich, Rolf Niedermeier</authors><categories>cs.LG cs.CC cs.DS cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the computational complexity of training simple neural networks
with rectified linear units (ReLUs) has recently been a subject of intensive
research. Closing gaps and complementing results from the literature, we
present several results on the parameterized complexity of training two-layer
ReLU networks with respect to various loss functions. After a brief discussion
of other parameters, we focus on analyzing the influence of the dimension $d$
of the training data on the computational complexity. We provide running time
lower bounds in terms of W[1]-hardness for parameter $d$ and prove that known
brute-force strategies are essentially optimal (assuming the Exponential Time
Hypothesis). In comparison with previous work, our results hold for a broad(er)
range of loss functions, including $\ell^p$-loss for all $p\in[0,\infty]$. In
particular, we extend a known polynomial-time algorithm for constant $d$ and
convex loss functions to a more general class of loss functions, matching our
running time lower bounds also in these cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08765</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08765</id><submitter>Matthew McCoy</submitter><version version="v1"><date>Tue, 18 May 2021 18:31:19 GMT</date><size>5662kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 21:23:53 GMT</date><size>5657kb</size><source_type>D</source_type></version><title>Moving Mesh with Streamline Upwind Petrov-Galerkin (MM-SUPG) Method for
  Convection-Diffusion Problems</title><authors>Xianping Li and Matthew McCoy</authors><categories>math.NA cs.NA</categories><comments>There are 17 pages, 12 figures, and 3 tables. Added additional
  references and fixed typos</comments><msc-class>65M50, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the effect of the streamline upwind Petrov-Galerkin method
(SUPG) as it relates to the moving mesh partial differential equation (MMPDE)
method for convection-diffusion problems in the presence of vanishing
diffusivity. We first discretize in space using linear finite elements and then
use a $\theta$-scheme to discretize in time. On a fixed mesh, SUPG (FM-SUPG) is
shown to enhance the stability and resolves spurious oscillations when compared
to the classic Galerkin method (FM-FEM) when diffusivity is small. However, it
falls short when the layer-gradient is large. In this paper, we develop a
moving mesh upwind Petrov-Galerkin (MM-SUPG) method by integrating the SUPG
method with the MMPDE method. Numerical results show that our MM-SUPG works
well for these types of problems and performs better than FM-SUPG as well as
MMPDE without SUPG.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08869</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08869</id><submitter>Tianchen Zhou</submitter><version version="v1"><date>Wed, 19 May 2021 01:06:32 GMT</date><size>4527kb</size></version><version version="v2"><date>Thu, 20 May 2021 05:44:36 GMT</date><size>4526kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 03:15:05 GMT</date><size>4527kb</size><source_type>D</source_type></version><title>Incentivized Bandit Learning with Self-Reinforcing User Preferences</title><authors>Tianchen Zhou, Jia Liu, Chaosheng Dong, Jingyuan Deng</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a new multi-armed bandit (MAB) online learning
model that considers real-world phenomena in many recommender systems: (i) the
learning agent cannot pull the arms by itself and thus has to offer rewards to
users to incentivize arm-pulling indirectly; and (ii) if users with specific
arm preferences are well rewarded, they induce a &quot;self-reinforcing&quot; effect in
the sense that they will attract more users of similar arm preferences. Besides
addressing the tradeoff of exploration and exploitation, another key feature of
this new MAB model is to balance reward and incentivizing payment. The goal of
the agent is to maximize the total reward over a fixed time horizon $T$ with a
low total payment. Our contributions in this paper are two-fold: (i) We propose
a new MAB model with random arm selection that considers the relationship of
users' self-reinforcing preferences and incentives; and (ii) We leverage the
properties of a multi-color Polya urn with nonlinear feedback model to propose
two MAB policies termed &quot;At-Least-$n$ Explore-Then-Commit&quot; and &quot;UCB-List&quot;. We
prove that both policies achieve $O(log T)$ expected regret with $O(log T)$
expected payment over a time horizon $T$. We conduct numerical simulations to
demonstrate and verify the performances of these two policies and study their
robustness under various settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09004</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09004</id><submitter>Mario Di Mauro</submitter><version version="v1"><date>Wed, 19 May 2021 09:13:52 GMT</date><size>18202kb</size><source_type>D</source_type></version><title>Performability of Network Service Chains: Stochastic Modeling and
  Assessment of Softwarized IP Multimedia Subsystem</title><authors>Mario Di Mauro, Giovanni Galatro, Fabio Postiglione, Marco Tambasco</authors><categories>cs.NI cs.PF</categories><doi>10.1109/TDSC.2021.3082626</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service provisioning mechanisms implemented across 5G infrastructures take
broadly into use the network service chain concept. Typically, it is coupled
with Network Function Virtualization (NFV) paradigm, and consists in defining a
pre-determined path traversed by a set of softwarized network nodes to provide
specific services. A well known chain-like framework is the IP Multimedia
Subsystem (IMS), a key infrastructure of 5G networks, that we characterize both
by a performance and an availability perspective. Precisely, supported by a
designed from scratch testbed realized through Clearwater platform, we perform
a stochastic assessment of a softwarized IMS (softIMS) architecture where two
main stages stand out: i) a performance analysis, where, exploiting the
queueing network decomposition method, we formalize an optimization problem of
resource allocation by modeling each softIMS node as an M/G/c system; ii) an
availability assessment, where, adopting the Stochastic Reward Net methodology,
we are able to characterize the behavior of softIMS in terms of failure/repair
events, and to derive a set of optimal configurations satisfying a given
availability requirement (e.g. five nines) while minimizing deployment costs.
Two routines dubbed OptCNT and OptSearchChain have been devised to govern the
performance and availability analyses, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09085</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09085</id><submitter>Jinhong Zhang</submitter><version version="v1"><date>Wed, 19 May 2021 12:17:07 GMT</date><size>356kb</size></version><version version="v2"><date>Sun, 30 May 2021 03:38:13 GMT</date><size>378kb</size></version><title>Combining GCN and Transformer for Chinese Grammatical Error Detection</title><authors>Jinhong Zhang</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes our system at NLPTEA-2020 Task: Chinese Grammatical
Error Diagnosis (CGED). The goal of CGED is to diagnose four types of
grammatical errors: word selection (S), redundant words (R), missing words (M),
and disordered words (W). The automatic CGED system contains two parts
including error detection and error correction and our system is designed to
solve the error detection problem. Our system is built on three models: 1) a
BERT-based model leveraging syntactic information; 2) a BERT-based model
leveraging contextual embeddings; 3) a lexicon-based graph neural network
leveraging lexical information. We also design an ensemble mechanism to improve
the single model's performance. Finally, our system achieves the highest F1
scores at detection level and identification level among all teams
participating in the CGED 2020 task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09365</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09365</id><submitter>Onur Boyar</submitter><version version="v1"><date>Wed, 19 May 2021 19:15:31 GMT</date><size>1907kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 13:04:05 GMT</date><size>1907kb</size><source_type>D</source_type></version><title>Exploring The Limits Of Data Augmentation For Retinal Vessel
  Segmentation</title><authors>Enes Sadi Uysal, M.\c{S}afak Bilici, B. Selin Zaza, M. Yi\u{g}it
  \&quot;Ozgen\c{c}, Onur Boyar</authors><categories>eess.IV cs.CV cs.LG</categories><comments>7 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Retinal Vessel Segmentation is important for the diagnosis of various
diseases. The research on retinal vessel segmentation focuses mainly on the
improvement of the segmentation model which is usually based on U-Net
architecture. In our study, we use the U-Net architecture and we rely on heavy
data augmentation in order to achieve better performance. The success of the
data augmentation relies on successfully addressing the problem of input
images. By analyzing input images and performing the augmentation accordingly
we show that the performance of the U-Net model can be increased dramatically.
Results are reported using the most widely used retina dataset, DRIVE.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09458</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09458</id><submitter>Ningyu Zhang</submitter><version version="v1"><date>Thu, 20 May 2021 02:29:03 GMT</date><size>5277kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 23 May 2021 10:16:22 GMT</date><size>5277kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 04:40:06 GMT</date><size>5277kb</size><source_type>D</source_type></version><title>MLBiNet: A Cross-Sentence Collective Event Detection Network</title><authors>Dongfang Lou, Zhilin Liao, Shumin Deng, Ningyu Zhang, Huajun Chen</authors><categories>cs.CL cs.AI</categories><comments>Accepted by ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of collectively detecting multiple events,
particularly in cross-sentence settings. The key to dealing with the problem is
to encode semantic information and model event inter-dependency at a
document-level. In this paper, we reformulate it as a Seq2Seq task and propose
a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level
association of events and semantic information simultaneously. Specifically, a
bidirectional decoder is firstly devised to model event inter-dependency within
a sentence when decoding the event tag vector sequence. Secondly, an
information aggregation module is employed to aggregate sentence-level semantic
and event tag information. Finally, we stack multiple bidirectional decoders
and feed cross-sentence information, forming a multi-layer bidirectional
tagging architecture to iteratively propagate information across sentences. We
show that our approach provides significant improvement in performance compared
to the current state-of-the-art results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09474</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09474</id><submitter>Stanley Lazic</submitter><version version="v1"><date>Tue, 18 May 2021 18:54:54 GMT</date><size>489kb</size><source_type>D</source_type></version><title>Quantifying sources of uncertainty in drug discovery predictions with
  probabilistic models</title><authors>Stanley E. Lazic, Dominic P. Williams</authors><categories>cs.LG stat.AP</categories><comments>34 pages, 9 figures</comments><journal-ref>Artificial Intelligence in the Life Sciences (2021)</journal-ref><doi>10.1016/j.ailsci.2021.100004</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Knowing the uncertainty in a prediction is critical when making expensive
investment decisions and when patient safety is paramount, but machine learning
(ML) models in drug discovery typically provide only a single best estimate and
ignore all sources of uncertainty. Predictions from these models may therefore
be over-confident, which can put patients at risk and waste resources when
compounds that are destined to fail are further developed. Probabilistic
predictive models (PPMs) can incorporate uncertainty in both the data and
model, and return a distribution of predicted values that represents the
uncertainty in the prediction. PPMs not only let users know when predictions
are uncertain, but the intuitive output from these models makes communicating
risk easier and decision making better. Many popular machine learning methods
have a PPM or Bayesian analogue, making PPMs easy to fit into current
workflows. We use toxicity prediction as a running example, but the same
principles apply for all prediction models used in drug discovery. The
consequences of ignoring uncertainty and how PPMs account for uncertainty are
also described. We aim to make the discussion accessible to a broad
non-mathematical audience. Equations are provided to make ideas concrete for
mathematical readers (but can be skipped without loss of understanding) and
code is available for computational researchers
(https://github.com/stanlazic/ML_uncertainty_quantification).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09497</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09497</id><submitter>Daniel Z. Huang</submitter><version version="v1"><date>Thu, 20 May 2021 03:43:20 GMT</date><size>2295kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 21 May 2021 06:11:56 GMT</date><size>2295kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 05:48:05 GMT</date><size>2527kb</size><source_type>D</source_type></version><title>Bayesian Calibration for Large-Scale Fluid Structure Interaction
  Problems Under Embedded/Immersed Boundary Framework</title><authors>Shunxiang Cao, Daniel Zhengyu Huang</authors><categories>math.NA cs.NA</categories><comments>23pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian calibration is widely used for inverse analysis and uncertainty
analysis for complex systems in the presence of both computer models and
observation data. In the present work, we focus on large-scale fluid-structure
interaction systems characterized by large structural deformations. Numerical
methods to solve these problems, including embedded/immersed boundary methods,
are typically not differentiable and lack smoothness. We propose a framework
that is built on unscented Kalman filter/inversion to efficiently calibrate and
provide uncertainty estimations of such complicated models with noisy
observation data. The approach is derivative-free and non-intrusive, and is of
particular value for the forward model that is computationally expensive and
provided as a black box which is impractical to differentiate. The framework is
demonstrated and validated by successfully calibrating the model parameters of
a piston problem and identifying the damage field of an airfoil under transonic
buffeting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09509</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09509</id><submitter>Shirong Shen</submitter><version version="v1"><date>Thu, 20 May 2021 04:26:26 GMT</date><size>9021kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 14:17:57 GMT</date><size>9021kb</size><source_type>D</source_type></version><title>Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event
  Detection</title><authors>Shirong Shen and Tongtong Wu and Guilin Qi and Yuan-Fang Li and
  Gholamreza Haffari and Sheng Bi</authors><categories>cs.CL cs.AI</categories><comments>Accepted by ACL2021 Findings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event detection (ED) aims at detecting event trigger words in sentences and
classifying them into specific event types. In real-world applications, ED
typically does not have sufficient labelled data, thus can be formulated as a
few-shot learning problem. To tackle the issue of low sample diversity in
few-shot ED, we propose a novel knowledge-based few-shot event detection method
which uses a definition-based encoder to introduce external event knowledge as
the knowledge prior of event types. Furthermore, as external knowledge
typically provides limited and imperfect coverage of event types, we introduce
an adaptive knowledge-enhanced Bayesian meta-learning method to dynamically
adjust the knowledge prior of event types. Experiments show our method
consistently and substantially outperforms a number of baselines by at least 15
absolute F1 points under the same few-shot settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09511</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09511</id><submitter>Shaohua Li</submitter><version version="v1"><date>Thu, 20 May 2021 04:45:47 GMT</date><size>3296kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 23 May 2021 12:11:20 GMT</date><size>3273kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 02:42:19 GMT</date><size>3273kb</size><source_type>D</source_type></version><title>Medical Image Segmentation Using Squeeze-and-Expansion Transformers</title><authors>Shaohua Li, Xiuchao Sui, Xiangde Luo, Xinxing Xu, Yong Liu, Rick Goh</authors><categories>eess.IV cs.CV</categories><comments>Camera ready for IJCAI'2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical image segmentation is important for computer-aided diagnosis. Good
segmentation demands the model to see the big picture and fine details
simultaneously, i.e., to learn image features that incorporate large context
while keep high spatial resolutions. To approach this goal, the most widely
used methods -- U-Net and variants, extract and fuse multi-scale features.
However, the fused features still have small &quot;effective receptive fields&quot; with
a focus on local image cues, limiting their performance. In this work, we
propose Segtran, an alternative segmentation framework based on transformers,
which have unlimited &quot;effective receptive fields&quot; even at high feature
resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:
a squeezed attention block regularizes the self attention of transformers, and
an expansion block learns diversified representations. Additionally, we propose
a new positional encoding scheme for transformers, imposing a continuity
inductive bias for images. Experiments were performed on 2D and 3D medical
image segmentation tasks: optic disc/cup segmentation in fundus images
(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain
tumor segmentation in MRI scans (BraTS'19 challenge). Compared with
representative existing methods, Segtran consistently achieved the highest
segmentation accuracy, and exhibited good cross-domain generalization
capabilities. The source code of Segtran is released at
https://github.com/askerlee/segtran.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09553</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09553</id><submitter>Monireh Ghasri</submitter><version version="v1"><date>Thu, 20 May 2021 07:16:45 GMT</date><size>2650kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 10:34:36 GMT</date><size>2650kb</size><source_type>D</source_type></version><title>A New Dynamic Optimal Relay Selection and RF interfaces Setting
  Algorithm (DORSA) in M2M for IoT Applications</title><authors>Monireh Allah Gholi Ghasri, Ali Mohammad Afshin Hemmatyar</authors><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine-to-Machine (M2M) communication is one of the main communications in
the Internet of Things (IoT). How to send data in these high-density
communications using relay selection can help better performance of this type
of communications in various applications. In addition, the possibility of
simultaneous use of different Radio Frequency (RF) interfaces helps to make
better use of the network radio frequencies.
  Therefore, in this work, we try to further use of machine communication
equipment and improve the average data rate of networks in different
applications such as the Internet of Things, which have different bandwidth
requirements, by providing an optimization algorithm for relay selection as
well as the simultaneous and dynamic multiple M2M RF interfaces setting that
called Dynamic Optimal Relay Selection and RF interfaces Setting Algorithm
(DORSA).
  The simulation results show that the average DORSA\_W-B-Z data rate is
improved by 0.8 to 10% compared to the studied algorithms such as direct
transmission as well as relay selection algorithms with static RF interface
setting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09679</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09679</id><submitter>Koujin Takeda</submitter><version version="v1"><date>Thu, 20 May 2021 11:37:38 GMT</date><size>6639kb</size></version><title>Improved Neuronal Ensemble Inference with Generative Model and MCMC</title><authors>Shun Kimura, Keisuke Ota, Koujin Takeda</authors><categories>cond-mat.dis-nn cs.LG q-bio.NC stat.ML</categories><comments>23 pages, 8 figures, partially overlapped with arXiv:1911.06509</comments><journal-ref>J. Stat. Mech. (2021) 063501</journal-ref><doi>10.1088/1742-5468/abffd5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuronal ensemble inference is a significant problem in the study of
biological neural networks. Various methods have been proposed for ensemble
inference from experimental data of neuronal activity. Among them, Bayesian
inference approach with generative model was proposed recently. However, this
method requires large computational cost for appropriate inference. In this
work, we give an improved Bayesian inference algorithm by modifying update rule
in Markov chain Monte Carlo method and introducing the idea of simulated
annealing for hyperparameter control. We compare the performance of ensemble
inference between our algorithm and the original one, and discuss the advantage
of our method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09917</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09917</id><submitter>Aleksandr Beknazaryan</submitter><version version="v1"><date>Thu, 20 May 2021 17:29:08 GMT</date><size>7kb</size></version><version version="v2"><date>Sat, 29 May 2021 06:40:28 GMT</date><size>7kb</size></version><title>Neural networks with superexpressive activations and integer weights</title><authors>Aleksandr Beknazaryan</authors><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An example of an activation function $\sigma$ is given such that networks
with activations $\{\sigma, \lfloor\cdot\rfloor\}$, integer weights and a fixed
architecture depending on $d$ approximate continuous functions on $[0,1]^d$.
The range of integer weights required for $\varepsilon$-approximation of
H\&quot;older continuous functions is derived, which leads to a convergence rate of
order $n^{\frac{-2\beta}{2\beta+d}}\log_2n$ for neural network regression
estimation of unknown $\beta$-H\&quot;older continuous function with given $n$
samples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09984</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09984</id><submitter>Shivani Kumar</submitter><version version="v1"><date>Thu, 20 May 2021 18:33:55 GMT</date><size>1806kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 08:06:23 GMT</date><size>913kb</size><source_type>D</source_type></version><title>Multi-modal Sarcasm Detection and Humor Classification in Code-mixed
  Conversations</title><authors>Manjot Bedi, Shivani Kumar, Md Shad Akhtar, and Tanmoy Chakraborty</authors><categories>cs.CL</categories><comments>13 pages, 4 figures, 9 tables</comments><doi>10.1109/TAFFC.2021.3083522</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sarcasm detection and humor classification are inherently subtle problems,
primarily due to their dependence on the contextual and non-verbal information.
Furthermore, existing studies in these two topics are usually constrained in
non-English languages such as Hindi, due to the unavailability of qualitative
annotated datasets. In this work, we make two major contributions considering
the above limitations: (1) we develop a Hindi-English code-mixed dataset,
MaSaC, for the multi-modal sarcasm detection and humor classification in
conversational dialog, which to our knowledge is the first dataset of its kind;
(2) we propose MSH-COMICS, a novel attention-rich neural architecture for the
utterance classification. We learn efficient utterance representation utilizing
a hierarchical attention mechanism that attends to a small portion of the input
sentence at a time. Further, we incorporate dialog-level contextual attention
mechanism to leverage the dialog history for the multi-modal classification. We
perform extensive experiments for both the tasks by varying multi-modal inputs
and various submodules of MSH-COMICS. We also conduct comparative analysis
against existing approaches. We observe that MSH-COMICS attains superior
performance over the existing models by &gt; 1 F1-score point for the sarcasm
detection and 10 F1-score points in humor classification. We diagnose our model
and perform thorough analysis of the results to understand the superiority and
pitfalls.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.09992</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.09992</id><submitter>Mathieu Seurin</submitter><version version="v1"><date>Thu, 20 May 2021 18:55:11 GMT</date><size>3901kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 09:03:06 GMT</date><size>3901kb</size><source_type>D</source_type></version><title>Don't Do What Doesn't Matter: Intrinsic Motivation with Action
  Usefulness</title><authors>Mathieu Seurin, Florian Strub, Philippe Preux, Olivier Pietquin</authors><categories>cs.LG</categories><comments>Accepted at Internationnal Joint Conference on Artificial
  Intelligence (IJCAI'21) and Self-Supervision for Reinforcement Learning
  Workshop (SSL-RL @ICLR'21)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse rewards are double-edged training signals in reinforcement learning:
easy to design but hard to optimize. Intrinsic motivation guidances have thus
been developed toward alleviating the resulting exploration problem. They
usually incentivize agents to look for new states through novelty signals. Yet,
such methods encourage exhaustive exploration of the state space rather than
focusing on the environment's salient interaction opportunities. We propose a
new exploration method, called Don't Do What Doesn't Matter (DoWhaM), shifting
the emphasis from state novelty to state with relevant actions. While most
actions consistently change the state when used, \textit{e.g.} moving the
agent, some actions are only effective in specific states, \textit{e.g.},
\emph{opening} a door, \emph{grabbing} an object. DoWhaM detects and rewards
actions that seldom affect the environment. We evaluate DoWhaM on the
procedurally-generated environment MiniGrid, against state-of-the-art methods
and show that DoWhaM greatly reduces sample complexity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10005</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10005</id><submitter>C.-H. Huck Yang</submitter><version version="v1"><date>Thu, 20 May 2021 19:38:03 GMT</date><size>194kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 27 May 2021 14:29:32 GMT</date><size>201kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 01:36:22 GMT</date><size>201kb</size><source_type>D</source_type></version><title>Robust Unsupervised Multi-Object Tracking in Noisy Environments</title><authors>C.-H. Huck Yang, Mohit Chhabra, Y.-C. Liu, Quan Kong, Tomoaki
  Yoshinaga, Tomokazu Murakami</authors><categories>cs.CV cs.AI cs.LG cs.MM cs.NE</categories><comments>Accepted to IEEE ICIP 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Camera movement and unpredictable environmental conditions like dust and wind
induce noise into video feeds. We observe that popular unsupervised MOT methods
are dependent on noise-free conditions. We show that the addition of a small
amount of artificial random noise causes a sharp degradation in model
performance on benchmark metrics. We resolve this problem by introducing a
robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed
single-head attention model helps limit the negative impact of noise by
learning visual representations at different segment scales. AttU-Net shows
better unsupervised MOT tracking performance over variational inference-based
state-of-the-art baselines. We evaluate our method in the MNIST and the Atari
game video benchmark. We also provide two extended video datasets consisting of
complex visual patterns that include Kuzushiji characters and fashion images to
validate the effectiveness of the proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10077</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10077</id><submitter>Leman Akoglu</submitter><version version="v1"><date>Fri, 21 May 2021 00:56:25 GMT</date><size>176kb</size></version><version version="v2"><date>Mon, 31 May 2021 13:29:47 GMT</date><size>151kb</size></version><title>Anomaly Mining -- Past, Present and Future</title><authors>Leman Akoglu</authors><categories>cs.LG cs.SI</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Anomaly mining is an important problem that finds numerous applications in
various real world domains such as environmental monitoring, cybersecurity,
finance, healthcare and medicine, to name a few. In this article, I focus on
two areas, (1) point-cloud and (2) graph-based anomaly mining. I aim to present
a broad view of each area, and discuss classes of main research problems,
recent trends and future directions. I conclude with key take-aways and
overarching open problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10188</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10188</id><submitter>Xuefeng Bai</submitter><version version="v1"><date>Fri, 21 May 2021 07:55:07 GMT</date><size>561kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 16:03:54 GMT</date><size>775kb</size><source_type>D</source_type></version><title>Semantic Representation for Dialogue Modeling</title><authors>Xuefeng Bai, Yulong Chen, Linfeng Song, Yue Zhang</authors><categories>cs.CL</categories><comments>Final camera ready version, to appear in ACL2021 main conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Although neural models have achieved competitive results in dialogue systems,
they have shown limited ability in representing core semantics, such as
ignoring important entities. To this end, we exploit Abstract Meaning
Representation (AMR) to help dialogue modeling. Compared with the textual
input, AMR explicitly provides core semantic knowledge and reduces data
sparsity. We develop an algorithm to construct dialogue-level AMR graphs from
sentence-level AMRs and explore two ways to incorporate AMRs into dialogue
systems. Experimental results on both dialogue understanding and response
generation tasks show the superiority of our model. To our knowledge, we are
the first to leverage a formal semantic representation into neural dialogue
modeling.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10362</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10362</id><submitter>Stanis{\l}aw Ambroszkiewicz</submitter><version version="v1"><date>Fri, 21 May 2021 15:28:49 GMT</date><size>1558kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 19:10:02 GMT</date><size>1614kb</size><source_type>D</source_type></version><title>Functionals in the Clouds: An abstract architecture of serverless
  Cloud-Native Apps</title><authors>Stanislaw Ambroszkiewicz, Waldemar Bartyna and Stanislaw Bylka</authors><categories>cs.CL cs.LO</categories><comments>Work in progress. Three figures (related to functionals) were added</comments><msc-class>68M14</msc-class><acm-class>F.4.3; F.1.1</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Cloud Native Application CNApp (as a distributed system) is a collection of
independent components (micro-services) interacting via communication
protocols. This gives rise to present an abstract architecture of CNApp as
dynamically re-configurable acyclic directed multi graph where vertices are
microservices, and edges are the protocols. Generic mechanisms for such
reconfigurations evidently correspond to higher-level functions (functionals).
This implies also internal abstract architecture of microservice as a
collection of event-triggered serverless functions (including functions
implementing the protocols) that are dynamically composed into event-dependent
data-flow graphs. Again, generic mechanisms for such compositions correspond to
calculus of functionals and relations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10375</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10375</id><submitter>Kai Wang</submitter><version version="v1"><date>Fri, 21 May 2021 14:34:00 GMT</date><size>620kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 27 May 2021 14:27:03 GMT</date><size>685kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 3 Jun 2021 13:20:29 GMT</date><size>684kb</size><source_type>D</source_type></version><title>An Efficient Training Approach for Very Large Scale Face Recognition</title><authors>Kai Wang, Shuo Wang, Zhipeng Zhou, Xiaobo Wang, Xiaojiang Peng, Baigui
  Sun, Hao Li, Yang You</authors><categories>cs.CV</categories><comments>This is a very effcient framework for ultra-large-scale
  classification tasks. Our code is available at
  https://github.com/tiandunx/FFC. We will keep updating!!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition has achieved significant progress in deep-learning era due
to the ultra-large-scale and well-labeled datasets. However, training on
ultra-large-scale datasets is time-consuming and takes up a lot of hardware
resource. Therefore, designing an efficient training approach is crucial and
indispensable. The heavy computational and memory costs mainly result from the
high dimensionality of the Fully-Connected (FC) layer. Specifically, the
dimensionality is determined by the number of face identities, which can be
million-level or even more. To this end, we propose a novel training approach
for ultra-large-scale face datasets, termed Faster Face Classification
(F$^2$C). In F$^2$C, we first define a Gallery Net and a Probe Net that are
used to generate identities' centers and extract faces' features for face
recognition, respectively. Gallery Net has the same structure as Probe Net and
inherits the parameters from Probe Net with a moving average paradigm. After
that, to reduce the training time and hardware costs of the FC layer, we
propose a Dynamic Class Pool (DCP) that stores the features from Gallery Net
and calculates the inner product (logits) with positive samples (whose
identities are in the DCP) in each mini-batch. DCP can be regarded as a
substitute for the FC layer but it is far smaller, thus greatly reducing the
computational and memory costs. For negative samples (whose identities are not
in DCP), we minimize the cosine similarities between negative samples and those
in DCP. Then, to improve the update efficiency of DCP's parameters, we design a
dual data-loader including identity-based and instance-based loaders to
generate a certain of identities and samples in mini-batches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10500</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10500</id><submitter>Yingjie Zhou</submitter><version version="v1"><date>Sat, 22 May 2021 16:23:05 GMT</date><size>7265kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 11:42:19 GMT</date><size>7266kb</size><source_type>D</source_type></version><title>Feature Encoding with AutoEncoders for Weakly-supervised Anomaly
  Detection</title><authors>Yingjie Zhou, Xucheng Song, Yanru Zhang, Fanxing Liu, Ce Zhu and
  Lingqiao Liu</authors><categories>cs.LG cs.NI</categories><comments>12pages,4 figures, accepted by IEEE Transactions on Neural Networks
  and Learning Systems, 2021</comments><msc-class>68W99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weakly-supervised anomaly detection aims at learning an anomaly detector from
a limited amount of labeled data and abundant unlabeled data. Recent works
build deep neural networks for anomaly detection by discriminatively mapping
the normal samples and abnormal samples to different regions in the feature
space or fitting different distributions. However, due to the limited number of
annotated anomaly samples, directly training networks with the discriminative
loss may not be sufficient. To overcome this issue, this paper proposes a novel
strategy to transform the input data into a more meaningful representation that
could be used for anomaly detection. Specifically, we leverage an autoencoder
to encode the input data and utilize three factors, hidden representation,
reconstruction residual vector, and reconstruction error, as the new
representation for the input data. This representation amounts to encode a test
sample with its projection on the training data manifold, its direction to its
projection and its distance to its projection. In addition to this encoding, we
also propose a novel network architecture to seamlessly incorporate those three
factors. From our extensive experiments, the benefits of the proposed strategy
are clearly demonstrated by its superior performance over the competitive
methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10588</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10588</id><submitter>Michele Xiloyannis</submitter><version version="v1"><date>Fri, 21 May 2021 21:56:18 GMT</date><size>3680kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 07:26:51 GMT</date><size>4630kb</size><source_type>D</source_type></version><title>Soft robotic suits: State of the art, core technologies and open
  challenges</title><authors>Michele Xiloyannis, Ryan Alicea, Anna-Maria Georgarakis, Florian L.
  Haufe, Peter Wolf, Lorenzo Masia and Robert Riener</authors><categories>cs.RO cs.SY eess.SY</categories><comments>Accepted as a Survey Paper on IEEE Transaction on Robotics</comments><journal-ref>IEEE Transactions on Robotics (TRO). Copyright 2021 IEEE</journal-ref><doi>10.1109/TRO.2021.3084466</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wearable robots are undergoing a disruptive transition, from the rigid
machines that populated the science-fiction world in the early eighties to
lightweight robotic apparel, hardly distinguishable from our daily clothes. In
less than a decade of development, soft robotic suits have achieved important
results in human motor assistance and augmentation. In this paper, we start by
giving a definition of soft robotic suits and proposing a taxonomy to classify
existing systems. We then critically review the modes of actuation, the
physical human-robot interface and the intention-detection strategies of state
of the art soft robotic suits, highlighting the advantages and limitations of
different approaches. Finally, we discuss the impact of this new technology on
human movements, for both augmenting human function and supporting motor
impairments, and identify areas that are in need of further development.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10591</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10591</id><submitter>Amir Gilad</submitter><version version="v1"><date>Fri, 21 May 2021 22:24:18 GMT</date><size>1490kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 16:41:38 GMT</date><size>1490kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 11:21:44 GMT</date><size>1490kb</size><source_type>D</source_type></version><title>Detecting Treatment Effect Modifiers in Social Networks</title><authors>Amir Gilad, Harsh Parikh, Sudeepa Roy, Babak Salimi</authors><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study treatment effect modifiers for causal analysis in a social network,
where neighbors' characteristics or network structure may affect the outcome of
a unit, and the goal is to identify sub-populations with varying treatment
effects using such network properties. We propose a novel framework called
Testing-for-Effect-Modifier (TEEM) for this purpose that facilitates
data-driven decision making by testing hypotheses about complex effect
modifiers in terms of network features or network patterns (e.g.,
characteristics of neighbors of a unit or belonging to a triangle), and by
identifying sub-populations for which a treatment is likely to be effective or
harmful. We describe a hypothesis testing approach that accounts for unit's
covariates, their neighbors' covariates and patterns in the social network, and
devise an algorithm incorporating ideas from causal inference, hypothesis
testing, and graph theory to verify a hypothesized effect modifier. We perform
extensive experimental evaluations with a real development economics dataset
about the treatment effect of belonging to a financial support network called
self-help groups on risk tolerance, and also with a synthetic dataset with
known ground truths simulating a vaccine efficacy trial, to evaluate our
framework and algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10606</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10606</id><submitter>Parag Dakle</submitter><version version="v1"><date>Fri, 21 May 2021 23:40:12 GMT</date><size>7100kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 03:08:01 GMT</date><size>7101kb</size></version><title>CEREC: A Corpus for Entity Resolution in Email Conversations</title><authors>Parag Pravin Dakle and Dan I. Moldovan</authors><categories>cs.CL cs.AI</categories><journal-ref>Proceedings of the 28th International Conference on Computational
  Linguistics, pp. 339-349. 2020</journal-ref><doi>10.18653/v1/2020.coling-main.30</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present the first large scale corpus for entity resolution in email
conversations (CEREC). The corpus consists of 6001 email threads from the Enron
Email Corpus containing 36,448 email messages and 60,383 entity coreference
chains. The annotation is carried out as a two-step process with minimal manual
effort. Experiments are carried out for evaluating different features and
performance of four baselines on the created corpus. For the task of mention
identification and coreference resolution, a best performance of 59.2 F1 is
reported, highlighting the room for improvement. An in-depth qualitative and
quantitative error analysis is presented to understand the limitations of the
baselines considered.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10755</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10755</id><submitter>Sai Teja Suggala</submitter><version version="v1"><date>Sat, 22 May 2021 16:10:54 GMT</date><size>2445kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 18:52:59 GMT</date><size>2444kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 08:20:16 GMT</date><size>2445kb</size><source_type>D</source_type></version><title>SDN assisted UAV communication systems : Efficient Deployment Strategies</title><authors>Sai Teja Suggala, Siddhartha Pothukuchi and Naimat Ali Khan</authors><categories>cs.NI cs.IT math.IT</categories><comments>7 pages, 17 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recently, Unmanned Aerial Vehicle (UAV) based communications systems have
attracted increasing research and commercial interest due to their cost
effective deployment and ease of mobility.During natural disasters and
emergencies, such networks are extremely useful to provide communication
service. In such scenarios, UAVs position and trajectory must be optimal to
maintain Quality of Service at the user end. This paper focuses on the
deployment of an SDN-based UAV network providing communication service to the
users. We consider the deployment of the system in stadiums and events. In this
paper, we propose a scheme to allocate UAVs to the users and a traffic
congestion algorithm to reduce the number of packets dropped to avoid
re-transmissions from the user end. We also propose an energy efficient multi
hop routing mechanism to avoid the high power requirement to transmit longer
distances. We assume that all the back-haul links have sufficient capacities to
carry all the traffic from the front-haul links and the design of UAVs must
consider their power requirements for both flight and transmission.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10783</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10783</id><submitter>Srinjita Bhaduri</submitter><version version="v1"><date>Sat, 22 May 2021 18:15:39 GMT</date><size>1654kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 02:51:30 GMT</date><size>1654kb</size></version><title>3DARVisualizer: Debugging 3D Models using Augmented Reality</title><authors>Srinjita Bhaduri, Peter Gyory, Tamara Sumner</authors><categories>cs.HC</categories><comments>Presented virtually as a Demo at the FabLearn Flagship conference,
  NYC 2020</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Often neglected in traditional education, spatial thinking has played a
critical role in science, technology, engineering, and mathematics (STEM)
education. Spatial thinking skills can be enhanced by training, life
experience, and practice. One approach to train these skills is through 3D
modeling (also known as Computer-Aided Design or CAD). Although 3D modeling
tools have shown promising results in training and enhancing spatial thinking
skills in undergraduate engineering students when it comes to novices,
especially middle and high-school students, they are not sufficient to provide
rich 3D experience since the 3D models created in CAD are isolated the actual
3D physical world. Resulting in novice students finding it difficult to create
error-free 3D models that would 3D print successfully. This leads to student
frustration where students are not motivated to create 3D models themselves;
instead, they prefer to download them from online repositories. To address this
problem, researchers are focusing on integrating 3D models and displays into
the physical world with the help of technologies like Augmented Reality (AR).
In this demo, we present an AR application, 3DARVisualizer, that helps us
explore the role of AR as a 3D model debugger, including enhancing 3D modeling
abilities and spatial thinking skills of middle- and high-school students.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10852</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10852</id><submitter>Mouhamed Abdulla Ph.D.</submitter><version version="v1"><date>Sun, 23 May 2021 03:53:37 GMT</date><size>5405kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 19:06:41 GMT</date><size>5524kb</size><source_type>D</source_type></version><title>Latency of Concatenating Unlicensed LPWAN with Cellular IoT: An
  Experimental QoE Study</title><authors>Alvin Ramoutar and Zohreh Motamedi and Mouhamed Abdulla</authors><categories>cs.NI cs.AR cs.IT cs.PF math.IT</categories><comments>Experimental dataset is openly available here:
  https://dx.doi.org/10.21227/zzax-g919</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Developing low-power wide-area network (LPWAN) solutions that are efficient
to adopt, deploy and maintain are vital for smart cities. The poor
quality-of-service of unlicensed LPWAN, and the high service cost of
LTE-M/NB-IoT are key disadvantages of these technologies. Concatenating
unlicensed with licensed LPWANs can overcome these limitations and harness
their benefits. However, a concatenated LPWAN architecture will inevitably
result in excess latency which may impact users' quality-of-experience (QoE).
To evaluate the real-life feasibility of this system, we first propose a
concatenated LPWAN architecture and experimentally measure the statistics of
end-to-end (E2E) latencies. The concatenated delay margin is determined by
benchmarking the latencies with different LPWAN architecture schemes, namely
with unlicensed IoT (standalone LoRa), cellular IoT (standalone LTE-M), and
concatenated IoT (LoRa interfaced with LTE-M). Through extensive experimental
measurement campaigns of 30,000 data points of E2E latencies, we show that the
excess delay due to LPWAN interfacing introduces on average less than 300
milliseconds. The proof-of-concept results suggest that the latency for
concatenating unlicensed LPWAN with cellular IoT is negligible for smart city
use cases where human perception and decision making is in the loop.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.10968</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.10968</id><submitter>Thomas Gilles</submitter><version version="v1"><date>Sun, 23 May 2021 16:27:04 GMT</date><size>6975kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 11:26:47 GMT</date><size>6975kb</size><source_type>D</source_type></version><title>HOME: Heatmap Output for future Motion Estimation</title><authors>Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan
  Stanciulescu, Fabien Moutarde</authors><categories>cs.CV cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose HOME, a framework tackling the motion forecasting
problem with an image output representing the probability distribution of the
agent's future location. This method allows for a simple architecture with
classic convolution networks coupled with attention mechanism for agent
interactions, and outputs an unconstrained 2D top-view representation of the
agent's possible future. Based on this output, we design two methods to sample
a finite set of agent's future locations. These methods allow us to control the
optimization trade-off between miss rate and final displacement error for
multiple modalities without having to retrain any part of the model. We apply
our method to the Argoverse Motion Forecasting Benchmark and achieve 1st place
on the online leaderboard.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11098</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11098</id><submitter>Fandong Meng</submitter><version version="v1"><date>Mon, 24 May 2021 05:34:09 GMT</date><size>580kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:57:08 GMT</date><size>702kb</size><source_type>D</source_type></version><title>Prevent the Language Model from being Overconfident in Neural Machine
  Translation</title><authors>Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, Jie Zhou</authors><categories>cs.CL cs.AI</categories><comments>Accepted as a long paper at ACL 2021. Code is available at:
  https://github.com/Mlair77/nmt_adequacy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Neural Machine Translation (NMT) model is essentially a joint language
model conditioned on both the source sentence and partial translation.
Therefore, the NMT model naturally involves the mechanism of the Language Model
(LM) that predicts the next token only based on partial translation. Despite
its success, NMT still suffers from the hallucination problem, generating
fluent but inadequate translations. The main reason is that NMT pays excessive
attention to the partial translation while neglecting the source sentence to
some extent, namely overconfidence of the LM. Accordingly, we define the Margin
between the NMT and the LM, calculated by subtracting the predicted probability
of the LM from that of the NMT model for each token. The Margin is negatively
correlated to the overconfidence degree of the LM. Based on the property, we
propose a Margin-based Token-level Objective (MTO) and a Margin-based
Sentencelevel Objective (MSO) to maximize the Margin for preventing the LM from
being overconfident. Experiments on WMT14 English-to-German, WMT19
Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate
the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements,
respectively, compared to the Transformer baseline. The human evaluation
further verifies that our approaches improve translation adequacy as well as
fluency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11108</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11108</id><submitter>Lixin Zou</submitter><version version="v1"><date>Mon, 24 May 2021 06:12:06 GMT</date><size>6173kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 16:03:00 GMT</date><size>6173kb</size><source_type>D</source_type></version><title>Pre-trained Language Model based Ranking in Baidu Search</title><authors>Lixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma, Suqi Cheng,
  Daiting Shi, Shuaiqiang Wang, Zhicong Cheng, Dawei Yin</authors><categories>cs.IR</categories><comments>9-pages, 3 figures, 7 tables, SIGKDD 2021 accepted paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the heart of a search engine, the ranking system plays a crucial role in
satisfying users' information demands. More recently, neural rankers fine-tuned
from pre-trained language models (PLMs) establish state-of-the-art ranking
effectiveness. However, it is nontrivial to directly apply these PLM-based
rankers to the large-scale web search system due to the following challenging
issues:(1) the prohibitively expensive computations of massive neural PLMs,
especially for long texts in the web-document, prohibit their deployments in an
online ranking system that demands extremely low latency;(2) the discrepancy
between existing ranking-agnostic pre-training objectives and the ad-hoc
retrieval scenarios that demand comprehensive relevance modeling is another
main barrier for improving the online ranking system;(3) a real-world search
engine typically involves a committee of ranking components, and thus the
compatibility of the individually fine-tuned ranking model is critical for a
cooperative ranking system. In this work, we contribute a series of
successfully applied techniques in tackling these exposed issues when deploying
the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the
online search engine system. We first articulate a novel practice to
cost-efficiently summarize the web document and contextualize the resultant
summary content with the query using a cheap yet powerful Pyramid-ERNIE
architecture. Then we endow an innovative paradigm to finely exploit the
large-scale noisy and biased post-click behavioral data for relevance-oriented
pre-training. We also propose a human-anchored fine-tuning strategy tailored
for the online ranking system, aiming to stabilize the ranking signals across
various online components. Extensive offline and online experimental results
show that the proposed techniques significantly boost the search engine's
performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11160</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11160</id><submitter>Celia Cintas</submitter><version version="v1"><date>Mon, 24 May 2021 09:04:47 GMT</date><size>30569kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 25 May 2021 11:53:15 GMT</date><size>30569kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 2 Jun 2021 07:40:54 GMT</date><size>28081kb</size><source_type>D</source_type></version><title>Out-of-Distribution Detection in Dermatology using Input Perturbation
  and Subset Scanning</title><authors>Hannah Kim, Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Kush
  Varshney</authors><categories>cs.CV cs.LG</categories><comments>Under review for 6th Outlier Detection &amp; Description Workshop</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in deep learning have led to breakthroughs in the development
of automated skin disease classification. As we observe an increasing interest
in these models in the dermatology space, it is crucial to address aspects such
as the robustness towards input data distribution shifts. Current skin disease
models could make incorrect inferences for test samples from different hardware
devices and clinical settings or unknown disease samples, which are
out-of-distribution (OOD) from the training samples. To this end, we propose a
simple yet effective approach that detect these OOD samples prior to making any
decision. The detection is performed via scanning in the latent space
representation (e.g., activations of the inner layers of any pre-trained skin
disease classifier). The input samples could also perturbed to maximise
divergence of OOD samples. We validate our ODD detection approach in two use
cases: 1) identify samples collected from different protocols, and 2) detect
samples from unknown disease classes. Additionally, we evaluate the performance
of the proposed approach and compare it with other state-of-the-art methods.
Furthermore, data-driven dermatology applications may deepen the disparity in
clinical care across racial and ethnic groups since most datasets are reported
to suffer from bias in skin tone distribution. Therefore, we also evaluate the
fairness of these OOD detection methods across different skin tones. Our
experiments resulted in competitive performance across multiple datasets in
detecting OOD samples, which could be used (in the future) to design more
effective transfer learning techniques prior to inferring on these samples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11196</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11196</id><submitter>Andr\'e Mateus</submitter><version version="v1"><date>Mon, 24 May 2021 10:45:46 GMT</date><size>12798kb</size><source_type>D</source_type></version><title>On Incremental Structure-from-Motion using Lines</title><authors>Andr\'e Mateus, Omar Tahri, A. Pedro Aguiar, Pedro U. Lima, and Pedro
  Miraldo</authors><categories>cs.RO cs.SY eess.SY</categories><comments>To appear in IEEE Transactions on Robotics \c{opyright} 2021 IEEE</comments><doi>10.1109/TRO.2021.3085487</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans tend to build environments with structure, which consists of mainly
planar surfaces. From the intersection of planar surfaces arise straight lines.
Lines have more degrees-of-freedom than points. Thus, line-based
Structure-from-Motion (SfM) provides more information about the environment. In
this paper, we present solutions for SfM using lines, namely, incremental SfM.
These approaches consist of designing state observers for a camera's dynamical
visual system looking at a 3D line. We start by presenting a model that uses
spherical coordinates for representing the line's moment vector. We show that
this parameterization has singularities, and therefore we introduce a more
suitable model that considers the line's moment and shortest viewing ray.
Concerning the observers, we present two different methodologies. The first
uses a memory-less state-of-the-art framework for dynamic visual systems. Since
the previous states of the robotic agent are accessible -- while performing the
3D mapping of the environment -- the second approach aims at exploiting the use
of memory to improve the estimation accuracy and convergence speed. The two
models and the two observers are evaluated in simulation and real data, where
mobile and manipulator robots are used.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11225</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11225</id><submitter>Tianming Liang</submitter><version version="v1"><date>Mon, 24 May 2021 12:02:32 GMT</date><size>2452kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 09:09:32 GMT</date><size>2353kb</size><source_type>D</source_type></version><title>Distantly-Supervised Long-Tailed Relation Extraction Using Constraint
  Graphs</title><authors>Tianming Liang, Yang Liu, Xiaoyan Liu, Gaurav Sharma and Maozu Guo</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Label noise and long-tailed distributions are two major challenges in
distantly supervised relation extraction. Recent studies have shown great
progress on denoising, but pay little attention to the problem of long-tailed
relations. In this paper, we introduce constraint graphs to model the
dependencies between relation labels. On top of that, we further propose a
novel constraint graph-based relation extraction framework(CGRE) to handle the
two challenges simultaneously. CGRE employs graph convolution networks (GCNs)
to propagate information from data-rich relation nodes to data-poor relation
nodes, and thus boosts the representation learning of long-tailed relations. To
further improve the noise immunity, a constraint-aware attention module is
designed in CGRE to integrate the constraint information. Experimental results
on a widely-used benchmark dataset indicate that our approach achieves
significant improvements over the previous methods for both denoising and
long-tailed relation extraction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11237</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11237</id><submitter>Jinlong Peng</submitter><version version="v1"><date>Mon, 24 May 2021 12:21:25 GMT</date><size>3926kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 02:47:07 GMT</date><size>3925kb</size><source_type>D</source_type></version><title>SiamRCR: Reciprocal Classification and Regression for Visual Object
  Tracking</title><authors>Jinlong Peng, Zhengkai Jiang, Yueyang Gu, Yang Wu, Yabiao Wang, Ying
  Tai, Chengjie Wang, Weiyao Lin</authors><categories>cs.CV</categories><comments>The 30th International Joint Conference on Artificial Intelligence
  (IJCAI 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, most siamese network based trackers locate targets via object
classification and bounding-box regression. Generally, they select the
bounding-box with maximum classification confidence as the final prediction.
This strategy may miss the right result due to the accuracy misalignment
between classification and regression. In this paper, we propose a novel
siamese tracking algorithm called SiamRCR, addressing this problem with a
simple, light and effective solution. It builds reciprocal links between
classification and regression branches, which can dynamically re-weight their
losses for each positive sample. In addition, we add a localization branch to
predict the localization accuracy, so that it can work as the replacement of
the regression assistance link during inference. This branch makes the training
and inference more consistent. Extensive experimental results demonstrate the
effectiveness of SiamRCR and its superiority over the state-of-the-art
competitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.
Moreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11239</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11239</id><submitter>Fernando P\'erez-Garc\'ia</submitter><version version="v1"><date>Mon, 24 May 2021 12:27:06 GMT</date><size>18890kb</size><source_type>D</source_type></version><title>A self-supervised learning strategy for postoperative brain cavity
  segmentation simulating resections</title><authors>Fernando P\'erez-Garc\'ia, Reuben Dorent, Michele Rizzi, Francesco
  Cardinale, Valerio Frazzini, Vincent Navarro, Caroline Essert, Ir\`ene
  Ollivier, Tom Vercauteren, Rachel Sparks, John S. Duncan and S\'ebastien
  Ourselin</authors><categories>eess.IV cs.CV</categories><comments>To be published in the International Journal of Computer Assisted
  Radiology and Surgery (IJCARS) - Special issue MICCAI 2020</comments><doi>10.1007/s11548-021-02420-2</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Accurate segmentation of brain resection cavities (RCs) aids in postoperative
analysis and determining follow-up treatment. Convolutional neural networks
(CNNs) are the state-of-the-art image segmentation technique, but require large
annotated datasets for training. Annotation of 3D medical images is
time-consuming, requires highly-trained raters, and may suffer from high
inter-rater variability. Self-supervised learning strategies can leverage
unlabeled data for training.
  We developed an algorithm to simulate resections from preoperative magnetic
resonance images (MRIs). We performed self-supervised training of a 3D CNN for
RC segmentation using our simulation method. We curated EPISURG, a dataset
comprising 430 postoperative and 268 preoperative MRIs from 430 refractory
epilepsy patients who underwent resective neurosurgery. We fine-tuned our model
on three small annotated datasets from different institutions and on the
annotated images in EPISURG, comprising 20, 33, 19 and 133 subjects.
  The model trained on data with simulated resections obtained median
(interquartile range) Dice score coefficients (DSCs) of 81.7 (16.4), 82.4
(36.4), 74.9 (24.2) and 80.5 (18.7) for each of the four datasets. After
fine-tuning, DSCs were 89.2 (13.3), 84.1 (19.8), 80.2 (20.1) and 85.2 (10.8).
For comparison, inter-rater agreement between human annotators from our
previous study was 84.0 (9.9).
  We present a self-supervised learning strategy for 3D CNNs using simulated
RCs to accurately segment real RCs on postoperative MRI. Our method generalizes
well to data from different institutions, pathologies and modalities. Source
code, segmentation models and the EPISURG dataset are available at
https://github.com/fepegar/ressegijcars .
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11259</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11259</id><submitter>Han Xu</submitter><version version="v1"><date>Mon, 24 May 2021 13:24:02 GMT</date><size>308kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 03:32:06 GMT</date><size>338kb</size><source_type>D</source_type></version><title>PTR: Prompt Tuning with Rules for Text Classification</title><authors>Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, Maosong Sun</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine-tuned pre-trained language models (PLMs) have achieved awesome
performance on almost all NLP tasks. By using additional prompts to fine-tune
PLMs, we can further stimulate the rich knowledge distributed in PLMs to better
serve downstream task. Prompt tuning has achieved promising results on some
few-class classification tasks such as sentiment classification and natural
language inference. However, manually designing lots of language prompts is
cumbersome and fallible. For those auto-generated prompts, it is also expensive
and time-consuming to verify their effectiveness in non-few-shot scenarios.
Hence, it is challenging for prompt tuning to address many-class classification
tasks. To this end, we propose prompt tuning with rules (PTR) for many-class
text classification, and apply logic rules to construct prompts with several
sub-prompts. In this way, PTR is able to encode prior knowledge of each class
into prompt tuning. We conduct experiments on relation classification, a
typical many-class classification task, and the results on benchmarks show that
PTR can significantly and consistently outperform existing state-of-the-art
baselines. This indicates that PTR is a promising approach to take advantage of
PLMs for those complicated classification tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11269</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11269</id><submitter>Deng Cai</submitter><version version="v1"><date>Mon, 24 May 2021 13:35:19 GMT</date><size>4460kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 07:41:52 GMT</date><size>1892kb</size><source_type>D</source_type></version><title>Neural Machine Translation with Monolingual Translation Memory</title><authors>Deng Cai and Yan Wang and Huayang Li and Wai Lam and Lemao Liu</authors><categories>cs.CL cs.AI</categories><comments>ACL2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Prior work has proved that Translation memory (TM) can boost the performance
of Neural Machine Translation (NMT). In contrast to existing work that uses
bilingual corpus as TM and employs source-side similarity search for memory
retrieval, we propose a new framework that uses monolingual memory and performs
learnable memory retrieval in a cross-lingual manner. Our framework has unique
advantages. First, the cross-lingual memory retriever allows abundant
monolingual data to be TM. Second, the memory retriever and NMT model can be
jointly optimized for the ultimate translation goal. Experiments show that the
proposed method obtains substantial improvements. Remarkably, it even
outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the
ability to leverage monolingual data, our model also demonstrates effectiveness
in low-resource and domain adaptation scenarios.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11522</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11522</id><submitter>Marco Ballesio</submitter><version version="v1"><date>Mon, 24 May 2021 20:31:48 GMT</date><size>900kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:56:49 GMT</date><size>900kb</size><source_type>D</source_type></version><title>Unbiased Estimation of the Gradient of the Log-Likelihood for a Class of
  Continuous-Time State-Space Models</title><authors>Marco Ballesio and Ajay Jasra</authors><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we consider static parameter estimation for a class of
continuous-time state-space models. Our goal is to obtain an unbiased estimate
of the gradient of the log-likelihood (score function), which is an estimate
that is unbiased even if the stochastic processes involved in the model must be
discretized in time. To achieve this goal, we apply a doubly randomized scheme,
that involves a novel coupled conditional particle filter (CCPF) on the second
level of randomization. Our novel estimate helps facilitate the application of
gradient-based estimation algorithms, such as stochastic-gradient Langevin
descent. We illustrate our methodology in the context of stochastic gradient
descent (SGD) in several numerical examples and compare with the Rhee &amp; Glynn
estimator.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11545</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11545</id><submitter>Nasim Baharisangari</submitter><version version="v1"><date>Mon, 24 May 2021 21:26:57 GMT</date><size>3680kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 17:02:42 GMT</date><size>3682kb</size><source_type>D</source_type></version><title>Uncertainty-Aware Signal Temporal Logic Inference</title><authors>Nasim Baharisangari, Jean-Rapha\&quot;el Gaglione, Daniel Neider, Ufuk
  Topcu, Zhe Xu</authors><categories>cs.AI</categories><comments>11 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal logic inference is the process of extracting formal descriptions of
system behaviors from data in the form of temporal logic formulas. The existing
temporal logic inference methods mostly neglect uncertainties in the data,
which results in limited applicability of such methods in real-world
deployments. In this paper, we first investigate the uncertainties associated
with trajectories of a system and represent such uncertainties in the form of
interval trajectories. We then propose two uncertainty-aware signal temporal
logic (STL) inference approaches to classify the undesired behaviors and
desired behaviors of a system. Instead of classifying finitely many
trajectories, we classify infinitely many trajectories within the interval
trajectories. In the first approach, we incorporate robust semantics of STL
formulas with respect to an interval trajectory to quantify the margin at which
an STL formula is satisfied or violated by the interval trajectory. The second
approach relies on the first learning algorithm and exploits the decision tree
to infer STL formulas to classify behaviors of a given system. The proposed
approaches also work for non-separable data by optimizing the worst-case
robustness in inferring an STL formula. Finally, we evaluate the performance of
the proposed algorithms in two case studies, where the proposed algorithms show
reductions in the computation time by up to four orders of magnitude in
comparison with the sampling-based baseline algorithms (for a dataset with 800
sampled trajectories in total).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11592</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11592</id><submitter>Anupama Mampage</submitter><version version="v1"><date>Tue, 25 May 2021 00:55:03 GMT</date><size>852kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 06:00:50 GMT</date><size>852kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 1 Jun 2021 01:33:17 GMT</date><size>852kb</size><source_type>D</source_type></version><title>A Holistic View on Resource Management in Serverless Computing
  Environments: Taxonomy and Future Directions</title><authors>Anupama Mampage, Shanika Karunasekera and Rajkumar Buyya</authors><categories>cs.DC</categories><comments>32 pages, 4 figures</comments><acm-class>A.1; C.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Serverless computing has emerged as an attractive deployment option for cloud
applications in recent times. The unique features of this computing model
include, rapid auto-scaling, strong isolation, fine-grained billing options and
access to a massive service ecosystem which autonomously handles resource
management decisions. This model is increasingly being explored for deployments
in geographically distributed edge and fog computing networks as well, due to
these characteristics. Effective management of computing resources has always
gained a lot of attention among researchers. The need to automate the entire
process of resource provisioning, allocation, scheduling, monitoring and
scaling, has resulted in the need for specialized focus on resource management
under the serverless model. In this article, we identify the major aspects
covering the broader concept of resource management in serverless environments
and propose a taxonomy of elements which influence these aspects, encompassing
characteristics of system design, workload attributes and stakeholder
expectations. We take a holistic view on serverless environments deployed
across edge, fog and cloud computing networks. We also analyse existing works
discussing aspects of serverless resource management using this taxonomy. This
article further identifies gaps in literature and highlights future research
directions for improving capabilities of this computing model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11605</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11605</id><submitter>Tianxing Xu</submitter><version version="v1"><date>Tue, 25 May 2021 01:54:31 GMT</date><size>2413kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 09:38:58 GMT</date><size>2424kb</size><source_type>D</source_type></version><title>TransLoc3D : Point Cloud based Large-scale Place Recognition using
  Adaptive Receptive Fields</title><authors>Tian-Xing Xu, Yuan-Chen Guo, Yu-Kun Lai, Song-Hai Zhang</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Place recognition plays an essential role in the field of autonomous driving
and robot navigation. Although a number of point cloud based methods have been
proposed and achieved promising results, few of them take the size difference
of objects into consideration. For small objects like pedestrians and vehicles,
large receptive fields will capture unrelated information, while small
receptive fields would fail to encode complete geometric information for large
objects such as buildings. We argue that fixed receptive fields are not well
suited for place recognition, and propose a novel Adaptive Receptive Field
Module (ARFM), which can adaptively adjust the size of the receptive field
based on the input point cloud. We also present a novel network architecture,
named TransLoc3D, to obtain discriminative global descriptors of point clouds
for the place recognition task. TransLoc3D consists of a 3D sparse
convolutional module, an ARFM module, an external transformer network which
aims to capture long range dependency and a NetVLAD layer. Experiments show
that our method outperforms prior state-of-the-art results, with an improvement
of 1.1\% on average recall@1 on the Oxford RobotCar dataset, and 0.8\% on the
B.D. dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11675</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11675</id><submitter>Zhiwei Wang</submitter><version version="v1"><date>Tue, 25 May 2021 05:27:11 GMT</date><size>257kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 07:40:24 GMT</date><size>260kb</size><source_type>D</source_type></version><title>An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural
  Network</title><authors>Tao Luo, Zheng Ma, Zhiwei Wang, Zhi-Qin John Xu, Yaoyu Zhang</authors><categories>cs.LG cs.NA math.NA</categories><comments>arXiv admin note: substantial text overlap with arXiv:2012.03238</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Deep neural network (DNN) usually learns the target function from low to high
frequency, which is called frequency principle or spectral bias. This frequency
principle sheds light on a high-frequency curse of DNNs -- difficult to learn
high-frequency information. Inspired by the frequency principle, a series of
works are devoted to develop algorithms for overcoming the high-frequency
curse. A natural question arises: what is the upper limit of the decaying rate
w.r.t. frequency when one trains a DNN? In this work, our theory, confirmed by
numerical experiments, suggests that there is a critical decaying rate w.r.t.
frequency in DNN training. Below the upper limit of the decaying rate, the DNN
interpolates the training data by a function with a certain regularity.
However, above the upper limit, the DNN interpolates the training data by a
trivial function, i.e., a function is only non-zero at training data points.
Our results indicate a better way to overcome the high-frequency curse is to
design a proper pre-condition approach to shift high-frequency information to
low-frequency one, which coincides with several previous developed algorithms
for fast learning high-frequency information. More importantly, this work
rigorously proves that the high-frequency curse is an intrinsic difficulty of
DNNs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11686</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11686</id><submitter>Hanxu Zhou</submitter><version version="v1"><date>Tue, 25 May 2021 05:47:55 GMT</date><size>20395kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 04:23:57 GMT</date><size>20278kb</size><source_type>D</source_type></version><title>Towards Understanding the Condensation of Two-layer Neural Networks at
  Initial Training</title><authors>Zhi-Qin John Xu, Hanxu Zhou, Tao Luo, Yaoyu Zhang</authors><categories>cs.LG cs.AI</categories><msc-class>68T07</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  It is important to study what implicit regularization is imposed on the loss
function during the training that leads over-parameterized neural networks
(NNs) to good performance on real dataset. Empirically, existing works have
shown that weights of NNs condense on isolated orientations with small
initialization. The condensation implies that the NN learns features from the
training data and is effectively a much smaller network. In this work, we show
that the singularity of the activation function at original point is a key
factor to understanding the condensation at initial training stage. Our
experiments suggest that the maximal number of condensed orientations is twice
of the singularity order. Our theoretical analysis confirms experiments for two
cases, one is for the first-order singularity activation function and the other
is for the one-dimensional input. This work takes a step towards understanding
how small initialization implicitly leads NNs to condensation at initial
training, which is crucial to understand the training and the learning of deep
NNs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11752</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11752</id><submitter>Milad Alshomary</submitter><version version="v1"><date>Tue, 25 May 2021 08:39:14 GMT</date><size>4609kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 08:14:09 GMT</date><size>4607kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 31 May 2021 08:12:14 GMT</date><size>4432kb</size><source_type>D</source_type></version><title>Argument Undermining: Counter-Argument Generation by Attacking Weak
  Premises</title><authors>Milad Alshomary, Shahbaz Syed, Arkajit Dhar, Martin Potthast and
  Henning Wachsmuth</authors><categories>cs.CL</categories><comments>9 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Text generation has received a lot of attention in computational
argumentation research as of recent. A particularly challenging task is the
generation of counter-arguments. So far, approaches primarily focus on
rebutting a given conclusion, yet other ways to counter an argument exist. In
this work, we go beyond previous research by exploring argument undermining,
that is, countering an argument by attacking one of its premises. We
hypothesize that identifying the argument's weak premises is key to effective
countering. Accordingly, we propose a pipeline approach that first assesses the
premises' strength and then generates a counter-argument targeting the weak
ones. On the one hand, both manual and automatic evaluation proves the
importance of identifying weak premises in counter-argument generation. On the
other hand, when considering correctness and content richness, human annotators
favored our approach over state-of-the-art counter-argument generation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11763</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11763</id><submitter>Emilio Gamba</submitter><version version="v1"><date>Tue, 25 May 2021 08:57:43 GMT</date><size>1000kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 15:32:09 GMT</date><size>2014kb</size><source_type>D</source_type></version><title>Efficiently Explaining CSPs with Unsatisfiable Subset Optimization</title><authors>Emilio Gamba, Bart Bogaerts and Tias Guns</authors><categories>cs.AI cs.LO</categories><msc-class>68T27</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build on a recently proposed method for explaining solutions of constraint
satisfaction problems. An explanation here is a sequence of simple inference
steps, where the simplicity of an inference step is measured by the number and
types of constraints and facts used, and where the sequence explains all
logical consequences of the problem. We build on these formal foundations and
tackle two emerging questions, namely how to generate explanations that are
provably optimal (with respect to the given cost metric) and how to generate
them efficiently. To answer these questions, we develop 1) an implicit hitting
set algorithm for finding optimal unsatisfiable subsets; 2) a method to reduce
multiple calls for (optimal) unsatisfiable subsets to a single call that takes
constraints on the subset into account, and 3) a method for re-using relevant
information over multiple calls to these algorithms. The method is also
applicable to other problems that require finding cost-optimal unsatiable
subsets. We specifically show that this approach can be used to effectively
find sequences of optimal explanation steps for constraint satisfaction
problems like logic grid puzzles.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11989</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11989</id><submitter>Rushabh Patel</submitter><version version="v1"><date>Tue, 25 May 2021 14:47:07 GMT</date><size>693kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 18:28:30 GMT</date><size>715kb</size></version><title>Graph Based Link Prediction between Human Phenotypes and Genes</title><authors>Rushabh Patel, Yanhui Guo</authors><categories>cs.AI cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The learning of genotype-phenotype associations and history of
human disease by doing detailed and precise analysis of phenotypic
abnormalities can be defined as deep phenotyping. To understand and detect this
interaction between phenotype and genotype is a fundamental step when
translating precision medicine to clinical practice. The recent advances in the
field of machine learning is efficient to predict these interactions between
abnormal human phenotypes and genes.
  Methods: In this study, we developed a framework to predict links between
human phenotype ontology (HPO) and genes. The annotation data from the
heterogeneous knowledge resources i.e., orphanet, is used to parse human
phenotype-gene associations. To generate the embeddings for the nodes (HPO &amp;
genes), an algorithm called node2vec was used. It performs node sampling on
this graph based on random walks, then learns features over these sampled nodes
to generate embeddings. These embeddings were used to perform the downstream
task to predict the presence of the link between these nodes using 5 different
supervised machine learning algorithms.
  Results: The downstream link prediction task shows that the Gradient Boosting
Decision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR
0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87.
Compared to the other 4 methods LightGBM is able to find more accurate
interaction/link between human phenotype &amp; gene pairs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.11992</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.11992</id><submitter>Richard Santiago</submitter><version version="v1"><date>Tue, 25 May 2021 14:55:37 GMT</date><size>556kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 15:29:22 GMT</date><size>556kb</size><source_type>D</source_type></version><title>An Optimal Monotone Contention Resolution Scheme for Uniform and
  Partition Matroids</title><authors>Danish Kashaev, Richard Santiago</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common approach to solve a combinatorial optimization problem is to first
solve a continous relaxation and then round the fractional solution. For the
latter, the framework of contention resolution schemes (or CR schemes)
introduced by Chekuri, Vondrak, and Zenklusen, has become a general and
successful tool. A CR scheme takes a fractional point $x$ in a relaxation
polytope, rounds each coordinate $x_i$ independently to get a possibly
non-feasible set, and then drops some elements in order to satisfy the
independence constraints. Intuitively, a CR scheme is $c$-balanced if every
element $i$ is selected with probability at least $c \cdot x_i$.
  It is known that general matroids admit a $(1-1/e)$-balanced CR scheme, and
that this is (asymptotically) optimal. This is in particular true for the
special case of uniform matroids of rank one. In this work, we provide a simple
monotone CR scheme with a balancedness factor of $1 - e^{-k}k^k/k!$ for uniform
matroids of rank $k$, and show that this is optimal. This result generalizes
the $1-1/e$ optimal factor for the rank one (i.e. $k=1$) case, and improves it
for any $k&gt;1$. Moreover, this scheme generalizes into an optimal CR scheme for
partition matroids.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12021</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12021</id><submitter>Tianqi Zheng</submitter><version version="v1"><date>Tue, 25 May 2021 15:40:38 GMT</date><size>202kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 12:24:04 GMT</date><size>202kb</size><source_type>D</source_type></version><title>Tight Inner Approximations of the Positive-Semidefinite Cone via
  Grassmannian Packings</title><authors>Tianqi Zheng, James Guthrie, and Enrique Mallada</authors><categories>math.OC cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate the problem of finding tight innerapproximations of large
dimensional positive semidefinite (PSD)cones. To solve this problem, we develop
a novel decompositionframework of the PSD cone by means of conical
combinationsof smaller dimensional sub-cones. We show that many
innerapproximation techniques could be summarized within thisframework,
including the set of (scaled) diagonally dominantmatrices,
Factor-widthkmatrices, and Chordal Sparse ma-trices. Furthermore, we provide a
more flexible family ofinner approximations of the PSD cone, where we aim
toarrange the sub-cones so that they are maximally separatedfrom each other. In
doing so, these approximations tend tooccupy large fractions of the volume of
the PSD cone. Theproposed approach is connected to a classical packing
problemin Riemannian Geometry. Precisely, we show that the problemof finding
maximally distant sub-cones in an ambient PSD coneis equivalent to the problem
of packing sub-spaces in a Grass-mannian Manifold. We further leverage existing
computationalmethod for constructing packings in Grassmannian manifoldsto build
tighter approximations of the PSD cone. Numericalexperiments show how the
proposed framework can balancebetween accuracy and computational complexity, to
efficientlysolve positive-semidefinite programs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12152</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12152</id><submitter>Christian Horvat</submitter><version version="v1"><date>Tue, 25 May 2021 18:08:09 GMT</date><size>3770kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 12:42:45 GMT</date><size>3770kb</size><source_type>D</source_type></version><title>Density estimation on low-dimensional manifolds: an inflation-deflation
  approach</title><authors>Christian Horvat, Jean-Pascal Pfister</authors><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Normalizing Flows (NFs) are universal density estimators based on Neuronal
Networks. However, this universality is limited: the density's support needs to
be diffeomorphic to a Euclidean space. In this paper, we propose a novel method
to overcome this limitation without sacrificing universality. The proposed
method inflates the data manifold by adding noise in the normal space, trains
an NF on this inflated manifold, and, finally, deflates the learned density.
Our main result provides sufficient conditions on the manifold and the specific
choice of noise under which the corresponding estimator is exact. Our method
has the same computational complexity as NFs and does not require computing an
inverse flow. We also show that, if the embedding dimension is much larger than
the manifold dimension, noise in the normal space can be well approximated by
Gaussian noise. This allows to use our method for approximating arbitrary
densities on non-flat manifolds provided that the manifold dimension is known.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12195</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12195</id><submitter>Joymallya Chakraborty Mr.</submitter><version version="v1"><date>Tue, 25 May 2021 20:15:50 GMT</date><size>929kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 21:37:02 GMT</date><size>1873kb</size><source_type>D</source_type></version><title>Bias in Machine Learning Software: Why? How? What to do?</title><authors>Joymallya Chakraborty, Suvodeep Majumder, Tim Menzies</authors><categories>cs.LG cs.SE</categories><journal-ref>ESEC/FSE'2021: The 29th ACM Joint European Software Engineering
  Conference and Symposium on the Foundations of Software Engineering
  (ESEC/FSE), Athens, Greece, August 23-28, 2021</journal-ref><doi>10.1145/3468264.3468537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasingly, software is making autonomous decisions in case of criminal
sentencing, approving credit cards, hiring employees, and so on. Some of these
decisions show bias and adversely affect certain social groups (e.g. those
defined by sex, race, age, marital status). Many prior works on bias mitigation
take the following form: change the data or learners in multiple ways, then see
if any of that improves fairness. Perhaps a better approach is to postulate
root causes of bias and then applying some resolution strategy. This paper
postulates that the root causes of bias are the prior decisions that affect-
(a) what data was selected and (b) the labels assigned to those examples. Our
Fair-SMOTE algorithm removes biased labels; and rebalances internal
distributions such that based on sensitive attribute, examples are equal in
both positive and negative classes. On testing, it was seen that this method
was just as effective at reducing bias as prior approaches. Further, models
generated via Fair-SMOTE achieve higher performance (measured in terms of
recall and F1) than other state-of-the-art fairness improvement algorithms. To
the best of our knowledge, measured in terms of number of analyzed learners and
datasets, this study is one of the largest studies on bias mitigation yet
presented in the literature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12210</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12210</id><submitter>George Philipp</submitter><version version="v1"><date>Tue, 25 May 2021 20:47:43 GMT</date><size>15188kb</size><source_type>D</source_type></version><title>The Nonlinearity Coefficient - A Practical Guide to Neural Architecture
  Design</title><authors>George Philipp</authors><categories>cs.LG cs.CV cs.NE</categories><comments>This work is based on the PhD thesis with the same name, author, year
  and institution. Both works may be cited interchangeably</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In essence, a neural network is an arbitrary differentiable, parametrized
function. Choosing a neural network architecture for any task is as complex as
searching the space of those functions. For the last few years, 'neural
architecture design' has been largely synonymous with 'neural architecture
search' (NAS), i.e. brute-force, large-scale search. NAS has yielded
significant gains on practical tasks. However, NAS methods end up searching for
a local optimum in architecture space in a small neighborhood around
architectures that often go back decades, based on CNN or LSTM.
  In this work, we present a different and complementary approach to
architecture design, which we term 'zero-shot architecture design' (ZSAD). We
develop methods that can predict, without any training, whether an architecture
will achieve a relatively high test or training error on a task after training.
We then go on to explain the error in terms of the architecture definition
itself and develop tools for modifying the architecture based on this
explanation. This confers an unprecedented level of control on the deep
learning practitioner. They can make informed design decisions before the first
line of code is written, even for tasks for which no prior art exists.
  Our first major contribution is to show that the 'degree of nonlinearity' of
a neural architecture is a key causal driver behind its performance, and a
primary aspect of the architecture's model complexity. We introduce the
'nonlinearity coefficient' (NLC), a scalar metric for measuring nonlinearity.
Via extensive empirical study, we show that the value of the NLC in the
architecture's randomly initialized state before training is a powerful
predictor of test error after training and that attaining a right-sized NLC is
essential for attaining an optimal test error. The NLC is also conceptually
simple, well-defined for any feedforward network, easy and cheap to compute,
has extensive theoretical, empirical and conceptual grounding, follows
instructively from the architecture definition, and can be easily controlled
via our 'nonlinearity normalization' algorithm. We argue that the NLC is the
most powerful scalar statistic for architecture design specifically and neural
network analysis in general. Our analysis is fueled by mean field theory, which
we use to uncover the 'meta-distribution' of layers.
  Beyond the NLC, we uncover and flesh out a range of metrics and properties
that have a significant explanatory influence on test and training error. We go
on to explain the majority of the error variation across a wide range of
randomly generated architectures with these metrics and properties. We compile
our insights into a practical guide for architecture designers, which we argue
can significantly shorten the trial-and-error phase of deep learning
deployment.
  Our results are grounded in an experimental protocol that exceeds that of the
vast majority of other deep learning studies in terms of carefulness and rigor.
We study the impact of e.g. dataset, learning rate, floating-point precision,
loss function, statistical estimation error and batch inter-dependency on
performance and other key properties. We promote research practices that we
believe can significantly accelerate progress in architecture design research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12228</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12228</id><submitter>Marco Minghini PhD</submitter><version version="v1"><date>Tue, 25 May 2021 21:37:58 GMT</date><size>1210kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 09:38:40 GMT</date><size>1210kb</size></version><title>INSPIRE: The Entry Point to Europe's Big Geospatial Data Infrastructure</title><authors>Marco Minghini, Vlado Cetl, Alexander Kotsev, Robert Tomas, Michael
  Lutz</authors><categories>cs.DB</categories><comments>21 pages, 6 figures</comments><journal-ref>In: Werner M., Chiang YY. (eds) Handbook of Big Geospatial Data.
  Springer, Cham (2021)</journal-ref><doi>10.1007/978-3-030-55462-0_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Initiated in 2007, the INSPIRE Directive has set a legal framework to create
a European-wide Spatial Data Infrastructure (SDI) to support the European Union
(EU) environmental policies. This chapter analyses the INSPIRE infrastructure
from a Big Geospatial Data perspective, describing how data is shared in an
interoperable way by public sector organisations in the EU Member States and
how it is made available in and accessible within the infrastructure. The
INSPIRE Geoportal, which is the entry point to the whole infrastructure, is
presented in detail. To justify its nature of a Big Geospatial Data
infrastructure, the characteristics of INSPIRE data are mapped to those of Big
Data's six 'Vs'. Despite many good results achieved in terms of data sharing,
some challenges still remain related to data consumption from the user side.
The chapter concludes with a dedicated discussion on how INSPIRE, and
traditional SDIs in general, should evolve into modern data ecosystems to
address these challenges while also embracing the modern practices of data
sharing through the web.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12247</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12247</id><submitter>Sayan Nag</submitter><version version="v1"><date>Tue, 25 May 2021 22:34:19 GMT</date><size>294kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 04:51:26 GMT</date><size>2333kb</size><source_type>D</source_type></version><title>Graph Self Supervised Learning: the BT, the HSIC, and the VICReg</title><authors>Sayan Nag</authors><categories>cs.LG cs.AI cs.CG cs.CV stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Self-supervised learning and pre-training strategies have developed over the
last few years especially for Convolutional Neural Networks (CNNs). Recently
application of such methods can also be noticed for Graph Neural Networks
(GNNs). In this paper, we have used a graph based self-supervised learning
strategy with different loss functions (Barlow Twins[ 7], HSIC[ 4], VICReg[ 1])
which have shown promising results when applied with CNNs previously. We have
also proposed a hybrid loss function combining the advantages of VICReg and
HSIC and called it as VICRegHSIC. The performance of these aforementioned
methods have been compared when applied to two different datasets namely MUTAG
and PROTEINS. Moreover, the impact of different batch sizes, projector
dimensions and data augmentation strategies have also been explored. The
results are preliminary and we will be continuing to explore with other
datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12273</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12273</id><submitter>Karishma Patnaik</submitter><version version="v1"><date>Wed, 26 May 2021 00:32:31 GMT</date><size>28248kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 18:09:43 GMT</date><size>28248kb</size><source_type>D</source_type></version><title>Collision Recovery Control of a Foldable Quadrotor</title><authors>Karishma Patnaik, Shatadal Mishra, Zachary Chase, and Wenlong Zhang</authors><categories>cs.RO cs.SY eess.SY</categories><comments>7 pages, 9 figures, accepted for publication in IEEE/ASME
  International Conference on Advanced Intelligent Mechatronics (AIM) 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous missions of small unmanned aerial vehicles (UAVs) are prone to
collisions owing to environmental disturbances and localization errors.
Consequently, a UAV that can endure collisions and perform recovery control in
critical aerial missions is desirable to prevent loss of the vehicle and/or
payload. We address this problem by proposing a novel foldable quadrotor system
which can sustain collisions and recover safely. The quadrotor is designed with
integrated mechanical compliance using a torsional spring such that the impact
time is increased and the net impact force on the main body is decreased. The
post-collision dynamics is analysed and a recovery controller is proposed which
stabilizes the system to a hovering location without additional collisions.
Flight test results on the proposed and a conventional quadrotor demonstrate
that for the former, integrated spring-damper characteristics reduce the
rebound velocity and lead to simple recovery control algorithms in the event of
unintended collisions as compared to a rigid quadrotor of the same dimension.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12400</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12400</id><submitter>Fanchao Qi</submitter><version version="v1"><date>Wed, 26 May 2021 08:54:19 GMT</date><size>150kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 08:27:11 GMT</date><size>150kb</size><source_type>D</source_type></version><title>Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger</title><authors>Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu,
  Yasheng Wang, Maosong Sun</authors><categories>cs.CL cs.CR</categories><comments>Accepted by ACL-IJCNLP 2021 as a long paper. Camera-ready version</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Backdoor attacks are a kind of insidious security threat against machine
learning models. After being injected with a backdoor in training, the victim
model will produce adversary-specified outputs on the inputs embedded with
predesigned triggers but behave properly on normal inputs during inference. As
a sort of emergent attack, backdoor attacks in natural language processing
(NLP) are investigated insufficiently. As far as we know, almost all existing
textual backdoor attack methods insert additional contents into normal samples
as triggers, which causes the trigger-embedded samples to be detected and the
backdoor attacks to be blocked without much effort. In this paper, we propose
to use the syntactic structure as the trigger in textual backdoor attacks. We
conduct extensive experiments to demonstrate that the syntactic trigger-based
attack method can achieve comparable attack performance (almost 100% success
rate) to the insertion-based methods but possesses much higher invisibility and
stronger resistance to defenses. These results also reveal the significant
insidiousness and harmfulness of textual backdoor attacks. All the code and
data of this paper can be obtained at https://github.com/thunlp/HiddenKiller.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12585</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12585</id><submitter>Fanchao Qi</submitter><version version="v1"><date>Wed, 26 May 2021 14:41:01 GMT</date><size>153kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 08:21:55 GMT</date><size>147kb</size><source_type>D</source_type></version><title>Automatic Construction of Sememe Knowledge Bases via Dictionaries</title><authors>Fanchao Qi, Yangyi Chen, Fengyu Wang, Zhiyuan Liu, Xiao Chen, Maosong
  Sun</authors><categories>cs.CL cs.AI</categories><comments>Accepted by Findings of ACL at ACL-IJCNLP 2021. Camera-ready version</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A sememe is defined as the minimum semantic unit in linguistics. Sememe
knowledge bases (SKBs), which comprise words annotated with sememes, enable
sememes to be applied to natural language processing. So far a large body of
research has showcased the unique advantages and effectiveness of SKBs in
various tasks. However, most languages have no SKBs, and manual construction of
SKBs is time-consuming and labor-intensive. To tackle this challenge, we
propose a simple and fully automatic method of building an SKB via an existing
dictionary. We use this method to build an English SKB and a French SKB, and
conduct comprehensive evaluations from both intrinsic and extrinsic
perspectives. Experimental results demonstrate that the automatically built
English SKB is even superior to HowNet, the most widely used SKB that takes
decades to build manually. And both the English and French SKBs can bring
obvious performance enhancement in multiple downstream tasks. All the code and
data of this paper (except the copyrighted dictionaries) can be obtained at
https://github.com/thunlp/DictSKB.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12626</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12626</id><submitter>Sergio Altares</submitter><version version="v1"><date>Wed, 26 May 2021 15:31:10 GMT</date><size>634kb</size><source_type>D</source_type></version><title>Automatic design of quantum feature maps</title><authors>Sergio Altares-L\'opez, Angela Ribeiro, Juan Jos\'e Garc\'ia-Ripoll</authors><categories>quant-ph cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a new technique for the automatic generation of optimal ad-hoc
ans\&quot;atze for classification by using quantum support vector machine (QSVM).
This efficient method is based on NSGA-II multiobjective genetic algorithms
which allow both maximize the accuracy and minimize the ansatz size. It is
demonstrated the validity of the technique by a practical example with a
non-linear dataset, interpreting the resulting circuit and its outputs. We also
show other application fields of the technique that reinforce the validity of
the method, and a comparison with classical classifiers in order to understand
the advantages of using quantum machine learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12697</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12697</id><submitter>Matej Zecevic</submitter><version version="v1"><date>Wed, 26 May 2021 17:19:22 GMT</date><size>5152kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 29 May 2021 09:13:12 GMT</date><size>5152kb</size><source_type>D</source_type></version><title>Intriguing Parameters of Structural Causal Models</title><authors>Matej Ze\v{c}evi\'c, Devendra Singh Dhami and Kristian Kersting</authors><categories>cs.LG cs.CR</categories><comments>Main paper: 9 pages, References: 2 pages, Supplement: 2 pages. Main
  paper: 3 figures, Supplement: 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been a lot of focus on adversarial attacks,
especially on deep neural networks. Here, we argue that they are more general
in nature and can easily affect a larger class of models, e.g., any
differentiable perturbed optimizers. We further show that such attacks can be
determined by the hidden confounders in a domain, thus drawing a novel
connection between such attacks and causality. Establishing this causal
perspective is characterized by the influence of the structural causal model's
data generating process on the subsequent optimization thereby exhibiting
intriguing parameters of the former. We reveal the existence of such parameters
for three combinatorial optimization problems, namely linear assignment,
shortest path and a real world problem of energy systems. Our empirical
examination also unveils worrisome consequences of these attacks on
differentiable perturbed optimizers thereby highlighting the criticality of our
findings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12827</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12827</id><submitter>Evgeny Bobrov</submitter><version version="v1"><date>Thu, 29 Apr 2021 06:02:24 GMT</date><size>451kb</size></version><version version="v2"><date>Mon, 31 May 2021 07:40:32 GMT</date><size>511kb</size></version><title>Massive MIMO Adaptive Modulation and Coding Using Online Deep Learning</title><authors>Evgeny Bobrov (1, 2), Dmitry Kropotov (2, 3), Hao Lu (1), Danila Zaev
  (1) ((1) Moscow Research Center, Huawei Technologies, Russia, (2) M. V.
  Lomonosov Moscow State University, Russia, (3) National Research University
  Higher School of Economics, Russia)</authors><categories>cs.NI eess.SP</categories><comments>The paper has been submitted to the IEEE WCL journal and has 4 pages
  and 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper describes an online deep learning algorithm for the adaptive
modulation and coding in 5G Massive MIMO. The algorithm is based on a fully
connected neural network, which is initially trained on the output of the
traditional algorithm and then is incrementally retrained by the service
feedback of its output. We show the advantage of our solution over the
state-of-the-art Q-Learning approach. We provide system-level simulation
results to support this conclusion in various scenarios with different channel
characteristics and different user speeds. Compared with traditional OLLA our
algorithm shows 10% to 20% improvement of user throughput in full buffer case.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12837</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12837</id><submitter>Hubert Baniecki</submitter><version version="v1"><date>Wed, 26 May 2021 20:58:04 GMT</date><size>667kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 17:10:23 GMT</date><size>668kb</size><source_type>D</source_type></version><title>Fooling Partial Dependence via Data Poisoning</title><authors>Hubert Baniecki, Wojciech Kretowicz, Przemyslaw Biecek</authors><categories>cs.LG stat.ML</categories><comments>Code for this work is available at
  https://github.com/MI2DataLab/fooling-partial-dependence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many methods have been developed to understand complex predictive models and
high expectations are placed on post-hoc model explainability. It turns out
that such explanations are not robust nor trustworthy, and they can be fooled.
This paper presents techniques for attacking Partial Dependence (plots,
profiles, PDP), which are among the most popular methods of explaining any
predictive model trained on tabular data. We showcase that PD can be
manipulated in an adversarial manner, which is alarming, especially in
financial or medical applications where auditability became a must-have trait
supporting black-box models. The fooling is performed via poisoning the data to
bend and shift explanations in the desired direction using genetic and gradient
algorithms. To the best of our knowledge, this is the first work performing
attacks on variable dependence explanations. The novel approach of using a
genetic algorithm for doing so is highly transferable as it generalizes both
ways: in a model-agnostic and an explanation-agnostic manner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12848</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12848</id><submitter>Yinghao Li</submitter><version version="v1"><date>Wed, 26 May 2021 21:18:48 GMT</date><size>426kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 13:53:14 GMT</date><size>426kb</size><source_type>D</source_type></version><title>BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised
  Named Entity Recognition</title><authors>Yinghao Li, Pranav Shetty, Lucas Liu, Chao Zhang, Le Song</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of learning a named entity recognition (NER) tagger
using noisy labels from multiple weak supervision sources. Though cheap to
obtain, the labels from weak supervision sources are often incomplete,
inaccurate, and contradictory, making it difficult to learn an accurate NER
model. To address this challenge, we propose a conditional hidden Markov model
(CHMM), which can effectively infer true labels from multi-source noisy labels
in an unsupervised way. CHMM enhances the classic hidden Markov model with the
contextual representation power of pre-trained language models. Specifically,
CHMM learns token-wise transition and emission probabilities from the BERT
embeddings of the input tokens to infer the latent true labels from noisy
observations. We further refine CHMM with an alternate-training approach
(CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM,
and this BERT-NER's output is regarded as an additional weak source to train
the CHMM in return. Experiments on four NER benchmarks from various domains
show that our method outperforms state-of-the-art weakly supervised NER models
by wide margins.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12932</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12932</id><submitter>Xiaofei Ma</submitter><version version="v1"><date>Thu, 27 May 2021 04:00:22 GMT</date><size>5792kb</size><source_type>D</source_type></version><title>Contrastive Fine-tuning Improves Robustness for Neural Rankers</title><authors>Xiaofei Ma, Cicero Nogueira dos Santos and Andrew O. Arnold</authors><categories>cs.IR cs.CL</categories><journal-ref>Findings of ACL 2021</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of state-of-the-art neural rankers can deteriorate
substantially when exposed to noisy inputs or applied to a new domain. In this
paper, we present a novel method for fine-tuning neural rankers that can
significantly improve their robustness to out-of-domain data and query
perturbations. Specifically, a contrastive loss that compares data points in
the representation space is combined with the standard ranking loss during
fine-tuning. We use relevance labels to denote similar/dissimilar pairs, which
allows the model to learn the underlying matching semantics across different
query-document pairs and leads to improved robustness. In experiments with four
passage ranking datasets, the proposed contrastive fine-tuning method obtains
improvements on robustness to query reformulations, noise perturbations, and
zero-shot transfer for both BERT and BART based rankers. Additionally, our
experiments show that contrastive fine-tuning outperforms data augmentation for
robustifying neural rankers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12964</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12964</id><submitter>Tan Sixiang</submitter><version version="v1"><date>Thu, 27 May 2021 06:47:02 GMT</date><size>11578kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 15:56:13 GMT</date><size>11519kb</size><source_type>D</source_type></version><title>Feature Reuse and Fusion for Real-time Semantic segmentation</title><authors>Tan Sixiang</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For real-time semantic segmentation, how to increase the speed while
maintaining high resolution is a problem that has been discussed and solved.
Backbone design and fusion design have always been two essential parts of
real-time semantic segmentation. We hope to design a light-weight network based
on previous design experience and reach the level of state-of-the-art real-time
semantic segmentation without any pre-training. To achieve this goal, a
encoder-decoder architectures are proposed to solve this problem by applying a
decoder network onto a backbone model designed for real-time segmentation tasks
and designed three different ways to fuse semantics and detailed information in
the aggregation phase. We have conducted extensive experiments on two semantic
segmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show
that the proposed FRFNet strikes a balance between speed calculation and
accuracy. It achieves 69% Mean Intersection over Union (mIoU%) on the
Cityscapes test dataset with the speed of 132on a single RTX 2080Ti card. The
Code is available at https://github.com/favoMJ/FRFNet.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12969</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12969</id><submitter>Tiezheng Yu</submitter><version version="v1"><date>Thu, 27 May 2021 06:58:42 GMT</date><size>98kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 May 2021 04:38:58 GMT</date><size>98kb</size><source_type>D</source_type></version><title>Improve Query Focused Abstractive Summarization by Incorporating Answer
  Relevance</title><authors>Dan Su, Tiezheng Yu, Pascale Fung</authors><categories>cs.CL</categories><comments>The two authors contribute equally. Accepted as a short paper in
  Findings of ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Query focused summarization (QFS) models aim to generate summaries from
source documents that can answer the given query. Most previous work on QFS
only considers the query relevance criterion when producing the summary.
However, studying the effect of answer relevance in the summary generating
process is also important. In this paper, we propose QFS-BART, a model that
incorporates the explicit answer relevance of the source documents given the
query via a question answering model, to generate coherent and answer-related
summaries. Furthermore, our model can take advantage of large pre-trained
models which improve the summarization performance significantly. Empirical
results on the Debatepedia dataset show that the proposed model achieves the
new state-of-the-art performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.12972</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.12972</id><submitter>Tomohiro Nishiyama</submitter><version version="v1"><date>Thu, 27 May 2021 07:29:21 GMT</date><size>84kb</size></version><version version="v2"><date>Sun, 30 May 2021 07:31:49 GMT</date><size>84kb</size></version><title>Tight Lower Bounds for $\alpha$-Divergences Under Moment Constraints and
  Relations Between Different $\alpha$</title><authors>Tomohiro Nishiyama</authors><categories>cs.IT math.IT math.ST stat.TH</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\alpha$-divergences include Kullback-Leibler divergence, Hellinger
distance and $\chi^2$-divergence. We derive differntial and integral relations
between $\alpha$-divergences that are generalizations of the relation between
the Kullback-Leibler divergence and the $\chi^2$-divergence. We also show tight
lower bounds for $\alpha$-divergences under given means and variances. In
particular, we show a necessary and sufficient condition such that the binary
divergences, which are divergences between probability measures on the same
$2$-point set, always attain lower bounds. Kullback-Leibler divergence,
Hellinger distance, and $\chi^2$-divergence satisfy this condition.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13010</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13010</id><submitter>Yunfei Yang</submitter><version version="v1"><date>Thu, 27 May 2021 08:55:19 GMT</date><size>44kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 07:28:00 GMT</date><size>45kb</size></version><title>An error analysis of generative adversarial networks for learning
  distributions</title><authors>Jian Huang, Yuling Jiao, Zhen Li, Shiao Liu, Yang Wang, Yunfei Yang</authors><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies how well generative adversarial networks (GANs) learn
probability distributions from finite samples. Our main results estimate the
convergence rates of GANs under a collection of integral probability metrics
defined through H\&quot;older classes, including the Wasserstein distance as a
special case. We also show that GANs are able to adaptively learn data
distributions with low-dimensional structure or have H\&quot;older densities, when
the network architectures are chosen properly. In particular, for distributions
concentrate around a low-dimensional set, it is proved that the learning rates
of GANs do not depend on the high ambient dimension, but on the lower intrinsic
dimension. Our analysis is based on a new oracle inequality decomposing the
estimation error into generator and discriminator approximation error and
statistical error, which may be of independent interest.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13011</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13011</id><submitter>Subhayan De</submitter><version version="v1"><date>Thu, 27 May 2021 08:56:17 GMT</date><size>8051kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 19:54:39 GMT</date><size>11527kb</size><source_type>D</source_type></version><title>Neural Network Training Using $\ell_1$-Regularization and Bi-fidelity
  Data</title><authors>Subhayan De and Alireza Doostan</authors><categories>stat.ML cs.LG</categories><comments>28 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the capability of accurately representing a functional relationship
between the inputs of a physical system's model and output quantities of
interest, neural networks have become popular for surrogate modeling in
scientific applications. However, as these networks are over-parameterized,
their training often requires a large amount of data. To prevent overfitting
and improve generalization error, regularization based on, e.g., $\ell_1$- and
$\ell_2$-norms of the parameters is applied. Similarly, multiple connections of
the network may be pruned to increase sparsity in the network parameters. In
this paper, we explore the effects of sparsity promoting
$\ell_1$-regularization on training neural networks when only a small training
dataset from a high-fidelity model is available. As opposed to standard
$\ell_1$-regularization that is known to be inadequate, we consider two
variants of $\ell_1$-regularization informed by the parameters of an identical
network trained using data from lower-fidelity models of the problem at hand.
These bi-fidelity strategies are generalizations of transfer learning of neural
networks that uses the parameters learned from a large low-fidelity dataset to
efficiently train networks for a small high-fidelity dataset. We also compare
the bi-fidelity strategies with two $\ell_1$-regularization methods that only
use the high-fidelity dataset. Three numerical examples for propagating
uncertainty through physical systems are used to show that the proposed
bi-fidelity $\ell_1$-regularization strategies produce errors that are one
order of magnitude smaller than those of networks trained only using datasets
from the high-fidelity models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13014</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13014</id><submitter>Kazunori Matsui</submitter><version version="v1"><date>Thu, 27 May 2021 09:05:00 GMT</date><size>505kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 08:21:18 GMT</date><size>506kb</size><source_type>D</source_type></version><title>A projection method for Navier-Stokes equations with a boundary
  condition including the total pressure</title><authors>Kazunori Matsui</authors><categories>math.NA cs.NA math.AP</categories><comments>30 pages</comments><msc-class>65M12, 35Q30, 76D03, 76D05, 76M10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a projection method for time-dependent incompressible
Navier-Stokes equations with a total pressure boundary condition. The
projection method is one of the numerical calculation methods for
incompressible viscous fluids often used in engineering. In general, the
projection method needs additional boundary conditions to solve a
pressure-Poisson equation, which does not appear in the original Navier-Stokes
problem. On the other hand, many mechanisms generate flow by creating a
pressure difference, such as water distribution systems and blood circulation.
We propose a new additional boundary condition for the projection method with a
Dirichlet-type pressure boundary condition and no tangent flow. We demonstrate
stability for the scheme and establish error estimates for the velocity and
pressure under suitable norms. A numerical experiment verifies the theoretical
convergence results. Furthermore, the existence of a weak solution to the
original Navier-Stokes problem is proved by using the stability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13099</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13099</id><submitter>Nicolas Keriven</submitter><version version="v1"><date>Thu, 27 May 2021 12:52:36 GMT</date><size>2846kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 20:23:31 GMT</date><size>2893kb</size><source_type>D</source_type></version><title>On the Universality of Graph Neural Networks on Large Random Graphs</title><authors>Nicolas Keriven, Alberto Bietti, Samuel Vaiter</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximation power of Graph Neural Networks (GNNs) on latent
position random graphs. In the large graph limit, GNNs are known to converge to
certain &quot;continuous&quot; models known as c-GNNs, which directly enables a study of
their approximation power on random graph models. In the absence of input node
features however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism
test, c-GNNs will be severely limited on simple random graph models. For
instance, they will fail to distinguish the communities of a well-separated
Stochastic Block Model (SBM) with constant degree function. Thus, we consider
recently proposed architectures that augment GNNs with unique node identifiers,
referred to as Structural GNNs here (SGNNs). We study the convergence of SGNNs
to their continuous counterpart (c-SGNNs) in the large random graph limit,
under new conditions on the node identifiers. We then show that c-SGNNs are
strictly more powerful than c-GNNs in the continuous limit, and prove their
universality on several random graph models of interest, including most SBMs
and a large class of random geometric graphs. Our results cover both
permutation-invariant and permutation-equivariant architectures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13150</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13150</id><submitter>Haibo Jin</submitter><version version="v1"><date>Thu, 27 May 2021 13:51:42 GMT</date><size>1664kb</size></version><version version="v2"><date>Wed, 2 Jun 2021 10:07:37 GMT</date><size>1666kb</size></version><title>When Liebig's Barrel Meets Facial Landmark Detection: A Practical Model</title><authors>Haibo Jin, Jinpeng Li, Shengcai Liao, Ling Shao</authors><categories>cs.CV</categories><comments>Fix minor errors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, significant progress has been made in the research of facial
landmark detection. However, few prior works have thoroughly discussed about
models for practical applications. Instead, they often focus on improving a
couple of issues at a time while ignoring the others. To bridge this gap, we
aim to explore a practical model that is accurate, robust, efficient,
generalizable, and end-to-end trainable at the same time. To this end, we first
propose a baseline model equipped with one transformer decoder as detection
head. In order to achieve a better accuracy, we further propose two lightweight
modules, namely dynamic query initialization (DQInit) and query-aware memory
(QAMem). Specifically, DQInit dynamically initializes the queries of decoder
from the inputs, enabling the model to achieve as good accuracy as the ones
with multiple decoder layers. QAMem is designed to enhance the discriminative
ability of queries on low-resolution feature maps by assigning separate memory
values to each query rather than a shared one. With the help of QAMem, our
model removes the dependence on high-resolution feature maps and is still able
to obtain superior accuracy. Extensive experiments and analysis on three
popular benchmarks show the effectiveness and practical advantages of the
proposed model. Notably, our model achieves new state of the art on WFLW as
well as competitive results on 300W and COFW, while still running at 50+ FPS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13183</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13183</id><submitter>Xixi Tao</submitter><version version="v1"><date>Thu, 27 May 2021 14:37:08 GMT</date><size>1639kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 08:34:59 GMT</date><size>1856kb</size><source_type>D</source_type></version><title>An Efficient Style Virtual Try on Network for Clothing Business Industry</title><authors>Shanchen Pang, Xixi Tao, Neal N. Xiong, Yukun Dong</authors><categories>cs.CV</categories><comments>10 pages,9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the increasing development of garment manufacturing industry, the method
of combining neural network with industry to reduce product redundancy has been
paid more and more attention.In order to reduce garment redundancy and achieve
personalized customization, more researchers have appeared in the field of
virtual trying on.They try to transfer the target clothing to the reference
figure, and then stylize the clothes to meet user's requirements for
fashion.But the biggest problem of virtual try on is that the shape and motion
blocking distort the clothes, causing the patterns and texture on the clothes
to be impossible to restore. This paper proposed a new stylized virtual try on
network, which can not only retain the authenticity of clothing texture and
pattern, but also obtain the undifferentiated stylized try on. The network is
divided into three sub-networks, the first is the user image, the front of the
target clothing image, the semantic segmentation image and the posture heat map
to generate a more detailed human parsing map. Second, UV position map and
dense correspondence are used to map patterns and textures to the deformed
silhouettes in real time, so that they can be retained in real time, and the
rationality of spatial structure can be guaranteed on the basis of improving
the authenticity of images. Third,Stylize and adjust the generated virtual try
on image. Through the most subtle changes, users can choose the texture, color
and style of clothing to improve the user's experience.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13184</identifier>
 <datestamp>2021-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13184</id><submitter>Karjoun Hasan</submitter><version version="v1"><date>Tue, 25 May 2021 12:33:56 GMT</date><size>968kb</size></version><version version="v2"><date>Tue, 1 Jun 2021 20:02:08 GMT</date><size>968kb</size></version><title>Modelling of coupled surface and subsurface water flows</title><authors>Hasan Karjoun and Abdelaziz Beljadid</authors><categories>math.NA cs.NA</categories><comments>CSCE 2021 Annual Conference</comments><journal-ref>Springer 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to use the HLL finite volume scheme combined with
implicit techniques for modelling the coupled surface and subsurface water
flows. In our approach, we used the shallow water equations modelling surface
water flow with different source terms such as variable bottom topography,
friction effect, precipitation and infiltration. For subsurface water flow, the
Green-Amp equation is used to simulate the infiltration process through soils.
For solving the resulting nonlinear-coupled system of shallow water flow and
the Green-Ampt infiltration equations, the HLL finite volume schemes with
linear reconstructions of the solutions at the discrete level are implemented
in order to achieve the second-order accuracy of the scheme. Appropriate
discretization techniques are used for the source terms to guarantee the
well-balanced property of our numerical scheme. Numerical experiments are
performed to test the capability of the developed numerical scheme to simulate
the coupled surface and subsurface water flows.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13189</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13189</id><submitter>Zhiyong Zhou</submitter><version version="v1"><date>Wed, 26 May 2021 11:36:01 GMT</date><size>1924kb</size></version><version version="v2"><date>Thu, 3 Jun 2021 01:57:48 GMT</date><size>1924kb</size></version><title>Sparse recovery based on the generalized error function</title><authors>Zhiyong Zhou</authors><categories>math.NA cs.LG cs.NA eess.IV stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a novel sparse recovery method based on the
generalized error function. The penalty function introduced involves both the
shape and the scale parameters, making it very flexible. The theoretical
analysis results in terms of the null space property, the spherical section
property and the restricted invertibility factor are established for both
constrained and unconstrained models. The practical algorithms via both the
iteratively reweighted $\ell_1$ and the difference of convex functions
algorithms are presented. Numerical experiments are conducted to illustrate the
improvement provided by the proposed approach in various scenarios. Its
practical application in magnetic resonance imaging (MRI) reconstruction is
studied as well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13228</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13228</id><submitter>Zenan Ling</submitter><version version="v1"><date>Thu, 27 May 2021 15:17:41 GMT</date><size>51kb</size></version><version version="v2"><date>Mon, 31 May 2021 12:48:54 GMT</date><size>53kb</size></version><title>Optimization Induced Equilibrium Networks</title><authors>Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Yisen Wang, Guangcan Liu,
  Zhouchen Lin</authors><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implicit equilibrium models, i.e., deep neural networks (DNNs) defined by
implicit equations, have been becoming more and more attractive recently. In
this paper, we investigate an emerging question: can an implicit equilibrium
model's equilibrium point be regarded as the solution of an optimization
problem? To this end, we first decompose DNNs into a new class of unit layer
that is the proximal operator of an implicit convex function while keeping its
output unchanged. Then, the equilibrium model of the unit layer can be derived,
named Optimization Induced Equilibrium Networks (OptEq), which can be easily
extended to deep layers. The equilibrium point of OptEq can be theoretically
connected to the solution of its corresponding convex optimization problem with
explicit objectives. Based on this, we can flexibly introduce prior properties
to the equilibrium points: 1) modifying the underlying convex problems
explicitly so as to change the architectures of OptEq; and 2) merging the
information into the fixed point iteration, which guarantees to choose the
desired equilibrium point when the fixed point set is non-singleton. We show
that deep OptEq outperforms previous implicit models even with fewer
parameters. This work establishes the first step towards the
optimization-guided design of deep models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13244</identifier>
 <datestamp>2021-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13244</id><submitter>Roderick Karlemstrand</submitter><version version="v1"><date>Thu, 27 May 2021 15:41:45 GMT</date><size>1613kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 1 Jun 2021 14:57:46 GMT</date><size>1613kb</size><source_type>D</source_type></version><title>Using Early-Learning Regularization to Classify Real-World Noisy Data</title><authors>Alessio Galatolo, Alfred Nilsson, Roderick Karlemstrand, Yineng Wang</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The memorization problem is well-known in the field of computer vision. Liu
et al. propose a technique called Early-Learning Regularization, which improves
accuracy on the CIFAR datasets when label noise is present. This project
replicates their experiments and investigates the performance on a real-world
dataset with intrinsic noise. Results show that their experimental results are
consistent. We also explore Sharpness-Aware Minimization in addition to SGD and
observed a further 14.6 percentage points improvement. Future work includes
using all 6 million images and manually clean a fraction of the images to
fine-tune a transfer learning model. Last but not the least, having access to
clean data for testing would also improve the measurement of accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13262</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13262</id><submitter>Harideep Nair</submitter><version version="v1"><date>Thu, 27 May 2021 15:59:54 GMT</date><size>9726kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Jun 2021 21:51:41 GMT</date><size>10921kb</size><source_type>D</source_type></version><title>A Microarchitecture Implementation Framework for Online Learning with
  Temporal Neural Networks</title><authors>Harideep Nair, John Paul Shen and James E. Smith</authors><categories>cs.AR cs.ET cs.LG cs.NE</categories><comments>To be published in ISVLSI 2021. arXiv admin note: substantial text
  overlap with arXiv:2009.00457</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal Neural Networks (TNNs) are spiking neural networks that use time as
a resource to represent and process information, similar to the mammalian
neocortex. In contrast to compute-intensive deep neural networks that employ
separate training and inference phases, TNNs are capable of extremely efficient
online incremental/continual learning and are excellent candidates for building
edge-native sensory processing units. This work proposes a microarchitecture
framework for implementing TNNs using standard CMOS. Gate-level implementations
of three key building blocks are presented: 1) multi-synapse neurons, 2)
multi-neuron columns, and 3) unsupervised and supervised online learning
algorithms based on Spike Timing Dependent Plasticity (STDP). The proposed
microarchitecture is embodied in a set of characteristic scaling equations for
assessing the gate count, area, delay and power for any TNN design.
Post-synthesis results (in 45nm CMOS) for the proposed designs are presented,
and their online incremental learning capability is demonstrated.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13290</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13290</id><submitter>Ming Ding</submitter><version version="v1"><date>Wed, 26 May 2021 16:52:53 GMT</date><size>18378kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 28 May 2021 18:05:31 GMT</date><size>18756kb</size><source_type>D</source_type></version><title>CogView: Mastering Text-to-Image Generation via Transformers</title><authors>Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin,
  Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text-to-Image generation in the general domain has long been an open problem,
which requires both a powerful generative model and cross-modal understanding.
We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to
advance this problem. We also demonstrate the finetuning strategies for various
downstream tasks, e.g. style learning, super-resolution, text-image ranking and
fashion design, and methods to stabilize pretraining, e.g. eliminating NaN
losses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS
COCO, outperforms previous GAN-based models and a recent similar work DALL-E.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13345</identifier>
 <datestamp>2021-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13345</id><submitter>Ishan Durugkar</submitter><version version="v1"><date>Thu, 27 May 2021 17:51:34 GMT</date><size>3159kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 30 May 2021 22:13:00 GMT</date><size>2513kb</size><source_type>D</source_type></version><title>Adversarial Intrinsic Motivation for Reinforcement Learning</title><authors>Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning with an objective to minimize the mismatch with a reference
distribution has been shown to be useful for generative modeling and imitation
learning. In this paper, we investigate whether one such objective, the
Wasserstein-1 distance between a policy's state visitation distribution and a
target distribution, can be utilized effectively for reinforcement learning
(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement
learning where the idealized (unachievable) target distribution has full
measure at the goal. We introduce a quasimetric specific to Markov Decision
Processes (MDPs), and show that the policy that minimizes the Wasserstein-1
distance of its state visitation distribution to this target distribution under
this quasimetric is the policy that reaches the goal in as few steps as
possible. Our approach, termed Adversarial Intrinsic Motivation (AIM),
estimates this Wasserstein-1 distance through its dual objective and uses it to
compute a supplemental reward function. Our experiments show that this reward
function changes smoothly with respect to transitions in the MDP and assists
the agent in learning. Additionally, we combine AIM with Hindsight Experience
Replay (HER) and show that the resulting algorithm accelerates learning
significantly on several simulated robotics tasks when compared to HER with a
sparse positive reward at the goal state.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.13351</identifier>
 <datestamp>2021-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.13351</id><submitter>Drew Linsley</submitter><version version="v1"><date>Thu, 27 May 2021 17:56:37 GMT</date><size>20020kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 3 Jun 2021 00:26:33 GMT</date><size>20021kb</size><source_type>D</source_type></version><title>Tracking Without Re-recognition in Humans and Machines</title><authors>Drew Linsley, Girik Malik, Junkyung Kim, Lakshmi N Govindarajan, Ennio
  Mingolla, and Thomas Serre</authors><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Imagine trying to track one particular fruitfly in a swarm of hundreds.
Higher biological visual systems have evolved to track moving objects by
relying on both appearance and motion features. We investigate if
state-of-the-art deep neural networks for visual tracking are capable of the
same. For this, we introduce PathTracker, a synthetic visual challenge that
asks human observers and machines to track a target object in the midst of
identical-looking &quot;distractor&quot; objects. While humans effortlessly learn
PathTracker and generalize to systematic variations in task design,
state-of-the-art deep networks struggle. To address this limitation, we
identify and model circuit mechanisms in biological brains that are implicated
in tracking objects based on motion cues. When instantiated as a recurrent
network, our circuit model learns to solve PathTracker with a robust visual
strategy that rivals human performance and explains a significant proportion of
their decision-making on the challenge. We also show that the success of this
circuit model extends to object tracking in natural videos. Adding it to a
transformer-based architecture for object tracking builds tolerance to visual
nuisances that affect object appearance, resulting in a new state-of-the-art
performance on the large-scale TrackingNet object tracking challenge. Our work
highlights the importance of building artificial vision models that can help us
better understand human vision and improve computer vision.
</abstract></arXivRaw>
</metadata>
</record>
<resumptionToken cursor="0" completeListSize="2583">5392151|1001</resumptionToken>
</ListRecords>
</OAI-PMH>
