<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2021-05-19T07:18:15Z</responseDate>
<request verb="ListRecords" from="2021-05-18" metadataPrefix="arXivRaw" set="cs">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0628</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1411.0628</id><submitter>Valerii Sopin</submitter><version version="v1"><date>Mon, 3 Nov 2014 19:41:08 GMT</date><size>4kb</size></version><version version="v10"><date>Mon, 19 Sep 2016 19:40:19 GMT</date><size>5kb</size></version><version version="v11"><date>Thu, 29 Apr 2021 17:42:35 GMT</date><size>8kb</size></version><version version="v12"><date>Mon, 3 May 2021 07:16:41 GMT</date><size>8kb</size></version><version version="v13"><date>Sat, 15 May 2021 06:36:59 GMT</date><size>8kb</size></version><version version="v2"><date>Tue, 4 Nov 2014 15:29:22 GMT</date><size>6kb</size></version><version version="v3"><date>Wed, 5 Nov 2014 01:20:47 GMT</date><size>6kb</size></version><version version="v4"><date>Tue, 18 Nov 2014 22:45:44 GMT</date><size>6kb</size></version><version version="v5"><date>Wed, 21 Jan 2015 08:11:18 GMT</date><size>5kb</size></version><version version="v6"><date>Tue, 3 Feb 2015 02:14:00 GMT</date><size>5kb</size></version><version version="v7"><date>Wed, 4 Feb 2015 23:09:45 GMT</date><size>6kb</size></version><version version="v8"><date>Sun, 8 Feb 2015 01:49:50 GMT</date><size>6kb</size></version><version version="v9"><date>Fri, 8 May 2015 15:58:54 GMT</date><size>0kb</size><source_type>I</source_type></version><title>PH = PSPACE</title><authors>Valerii Sopin</authors><categories>cs.CC cs.DS</categories><comments>The author greatly appreciate suggestions and help of Professor Lew
  Gordeew. The Computability in Europe conference (CiE), 5-9 July 2021</comments><msc-class>03G05, 03B70, 68Q12, 68Q15, 68Q25</msc-class><acm-class>F.2; F.4; G.2; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that PSPACE is equal to 4th level in the polynomial
hierarchy. We also deduce a lot of important consequences.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1501.05683</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1501.05683</id><submitter>Ling Liu</submitter><version version="v1"><date>Thu, 22 Jan 2015 22:57:45 GMT</date><size>131kb</size></version><version version="v2"><date>Wed, 15 Apr 2015 23:39:47 GMT</date><size>131kb</size></version><version version="v3"><date>Mon, 2 Nov 2015 16:09:31 GMT</date><size>2375kb</size></version><version version="v4"><date>Mon, 26 Apr 2021 23:33:20 GMT</date><size>2828kb</size></version><version version="v5"><date>Sat, 15 May 2021 13:45:44 GMT</date><size>2828kb</size></version><title>Polar Lattices for Lossy Compression</title><authors>Ling Liu and Jinwen Shi and Cong Ling</authors><categories>cs.IT math.IT</categories><comments>63 pages, 18 figure</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Polar lattices, which are constructed from polar codes, have recently been
proved to be able to achieve the capacity of the additive white Gaussian noise
(AWGN) channel. In this work, we propose a new construction of polar lattices
to solve the dual problem, i.e., achieving the rate-distortion bound of a
memoryless Gaussian source, which means that polar lattices can also be good
for the lossy compression of continuous sources. The structure of the proposed
polar lattices enables us to integrate the post-entropy coding process into the
lattice quantizer, which simplifies the quantization process. The overall
complexity of encoding and decoding complexity is $O(N \log^2 N)$ for a
sub-exponentially decaying excess distortion. Moreover, the nesting structure
of polar lattices further provides solutions for some multi-terminal coding
problems. The Wyner-Ziv coding problem for a Gaussian source can be solved by
an AWGN capacity-achieving polar lattice nested in a rate-distortion bound
achieving one, and the Gelfand-Pinsker problem can be solved in a reversed
manner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1608.03533</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1608.03533</id><submitter>Chitta Ranjan</submitter><version version="v1"><date>Thu, 11 Aug 2016 16:59:19 GMT</date><size>1852kb</size><source_type>D</source_type></version><version version="v10"><date>Thu, 27 Feb 2020 19:47:52 GMT</date><size>855kb</size><source_type>D</source_type></version><version version="v11"><date>Wed, 4 Mar 2020 14:54:16 GMT</date><size>855kb</size><source_type>D</source_type></version><version version="v12"><date>Fri, 8 May 2020 20:03:02 GMT</date><size>891kb</size><source_type>D</source_type></version><version version="v13"><date>Thu, 29 Oct 2020 11:49:41 GMT</date><size>888kb</size><source_type>D</source_type></version><version version="v14"><date>Tue, 18 May 2021 00:03:21 GMT</date><size>893kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 12 Aug 2016 14:01:03 GMT</date><size>1852kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 23 Aug 2016 20:03:41 GMT</date><size>1853kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 28 Sep 2016 00:20:49 GMT</date><size>679kb</size><source_type>D</source_type></version><version version="v5"><date>Wed, 19 Oct 2016 05:04:59 GMT</date><size>419kb</size><source_type>D</source_type></version><version version="v6"><date>Sun, 27 Nov 2016 01:43:12 GMT</date><size>435kb</size><source_type>D</source_type></version><version version="v7"><date>Wed, 30 Nov 2016 06:35:26 GMT</date><size>435kb</size><source_type>D</source_type></version><version version="v8"><date>Tue, 31 Jan 2017 03:50:58 GMT</date><size>434kb</size><source_type>D</source_type></version><version version="v9"><date>Sun, 30 Apr 2017 07:21:43 GMT</date><size>568kb</size><source_type>D</source_type></version><title>Sequence Graph Transform (SGT): A Feature Embedding Function for
  Sequence Data Mining</title><authors>Chitta Ranjan, Samaneh Ebrahimi and Kamran Paynabar</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence feature embedding is a challenging task due to un-structuredness of
sequences -- arbitrary strings of arbitrary length. Existing methods are
efficient in extracting short-term dependencies but typically suffer from
computation issues for the long-term. Sequence Graph Transform (SGT), a feature
embedding function, that can extract varying amount of short- to long-term
dependencies without increasing the computation is proposed. SGT's properties
are analytically proved for interpretation under normal and uniform
distribution assumptions. SGT features yield significantly superior results in
sequence clustering and classification with higher accuracy and lower
computation as compared to the existing methods, including the state-of-the-art
sequence/string Kernels and LSTM.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1711.08264</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1711.08264</id><submitter>Priyanka Dey</submitter><version version="v1"><date>Wed, 22 Nov 2017 13:14:59 GMT</date><size>107kb</size></version><version version="v2"><date>Thu, 5 Mar 2020 07:31:47 GMT</date><size>46kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 05:41:40 GMT</date><size>67kb</size><source_type>D</source_type></version><title>Complexity of constrained sensor placement problems for optimal
  observability</title><authors>Priyanka Dey, Niranjan Balachandran, and Debasish Chatterjee</authors><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies two problems related to observability and efficient
constrained sensor placement in linear time-invariant discrete-time systems
with partial state observations. (i) We impose the condition that both the set
of outputs and the state that each output can measure are pre-specified. We
establish that for any fixed \(k &gt; 2\), the problem of placing the minimum
number of sensors/outputs required to ensure that the structural observability
index is at most \(k\), is NP-complete. Conversely, we identify a subclass of
systems whose structures are directed trees with self-loops at every state
vertex, for which the problem can be solved in linear time. (ii) Assuming that
the set of states that each given output can measure is given, we prove that
the problem of selecting a pre-assigned number of sensors in order to maximize
the number of states of the system that are structurally observable is also
NP-hard. As an application, we identify suitable conditions on the system
structure under which there exists an efficient greedy strategy, which we
provide, to obtain a \((1-\frac{1}{e})\)-approximate solution. An illustration
of the techniques developed for this problem is given on the benchmark IEEE
118-bus power network containing roughly \(400\) states in its linearized
model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.04573</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1801.04573</id><submitter>Paola Boito</submitter><version version="v1"><date>Sun, 14 Jan 2018 15:28:49 GMT</date><size>159kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 12 Feb 2020 20:47:43 GMT</date><size>283kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 12:55:25 GMT</date><size>213kb</size><source_type>D</source_type></version><title>Computing the Reciprocal of a $\phi$-function by Rational Approximation</title><authors>Paola Boito, Yuli Eidelman, Luca Gemignani</authors><categories>math.NA cs.NA</categories><msc-class>65F60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a family of rational approximations of the
reciprocal of a $\phi$-function involved in the explicit solutions of certain
linear differential equations, as well as in integration schemes evolving on
manifolds. The derivation and properties of this family of approximations
applied to scalar and matrix arguments are presented. Moreover, we show that
the matrix functions computed by these approximations exhibit decaying
properties comparable to the best existing theoretical bounds. Numerical
examples highlight the benefits of the proposed rational approximations
w.r.t.~the classical Taylor polynomials and other rational functions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1804.00308</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1804.00308</id><submitter>Matthew Jagielski</submitter><version version="v1"><date>Sun, 1 Apr 2018 15:56:43 GMT</date><size>974kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 22:17:47 GMT</date><size>1490kb</size><source_type>D</source_type></version><title>Manipulating Machine Learning: Poisoning Attacks and Countermeasures for
  Regression Learning</title><authors>Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina
  Nita-Rotaru, Bo Li</authors><categories>cs.CR cs.GT cs.LG</categories><comments>Preprint of the work accepted for publication at the 39th IEEE
  Symposium on Security and Privacy, San Francisco, CA, USA, May 21-23, 2018;
  May 14 '21 update: fixed bug in TRIM algorithm which led to incorrect results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As machine learning becomes widely used for automated decisions, attackers
have strong incentives to manipulate the results and models generated by
machine learning algorithms. In this paper, we perform the first systematic
study of poisoning attacks and their countermeasures for linear regression
models. In poisoning attacks, attackers deliberately influence the training
data to manipulate the results of a predictive model. We propose a
theoretically-grounded optimization framework specifically designed for linear
regression and demonstrate its effectiveness on a range of datasets and models.
We also introduce a fast statistical attack that requires limited knowledge of
the training process. Finally, we design a new principled defense method that
is highly resilient against all poisoning attacks. We provide formal guarantees
about its convergence and an upper bound on the effect of poisoning attacks
when the defense is deployed. We evaluate extensively our attacks and defenses
on three realistic datasets from health care, loan assessment, and real estate
domains.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1804.00836</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1804.00836</id><submitter>Canh Hao Nguyen</submitter><version version="v1"><date>Tue, 3 Apr 2018 06:16:35 GMT</date><size>130kb</size><source_type>D</source_type></version><title>Learning on Hypergraphs with Sparsity</title><authors>Canh Hao Nguyen, Hiroshi Mamitsuka</authors><categories>stat.ML cs.LG</categories><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence
  (2020)</journal-ref><doi>10.1109/TPAMI.2020.2974746</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypergraph is a general way of representing high-order relations on a set of
objects. It is a generalization of graph, in which only pairwise relations can
be represented. It finds applications in various domains where relationships of
more than two objects are observed. On a hypergraph, as a generalization of
graph, one wishes to learn a smooth function with respect to its topology. A
fundamental issue is to find suitable smoothness measures of functions on the
nodes of a graph/hypergraph. We show a general framework that generalizes
previously proposed smoothness measures and also gives rise to new ones. To
address the problem of irrelevant or noisy data, we wish to incorporate sparse
learning framework into learning on hypergraphs. We propose sparsely smooth
formulations that learn smooth functions and induce sparsity on hypergraphs at
both hyperedge and node levels. We show their properties and sparse support
recovery results. We conduct experiments to show that our sparsely smooth
models have benefits to irrelevant and noisy data, and usually give similar or
improved performances compared to dense models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04440</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1809.04440</id><submitter>Yunsheng Bai</submitter><version version="v1"><date>Mon, 10 Sep 2018 22:48:49 GMT</date><size>844kb</size></version><version version="v2"><date>Sun, 16 May 2021 20:23:19 GMT</date><size>8592kb</size><source_type>D</source_type></version><title>Learning-based Efficient Graph Similarity Computation via Multi-Scale
  Convolutional Set Matching</title><authors>Yunsheng Bai, Hao Ding, Yizhou Sun, Wei Wang</authors><categories>cs.LG stat.ML</categories><comments>AAAI 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph similarity computation is one of the core operations in many
graph-based applications, such as graph similarity search, graph database
analysis, graph clustering, etc. Since computing the exact distance/similarity
between two graphs is typically NP-hard, a series of approximate methods have
been proposed with a trade-off between accuracy and speed. Recently, several
data-driven approaches based on neural networks have been proposed, most of
which model the graph-graph similarity as the inner product of their
graph-level representations, with different techniques proposed for generating
one embedding per graph. However, using one fixed-dimensional embedding per
graph may fail to fully capture graphs in varying sizes and link structures, a
limitation that is especially problematic for the task of graph similarity
computation, where the goal is to find the fine-grained difference between two
graphs. In this paper, we address the problem of graph similarity computation
from another perspective, by directly matching two sets of node embeddings
without the need to use fixed-dimensional vectors to represent whole graphs for
their similarity computation. The model, GraphSim, achieves the
state-of-the-art performance on four real-world graph datasets under six out of
eight settings (here we count a specific dataset and metric combination as one
setting), compared to existing popular methods for approximate Graph Edit
Distance (GED) and Maximum Common Subgraph (MCS) computation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01187</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1810.01187</id><submitter>Zixin Zhong</submitter><version version="v1"><date>Tue, 2 Oct 2018 11:55:54 GMT</date><size>992kb</size></version><version version="v2"><date>Wed, 12 Jun 2019 13:08:10 GMT</date><size>619kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 8 May 2020 16:56:18 GMT</date><size>1977kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 16 May 2021 03:20:10 GMT</date><size>7604kb</size><source_type>D</source_type></version><title>Thompson Sampling Algorithms for Cascading Bandits</title><authors>Zixin Zhong, Wang Chi Cheung, Vincent Y. F. Tan</authors><categories>cs.LG stat.ML</categories><comments>62 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the pressing need for efficient optimization in online
recommender systems, we revisit the cascading bandit model proposed by Kveton
et al. (2015). While Thompson sampling (TS) algorithms have been shown to be
empirically superior to Upper Confidence Bound (UCB) algorithms for cascading
bandits, theoretical guarantees are only known for the latter. In this paper,
we first provide a problem-dependent upper bound on the regret of a TS
algorithm with Beta-Bernoulli updates; this upper bound is tighter than a
recent derivation under a more general setting by Huyuk and Tekin (2019). Next,
we design and analyze another TS algorithm with Gaussian updates, TS-Cascade.
TS-Cascade achieves the state-of-the-art regret bound for cascading bandits.
Complementarily, we consider a linear generalization of the cascading bandit
model, which allows efficient learning in large cascading bandit problem
instances. We introduce and analyze a TS algorithm, which enjoys a regret bound
that depends on the dimension of the linear model but not the number of items.
Finally, by using information-theoretic techniques and judiciously constructing
cascading bandit instances, we derive a nearly matching regret lower bound for
the standard model. Our paper establishes the first theoretical guarantees on
TS algorithms for stochastic combinatorial bandit problem model with partial
feedback. Numerical experiments demonstrate the superiority of the proposed TS
algorithms compared to existing UCB-based ones.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05835</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1810.05835</id><submitter>Manuel Soriano-Trigueros</submitter><version version="v1"><date>Sat, 13 Oct 2018 09:43:14 GMT</date><size>644kb</size><source_type>D</source_type></version><title>Characterising epithelial tissues using persistent entropy</title><authors>N. Atienza, L.M. Escudero, M.J. Jimenez, M. Soriano-Trigueros</authors><categories>eess.IV cs.CV q-bio.QM</categories><comments>12 pages, 7 figures, 4 tables</comments><msc-class>68T10, 92B99, 65D18 94A17, 55N99, 5504</msc-class><doi>10.1007/978-3-030-10828-1_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply persistent entropy, a novel topological statistic,
for characterization of images of epithelial tissues. We have found out that
persistent entropy is able to summarize topological and geometric information
encoded by \alpha-complexes and persistent homology. After using some
statistical tests, we can guarantee the existence of significant differences in
the studied tissues.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10188</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1810.10188</id><submitter>Sanket Biswas</submitter><version version="v1"><date>Wed, 24 Oct 2018 05:08:08 GMT</date><size>379kb</size></version><title>Fault Area Detection in Leaf Diseases using k-means Clustering</title><authors>Subhajit Maity, Sujan Sarkar, Avinaba Tapadar, Ayan Dutta, Sanket
  Biswas, Sayon Nayek, Pritam Saha</authors><categories>cs.CV</categories><comments>This article is of 5 pages in IEEE format. It has been presented as a
  full paper in International Conference on Trends in Electronics and
  Informatics (ICOEI 2018) and is currently under the proceedings of the
  conference and yet to be published in IEEE Xplore</comments><doi>10.1109/ICOEI.2018.8553913</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With increasing population the crisis of food is getting bigger day by day.In
this time of crisis,the leaf disease of crops is the biggest problem in the
food industry.In this paper, we have addressed that problem and proposed an
efficient method to detect leaf disease.Leaf diseases can be detected from
sample images of the leaf with the help of image processing and
segmentation.Using k-means clustering and Otsu's method the faulty region in a
leaf is detected which helps to determine proper course of action to be
taken.Further the ratio of normal and faulty region if calculated would be able
to predict if the leaf can be cured at all.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1812.01482</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1812.01482</id><submitter>Rogers Mathew</submitter><version version="v1"><date>Tue, 4 Dec 2018 15:23:48 GMT</date><size>10kb</size></version><version version="v2"><date>Mon, 19 Aug 2019 08:55:23 GMT</date><size>11kb</size></version><version version="v3"><date>Wed, 19 Feb 2020 05:04:11 GMT</date><size>102kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 10 Jun 2020 07:51:30 GMT</date><size>33kb</size></version><version version="v5"><date>Sun, 16 May 2021 12:55:37 GMT</date><size>28kb</size></version><title>Target Set Selection parameterized by vertex cover and more</title><authors>Suman Banerjee, Rogers Mathew, and Fahad Panolan</authors><categories>cs.CC cs.DS cs.SI</categories><comments>21 pages</comments><msc-class>68W25, 68Q17, 68R10,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a simple, undirected graph $G$ with a threshold function $\tau:V(G)
\rightarrow \mathbb{N}$, the \textsc{Target Set Selection} (TSS) problem is
about choosing a minimum cardinality set, say $S \subseteq V(G)$, such that
starting a diffusion process with $S$ as its seed set will eventually result in
activating all the nodes in $G$. For any non-negative integer $i$, we say a set
$T\subseteq V(G)$ is a &quot;degree-$i$ modulator&quot; of $G$ if the degree of any
vertex in the graph $G-T$ is at most $i$. Degree-$0$ modulators of a graph are
precisely its vertex covers. Consider a graph $G$ on $n$ vertices and $m$
edges. We have the following results on the TSS problem:
  -&gt; It was shown by Nichterlein et al. [Social Network Analysis and Mining,
2013] that it is possible to compute an optimal-sized target set in
$O(2^{(2^{t}+1)t}\cdot m)$ time, where $t$ denotes the cardinality of a minimum
degree-$0$ modulator of $G$. We improve this result by designing an algorithm
running in time $2^{O(t\log t)}n^{O(1)}$.
  -&gt; We design a $2^{2^{O(t)}}n^{O(1)}$ time algorithm to compute an optimal
target set for $G$, where $t$ is the size of a minimum degree-$1$ modulator of
$G$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1812.05506</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1812.05506</id><submitter>Kim Peter Wabersich</submitter><version version="v1"><date>Thu, 13 Dec 2018 16:31:39 GMT</date><size>441kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 4 Apr 2019 15:01:48 GMT</date><size>510kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 17 Feb 2021 13:57:28 GMT</date><size>6112kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 17 May 2021 14:34:01 GMT</date><size>3056kb</size><source_type>D</source_type></version><title>A predictive safety filter for learning-based control of constrained
  nonlinear dynamical systems</title><authors>Kim P. Wabersich and Melanie N. Zeilinger</authors><categories>cs.SY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transfer of reinforcement learning (RL) techniques into real-world
applications is challenged by safety requirements in the presence of physical
limitations. Most RL methods, in particular the most popular algorithms, do not
support explicit consideration of state and input constraints. In this paper,
we address this problem for nonlinear systems with continuous state and input
spaces by introducing a predictive safety filter, which is able to turn a
constrained dynamical system into an unconstrained safe system and to which any
RL algorithm can be applied `out-of-the-box'. The predictive safety filter
receives the proposed control input and decides, based on the current system
state, if it can be safely applied to the real system, or if it has to be
modified otherwise. Safety is thereby established by a continuously updated
safety policy, which is based on a model predictive control formulation using a
data-driven system model and considering state and input dependent
uncertainties.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.01522</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1902.01522</id><submitter>Jialei Chen</submitter><version version="v1"><date>Tue, 5 Feb 2019 02:49:09 GMT</date><size>7498kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 1 Sep 2019 14:53:14 GMT</date><size>7498kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 12 Sep 2019 22:58:31 GMT</date><size>8541kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 4 Aug 2020 15:04:32 GMT</date><size>10249kb</size><source_type>D</source_type></version><title>Active Image Synthesis for Efficient Labeling</title><authors>Jialei Chen, Yujia Xie, Kan Wang, Chuck Zhang, Mani A. Vannan, Ben
  Wang, Zhen Qian</authors><categories>cs.CV</categories><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2020</journal-ref><doi>10.1109/TPAMI.2020.2993221</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The great success achieved by deep neural networks attracts increasing
attention from the manufacturing and healthcare communities. However, the
limited availability of data and high costs of data collection are the major
challenges for the applications in those fields. We propose in this work AISEL,
an active image synthesis method for efficient labeling to improve the
performance of the small-data learning tasks. Specifically, a complementary
AISEL dataset is generated, with labels actively acquired via a physics-based
method to incorporate underlining physical knowledge at hand. An important
component of our AISEL method is the bidirectional generative invertible
network (GIN), which can extract interpretable features from the training
images and generate physically meaningful virtual images. Our AISEL method then
efficiently samples virtual images not only further exploits the uncertain
regions, but also explores the entire image space. We then discuss the
interpretability of GIN both theoretically and experimentally, demonstrating
clear visual improvements over the benchmarks. Finally, we demonstrate the
effectiveness of our AISEL framework on aortic stenosis application, in which
our method lower the labeling cost by $90\%$ while achieving a $15\%$
improvement in prediction accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.03667</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1902.03667</id><submitter>L. Thorne McCarty</submitter><version version="v1"><date>Sun, 10 Feb 2019 20:30:29 GMT</date><size>4014kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 17:13:45 GMT</date><size>7954kb</size><source_type>D</source_type></version><title>Differential Similarity in Higher Dimensional Spaces: Theory and
  Applications</title><authors>L. Thorne McCarty</authors><categories>cs.LG stat.ML</categories><comments>67 pages, 29 figures. Revised and expanded to include Section 7 on
  the CIFAR-10 dataset</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an extension and an elaboration of the theory of
differential similarity, which was originally proposed in arXiv:1401.2411
[cs.LG]. The goal is to develop an algorithm for clustering and coding that
combines a geometric model with a probabilistic model in a principled way. For
simplicity, the geometric model in the earlier paper was restricted to the
three-dimensional case. The present paper removes this restriction, and
considers the full $n$-dimensional case. Although the mathematical model is the
same, the strategies for computing solutions in the $n$-dimensional case are
different, and one of the main purposes of this paper is to develop and analyze
these strategies. Another main purpose is to devise techniques for estimating
the parameters of the model from sample data, again in $n$ dimensions. We
evaluate the solution strategies and the estimation techniques by applying them
to two familiar real-world examples: the classical MNIST dataset and the
CIFAR-10 dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.04864</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1902.04864</id><submitter>Shoujin Wang</submitter><version version="v1"><date>Wed, 13 Feb 2019 11:12:47 GMT</date><size>4048kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 13 Dec 2020 12:07:31 GMT</date><size>444kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 13:28:09 GMT</date><size>21251kb</size><source_type>D</source_type></version><title>A Survey on Session-based Recommender Systems</title><authors>Shoujin Wang, Longbing Cao, Yan Wang, Quan Z. Sheng, Mehmet Orgun,
  Defu Lian</authors><categories>cs.IR</categories><comments>Accepted by ACM Computing Surveys. 39 pages, 163 references, 11
  sections, 4 figures and 11 tables, the latest and most comprehensive survey
  on session-based recommender systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems (RSs) have been playing an increasingly important role
for informed consumption, services, and decision-making in the overloaded
information era and digitized economy. In recent years, session-based
recommender systems (SBRSs) have emerged as a new paradigm of RSs. Different
from other RSs such as content-based RSs and collaborative filtering-based RSs
which usually model long-term yet static user preferences, SBRSs aim to capture
short-term but dynamic user preferences to provide more timely and accurate
recommendations sensitive to the evolution of their session contexts. Although
SBRSs have been intensively studied, neither unified problem statements for
SBRSs nor in-depth elaboration of SBRS characteristics and challenges are
available. It is also unclear to what extent SBRS challenges have been
addressed and what the overall research landscape of SBRSs is. This
comprehensive review of SBRSs addresses the above aspects by exploring in depth
the SBRS entities (e.g., sessions), behaviours (e.g., users' clicks on items)
and their properties (e.g., session length). We propose a general problem
statement of SBRSs, summarize the diversified data characteristics and
challenges of SBRSs, and define a taxonomy to categorize the representative
SBRS research. Finally, we discuss new research opportunities in this exciting
and vibrant area.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.06467</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1902.06467</id><submitter>Manuel Soriano-Trigueros</submitter><version version="v1"><date>Mon, 18 Feb 2019 09:03:07 GMT</date><size>763kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 26 Feb 2019 16:57:45 GMT</date><size>665kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 27 Feb 2019 08:16:15 GMT</date><size>665kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 17 May 2019 14:23:54 GMT</date><size>2221kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 18 May 2021 12:33:52 GMT</date><size>3176kb</size><source_type>D</source_type></version><title>Stable Topological Summaries for Analyzing the Organization of Cells in
  a Packed Tissue</title><authors>N. Atienza, M. J. Jimenez, M. Soriano-Trigueros</authors><categories>cs.CV</categories><msc-class>68T10, 92B99, 65D18, 94A17, 55N99, 5504</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Topological Data Analysis tools for studying the inner organization of
cells in segmented images of epithelial tissues. More specifically, for each
segmented image, we compute different persistence barcodes, which codify
lifetime of homology classes (persistent homology) along different filtrations
(increasing nested sequences of simplicial complexes) that are built from the
regions representing the cells in the tissue. We use a complete and
well-grounded set of numerical variables over those persistence barcodes, also
known as topological summaries. A novel combination of normalization methods
for both, the set of input segmented images and the produced barcodes, allows
to prove stability results for those variables with respect to small changes in
the input, as well as invariance to image scale. Our study provides new
insights to this problem, such as a possible novel indicator for the
development of the drosophila wing disc tissue or the importance of centroids
distribution to differentiate some tissues from their CVT-path counterpart (a
mathematical model of epithelia based on Voronoi diagrams). We also show how
the use of topological summaries may improve the classification accuracy of
epithelial images using Random Forests algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00268</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1903.00268</id><submitter>Margarita Grinvald</submitter><version version="v1"><date>Fri, 1 Mar 2019 12:32:46 GMT</date><size>6312kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 10 Jul 2019 14:39:33 GMT</date><size>7325kb</size><source_type>D</source_type></version><title>Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery</title><authors>Margarita Grinvald, Fadri Furrer, Tonci Novkovic, Jen Jen Chung, Cesar
  Cadena, Roland Siegwart, Juan Nieto</authors><categories>cs.RO cs.CV</categories><comments>8 pages, 4 figures. To be published in IEEE Robotics and Automation
  Letters (RA-L) and 2019 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS). Accompanying video material can be found at
  http://youtu.be/Jvl42VJmYxg</comments><journal-ref>IEEE Robotics and Automation Letters, vol. 4, no. 3, pp.
  3037-3044, July 2019</journal-ref><doi>10.1109/LRA.2019.2923960</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To autonomously navigate and plan interactions in real-world environments,
robots require the ability to robustly perceive and map complex, unstructured
surrounding scenes. Besides building an internal representation of the observed
scene geometry, the key insight toward a truly functional understanding of the
environment is the usage of higher-level entities during mapping, such as
individual object instances. We propose an approach to incrementally build
volumetric object-centric maps during online scanning with a localized RGB-D
camera. First, a per-frame segmentation scheme combines an unsupervised
geometric approach with instance-aware semantic object predictions. This allows
us to detect and segment elements both from the set of known classes and from
other, previously unseen categories. Next, a data association step tracks the
predicted instances across the different frames. Finally, a map integration
strategy fuses information about their 3D shape, location, and, if available,
semantic class into a global volume. Evaluation on a publicly available dataset
shows that the proposed approach for building instance-level semantic maps is
competitive with state-of-the-art methods, while additionally able to discover
objects of unseen categories. The system is further evaluated within a
real-world robotic mapping setup, for which qualitative results highlight the
online nature of the method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00979</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1903.00979</id><submitter>Sarthak Chatterjee</submitter><version version="v1"><date>Sun, 3 Mar 2019 20:09:39 GMT</date><size>72kb</size></version><version version="v2"><date>Tue, 1 Oct 2019 17:48:29 GMT</date><size>72kb</size></version><version version="v3"><date>Mon, 18 Nov 2019 22:37:00 GMT</date><size>505kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 18 May 2021 04:46:31 GMT</date><size>979kb</size><source_type>D</source_type></version><title>Analysis of a Generalized Expectation-Maximization Algorithm for
  Gaussian Mixture Models: A Control Systems Perspective</title><authors>Sarthak Chatterjee, Orlando Romero, S\'ergio Pequito</authors><categories>math.OC cs.LG cs.SY math.DS stat.ML</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Expectation-Maximization (EM) algorithm is one of the most popular
methods used to solve the problem of parametric distribution-based clustering
in unsupervised learning. In this paper, we propose to analyze a generalized EM
(GEM) algorithm in the context of Gaussian mixture models, where the
maximization step in the EM is replaced by an increasing step. We show that
this GEM algorithm can be understood as a linear time-invariant (LTI) system
with a feedback nonlinearity. Therefore, we explore some of its convergence
properties by leveraging tools from robust control theory. Lastly, we explain
how the proposed GEM can be designed, and present a pedagogical example to
understand the advantages of the proposed approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12626</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1904.12626</id><submitter>Francisco Bischoff</submitter><version version="v1"><date>Thu, 18 Apr 2019 05:27:09 GMT</date><size>174kb</size><source_type>D</source_type></version><title>tsmp: An R Package for Time Series with Matrix Profile</title><authors>Francisco Bischoff, Pedro Pereira Rodrigues</authors><categories>cs.DB cs.IR</categories><doi>10.32614/RJ-2020-021</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This article describes tsmp, an R package that implements the matrix profile
concept for time series. The tsmp package is a toolkit that allows all-pairs
similarity joins, motif, discords and chains discovery, semantic segmentation,
etc. Here we describe how the tsmp package may be used by showing some of the
use-cases from the original articles and evaluate the algorithm speed in the R
environment. This package can be downloaded at
https://CRAN.R-project.org/package=tsmp.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13335</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1904.13335</id><submitter>Xin Du</submitter><version version="v1"><date>Tue, 30 Apr 2019 16:02:49 GMT</date><size>265kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 12 Nov 2019 11:59:03 GMT</date><size>273kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 16 Oct 2020 21:52:08 GMT</date><size>277kb</size><source_type>D</source_type></version><title>Adversarial Balancing-based Representation Learning for Causal Effect
  Inference with Observational Data</title><authors>Xin Du, Lei Sun, Wouter Duivesteijn, Alexander Nikolaev, Mykola
  Pechenizkiy</authors><categories>cs.LG stat.ML</categories><journal-ref>Data Mining and Knowledge Discovery, (2021), 1-26</journal-ref><doi>10.1007/s10618-021-00759-3</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Learning causal effects from observational data greatly benefits a variety of
domains such as health care, education and sociology. For instance, one could
estimate the impact of a new drug on specific individuals to assist the clinic
plan and improve the survival rate. In this paper, we focus on studying the
problem of estimating Conditional Average Treatment Effect (CATE) from
observational data. The challenges for this problem are two-fold: on the one
hand, we have to derive a causal estimator to estimate the causal quantity from
observational data, where there exists confounding bias; on the other hand, we
have to deal with the identification of CATE when the distribution of
covariates in treatment and control groups are imbalanced. To overcome these
challenges, we propose a neural network framework called Adversarial
Balancing-based representation learning for Causal Effect Inference (ABCEI),
based on the recent advances in representation learning. To ensure the
identification of CATE, ABCEI uses adversarial learning to balance the
distributions of covariates in treatment and control groups in the latent
representation space, without any assumption on the form of the treatment
selection/assignment function. In addition, during the representation learning
and balancing process, highly predictive information from the original
covariate space might be lost. ABCEI can tackle this information loss problem
by preserving useful information for predicting causal effects under the
regularization of a mutual information estimator. The experimental results show
that ABCEI is robust against treatment selection bias, and matches/outperforms
the state-of-the-art approaches. Our experiments show promising results on
several datasets, representing different health care domains among others.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00609</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.00609</id><submitter>Bin Liu</submitter><version version="v1"><date>Thu, 2 May 2019 08:13:05 GMT</date><size>312kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 20 Jun 2019 14:33:25 GMT</date><size>297kb</size><source_type>D</source_type></version><title>Synthetic Oversampling of Multi-Label Data based on Local Label
  Distribution</title><authors>Bin Liu and Grigorios Tsoumakas</authors><categories>cs.LG stat.ML</categories><journal-ref>ECML-PKDD 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Class-imbalance is an inherent characteristic of multi-label data which
affects the prediction accuracy of most multi-label learning methods. One
efficient strategy to deal with this problem is to employ resampling techniques
before training the classifier. Existing multilabel sampling methods alleviate
the (global) imbalance of multi-label datasets. However, performance
degradation is mainly due to rare subconcepts and overlapping of classes that
could be analysed by looking at the local characteristics of the minority
examples, rather than the imbalance of the whole dataset. We propose a new
method for synthetic oversampling of multi-label data that focuses on local
label distribution to generate more diverse and better labeled instances.
Experimental results on 13 multi-label datasets demonstrate the effectiveness
of the proposed approach in a variety of evaluation measures, particularly in
the case of an ensemble of classifiers trained on repeated samples of the
original data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.02266</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.02266</id><submitter>Guido Previde Massara</submitter><version version="v1"><date>Mon, 6 May 2019 21:15:46 GMT</date><size>357kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 16:19:53 GMT</date><size>393kb</size><source_type>D</source_type></version><title>Learning Clique Forests</title><authors>Guido Previde Massara and Tomaso Aste</authors><categories>stat.ML cs.LG</categories><comments>47 pages, 26 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a topological learning algorithm for the estimation of the
conditional dependency structure of large sets of random variables from sparse
and noisy data. The algorithm, named Maximally Filtered Clique Forest (MFCF),
produces a clique forest and an associated Markov Random Field (MRF) by
generalising Prim's minimum spanning tree algorithm. To the best of our
knowledge, the MFCF presents three elements of novelty with respect to existing
structure learning approaches. The first is the repeated application of a local
topological move, the clique expansion, that preserves the decomposability of
the underlying graph. Through this move the decomposability and calculation of
scores is performed incrementally at the variable (rather than edge) level, and
this provides better computational performance and an intuitive application of
multivariate statistical tests. The second is the capability to accommodate a
variety of score functions and, while this paper is focused on multivariate
normal distributions, it can be directly generalised to different types of
statistics. Finally, the third is the variable range of allowed clique sizes
which is an adjustable topological constraint that acts as a topological
penalizer providing a way to tackle sparsity at $l_0$ semi-norm level; this
allows a clean decoupling of structure learning and parameter estimation. The
MFCF produces a representation of the clique forest, together with a perfect
ordering of the cliques and a perfect elimination ordering for the vertices. As
an example we propose an application to covariance selection models and we show
that the MCFC outperforms the Graphical Lasso for a number of classes of
matrices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.08707</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.08707</id><submitter>Michal Branicki</submitter><version version="v1"><date>Tue, 21 May 2019 15:45:43 GMT</date><size>4762kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 16:38:27 GMT</date><size>2762kb</size><source_type>D</source_type></version><title>Lagrangian uncertainty quantification and information inequalities for
  stochastic flows</title><authors>Michal Branicki and Kenneth Uda</authors><categories>math.PR cs.IT math.DS math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a systematic information-theoretic framework for quantification
and mitigation of error in probabilistic Lagrangian (i.e., trajectory-based)
predictions which are obtained from (Eulerian) vector fields generating the
underlying dynamical system in a way which naturally applies in both
deterministic and stochastic settings. This work is motivated by the desire to
improve Lagrangian predictions in complex, multi-scale systems based on
simplified, data-driven models. Here, discrepancies between probability
measures $\mu$ and $\nu$ associated with the true dynamics and its
approximation are quantified via so-called $\varphi$-divergencies,
$\mathcal{D}_\varphi(\mu\|\nu)$, which are premetrics defined by a class of
strictly convex functions $\varphi$. We derive general information bounds on
the uncertainty in estimates, $\mathbb{E}^{\nu}[f]$, of `true' observables
$\mathbb{E}^{\mu}[f]$ in terms of $\varphi$-divergencies; we then derive two
distinct bounds on $\mathcal{D}_\varphi(\mu\|\nu)$ itself. First, an
analytically tractable bound on $\mathcal{D}_\varphi(\mu\|\nu)$ is derived from
differences between vector fields generating the true dynamics and its
approximations. The second bound on $\mathcal{D}_\varphi(\mu\|\nu)$ is based on
a difference of so-called finite-time divergence rate (FTDR) fields and it can
be exploited within a computational framework to mitigate the error in
Lagrangian predictions by tuning the fields of expansion rates obtained from
simplified models. This new framework provides a systematic link between
Eulerian (field-based) model error and the resulting uncertainty in Lagrangian
(trajectory-based) predictions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.10996</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1905.10996</id><submitter>Christoph David Hofer PhD MSc</submitter><version version="v1"><date>Mon, 27 May 2019 06:46:30 GMT</date><size>321kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 Feb 2020 07:54:26 GMT</date><size>1724kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 04:39:34 GMT</date><size>1723kb</size><source_type>D</source_type></version><title>Graph Filtration Learning</title><authors>Christoph D. Hofer, Florian Graf, Bastian Rieck, Marc Niethammer,
  Roland Kwitt</authors><categories>cs.LG math.AT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach to learning with graph-structured data in the problem
domain of graph classification. In particular, we present a novel type of
readout operation to aggregate node features into a graph-level representation.
To this end, we leverage persistent homology computed via a real-valued,
learnable, filter function. We establish the theoretical foundation for
differentiating through the persistent homology computation. Empirically, we
show that this type of readout operation compares favorably to previous
techniques, especially when the graph connectivity structure is informative for
the learning problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1906.04491</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1906.04491</id><submitter>Kees Middelburg</submitter><version version="v1"><date>Tue, 11 Jun 2019 10:50:01 GMT</date><size>35kb</size></version><version version="v2"><date>Sat, 21 Dec 2019 10:37:00 GMT</date><size>35kb</size></version><version version="v3"><date>Tue, 13 Oct 2020 11:23:52 GMT</date><size>40kb</size></version><title>Using Hoare logic in a process algebra setting</title><authors>J. A. Bergstra, C. A. Middelburg</authors><categories>cs.LO</categories><comments>24 pages; presentation improved, examples added</comments><acm-class>D.1.3; D.1.4; D.2.4; F.1.2; F.3.1</acm-class><journal-ref>Fundamenta Informaticae 179(4):321--344, 2021</journal-ref><doi>10.3233/FI-2021-2026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the relation between process algebra and Hoare logic. We
investigate the question whether and how a Hoare logic can be used for
reasoning about how data change in the course of a process when reasoning
equationally about that process. We introduce an extension of ACP (Algebra of
Communicating Processes) with features that are relevant to processes in which
data are involved, present a Hoare logic for the processes considered in this
process algebra, and discuss the use of this Hoare logic as a complement to
pure equational reasoning with the equational axioms of the process algebra.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.00359</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.00359</id><submitter>Krishna Balajirao Manoorkar</submitter><version version="v1"><date>Sun, 30 Jun 2019 10:35:20 GMT</date><size>84kb</size></version><version version="v2"><date>Mon, 3 Feb 2020 10:28:48 GMT</date><size>2607kb</size></version><title>Rough concepts</title><authors>Willem Conradie, Sabine Frittella, Krishna Manoorkar, Sajad Nazari,
  Alessandra Palmigiano, Apostolos Tzimoulis, Nachoem M. Wijnberg</authors><categories>cs.LO math.LO</categories><doi>10.1016/j.ins.2020.05.074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper proposes a novel way to unify Rough Set Theory and Formal
Concept Analysis. Our method stems from results and insights developed in the
algebraic theory of modal logic, and is based on the idea that Pawlak's
original approximation spaces can be seen as special instances of enriched
formal contexts, i.e. relational structures based on formal contexts from
Formal Concept Analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.01547</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.01547</id><submitter>Tim R\&quot;omer</submitter><version version="v1"><date>Tue, 2 Jul 2019 17:57:44 GMT</date><size>31kb</size></version><version version="v2"><date>Fri, 4 Sep 2020 15:23:07 GMT</date><size>40kb</size></version><title>Learning algebraic decompositions using Prony structures</title><authors>Stefan Kunis, Tim R\&quot;omer, Ulrich von der Ohe</authors><categories>math.AC cs.NA math.NA</categories><comments>33 pages; revised version. The third author was supported by an
  INdAM-DP-COFUND-2015/Marie Sk\l{}odowska-Curie Actions scholarship, grant
  number 713485</comments><msc-class>13P25, 94A12 (Primary), 13P10, 15B05, 30E05, 65F30 (Secondary)</msc-class><journal-ref>Adv. in Appl. Math. 118 (2020), 102044, 43 pp</journal-ref><doi>10.1016/j.aam.2020.102044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algebraic framework generalizing several variants of Prony's
method and explaining their relations. This includes Hankel and Toeplitz
variants of Prony's method for the decomposition of multivariate exponential
sums, polynomials (with respect to the monomial and Chebyshev bases),
Gau{\ss}ian sums, spherical harmonic sums, taking also into account whether
they have their support on an algebraic set.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.03956</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.03956</id><submitter>Changjoo Nam</submitter><version version="v1"><date>Tue, 9 Jul 2019 03:18:51 GMT</date><size>2869kb</size><source_type>D</source_type></version><title>Planning for target retrieval using a robotic manipulator in cluttered
  and occluded environments</title><authors>Changjoo Nam, Jinhwi Lee, Younggil Cho, Jeongho Lee, Dong Hwan Kim,
  ChangHwan Kim</authors><categories>cs.RO</categories><comments>8 pages, 14 figures</comments><doi>10.1109/TRO.2020.3047472</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents planning algorithms for a robotic manipulator with a
fixed base in order to grasp a target object in cluttered environments. We
consider a configuration of objects in a confined space with a high density so
no collision-free path to the target exists. The robot must relocate some
objects to retrieve the target while avoiding collisions. For fast completion
of the retrieval task, the robot needs to compute a plan optimizing an
appropriate objective value directly related to the execution time of the
relocation plan.
  We propose planning algorithms that aim to minimize the number of objects to
be relocated. Our objective value is appropriate for the object retrieval task
because grasping and releasing objects often dominate the total running time.
In addition to the algorithm working in fully known and static environments, we
propose algorithms that can deal with uncertain and dynamic situations incurred
by occluded views. The proposed algorithms are shown to be complete and run in
polynomial time. Our methods reduce the total running time significantly
compared to a baseline method (e.g., 25.1% of reduction in a known static
environment with 10 objects
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.07815</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.07815</id><submitter>Rupert H\&quot;olzl</submitter><version version="v1"><date>Wed, 17 Jul 2019 23:50:01 GMT</date><size>45kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 22 Feb 2021 10:24:04 GMT</date><size>47kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 11:49:20 GMT</date><size>47kb</size><source_type>D</source_type></version><title>Degrees of Randomized Computability</title><authors>Rupert H\&quot;olzl and Christopher P. Porter</authors><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this survey we discuss work of Levin and V'yugin on collections of
sequences that are non-negligible in the sense that they can be computed by a
probabilistic algorithm with positive probability. More precisely, Levin and
V'yugin introduced an ordering on collections of sequences that are closed
under Turing equivalence. Roughly speaking, given two such collections
$\mathcal{A}$ and $\mathcal{B}$, $\mathcal{A}$ is below $\mathcal{B}$ in this
ordering if $\mathcal{A}\setminus\mathcal{B}$ is negligible. The degree
structure associated with this ordering, the Levin-V'yugin degrees (or
LV-degrees), can be shown to be a Boolean algebra, and in fact a measure
algebra.
  We demonstrate the interactions of this work with recent results in
computability theory and algorithmic randomness: First, we recall the
definition of the Levin-V'yugin algebra and identify connections between its
properties and classical properties from computability theory. In particular,
we apply results on the interactions between notions of randomness and Turing
reducibility to establish new facts about specific LV-degrees, such as the
LV-degree of the collection of 1-generic sequences, that of the collection of
sequences of hyperimmune degree, and those collections corresponding to various
notions of effective randomness. Next, we provide a detailed explanation of a
complex technique developed by V'yugin that allows the construction of
semi-measures into which computability-theoretic properties can be encoded. We
provide two examples of the use of this technique by explicating a result of
V'yugin's about the LV-degree of the collection of Martin-L\&quot;of random
sequences and extending the result to the LV-degree of the collection of
sequences of DNC degree.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.11019</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.11019</id><submitter>Nidhi Rathi</submitter><version version="v1"><date>Thu, 25 Jul 2019 13:04:34 GMT</date><size>197kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 30 Sep 2019 14:16:35 GMT</date><size>199kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 09:09:59 GMT</date><size>629kb</size><source_type>D</source_type></version><title>Fair and Efficient Cake Division with Connected Pieces</title><authors>Eshwar Ram Arunachaleswaran, Siddharth Barman, Rachitesh Kumar, and
  Nidhi Rathi</authors><categories>cs.GT</categories><comments>29 pages</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic cake-cutting problem provides a model for addressing fair and
efficient allocation of a divisible, heterogeneous resource (metaphorically,
the cake) among agents with distinct preferences. Focusing on a standard
formulation of cake cutting, in which each agent must receive a contiguous
piece of the cake, this work establishes algorithmic and hardness results for
multiple fairness/efficiency measures.
  First, we consider the well-studied notion of envy-freeness and develop an
efficient algorithm that finds a cake division (with connected pieces) wherein
the envy is multiplicatively within a factor of $2 + o(1)$. The same algorithm
in fact achieves an approximation ratio of $3 + o(1)$ for the problem of
finding cake divisions with as large a Nash social welfare (NSW) as possible.
NSW is another standard measure of fairness and this work also establishes a
connection between envy-freeness and NSW: approximately envy-free cake
divisions (with connected pieces) always have near-optimal Nash social welfare.
Furthermore, we develop an approximation algorithm for maximizing the
$\rho$-mean welfare--this unifying objective, with different values of $\rho$,
interpolates between notions of fairness (NSW) and efficiency (average social
welfare). Finally, we complement these algorithmic results by proving that
maximizing NSW (and, in general, the $\rho$-mean welfare) is APX-hard in the
cake-division context.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1907.12439</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1907.12439</id><submitter>Hanbo Zhang</submitter><version version="v1"><date>Mon, 29 Jul 2019 13:59:42 GMT</date><size>1626kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 24 Sep 2019 15:16:59 GMT</date><size>1404kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 11 Feb 2020 02:00:15 GMT</date><size>1526kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 12 May 2021 14:24:39 GMT</date><size>9614kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 17 May 2021 06:09:53 GMT</date><size>9118kb</size><source_type>D</source_type></version><title>Hindsight Trust Region Policy Optimization</title><authors>Hanbo Zhang, Site Bai, Xuguang Lan, David Hsu, Nanning Zheng</authors><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted by IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement Learning(RL) with sparse rewards is a major challenge. We
propose \emph{Hindsight Trust Region Policy Optimization}(HTRPO), a new RL
algorithm that extends the highly successful TRPO algorithm with
\emph{hindsight} to tackle the challenge of sparse rewards. Hindsight refers to
the algorithm's ability to learn from information across goals, including ones
not intended for the current task. HTRPO leverages two main ideas. It
introduces QKL, a quadratic approximation to the KL divergence constraint on
the trust region, leading to reduced variance in KL divergence estimation and
improved stability in policy update. It also presents Hindsight Goal
Filtering(HGF) to select conductive hindsight goals. In experiments, we
evaluate HTRPO in various sparse reward tasks, including simple benchmarks,
image-based Atari games, and simulated robot control. Ablation studies indicate
that QKL and HGF contribute greatly to learning stability and high performance.
Comparison results show that in all tasks, HTRPO consistently outperforms both
TRPO and HPG, a state-of-the-art algorithm for RL with sparse rewards.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1908.05145</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1908.05145</id><submitter>Krishna Balajirao Manoorkar</submitter><version version="v1"><date>Wed, 14 Aug 2019 14:27:00 GMT</date><size>31kb</size></version><version version="v2"><date>Tue, 21 Jan 2020 15:19:00 GMT</date><size>698kb</size></version><title>Toward a Dempster-Shafer theory of concepts</title><authors>Sabine Frittella, Krishna Manoorkar, Alessandra Palmigiano, Apostolos
  Tzimoulis, Nachoem M. Wijnberg</authors><categories>cs.AI cs.LO</categories><doi>10.1016/j.ijar.2020.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we generalize the basic notions and results of Dempster-Shafer
theory from predicates to formal concepts. Results include the representation
of conceptual belief functions as inner measures of suitable probability
functions, and a Dempster-Shafer rule of combination on belief functions on
formal concepts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.00257</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.00257</id><submitter>Mitsuru Igami</submitter><version version="v1"><date>Sat, 31 Aug 2019 18:20:25 GMT</date><size>2748kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 4 Aug 2020 17:23:04 GMT</date><size>5660kb</size></version><version version="v3"><date>Fri, 18 Sep 2020 16:13:45 GMT</date><size>5679kb</size></version><version version="v4"><date>Tue, 18 May 2021 15:03:56 GMT</date><size>5680kb</size></version><title>Mapping Firms' Locations in Technological Space: A Topological Analysis
  of Patent Statistics</title><authors>Emerson G. Escolar, Yasuaki Hiraoka, Mitsuru Igami, Yasin Ozcan</authors><categories>econ.EM cs.DM math.AT math.CO</categories><comments>31 pages (main text), 10 pages (Appendix), 9 tables and 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method to characterize firms' inventive activities via
topological data analysis (TDA) that represents high-dimensional data in a
shape graph. Applying this method to 333 major firms' patents in 1976-2005
reveals substantial heterogeneity: some firms remain undifferentiated; others
develop unique portfolios. Firms with unique trajectories, which we define
graph-theoretically as &quot;flares&quot; in the Mapper graph, perform better. This
association is statistically and economically significant, and continues to
hold after we control for portfolio size and firm survivorship. Comparison with
existing techniques suggests the method's usefulness for data visualization and
exploratory empirical research more generally.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.06711</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.06711</id><submitter>Joseph Monaco</submitter><version version="v1"><date>Sun, 15 Sep 2019 02:02:22 GMT</date><size>2195kb</size><source_type>D</source_type></version><title>Cognitive swarming in complex environments with attractor dynamics and
  oscillatory computing</title><authors>Joseph D. Monaco, Grace M. Hwang, Kevin M. Schultz, Kechen Zhang</authors><categories>cs.MA cs.NE cs.RO nlin.AO q-bio.NC</categories><comments>16 pages, 7 figures</comments><journal-ref>Biol Cybern 114, 269-284 (2020)</journal-ref><doi>10.1007/s00422-020-00823-z</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neurobiological theories of spatial cognition developed with respect to
recording data from relatively small and/or simplistic environments compared to
animals' natural habitats. It has been unclear how to extend theoretical models
to large or complex spaces. Complementarily, in autonomous systems technology,
applications have been growing for distributed control methods that scale to
large numbers of low-footprint mobile platforms. Animals and many-robot groups
must solve common problems of navigating complex and uncertain environments.
Here, we introduce the 'NeuroSwarms' control framework to investigate whether
adaptive, autonomous swarm control of minimal artificial agents can be achieved
by direct analogy to neural circuits of rodent spatial cognition. NeuroSwarms
analogizes agents to neurons and swarming groups to recurrent networks. We
implemented neuron-like agent interactions in which mutually visible agents
operate as if they were reciprocally-connected place cells in an attractor
network. We attributed a phase state to agents to enable patterns of
oscillatory synchronization similar to hippocampal models of theta-rhythmic
(5-12 Hz) sequence generation. We demonstrate that multi-agent swarming and
reward-approach dynamics can be expressed as a mobile form of Hebbian learning
and that NeuroSwarms supports a single-entity paradigm that directly informs
theoretical models of animal cognition. We present emergent behaviors including
phase-organized rings and trajectory sequences that interact with environmental
cues and geometry in large, fragmented mazes. Thus, NeuroSwarms is a model
artificial spatial system that integrates autonomous control and theoretical
neuroscience to potentially uncover common principles to advance both domains.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.07079</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.07079</id><submitter>Huan Xiong</submitter><version version="v1"><date>Mon, 16 Sep 2019 09:21:22 GMT</date><size>66kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 14 Dec 2020 08:46:24 GMT</date><size>66kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 04:51:07 GMT</date><size>66kb</size><source_type>D</source_type></version><title>Fast Large-Scale Discrete Optimization Based on Principal Coordinate
  Descent</title><authors>Huan Xiong, Mengyang Yu, Li Liu, Fan Zhu, Fumin Shen, Ling Shao</authors><categories>math.OC cs.LG</categories><comments>14 pages</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Binary optimization, a representative subclass of discrete optimization,
plays an important role in mathematical optimization and has various
applications in computer vision and machine learning. Usually, binary
optimization problems are NP-hard and difficult to solve due to the binary
constraints, especially when the number of variables is very large. Existing
methods often suffer from high computational costs or large accumulated
quantization errors, or are only designed for specific tasks. In this paper, we
propose a fast algorithm to find effective approximate solutions for general
binary optimization problems. The proposed algorithm iteratively solves
minimization problems related to the linear surrogates of loss functions, which
leads to the updating of some binary variables most impacting the value of loss
functions in each step. Our method supports a wide class of empirical objective
functions with/without restrictions on the numbers of $1$s and $-1$s in the
binary variables. Furthermore, the theoretical convergence of our algorithm is
proven, and the explicit convergence rates are derived, for objective functions
with Lipschitz continuous gradients, which are commonly adopted in practice.
Extensive experiments on several binary optimization tasks and large-scale
datasets demonstrate the superiority of the proposed algorithm over several
state-of-the-art methods in terms of both effectiveness and efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.07105</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.07105</id><submitter>Yu Yol Shin</submitter><version version="v1"><date>Mon, 16 Sep 2019 10:30:53 GMT</date><size>866kb</size></version><version version="v2"><date>Wed, 4 Nov 2020 06:34:44 GMT</date><size>1112kb</size></version><title>Incorporating Dynamicity of Transportation Network with Multi-Weight
  Traffic Graph Convolution for Traffic Forecasting</title><authors>Yuyol Shin, Yoonjin Yoon</authors><categories>stat.ML cs.LG</categories><comments>11 pages, 7 figures, Accepted to IEEE Transactions on Intelligent
  Transportation Systems (2020)</comments><msc-class>68T99</msc-class><journal-ref>IEEE Trans. Intell. Transp. Syst., 0 (2020) 1-11</journal-ref><doi>10.1109/TITS.2020.3031331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic forecasting problem remains a challenging task in the intelligent
transportation system due to its spatio-temporal complexity. Although temporal
dependency has been well studied and discussed, spatial dependency is
relatively less explored due to its large variations, especially in the urban
environment. In this study, a novel graph convolutional network model,
Multi-Weight Traffic Graph Convolutional (MW-TGC) network, is proposed and
applied to two urban networks with contrasting geometric constraints. The model
conducts graph convolution operations on speed data with multi-weighted
adjacency matrices to combine the features, including speed limit, distance,
and angle. The spatially isolated dimension reduction operation is conducted on
the combined features to learn the dependencies among the features and reduce
the size of the output to a computationally feasible level. The output of
multi-weight graph convolution is applied to the sequence-to-sequence model
with Long Short-Term Memory units to learn temporal dependencies. When applied
to two urban sites, urban-core and urban-mix, MW-TGC network not only
outperformed the comparative models in both sites but also reduced variance in
the heterogeneous urban-mix network. We conclude that MW-TGC network can
provide a robust traffic forecasting performance across the variations in
spatial complexity, which can be a strong advantage in urban traffic
forecasting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1909.10917</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1909.10917</id><submitter>Husnu Ata Erbay</submitter><version version="v1"><date>Mon, 23 Sep 2019 10:37:07 GMT</date><size>87kb</size></version><title>A semi-discrete numerical method for convolution-type unidirectional
  wave equations</title><authors>H. A. Erbay, S. Erbay, A. Erkip</authors><categories>math.NA cs.NA math-ph math.AP math.MP</categories><comments>21 pages, to appear in Journal of Computational and Applied
  Mathematics. arXiv admin note: text overlap with arXiv:1805.07264</comments><msc-class>35Q53, 65M12, 65M20, 65Z05</msc-class><journal-ref>Journal of Computational and Applied Mathematics 387, Article
  Number: 112496 (2021)</journal-ref><doi>10.1016/j.cam.2019.112496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical approximation of a general class of nonlinear unidirectional wave
equations with a convolution-type nonlocality in space is considered. A
semi-discrete numerical method based on both a uniform space discretization and
the discrete convolution operator is introduced to solve the Cauchy problem.
The method is proved to be uniformly convergent as the mesh size goes to zero.
The order of convergence for the discretization error is linear or quadratic
depending on the smoothness of the convolution kernel. The discrete problem
defined on the whole spatial domain is then truncated to a finite domain.
Restricting the problem to a finite domain introduces a localization error and
it is proved that this localization error stays below a given threshold if the
finite domain is large enough. For two particular kernel functions, the
numerical examples concerning solitary wave solutions illustrate the expected
accuracy of the method. Our class of nonlocal wave equations includes the
Benjamin-Bona-Mahony equation as a special case and the present work is
inspired by the previous work of Bona, Pritchard and Scott on numerical
solution of the Benjamin-Bona-Mahony equation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.00879</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.00879</id><submitter>Isaac Matthews</submitter><version version="v1"><date>Wed, 2 Oct 2019 11:28:40 GMT</date><size>2267kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 16:34:09 GMT</date><size>4068kb</size><source_type>D</source_type></version><title>The Neural Moving Average Model for Scalable Variational Inference of
  State Space Models</title><authors>Tom Ryder, Dennis Prangle, Andrew Golightly, Isaac Matthews</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational inference has had great success in scaling approximate Bayesian
inference to big data by exploiting mini-batch training. To date, however, this
strategy has been most applicable to models of independent data. We propose an
extension to state space models of time series data based on a novel generative
model for latent temporal states: the neural moving average model. This permits
a subsequence to be sampled without drawing from the entire distribution,
enabling training iterations to use mini-batches of the time series at low
computational cost. We illustrate our method on autoregressive, Lotka-Volterra,
FitzHugh-Nagumo and stochastic volatility models, achieving accurate parameter
estimation in a short time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.06849</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.06849</id><submitter>Guohao Li</submitter><version version="v1"><date>Tue, 15 Oct 2019 15:10:34 GMT</date><size>7003kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 14 Jun 2020 19:38:05 GMT</date><size>7487kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 14 May 2021 21:35:58 GMT</date><size>13575kb</size><source_type>D</source_type></version><title>DeepGCNs: Making GCNs Go as Deep as CNNs</title><authors>Guohao Li, Matthias M\&quot;uller, Guocheng Qian, Itzel C. Delgadillo,
  Abdulellah Abualshour, Ali Thabet, Bernard Ghanem</authors><categories>cs.CV cs.LG eess.IV</categories><comments>Accepted at TPAMI. This work is a journal extension of our ICCV'19
  paper arXiv:1904.03751. The first three authors contributed equally</comments><doi>10.1109/TPAMI.2021.3074057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) have been very successful at solving a
variety of computer vision tasks such as object classification and detection,
semantic segmentation, activity understanding, to name just a few. One key
enabling factor for their great performance has been the ability to train very
deep networks. Despite their huge success in many tasks, CNNs do not work well
with non-Euclidean data, which is prevalent in many real-world applications.
Graph Convolutional Networks (GCNs) offer an alternative that allows for
non-Eucledian data input to a neural network. While GCNs already achieve
encouraging results, they are currently limited to architectures with a
relatively small number of layers, primarily due to vanishing gradients during
training. This work transfers concepts such as residual/dense connections and
dilated convolutions from CNNs to GCNs in order to successfully train very deep
GCNs. We show the benefit of using deep GCNs (with as many as 112 layers)
experimentally across various datasets and tasks. Specifically, we achieve very
promising performance in part segmentation and semantic segmentation on point
clouds and in node classification of protein functions across biological
protein-protein interaction (PPI) graphs. We believe that the insights in this
work will open avenues for future research on GCNs and their application to
further tasks not explored in this paper. The source code for this work is
available at https://github.com/lightaime/deep_gcns_torch and
https://github.com/lightaime/deep_gcns for PyTorch and TensorFlow
implementation respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.10376</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.10376</id><submitter>Zahed Rahmati</submitter><version version="v1"><date>Wed, 23 Oct 2019 06:08:15 GMT</date><size>5052kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 20:39:53 GMT</date><size>5340kb</size><source_type>D</source_type></version><title>Emanation Graph: A Plane Geometric Spanner with Steiner Points</title><authors>Bardia Hamedmohseni, Zahed Rahmati, Debajyoti Mondal</authors><categories>cs.CG</categories><comments>A preliminary version of this work was presented at the 30th Canadian
  Conference on Computational Geometry (CCCG) and the 46th International
  Conference on Current Trends in Theory and Practice of Computer Science
  (SOFSEM)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emanation graph of grade $k$ on a set of points is a plane spanner made by
shooting $2^{k+1}$ equally spaced rays from each point, where the shorter rays
stop the longer ones upon collision. The collision points are the Steiner
points of the spanner. Emanation graphs of grade one were studied by Mondal and
Nachmanson in the context of network visualization. They proved that the
spanning ratio of such a graph is bounded by $(2+\sqrt{2})\approx 3.41$. We
improve this upper bound to $\sqrt{10} \approx 3.162$ and show this to be
tight, i.e., there exist emanation graphs with spanning ratio $\sqrt{10}$.
  We also explore emanation graphs of grade two, which may have twice the
number of edges compared to grade one graphs. We introduce a method of
simplification that makes it an interesting geometric graph to be used for
network visualization and geometric routing. In particular, we compare
simplified emanation graphs against Shewchuk's constrained Delaunay
triangulations on both synthetic and real-life datasets. Our experimental
results reveal that the simplified emanation graphs outperform constrained
Delaunay triangulations in common quality measures (e.g., edge count, angular
resolution, average degree, total edge length) while maintain a comparable
spanning ratio and Steiner point count.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1910.12787</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1910.12787</id><submitter>Thomas Kahl</submitter><version version="v1"><date>Mon, 28 Oct 2019 16:30:42 GMT</date><size>26kb</size></version><version version="v2"><date>Wed, 13 Jan 2021 12:14:47 GMT</date><size>33kb</size></version><version version="v3"><date>Sun, 16 May 2021 16:53:31 GMT</date><size>37kb</size><source_type>D</source_type></version><title>Weak equivalence of higher-dimensional automata</title><authors>Thomas Kahl</authors><categories>cs.LO cs.FL math.AT</categories><msc-class>68Q85, 55N99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a notion of equivalence for higher-dimensional
automata, called weak equivalence. Weak equivalence focuses mainly on a
traditional trace language and a new homology language, which captures the
overall independence structure of an HDA. It is shown that weak equivalence is
compatible with both the tensor product and the coproduct of HDAs and that,
under certain conditions, HDAs may be reduced to weakly equivalent smaller ones
by merging and collapsing cubes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.03020</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.03020</id><submitter>Mohammad Yaghini</submitter><version version="v1"><date>Fri, 8 Nov 2019 03:41:03 GMT</date><size>1232kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 12:41:14 GMT</date><size>2343kb</size><source_type>D</source_type></version><title>A Human-in-the-loop Framework to Construct Context-aware Mathematical
  Notions of Outcome Fairness</title><authors>Mohammad Yaghini, Andreas Krause, and Hoda Heidari</authors><categories>cs.AI cs.CY</categories><comments>In the forth AAAI/ACM Conference on Artificial Intelligence, Ethics,
  and Society (AIES-2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing mathematical notions of fairness fail to account for the context of
decision-making. We argue that moral consideration of contextual factors is an
inherently human task. So we present a framework to learn context-aware
mathematical formulations of fairness by eliciting people's situated fairness
assessments. Our family of fairness notions corresponds to a new interpretation
of economic models of Equality of Opportunity (EOP), and it includes most
existing notions of fairness as special cases. Our human-in-the-loop approach
is designed to learn the appropriate parameters of the EOP family by utilizing
human responses to pair-wise questions about decision subjects' circumstance
and deservingness, and the harm/benefit imposed on them. We illustrate our
framework in a hypothetical criminal risk assessment scenario by conducting a
series of human-subject experiments on Amazon Mechanical Turk. Our work takes
an important initial step toward empowering stakeholders to have a voice in the
formulation of fairness for Machine Learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.03047</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.03047</id><submitter>Woon Sang Cho</submitter><version version="v1"><date>Fri, 8 Nov 2019 04:45:55 GMT</date><size>7819kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 29 Apr 2020 13:16:57 GMT</date><size>7495kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 11:36:50 GMT</date><size>7094kb</size><source_type>D</source_type></version><title>Contrastive Multi-document Question Generation</title><authors>Woon Sang Cho, Yizhe Zhang, Sudha Rao, Asli Celikyilmaz, Chenyan
  Xiong, Jianfeng Gao, Mengdi Wang, Bill Dolan</authors><categories>cs.CL</categories><comments>EACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-document question generation focuses on generating a question that
covers the common aspect of multiple documents. Such a model is useful in
generating clarifying options. However, a naive model trained only using the
targeted (&quot;positive&quot;) document set may generate too generic questions that
cover a larger scope than delineated by the document set. To address this
challenge, we introduce the contrastive learning strategy where given
&quot;positive&quot; and &quot;negative&quot; sets of documents, we generate a question that is
closely related to the &quot;positive&quot; set but is far away from the &quot;negative&quot; set.
This setting allows generated questions to be more specific and related to the
target document set. To generate such specific questions, we propose
Multi-Source Coordinated Question Generator (MSCQG), a novel framework that
includes a supervised learning (SL) stage and a reinforcement learning (RL)
stage. In the SL stage, a single-document question generator is trained. In the
RL stage, a coordinator model is trained to find optimal attention weights to
align multiple single-document generators, by optimizing a reward designed to
promote specificity of generated questions. We also develop an effective
auxiliary objective, named Set-induced Contrastive Regularization (SCR) that
improves the coordinator's contrastive learning during the RL stage. We show
that our model significantly outperforms several strong baselines, as measured
by automatic metrics and human evaluation. The source repository is publicly
available at \url{www.github.com/woonsangcho/contrast_qgen}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.03616</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.03616</id><submitter>Jerome Droniou</submitter><version version="v1"><date>Sat, 9 Nov 2019 06:21:28 GMT</date><size>34kb</size></version><version version="v2"><date>Mon, 17 May 2021 04:47:59 GMT</date><size>38kb</size></version><title>Fully discrete polynomial de Rham sequences of arbitrary degree on
  polygons and polyhedra</title><authors>Daniele A. Di Pietro, J\'er\^ome Droniou, and Francesca Rapetti</authors><categories>math.NA cs.NA</categories><msc-class>65N08, 65N30, 65N99</msc-class><journal-ref>Math. Models Methods Appl. Sci. 30 (9), pp. 1809-1855, 2020</journal-ref><doi>10.1142/S0218202520500372</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, merging ideas from compatible discretisations and polyhedral
methods, we construct novel fully discrete polynomial de Rham sequences of
arbitrary degree on polygons and polyhedra. The spaces and operators that
appear in these sequences are directly amenable to computer implementation.
Besides proving exactness, we show that the usual three-dimensional sequence of
trimmed Finite Element spaces forms, through appropriate interpolation
operators, a commutative diagram with our sequence, which ensures suitable
approximation properties. A discussion on reconstructions of potentials and
discrete $L^2$-products completes the exposition.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.04352</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.04352</id><submitter>Tizian Wenzel</submitter><version version="v1"><date>Mon, 11 Nov 2019 15:50:29 GMT</date><size>71kb</size></version><title>A novel class of stabilized greedy kernel approximation algorithms:
  Convergence, stability &amp; uniform point distribution</title><authors>Tizian Wenzel, Gabriele Santin, Bernard Haasdonk</authors><categories>math.NA cs.NA</categories><journal-ref>Journal of Approximation Theory Volume 262, February 2021</journal-ref><doi>10.1016/j.jat.2020.105508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel based methods provide a way to reconstruct potentially
high-dimensional functions from meshfree samples, i.e., sampling points and
corresponding target values. A crucial ingredient for this to be successful is
the distribution of the sampling points. Since the computation of an optimal
selection of sampling points may be an infeasible task, one promising option is
to use greedy methods.
  Although these methods may be very effective, depending on the specific
greedy criterion the chosen points might quickly lead to instabilities in the
computation. To circumvent this problem, we introduce and investigate a new
class of \textit{stabilized} greedy kernel algorithms, which can be used to
create a scale of new selection strategies.
  We analyze these algorithms, and in particular we prove convergence results
and quantify in a precise way the distribution of the selected points. These
results allow to prove, in the case of certain Sobolev kernels, that the
algorithms have optimal stability and optimal convergence rates, including for
functions outside the native space of the kernel. The results also apply to the
case of the usual $P$-greedy algorithm, significantly improving
state-of-the-art results available in the literature. Illustrative experiments
are presented that support the theoretical findings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.08907</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.08907</id><submitter>Ruobing Han</submitter><version version="v1"><date>Wed, 20 Nov 2019 13:41:45 GMT</date><size>1805kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 11 Sep 2020 04:39:01 GMT</date><size>2702kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 11:08:55 GMT</date><size>3929kb</size><source_type>D</source_type></version><title>Auto-Precision Scaling for Distributed Deep Learning</title><authors>Ruobing Han, James Demmel, Yang You</authors><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been reported that the communication cost for synchronizing gradients
can be a bottleneck, which limits the scalability of distributed deep learning.
Using low-precision gradients is a promising technique for reducing the
bandwidth requirement. In this work, we propose Auto Precision Scaling (APS),
an algorithm that can improve the accuracy when we communicate gradients by
low-precision floating-point values. APS can improve the accuracy for all
precisions with a trivial communication cost. Our experimental results show
that for many applications, APS can train state-of-the-art models by 8-bit
gradients with no or only a tiny accuracy loss (&lt;0.05%). Furthermore, we can
avoid any accuracy loss by designing a hybrid-precision technique. Finally, we
propose a performance model to evaluate the proposed method. Our experimental
results show that APS can get a significant speedup over state-of-the-art
methods. To make it available to researchers and developers, we design and
implement CPD (Customized-Precision Deep Learning) system, which can simulate
the training process using an arbitrary low-precision customized floating-point
format. We integrate CPD into PyTorch and make it open-source.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1911.09412</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1911.09412</id><submitter>Michal Kocvara</submitter><version version="v1"><date>Thu, 21 Nov 2019 11:11:48 GMT</date><size>686kb</size><source_type>D</source_type></version><title>Decomposition of arrow type positive semidefinite matrices with
  application to topology optimization</title><authors>Michal Kocvara</authors><categories>math.OC cs.NA math.NA</categories><msc-class>90C22, 74P05, 65N55, 05C69</msc-class><doi>10.1007/s10107-020-01526-w</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposition of large matrix inequalities for matrices with chordal sparsity
graph has been recently used by Kojima et al.\ \cite{kim2011exploiting} to
reduce problem size of large scale semidefinite optimization (SDO) problems and
thus increase efficiency of standard SDO software. A by-product of such a
decomposition is the introduction of new dense small-size matrix variables. We
will show that for arrow type matrices satisfying suitable assumptions, the
additional matrix variables have rank one and can thus be replaced by vector
variables of the same dimensions. This leads to significant improvement in
efficiency of standard SDO software. We will apply this idea to the problem of
topology optimization formulated as a large scale linear semidefinite
optimization problem. Numerical examples will demonstrate tremendous speed-up
in the solution of the decomposed problems, as compared to the original large
scale problem. In our numerical example the decomposed problems exhibit linear
growth in complexity, compared to the more than cubic growth in the original
problem formulation. We will also give a connection of our approach to the
standard theory of domain decomposition and show that the additional vector
variables are outcomes of the corresponding discrete Steklov-Poincar\'{e}
operators.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.06105</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.06105</id><submitter>Elias Riedel G{\aa}rding</submitter><version version="v1"><date>Thu, 12 Dec 2019 18:04:28 GMT</date><size>3637kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 2 Apr 2020 20:12:28 GMT</date><size>5061kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 15 May 2020 14:00:18 GMT</date><size>5116kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 16 May 2021 21:53:57 GMT</date><size>11415kb</size><source_type>D</source_type></version><title>Bell Diagonal and Werner state generation: entanglement, non-locality,
  steering and discord on the IBM quantum computer</title><authors>Elias Riedel G{\aa}rding, Nicolas Schwaller, Su Yeon Chang, Samuel
  Bosch, Willy Robert Laborde, Javier Naya Hernandez, Chun Lam Chan,
  Fr\'ed\'eric Gessler, Xinyu Si, Marc-Andr\'e Dupertuis and Nicolas Macris</authors><categories>quant-ph cs.IT math.IT</categories><comments>20 pages, 23 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the first correct special-purpose quantum circuits for preparation
of Bell-diagonal states (BDS), and implement them on the IBM Quantum computer,
characterizing and testing complex aspects of their quantum correlations in the
full parameter space. Among the circuits proposed, one involves only two
quantum bits but requires adapted quantum tomography routines handling
classical bits in parallel. The entire class of Bell-diagonal states is
generated, and a number of characteristic indicators, namely entanglement of
formation, CHSH non-locality, steering and discord, are experimentally
evaluated over the full parameter space and compared with theory. As a
by-product of this work we also find a remarkable general inequality between
&quot;quantum discord&quot; and &quot;asymmetric relative entropy of discord&quot;: the former
never exceeds the latter. We also prove that for all BDS the two coincide.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.09815</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.09815</id><submitter>Thomas Quinn-Gregson</submitter><version version="v1"><date>Fri, 20 Dec 2019 13:33:00 GMT</date><size>119kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 31 Jan 2020 09:40:18 GMT</date><size>40kb</size><source_type>D</source_type></version><title>Solving Equation Systems in $\omega$-categorical Algebras</title><authors>Manuel Bodirsky and Thomas Quinn-Gregson</authors><categories>math.LO cs.CC math.RA</categories><comments>28 pages, 1 figure</comments><msc-class>03C10, 08A70, 08A40</msc-class><acm-class>F.4.1; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of deciding whether a given set of term
equalities and inequalities has a solution in an $\omega$-categorical algebra
$\mathfrak{A}$. There are $\omega$-categorical groups where this problem is
undecidable. We show that if $\mathfrak{A}$ is an $\omega$-categorical
semilattice or an abelian group, then the problem is in P or NP-hard. The hard
cases are precisely those where Pol$(\mathfrak{A},\neq)$ has a uniformly
continuous minor-preserving map to the clone of projections on a two-element
set. The results provide information about algebras $\mathfrak{A}$ such that
Pol$(\mathfrak{A},\neq)$ does not satisfy this condition, and they are of
independent interest in universal algebra. In our proofs we rely on the
Barto-Pinsker theorem about the existence of pseudo-Siggers polymorphisms. To
the best of our knowledge, this is the first time that the pseudo-Siggers
identity has been used to prove a complexity dichotomy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.09989</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.09989</id><submitter>Hai Shu</submitter><version version="v1"><date>Fri, 20 Dec 2019 18:21:19 GMT</date><size>4096kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 20 Mar 2020 07:01:37 GMT</date><size>5356kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 16:44:44 GMT</date><size>11227kb</size><source_type>D</source_type></version><title>CDPA: Common and Distinctive Pattern Analysis between High-dimensional
  Datasets</title><authors>Hai Shu, Zhe Qu</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A representative model in integrative analysis of two high-dimensional
correlated datasets is to decompose each data matrix into a low-rank common
matrix generated by latent factors shared across datasets, a low-rank
distinctive matrix corresponding to each dataset, and an additive noise matrix.
Existing decomposition methods claim that their common matrices capture the
common pattern of the two datasets. However, their so-called common pattern
only denotes the common latent factors but ignores the common pattern between
the two coefficient matrices of these common latent factors. We propose a new
unsupervised learning method, called the common and distinctive pattern
analysis (CDPA), which appropriately defines the two types of data patterns by
further incorporating the common and distinctive patterns of the coefficient
matrices. A consistent estimation approach is developed for high-dimensional
settings, and shows reasonably good finite-sample performance in simulations.
Our simulation studies and real data analysis corroborate that the proposed
CDPA can provide better characterization of common and distinctive patterns and
thereby benefit data mining.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.10616</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.10616</id><submitter>Mark Dras</submitter><version version="v1"><date>Mon, 23 Dec 2019 04:38:00 GMT</date><size>161kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 21 Jan 2020 06:48:40 GMT</date><size>202kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 08:23:28 GMT</date><size>207kb</size><source_type>D</source_type></version><title>Siamese Networks for Large-Scale Author Identification</title><authors>Chakaveh Saedi and Mark Dras</authors><categories>cs.CL</categories><comments>28 pages. Accepted by Computer Speech and Language</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authorship attribution is the process of identifying the author of a text.
Approaches to tackling it have been conventionally divided into
classification-based ones, which work well for small numbers of candidate
authors, and similarity-based methods, which are applicable for larger numbers
of authors or for authors beyond the training set; these existing
similarity-based methods have only embodied static notions of similarity. Deep
learning methods, which blur the boundaries between classification-based and
similarity-based approaches, are promising in terms of ability to learn a
notion of similarity, but have previously only been used in a conventional
small-closed-class classification setup.
  Siamese networks have been used to develop learned notions of similarity in
one-shot image tasks, and also for tasks of mostly semantic relatedness in NLP.
We examine their application to the stylistic task of authorship attribution on
datasets with large numbers of authors, looking at multiple energy functions
and neural network architectures, and show that they can substantially
outperform previous approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.11300</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.11300</id><submitter>Lakshmi B Narayana V S Ch</submitter><version version="v1"><date>Tue, 24 Dec 2019 11:41:57 GMT</date><size>1281kb</size></version><version version="v10"><date>Wed, 10 Feb 2021 08:17:50 GMT</date><size>357kb</size></version><version version="v11"><date>Mon, 17 May 2021 05:56:22 GMT</date><size>356kb</size></version><version version="v2"><date>Mon, 30 Dec 2019 09:08:16 GMT</date><size>1285kb</size></version><version version="v3"><date>Tue, 7 Jan 2020 12:56:03 GMT</date><size>1293kb</size></version><version version="v4"><date>Sat, 25 Jan 2020 06:21:50 GMT</date><size>1295kb</size></version><version version="v5"><date>Mon, 9 Mar 2020 08:24:23 GMT</date><size>1296kb</size></version><version version="v6"><date>Wed, 11 Mar 2020 05:40:48 GMT</date><size>1295kb</size></version><version version="v7"><date>Wed, 8 Apr 2020 12:31:04 GMT</date><size>1295kb</size></version><version version="v8"><date>Thu, 4 Jun 2020 15:34:41 GMT</date><size>383kb</size></version><version version="v9"><date>Wed, 10 Jun 2020 19:56:51 GMT</date><size>383kb</size></version><title>RetroRenting: An Online Policy for Service Caching at the Edge</title><authors>V S Ch Lakshmi Narayana, Sharayu Moharir, Nikhil Karamchandani</authors><categories>cs.NI cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid proliferation of shared edge computing platforms has enabled
application service providers to deploy a wide variety of services with
stringent latency and high bandwidth requirements. A key advantage of these
platforms is that they provide pay-as-you-go flexibility by charging clients in
proportion to their resource usage through short-term contracts. This affords
the client significant cost-saving opportunities, by dynamically deciding when
to host its service on the platform, depending on the changing intensity of
requests.
  A natural policy for our setting is the Time-To-Live (TTL) policy. We show
that TTL performs poorly both in the adversarial arrival setting, i.e., in
terms of the competitive ratio, and for i.i.d. stochastic arrivals with low
arrival rates, irrespective of the value of the TTL timer.
  We propose an online policy called RetroRenting (RR) and show that in the
class of deterministic online policies, RR is order-optimal with respect to the
competitive ratio.
  In addition, we provide performance guarantees for RR for i.i.d. stochastic
arrival processes coupled with negatively associated rent cost sequences and
prove that it compares well with the optimal online policy. Further, we conduct
simulations using both synthetic and real world traces to compare the
performance of RR with the optimal offline and online policies. The simulations
show that the performance of RR is near optimal for all settings considered.
Our results illustrate the universality of RR.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.11443</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.11443</id><submitter>Julian G\&quot;oltz</submitter><version version="v1"><date>Tue, 24 Dec 2019 17:18:07 GMT</date><size>5932kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 31 Aug 2020 16:27:45 GMT</date><size>7015kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 19 Nov 2020 18:43:48 GMT</date><size>7019kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 17 May 2021 15:35:57 GMT</date><size>7527kb</size><source_type>D</source_type></version><title>Fast and energy-efficient neuromorphic deep learning with first-spike
  times</title><authors>Julian G\&quot;oltz, Laura Kriener, Andreas Baumbach, Sebastian
  Billaudelle, Oliver Breitwieser, Benjamin Cramer, Dominik Dold, Akos Ferenc
  Kungl, Walter Senn, Johannes Schemmel, Karlheinz Meier, Mihai Alexandru
  Petrovici</authors><categories>cs.NE cs.ET q-bio.NC stat.ML</categories><comments>24 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  For a biological agent operating under environmental pressure, energy
consumption and reaction times are of critical importance. Similarly,
engineered systems are optimized for short time-to-solution and low
energy-to-solution characteristics. At the level of neuronal implementation,
this implies achieving the desired results with as few and as early spikes as
possible. With time-to-first-spike coding both of these goals are inherently
emerging features of learning. Here, we describe a rigorous derivation of a
learning rule for such first-spike times in networks of leaky
integrate-and-fire neurons, relying solely on input and output spike times, and
show how this mechanism can implement error backpropagation in hierarchical
spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2
neuromorphic system and demonstrate its capability of harnessing the system's
speed and energy characteristics. Finally, we examine how our approach
generalizes to other neuromorphic platforms by studying how its performance is
affected by typical distortive effects induced by neuromorphic substrates.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.11611</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.11611</id><submitter>Yupan Liu</submitter><version version="v1"><date>Wed, 25 Dec 2019 07:21:57 GMT</date><size>27kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 16 Jul 2020 21:14:46 GMT</date><size>27kb</size><source_type>D</source_type></version><title>Towards a quantum-inspired proof for IP = PSPACE</title><authors>Ayal Green, Guy Kindler, Yupan Liu</authors><categories>quant-ph cs.CC</categories><comments>10 pages</comments><journal-ref>Quantum Inf. Comput. 21(5&amp;6): 377-386 (2021)</journal-ref><doi>10.26421/QIC21.5-6-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore quantum-inspired interactive proof systems where the prover is
limited. Namely, we improve on a result by [AG17] showing a quantum-inspired
interactive protocol ($\sf IP$) for $\sf PreciseBQP$ where the prover is only
assumed to be a $\sf PreciseBQP$ machine, and show that the result can be
strengthened to show an $\sf IP$ for $\sf NP^{PP}$ with a prover which is only
assumed to be an $\sf NP^{PP}$ machine - which was not known before. We also
show how the protocol can be used to directly verify $\sf QMA$ computations,
thus connecting the sum-check protocol by [AAV13] with the result of [AG17,
LFKN90]. Our results shed light on a quantum-inspired proof for ${\sf IP} =
{\sf PSPACE}$, as $\sf PreciseQMA$ captures the full $\sf PSPACE$ power.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1912.12150</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>1912.12150</id><submitter>Cencheng Shen</submitter><version version="v1"><date>Fri, 27 Dec 2019 15:16:40 GMT</date><size>144kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 7 Jan 2020 15:08:41 GMT</date><size>142kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 22 Jan 2020 21:53:47 GMT</date><size>143kb</size><source_type>D</source_type></version><version version="v4"><date>Fri, 21 Feb 2020 21:35:39 GMT</date><size>143kb</size><source_type>D</source_type></version><version version="v5"><date>Fri, 14 May 2021 18:09:51 GMT</date><size>154kb</size><source_type>D</source_type></version><title>The Chi-Square Test of Distance Correlation</title><authors>Cencheng Shen, Sambit Panda, Joshua T. Vogelstein</authors><categories>stat.ML cs.LG math.ST stat.ME stat.TH</categories><comments>21 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance correlation has gained much recent attention in the data science
community: the sample statistic is straightforward to compute and
asymptotically equals zero if and only if independence, making it an ideal
choice to discover any type of dependency structure given sufficient sample
size. One major bottleneck is the testing process: because the null
distribution of distance correlation depends on the underlying random variables
and metric choice, it typically requires a permutation test to estimate the
null and compute the p-value, which is very costly for large amount of data. To
overcome the difficulty, in this paper we propose a chi-square test for
distance correlation. Method-wise, the chi-square test is non-parametric,
extremely fast, and applicable to bias-corrected distance correlation using any
strong negative type metric or characteristic kernel. The test exhibits a
similar testing power as the standard permutation test, and can be utilized for
K-sample and partial testing. Theory-wise, we show that the underlying
chi-square distribution well approximates and dominates the limiting null
distribution in upper tail, prove the chi-square test can be valid and
universally consistent for testing independence, and establish a testing power
inequality with respect to the permutation test.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.00081</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.00081</id><submitter>Patrick Kelley</submitter><version version="v1"><date>Fri, 27 Dec 2019 10:27:05 GMT</date><size>1117kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 17:20:34 GMT</date><size>2350kb</size><source_type>D</source_type></version><title>Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial
  Intelligence in 8 Countries</title><authors>Patrick Gage Kelley, Yongwei Yang, Courtney Heldreth, Christopher
  Moessner, Aaron Sedley, Andreas Kramm, David T. Newman, and Allison Woodruff</authors><categories>cs.CY</categories><comments>12 pages, 2 figures, 3 tables. AIES 2021: Proceedings of the AAAI/ACM
  Conference on AI, Ethics, and Society</comments><acm-class>K.4.1; I.2</acm-class><doi>10.1145/3461702.3462605</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the influence and use of artificial intelligence (AI) have grown and its
transformative potential has become more apparent, many questions have been
raised regarding the economic, political, social, and ethical implications of
its use. Public opinion plays an important role in these discussions,
influencing product adoption, commercial development, research funding, and
regulation. In this paper we present results of an in-depth survey of public
opinion of artificial intelligence conducted with 10,005 respondents spanning
eight countries and six continents. We report widespread perception that AI
will have significant impact on society, accompanied by strong support for the
responsible development and use of AI, and also characterize the public's
sentiment towards AI with four key themes (exciting, useful, worrying, and
futuristic) whose prevalence distinguishes response to AI in different
countries.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.00712</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.00712</id><submitter>Quanyan Zhu</submitter><version version="v1"><date>Fri, 3 Jan 2020 03:39:53 GMT</date><size>4055kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 20:50:22 GMT</date><size>2284kb</size><source_type>D</source_type></version><title>Control Challenges for Resilient Control Systems</title><authors>Quanyan Zhu</authors><categories>eess.SY cs.GT cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this chapter, we introduce methods to address resiliency issues for
control systems. The main challenge for control systems is its cyber-physical
system nature which strongly couples the cyber systems with physical layer
dynamics. Hence the resiliency issues for control systems need to be addressed
by integrating the cyber resiliency with the physical layer resiliency. We
introduce frameworks that can provide a holistic view of the control system
resiliency and a quantitative design paradigm that can enable an optimal
cross-layer and cross-stage design at the planning, operation, and recovery
stage of control systems. The control systems are often large-scale systems in
industrial application and critical infrastructures. Decentralized control of
such systems is indispensable. We extend the resiliency framework to address
distributed and collaborative resiliency among decentralized control agents.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.02797</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.02797</id><submitter>Marc Schr\&quot;oder</submitter><version version="v1"><date>Thu, 9 Jan 2020 01:17:14 GMT</date><size>44kb</size></version><version version="v2"><date>Fri, 28 Feb 2020 06:50:53 GMT</date><size>48kb</size></version><version version="v3"><date>Mon, 2 Mar 2020 21:22:59 GMT</date><size>48kb</size></version><version version="v4"><date>Tue, 16 Jun 2020 06:36:23 GMT</date><size>52kb</size></version><version version="v5"><date>Wed, 17 Jun 2020 11:29:50 GMT</date><size>48kb</size></version><version version="v6"><date>Mon, 17 May 2021 11:54:38 GMT</date><size>59kb</size><source_type>D</source_type></version><title>Approximation and Convergence of Large Atomic Congestion Games</title><authors>Roberto Cominetti, Marco Scarsini, Marc Schr\&quot;oder, Nicol\'as
  Stier-Moses</authors><categories>cs.GT math.OC math.PR</categories><comments>37 pages, 5 figures</comments><msc-class>91A13, 91A06, 91A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of whether, and in what sense, Wardrop equilibria
provide a good approximation for Nash equilibria in atomic unsplittable
congestion games with a large number of small players. We examine two different
definitions of small players. In the first setting, we consider games where
each player's weight is small. We prove that when the number of players goes to
infinity and their weights to zero, the random flows in all (mixed) Nash
equilibria for the finite games converge in distribution to the set of Wardrop
equilibria of the corresponding nonatomic limit game. In the second setting, we
consider an increasing number of players with a unit weight that participate in
the game with a decreasingly small probability. In this case, the Nash
equilibrium flows converge in total variation towards Poisson random variables
whose expected values are Wardrop equilibria of a different nonatomic game with
suitably-defined costs. The latter can be viewed as symmetric equilibria in a
Poisson game in the sense of Myerson, establishing a plausible connection
between the Wardrop model for routing games and the stochastic fluctuations
observed in real traffic. In both settings we provide explicit approximation
bounds, and we study the convergence of the price of anarchy. Beyond the case
of congestion games, we prove a general result on the convergence of large
games with random players towards Poisson games.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.03515</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.03515</id><submitter>Francesco Del Duchetto</submitter><version version="v1"><date>Fri, 10 Jan 2020 15:38:07 GMT</date><size>3095kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 13 Jan 2020 09:32:05 GMT</date><size>3099kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 14:44:38 GMT</date><size>5584kb</size><source_type>D</source_type></version><title>Are you still with me? Continuous Engagement Assessment from a Robot's
  Point of View</title><authors>Francesco Del Duchetto, Paul Baxter, Marc Hanheide</authors><categories>cs.RO</categories><journal-ref>Front. Robot. AI 7:116 (2020)</journal-ref><doi>10.3389/frobt.2020.00116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuously measuring the engagement of users with a robot in a Human-Robot
Interaction (HRI) setting paves the way towards in-situ reinforcement learning,
improve metrics of interaction quality, and can guide interaction design and
behaviour optimisation. However, engagement is often considered very
multi-faceted and difficult to capture in a workable and generic computational
model that can serve as an overall measure of engagement. Building upon the
intuitive ways humans successfully can assess situation for a degree of
engagement when they see it, we propose a novel regression model (utilising CNN
and LSTM networks) enabling robots to compute a single scalar engagement during
interactions with humans from standard video streams, obtained from the point
of view of an interacting robot. The model is based on a long-term dataset from
an autonomous tour guide robot deployed in a public museum, with continuous
annotation of a numeric engagement assessment by three independent coders. We
show that this model not only can predict engagement very well in our own
application domain but show its successful transfer to an entirely different
dataset (with different tasks, environment, camera, robot and people). The
trained model and the software is available to the HRI community as a tool to
measure engagement in a variety of settings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.05137</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.05137</id><submitter>Maryam Hashemi Miss</submitter><version version="v1"><date>Wed, 15 Jan 2020 05:38:24 GMT</date><size>809kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 5 Mar 2020 05:29:38 GMT</date><size>357kb</size><source_type>D</source_type></version><title>CNN-based Driver Drowsiness Detection</title><authors>Maryam Hashemi, Alireza Mirrashid, Aliasghar Beheshti Shirazi</authors><categories>eess.IV cs.CV cs.LG</categories><doi>10.1007/s42979-020-00306-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel system for the problem of driver drowsiness
detection. In this system, Convolutional Neural Networks (CNN) are used for
driver eye monitoring with regarding two goals of real-time application,
including high accuracy and fastness, and introduce a new dataset for eye
closure detection. Three networks introduced as a potential network for eye
status classification in which one of them is a fully designed neural network
(FD-NN), and others use transfer learning with VGG16 and VGG19 with extra
designed layers (TL-VGG). Lack of an available and accurate eye dataset
strongly feels in the area of eye closure detection. Therefore, a new
comprehensive dataset proposed. The experimental results show the high accuracy
and low computational complexity of the estimations and the ability of the
proposed framework on drowsiness detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.05209</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.05209</id><submitter>Rohan Saphal Mr</submitter><version version="v1"><date>Wed, 15 Jan 2020 10:12:00 GMT</date><size>1309kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 13:35:02 GMT</date><size>2268kb</size><source_type>D</source_type></version><title>SEERL: Sample Efficient Ensemble Reinforcement Learning</title><authors>Rohan Saphal, Balaraman Ravindran, Dheevatsa Mudigere, Sasikanth
  Avancha, Bharat Kaul</authors><categories>cs.LG stat.ML</categories><comments>Accepted at Proceedings of the 20th International Conference on
  Autonomous Agents and MultiAgent Systems</comments><doi>10.5555/3463952.3464080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensemble learning is a very prevalent method employed in machine learning.
The relative success of ensemble methods is attributed to their ability to
tackle a wide range of instances and complex problems that require different
low-level approaches. However, ensemble methods are relatively less popular in
reinforcement learning owing to the high sample complexity and computational
expense involved in obtaining a diverse ensemble. We present a novel training
and model selection framework for model-free reinforcement algorithms that use
ensembles of policies obtained from a single training run. These policies are
diverse in nature and are learned through directed perturbation of the model
parameters at regular intervals. We show that learning and selecting an
adequately diverse set of policies is required for a good ensemble while
extreme diversity can prove detrimental to overall performance. Selection of an
adequately diverse set of policies is done through our novel policy selection
framework. We evaluate our approach on challenging discrete and continuous
control tasks and also discuss various ensembling strategies. Our framework is
substantially sample efficient, computationally inexpensive and is seen to
outperform state-of-the-art (SOTA) scores in Atari 2600 and Mujoco.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.05921</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.05921</id><submitter>Marc Hellmuth</submitter><version version="v1"><date>Thu, 16 Jan 2020 16:14:36 GMT</date><size>51kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 20 Jan 2021 08:51:04 GMT</date><size>87kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 11:13:11 GMT</date><size>94kb</size><source_type>D</source_type></version><title>Generalized Fitch Graphs III: Symmetrized Fitch maps and Sets of
  Symmetric Binary Relations that are explained by Unrooted Edge-labeled Trees</title><authors>Marc Hellmuth, Carsten R. Seemann and Peter F. Stadler</authors><categories>cs.DM cs.CC cs.DS math.CO</categories><msc-class>68R01, 05C05, 92D15</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Binary relations derived from labeled rooted trees play an import role in
mathematical biology as formal models of evolutionary relationships. The
(symmetrized) Fitch relation formalizes xenology as the pairs of genes
separated by at least one horizontal transfer event. As a natural
generalization, we consider symmetrized Fitch maps, that is, symmetric maps
$\varepsilon$ that assign a subset of colors to each pair of vertices in $X$
and that can be explained by a tree $T$ with edges that are labeled with
subsets of colors in the sense that the color $m$ appears in $\varepsilon(x,y)$
if and only if $m$ appears in a label along the unique path between $x$ and $y$
in $T$. We first give an alternative characterization of the monochromatic case
and then give a characterization of symmetrized Fitch maps in terms of
compatibility of a certain set of quartets. We show that recognition of
symmetrized Fitch maps is NP-complete. In the restricted case where
$|\varepsilon(x,y)|\leq 1$ the problem becomes polynomial, since such maps
coincide with class of monochromatic Fitch maps whose graph-representations
form precisely the class of complete multi-partite graphs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.07524</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.07524</id><submitter>Pushkar Mishra</submitter><version version="v1"><date>Fri, 17 Jan 2020 06:26:40 GMT</date><size>2049kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 20 Feb 2020 10:14:08 GMT</date><size>2448kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 16 Oct 2020 12:40:50 GMT</date><size>2405kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 16 May 2021 19:40:24 GMT</date><size>3356kb</size><source_type>D</source_type></version><title>Node Masking: Making Graph Neural Networks Generalize and Scale Better</title><authors>Pushkar Mishra, Aleksandra Piktus, Gerard Goossen, Fabrizio Silvestri</authors><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Neural Networks (GNNs) have received a lot of interest in the recent
times. From the early spectral architectures that could only operate on
undirected graphs per a transductive learning paradigm to the current state of
the art spatial ones that can apply inductively to arbitrary graphs, GNNs have
seen significant contributions from the research community. In this paper, we
utilize some theoretical tools to better visualize the operations performed by
state of the art spatial GNNs. We analyze the inner workings of these
architectures and introduce a simple concept, Node Masking, that allows them to
generalize and scale better. To empirically validate the concept, we perform
several experiments on some widely-used datasets for node classification in
both the transductive and inductive settings, hence laying down strong
benchmarks for future research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.08293</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.08293</id><submitter>Kostantinos Papadamou Mr</submitter><version version="v1"><date>Wed, 22 Jan 2020 21:47:54 GMT</date><size>1127kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 25 Jan 2020 12:40:34 GMT</date><size>1127kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 15 Jun 2020 11:55:45 GMT</date><size>446kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 16 Jun 2020 08:17:04 GMT</date><size>446kb</size><source_type>D</source_type></version><version version="v5"><date>Sun, 17 Jan 2021 16:35:06 GMT</date><size>2265kb</size><source_type>D</source_type></version><version version="v6"><date>Tue, 18 May 2021 09:24:47 GMT</date><size>2354kb</size><source_type>D</source_type></version><title>&quot;How over is it?&quot; Understanding the Incel Community on YouTube</title><authors>Kostantinos Papadamou, Savvas Zannettou, Jeremy Blackburn, Emiliano De
  Cristofaro, Gianluca Stringhini, and Michael Sirivianos</authors><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  YouTube is by far the largest host of user-generated video content worldwide.
Alas, the platform also hosts inappropriate, toxic, and hateful content. One
community that has often been linked to sharing and publishing hateful and
misogynistic content is the so-called Involuntary Celibates (Incels), a loosely
defined movement ostensibly focusing on men's issues. In this paper, we set out
to analyze the Incel community on YouTube by focusing on this community's
evolution over the last decade and understanding whether YouTube's
recommendation algorithm steers users towards Incel-related videos. We collect
videos shared on Incel communities within Reddit and perform a data-driven
characterization of the content posted on YouTube. Among other things, we find
that the Incel community on YouTube is getting traction and that during the
last decade, the number of Incel-related videos and comments rose
substantially. We also find that users have a 6.3% of being suggested an
Incel-related video by YouTube's recommendation algorithm within five hops when
starting from a non-Incel-related video. Overall, our findings paint an
alarming picture of online radicalization: not only Incel activity is
increasing over time, but platforms may also play an active role in steering
users towards such extreme content.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.09478</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.09478</id><submitter>Nick Dudley Ward</submitter><version version="v1"><date>Sun, 26 Jan 2020 16:19:41 GMT</date><size>1941kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 02:01:41 GMT</date><size>1941kb</size><source_type>D</source_type></version><title>A Discontinuous Galerkin method for three-dimensional elastic and
  poroelastic wave propagation: forward and adjoint problems</title><authors>Nick Dudley Ward, Simon Eveson and Timo Lahivaara</authors><categories>physics.comp-ph cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a numerical solver for three-dimensional wave propagation in
coupled poroelastic-elastic media, based on a high-order discontinuous Galerkin
(DG) method, with the Biot poroelastic wave equation formulated as a first
order conservative velocity/strain hyperbolic system. To derive an upwind
numerical flux, we find an exact solution to the Riemann problem, including the
poroelastic-elastic interface; we also consider attenuation mechanisms both in
Biot's low- and high-frequency regimes. Using either a low-storage explicit or
implicit-explicit (IMEX) Runge-Kutta scheme, according to the stiffness of the
problem, we study the convergence properties of the proposed DG scheme and
verify its numerical accuracy. In the Biot low frequency case, the wave can be
highly dissipative for small permeabilities; here, numerical errors associated
with the dissipation terms appear to dominate those arising from discretisation
of the main hyperbolic system.
  We then implement the adjoint method for this formulation of Biot's equation.
In contrast with the usual second order formulation of the Biot equation, we
are not dealing with a self-adjoint system but, with an appropriate inner
product, the adjoint may be identified with a non-conservative velocity/stress
formulation of the Biot equation. We derive dual fluxes for the adjoint and
present a simple but illuminating example of the application of the adjoint
method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.10290</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.10290</id><submitter>Markus P\&quot;uschel</submitter><version version="v1"><date>Tue, 28 Jan 2020 12:19:57 GMT</date><size>131kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 22 Oct 2020 09:10:41 GMT</date><size>786kb</size><source_type>D</source_type></version><title>Discrete Signal Processing with Set Functions</title><authors>Markus P\&quot;uschel and Chris Wendler</authors><categories>cs.IT cs.LG eess.SP math.IT</categories><comments>16 pages, submitted for publication</comments><journal-ref>IEEE Transactions on Signal Processing, Vol. 69, pp. 1039-1053,
  2021</journal-ref><doi>10.1109/TSP.2020.3046972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Set functions are functions (or signals) indexed by the powerset (set of all
subsets) of a finite set N. They are fundamental and ubiquitous in many
application domains and have been used, for example, to formally describe or
quantify loss functions for semantic image segmentation, the informativeness of
sensors in sensor networks the utility of sets of items in recommender systems,
cooperative games in game theory, or bidders in combinatorial auctions. In
particular, the subclass of submodular functions occurs in many optimization
and machine learning problems. In this paper, we derive discrete-set signal
processing (SP), a novel shift-invariant linear signal processing framework for
set functions. Discrete-set SP considers different notions of shift obtained
from set union and difference operations. For each shift it provides associated
notions of shift-invariant filters, convolution, Fourier transform, and
frequency response. We provide intuition for our framework using the concept of
generalized coverage function that we define, identify multivariate mutual
information as a special case of a discrete-set spectrum, and motivate
frequency ordering. Our work brings a new set of tools for analyzing and
processing set functions, and, in particular, for dealing with their
exponential nature. We show two prototypical applications and experiments:
compression in submodular function optimization and sampling for preference
elicitation in combinatorial auctions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.10818</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.10818</id><submitter>George Wynne</submitter><version version="v1"><date>Wed, 29 Jan 2020 13:28:27 GMT</date><size>73kb</size></version><version version="v2"><date>Tue, 9 Jun 2020 15:52:29 GMT</date><size>67kb</size></version><version version="v3"><date>Tue, 18 May 2021 12:33:03 GMT</date><size>68kb</size></version><title>Convergence Guarantees for Gaussian Process Means With Misspecified
  Likelihoods and Smoothness</title><authors>George Wynne, Fran\c{c}ois-Xavier Briol and Mark Girolami</authors><categories>math.ST cs.LG cs.NA math.NA stat.ML stat.TH</categories><comments>Accepted to JMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes are ubiquitous in machine learning, statistics, and
applied mathematics. They provide a flexible modelling framework for
approximating functions, whilst simultaneously quantifying uncertainty.
However, this is only true when the model is well-specified, which is often not
the case in practice. In this paper, we study the properties of Gaussian
process means when the smoothness of the model and the likelihood function are
misspecified. In this setting, an important theoretical question of practial
relevance is how accurate the Gaussian process approximations will be given the
difficulty of the problem, our model and the extent of the misspecification.
The answer to this problem is particularly useful since it can inform our
choice of model and experimental design. In particular, we describe how the
experimental design and choice of kernel and kernel hyperparameters can be
adapted to alleviate model misspecification.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.11560</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.11560</id><submitter>Jeremy Siek</submitter><version version="v1"><date>Thu, 30 Jan 2020 20:38:23 GMT</date><size>701kb</size></version><version version="v2"><date>Sat, 15 May 2021 16:05:00 GMT</date><size>61kb</size></version><title>Parameterized Cast Calculi and Reusable Meta-theory for Gradually Typed
  Lambda Calculi</title><authors>Jeremy G. Siek</authors><categories>cs.PL</categories><comments>In submission to Journal of Functional Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research on gradual typing has led to many variations on the Gradually
Typed Lambda Calculus (GTLC) of Siek and Taha (2006) and its underlying cast
calculus. For example, Wadler and Findler (2009) added blame tracking, Siek et
al. (2009) investigated alternate cast evaluation strategies, and Herman et al.
(2010) replaced casts with coercions for space-efficiency. The meta-theory for
the GTLC has also expanded beyond type safety to include blame safety
(Tobin-Hochstadt and Felleisen 2006), space consumption (Herman et al. 2010),
and the gradual guarantees (Siek et al. 2015). These results have been proven
for some variations of the GTLC but not others. Furthermore, researchers
continue to develop variations on the GTLC but establishing all of the
meta-theory for new variations is time consuming.
  This article identifies abstractions that capture similarities between many
cast calculi in the form of two parameterized cast calculi, one for the
purposes of language specification and the other to guide space-efficient
implementations. The article then develops reusable meta-theory for these two
calculi, proving type safety, blame safety, the gradual guarantees, and space
consumption. Finally, the article instantiates this meta-theory for eight cast
calculi including five from the literature and three new calculi. All of these
definitions and theorems, including the two parameterized calculi, the reusable
meta-theory, and the eight instantiations, are mechanized in Agda making
extensive use of module parameters and dependent records to define the
abstractions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2001.11905</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2001.11905</id><submitter>Laurens Devos</submitter><version version="v1"><date>Fri, 31 Jan 2020 15:31:23 GMT</date><size>81kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 16 Nov 2020 13:45:19 GMT</date><size>81kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 12:54:32 GMT</date><size>93kb</size><source_type>D</source_type></version><title>Verifying Tree Ensembles by Reasoning about Potential Instances</title><authors>Laurens Devos, Wannes Meert, Jesse Davis</authors><categories>cs.LG cs.AI stat.ML</categories><comments>Devos, Laurens, Wannes Meert, and Jesse Davis. &quot;Verifying tree
  ensembles by reasoning about potential instances.&quot; Proceedings of the 2021
  SIAM International Conference on Data Mining (SDM). Society for Industrial
  and Applied Mathematics, 2021</comments><doi>10.1137/1.9781611976700.51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imagine being able to ask questions to a black box model such as &quot;Which
adversarial examples exist?&quot;, &quot;Does a specific attribute have a
disproportionate effect on the model's prediction?&quot; or &quot;What kind of
predictions could possibly be made for a partially described example?&quot; This
last question is particularly important if your partial description does not
correspond to any observed example in your data, as it provides insight into
how the model will extrapolate to unseen data. These capabilities would be
extremely helpful as they would allow a user to better understand the model's
behavior, particularly as it relates to issues such as robustness, fairness,
and bias. In this paper, we propose such an approach for an ensemble of trees.
Since, in general, this task is intractable we present a strategy that (1) can
prune part of the input space given the question asked to simplify the problem;
and (2) follows a divide and conquer approach that is incremental and can
always return some answers and indicates which parts of the input domains are
still uncertain. The usefulness of our approach is shown on a diverse set of
use cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.02041</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.02041</id><submitter>Henry Adams</submitter><version version="v1"><date>Wed, 5 Feb 2020 23:57:26 GMT</date><size>798kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 9 Jun 2020 14:11:50 GMT</date><size>1973kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 14 May 2021 21:43:08 GMT</date><size>2604kb</size><source_type>D</source_type></version><title>An Adaptation for Iterative Structured Matrix Completion</title><authors>Henry Adams, Lara Kassab, Deanna Needell</authors><categories>math.NA cs.NA</categories><msc-class>15A83, 65F55 (Primary), 65F50 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of predicting missing entries of a matrix, from a subset of known
entries, is known as \textit{matrix completion}. In today's data-driven world,
data completion is essential whether it is the main goal or a pre-processing
step. Structured matrix completion includes any setting in which data is not
missing uniformly at random. In recent work, a modification to the standard
nuclear norm minimization (NNM) for matrix completion has been developed to
take into account \emph{sparsity-based} structure in the missing entries. This
notion of structure is motivated in many settings including recommender
systems, where the probability that an entry is observed depends on the value
of the entry. We propose adjusting an Iteratively Reweighted Least Squares
(IRLS) algorithm for low-rank matrix completion to take into account
sparsity-based structure in the missing entries. We also present an iterative
gradient-projection-based implementation of the algorithm that can handle
large-scale matrices. Finally, we present a robust array of numerical
experiments on matrices of varying sizes, ranks, and level of structure. We
show that our proposed method is comparable with the adjusted NNM on
small-sized matrices, and often outperforms the IRLS algorithm in structured
settings on matrices up to size $1000 \times 1000$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.02804</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.02804</id><submitter>Luis Larios-C\'ardenas</submitter><version version="v1"><date>Tue, 4 Feb 2020 00:49:47 GMT</date><size>9752kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 5 Dec 2020 04:46:47 GMT</date><size>5076kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 03:14:39 GMT</date><size>5070kb</size><source_type>D</source_type></version><title>A Deep Learning Approach for the Computation of Curvature in the
  Level-Set Method</title><authors>Luis \'Angel Larios-C\'ardenas and Frederic Gibou</authors><categories>math.NA cs.LG cs.NA stat.ML</categories><comments>To appear in SIAM Journal on Scientific Computing</comments><msc-class>68T99 (primary), 65Z05 (secondary), 65N06</msc-class><acm-class>I.2.6; G.1.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a deep learning strategy to estimate the mean curvature of
two-dimensional implicit interfaces in the level-set method. Our approach is
based on fitting feed-forward neural networks to synthetic data sets
constructed from circular interfaces immersed in uniform grids of various
resolutions. These multilayer perceptrons process the level-set values from
mesh points next to the free boundary and output the dimensionless curvature at
their closest locations on the interface. Accuracy analyses involving irregular
interfaces, in both uniform and adaptive grids, show that our models are
competitive with traditional numerical schemes in the $L^1$ and $L^2$ norms. In
particular, our neural networks approximate curvature with comparable precision
in coarse resolutions, when the interface features steep curvature regions, and
when the number of iterations to reinitialize the level-set function is small.
Although the conventional numerical approach is more robust than our framework,
our results have unveiled the potential of machine learning for dealing with
computational tasks where the level-set method is known to experience
difficulties. We also establish that an application-dependent map of local
resolutions to neural models can be devised to estimate mean curvature more
effectively than a universal neural network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.03979</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.03979</id><submitter>Wanrong Zhu</submitter><version version="v1"><date>Mon, 10 Feb 2020 17:46:10 GMT</date><size>685kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 02:16:56 GMT</date><size>1260kb</size><source_type>D</source_type></version><title>Online Covariance Matrix Estimation in Stochastic Gradient Descent</title><authors>Wanrong Zhu, Xi Chen, Wei Biao Wu</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic gradient descent (SGD) algorithm is widely used for parameter
estimation, especially for huge data sets and online learning. While this
recursive algorithm is popular for computation and memory efficiency,
quantifying variability and randomness of the solutions has been rarely
studied. This paper aims at conducting statistical inference of SGD-based
estimates in an online setting. In particular, we propose a fully online
estimator for the covariance matrix of averaged SGD iterates (ASGD) only using
the iterates from SGD. We formally establish our online estimator's consistency
and show that the convergence rate is comparable to offline counterparts. Based
on the classic asymptotic normality results of ASGD, we construct
asymptotically valid confidence intervals for model parameters. Upon receiving
new observations, we can quickly update the covariance matrix estimate and the
confidence intervals. This approach fits in an online setting and takes full
advantage of SGD: efficiency in computation and memory.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.04143</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.04143</id><submitter>Matthew Morse</submitter><version version="v1"><date>Tue, 11 Feb 2020 00:11:21 GMT</date><size>5620kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 12 May 2020 00:31:56 GMT</date><size>9105kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 16 Dec 2020 06:10:41 GMT</date><size>28441kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 18 May 2021 16:31:00 GMT</date><size>28494kb</size><source_type>D</source_type></version><title>A robust solver for elliptic PDEs in 3D complex geometries</title><authors>Matthew J. Morse, Abtin Rahimian, Denis Zorin</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a boundary integral equation solver for elliptic partial
differential equations on complex \threed geometries. Our method is efficient,
high-order accurate and robustly handles complex geometries. A key component is
our singular and near-singular layer potential evaluation scheme, \qbkix: a
simple extrapolation of the solution along a line to the boundary. We present a
series of geometry-processing algorithms required for \qbkix to run efficiently
with accuracy guarantees on arbitrary geometries and an adaptive upsampling
scheme based on a iteration-free heuristic for quadrature error. We validate
the accuracy and performance with a series of numerical tests and compare our
approach to a competing local evaluation method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.04805</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.04805</id><submitter>Christoph David Hofer PhD MSc</submitter><version version="v1"><date>Wed, 12 Feb 2020 05:25:15 GMT</date><size>1594kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 04:15:03 GMT</date><size>1598kb</size><source_type>D</source_type></version><title>Topologically Densified Distributions</title><authors>Christoph D. Hofer, Florian Graf, Marc Niethammer, Roland Kwitt</authors><categories>cs.LG math.AT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study regularization in the context of small sample-size learning with
over-parameterized neural networks. Specifically, we shift focus from
architectural properties, such as norms on the network weights, to properties
of the internal representations before a linear classifier. Specifically, we
impose a topological constraint on samples drawn from the probability measure
induced in that space. This provably leads to mass concentration effects around
the representations of training instances, i.e., a property beneficial for
generalization. By leveraging previous work to impose topological constraints
in a neural network setting, we provide empirical evidence (across various
vision benchmarks) to support our claim for better generalization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.07464</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.07464</id><submitter>Jihua Zhu</submitter><version version="v1"><date>Tue, 18 Feb 2020 10:04:51 GMT</date><size>962kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 9 Mar 2020 08:55:53 GMT</date><size>1003kb</size><source_type>D</source_type></version><title>Registration of multi-view point sets under the perspective of
  expectation-maximization</title><authors>Jihua Zhu, Jing Zhang, Huimin Lu, and Zhongyu Li</authors><categories>cs.CV</categories><journal-ref>IEEE TIP 2020</journal-ref><doi>10.1109/TIP.2020.3024096</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Registration of multi-view point sets is a prerequisite for 3D model
reconstruction. To solve this problem, most of previous approaches either
partially explore available information or blindly utilize unnecessary
information to align each point set, which may lead to the undesired results or
introduce extra computation complexity. To this end, this paper consider the
multi-view registration problem as a maximum likelihood estimation problem and
proposes a novel multi-view registration approach under the perspective of
Expectation-Maximization (EM). The basic idea of our approach is that different
data points are generated by the same number of Gaussian mixture models (GMMs).
For each data point in one point set, its nearest neighbors can be searched
from other well-aligned point sets. Then, we can suppose this data point is
generated by the special GMM, which is composed of each nearest neighbor
adhered with one Gaussian distribution. Based on this assumption, it is
reasonable to define the likelihood function including all rigid
transformations, which requires to be estimated for multi-view registration.
Subsequently, the EM algorithm is utilized to maximize the likelihood function
so as to estimate all rigid transformations. Finally, the proposed approach is
tested on several bench mark data sets and compared with some state-of-the-art
algorithms. Experimental results illustrate its super performance on accuracy,
robustness and efficiency for the registration of multi-view point sets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.08641</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.08641</id><submitter>Tanya Motwani</submitter><version version="v1"><date>Thu, 20 Feb 2020 09:51:48 GMT</date><size>548kb</size></version><version version="v2"><date>Mon, 17 May 2021 09:48:42 GMT</date><size>145kb</size><source_type>D</source_type></version><title>A Novel Framework for Selection of GANs for an Application</title><authors>Tanya Motwani and Manojkumar Parmar</authors><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>11 pages, 1 figure, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative Adversarial Network (GAN) is a current focal point of research.
The body of knowledge is fragmented, leading to a trial-error method while
selecting an appropriate GAN for a given scenario. We provide a comprehensive
summary of the evolution of GANs starting from its inception addressing issues
like mode collapse, vanishing gradient, unstable training and non-convergence.
We also provide a comparison of various GANs from the application point of
view, its behaviour and implementation details. We propose a novel framework to
identify candidate GANs for a specific use case based on architecture, loss,
regularization and divergence. We also discuss application of the framework
using an example, and we demonstrate a significant reduction in search space.
This efficient way to determine potential GANs lowers unit economics of AI
development for organizations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.11440</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.11440</id><submitter>L.A. Prashanth</submitter><version version="v1"><date>Wed, 26 Feb 2020 12:53:04 GMT</date><size>213kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 11:50:36 GMT</date><size>33kb</size></version><title>Non-asymptotic bounds for stochastic optimization with biased noisy
  gradient oracles</title><authors>Nirav Bhavsar and Prashanth L.A</authors><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce biased gradient oracles to capture a setting where the function
measurements have an estimation error that can be controlled through a batch
size parameter. Our proposed oracles are appealing in several practical
contexts, for instance, risk measure estimation from a batch of independent and
identically distributed (i.i.d.) samples, or simulation optimization, where the
function measurements are `biased' due to computational constraints. In either
case, increasing the batch size reduces the estimation error. We highlight the
applicability of our biased gradient oracles in a risk-sensitive reinforcement
learning setting. In the stochastic non-convex optimization context, we analyze
a variant of the randomized stochastic gradient (RSG) algorithm with a biased
gradient oracle. We quantify the convergence rate of this algorithm by deriving
non-asymptotic bounds on its performance. Next, in the stochastic convex
optimization setting, we derive non-asymptotic bounds for the last iterate of a
stochastic gradient descent (SGD) algorithm with a biased gradient oracle.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.11589</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.11589</id><submitter>Carolyn Kim</submitter><version version="v1"><date>Wed, 26 Feb 2020 16:17:05 GMT</date><size>764kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 00:14:54 GMT</date><size>765kb</size><source_type>D</source_type></version><title>Recommendation on a Budget: Column Space Recovery from Partially
  Observed Entries with Random or Active Sampling</title><authors>Carolyn Kim, Mohsen Bayati</authors><categories>cs.LG cs.IR stat.ML</categories><comments>A shorter version is accepted to AISTATS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze alternating minimization for column space recovery of a partially
observed, approximately low rank matrix with a growing number of columns and a
fixed budget of observations per column. In this work, we prove that if the
budget is greater than the rank of the matrix, column space recovery succeeds
-- as the number of columns grows, the estimate from alternating minimization
converges to the true column space with probability tending to one. From our
proof techniques, we naturally formulate an active sampling strategy for
choosing entries of a column that is theoretically and empirically (on
synthetic and real data) better than the commonly studied uniformly random
sampling strategy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2002.12104</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2002.12104</id><submitter>Hamid Usefi</submitter><version version="v1"><date>Thu, 27 Feb 2020 14:17:39 GMT</date><size>148kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 01:00:02 GMT</date><size>58kb</size></version><title>High-Dimensional Feature Selection for Genomic Datasets</title><authors>Majid Afshar, Hamid Usefi</authors><categories>cs.LG q-bio.QM stat.ML</categories><journal-ref>August 2020, Knowledge-Based Systems 206(4):106370</journal-ref><doi>10.1016/j.knosys.2020.106370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in machine learning and pattern recognition is the process
of recognizing the most important features. In this paper, we provide a new
feature selection method (DRPT) that consists of first removing the irrelevant
features and then detecting correlations between the remaining features. Let
$D=[A\mid \mathbf{b}]$ be a dataset, where $\mathbf{b}$ is the class label and
$A$ is a matrix whose columns are the features. We solve $A\mathbf{x} =
\mathbf{b}$ using the least squares method and the pseudo-inverse of $A$. Each
component of $\mathbf{x}$ can be viewed as an assigned weight to the
corresponding column (feature). We define a threshold based on the local maxima
of $\mathbf{x}$ and remove those features whose weights are smaller than the
threshold.
  To detect the correlations in the reduced matrix, which we still call $A$, we
consider a perturbation $\tilde A$ of $A$. We prove that correlations are
encoded in $\Delta\mathbf{x}=\mid \mathbf{x} -\tilde{\mathbf{x}}\mid $, where
$\tilde{\mathbf{x}}$ is the least quares solution of
  $\tilde A\tilde{\mathbf{x}}=\mathbf{b}$. We cluster features first based on
$\Delta\mathbf{x}$ and then using the entropy of features. Finally, a feature
is selected from each sub-cluster based on its weight and entropy. The
effectiveness of DRPT has been verified by performing a series of comparisons
with seven state-of-the-art feature selection methods over ten genetic datasets
ranging up from 9,117 to 267,604 features. The results show that, over all, the
performance of DRPT is favorable in several aspects compared to each feature
selection algorithm.
  \e
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.01491</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.01491</id><submitter>Jonathan Sterling</submitter><version version="v1"><date>Tue, 3 Mar 2020 12:59:54 GMT</date><size>100kb</size></version><version version="v2"><date>Sat, 15 May 2021 16:20:35 GMT</date><size>100kb</size></version><title>A Cubical Language for Bishop Sets</title><authors>Jonathan Sterling, Carlo Angiuli, Daniel Gratzer</authors><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present XTT, a version of Cartesian cubical type theory specialized for
Bishop sets \`a la Coquand, in which every type enjoys a definitional version
of the uniqueness of identity proofs. Using cubical notions, XTT reconstructs
many of the ideas underlying Observational Type Theory, a version of
intensional type theory that supports function extensionality. We prove the
canonicity property of XTT (that every closed boolean is definitionally equal
to a constant) by Artin gluing.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.04398</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.04398</id><submitter>Alexandru Hening</submitter><version version="v1"><date>Mon, 9 Mar 2020 20:24:36 GMT</date><size>2996kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 14:03:46 GMT</date><size>2987kb</size><source_type>D</source_type></version><title>Stationary distributions of persistent ecological systems</title><authors>Alexandru Hening and Yao Li</authors><categories>q-bio.PE cs.NA math.NA math.PR</categories><comments>48 pages, 14 figures</comments><msc-class>92D25, 37H15, 60H10, 60J05, 60J99, 65C05, 60H35, 37M25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze ecological systems that are influenced by random environmental
fluctuations. We first provide general conditions which ensure that the species
coexist and the system converges to a unique invariant probability measure
(stationary distribution). Since it is usually impossible to characterize this
invariant probability measure analytically, we develop a powerful method for
numerically approximating invariant probability measures. This allows us to
shed light upon how the various parameters of the ecosystem impact the
stationary distribution.
  We analyze different types of environmental fluctuations. At first we study
ecosystems modeled by stochastic differential equations. In the second setting
we look at piecewise deterministic Markov processes. These are processes where
one follows a system of differential equations for a random time, after which
the environmental state changes, and one follows a different set of
differential equations -- this procedure then gets repeated indefinitely.
Finally, we look at stochastic differential equations with switching, which
take into account both the white noise fluctuations and the random
environmental switches.
  As applications of our theoretical and numerical analysis, we look at
competitive Lotka--Volterra, Beddington-DeAngelis predator-prey, and
rock-paper-scissors dynamics. We highlight new biological insights by analyzing
the stationary distributions of the ecosystems and by seeing how various types
of environmental fluctuations influence the long term fate of populations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.04774</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.04774</id><submitter>Alexander Thebelt</submitter><version version="v1"><date>Tue, 10 Mar 2020 14:34:07 GMT</date><size>1252kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 12 Mar 2021 00:16:58 GMT</date><size>2942kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 15:10:36 GMT</date><size>2950kb</size><source_type>D</source_type></version><title>ENTMOOT: A Framework for Optimization over Ensemble Tree Models</title><authors>Alexander Thebelt, Jan Kronqvist, Miten Mistry, Robert M. Lee, Nathan
  Sudermann-Merx, Ruth Misener</authors><categories>stat.ML cs.AI cs.LG math.OC</categories><comments>33 pages, 10 figures, 2 tables</comments><doi>10.1016/j.compchemeng.2021.107343</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient boosted trees and other regression tree models perform well in a
wide range of real-world, industrial applications. These tree models (i) offer
insight into important prediction features, (ii) effectively manage sparse
data, and (iii) have excellent prediction capabilities. Despite their
advantages, they are generally unpopular for decision-making tasks and
black-box optimization, which is due to their difficult-to optimize structure
and the lack of a reliable uncertainty measure. ENTMOOT is our new framework
for integrating (already trained) tree models into larger optimization
problems. The contributions of ENTMOOT include: (i) explicitly introducing a
reliable uncertainty measure that is compatible with tree models, (ii) solving
the larger optimization problems that incorporate these uncertainty aware tree
models, (iii) proving that the solutions are globally optimal, i.e. no better
solution exists. In particular, we show how the ENTMOOT approach allows a
simple integration of tree models into decision-making and black-box
optimization, where it proves as a strong competitor to commonly-used
frameworks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.07441</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.07441</id><submitter>Gustav Grund Pihlgren</submitter><version version="v1"><date>Mon, 16 Mar 2020 21:08:43 GMT</date><size>454kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 15 Jul 2020 15:54:22 GMT</date><size>553kb</size><source_type>D</source_type></version><title>Pretraining Image Encoders without Reconstruction via Feature Prediction
  Loss</title><authors>Gustav Grund Pihlgren (1), Fredrik Sandin (1), Marcus Liwicki (1) ((1)
  Lule\r{a} University of Technology)</authors><categories>cs.CV cs.LG</categories><doi>10.1109/ICPR48806.2021.9412239</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates three methods for calculating loss for
autoencoder-based pretraining of image encoders: The commonly used
reconstruction loss, the more recently introduced deep perceptual similarity
loss, and a feature prediction loss proposed here; the latter turning out to be
the most efficient choice. Standard auto-encoder pretraining for deep learning
tasks is done by comparing the input image and the reconstructed image. Recent
work shows that predictions based on embeddings generated by image autoencoders
can be improved by training with perceptual loss, i.e., by adding a loss
network after the decoding step. So far the autoencoders trained with loss
networks implemented an explicit comparison of the original and reconstructed
images using the loss network. However, given such a loss network we show that
there is no need for the time-consuming task of decoding the entire image.
Instead, we propose to decode the features of the loss network, hence the name
&quot;feature prediction loss&quot;. To evaluate this method we perform experiments on
three standard publicly available datasets (LunarLander-v2, STL-10, and SVHN)
and compare six different procedures for training image encoders (pixel-wise,
perceptual similarity, and feature prediction losses; combined with two
variations of image and feature encoding/decoding). The embedding-based
prediction results show that encoders trained with feature prediction loss is
as good or better than those trained with the other two losses. Additionally,
the encoder is significantly faster to train using feature prediction loss in
comparison to the other losses. The method implementation used in this work is
available online: https://github.com/guspih/Perceptual-Autoencoders
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.09946</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.09946</id><submitter>Juan Maro\~nas</submitter><version version="v1"><date>Sun, 22 Mar 2020 16:54:31 GMT</date><size>227kb</size></version><version version="v2"><date>Sun, 12 Apr 2020 12:18:10 GMT</date><size>227kb</size></version><version version="v3"><date>Thu, 29 Oct 2020 07:54:20 GMT</date><size>227kb</size></version><version version="v4"><date>Thu, 28 Jan 2021 11:39:19 GMT</date><size>225kb</size></version><title>On Calibration of Mixup Training for Deep Neural Networks</title><authors>Juan Maro\~nas and Daniel Ramos and Roberto Paredes</authors><categories>cs.LG stat.ML</categories><comments>To appear in S+SSPR2020</comments><doi>10.1007/978-3-030-73973-7_7</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep Neural Networks (DNN) represent the state of the art in many tasks.
However, due to their overparameterization, their generalization capabilities
are in doubt and still a field under study. Consequently, DNN can overfit and
assign overconfident predictions -- effects that have been shown to affect the
calibration of the confidences assigned to unseen data. Data Augmentation (DA)
strategies have been proposed to regularize these models, being Mixup one of
the most popular due to its ability to improve the accuracy, the uncertainty
quantification and the calibration of DNN. In this work however we argue and
provide empirical evidence that, due to its fundamentals, Mixup does not
necessarily improve calibration. Based on our observations we propose a new
loss function that improves the calibration, and also sometimes the accuracy,
of DNN trained with this DA technique. Our loss is inspired by Bayes decision
theory and introduces a new training framework for designing losses for
probabilistic modelling. We provide state-of-the-art accuracy with consistent
improvements in calibration performance. Appendix and code are provided here:
https://github.com/jmaronas/calibration_MixupDNN_ARCLoss.pytorch.git
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2003.11409</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2003.11409</id><submitter>Yutaro Iiyama</submitter><version version="v1"><date>Wed, 25 Mar 2020 13:52:50 GMT</date><size>744kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 14:40:09 GMT</date><size>1871kb</size><source_type>D</source_type></version><title>Dynamo -- Handling Scientific Data Across Sites and Storage Media</title><authors>Yutaro Iiyama and Benedikt Maier and Daniel Abercrombie and Maxim
  Goncharov and Christoph Paus</authors><categories>cs.DC hep-ex</categories><comments>18 pages, 9 figures</comments><journal-ref>Computing and Software for Big Science 5, 11 (2021)</journal-ref><doi>10.1007/s41781-021-00054-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamo is a full-stack software solution for scientific data management.
Dynamo's architecture is modular, extensible, and customizable, making the
software suitable for managing data in a wide range of installation scales,
from a few terabytes stored at a single location to hundreds of petabytes
distributed across a worldwide computing grid. This article documents the core
system design of Dynamo and describes the applications that implement various
data management tasks. A brief report is also given on the operational
experiences of the system at the CMS experiment at the CERN Large Hadron
Collider and at a small scale analysis facility.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.00556</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.00556</id><submitter>Gabriele Santin</submitter><version version="v1"><date>Wed, 1 Apr 2020 16:34:02 GMT</date><size>150kb</size><source_type>D</source_type></version><title>Sampling based approximation of linear functionals in Reproducing Kernel
  Hilbert Spaces</title><authors>Gabriele Santin, Toni Karvonen, and Bernard Haasdonk</authors><categories>math.NA cs.NA</categories><msc-class>65D05, 65D15, 65D30, 65D32, 62C10, 41A05, 41A25</msc-class><doi>10.1007/s10543-021-00870-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze a greedy procedure to approximate a linear
functional defined in a Reproducing Kernel Hilbert Space by nodal values. This
procedure computes a quadrature rule which can be applied to general
functionals, including integration functionals. For a large class of
functionals, we prove convergence results for the approximation by means of
uniform and greedy points which generalize in various ways several known
results. A perturbation analysis of the weights and node computation is also
discussed. Beyond the theoretical investigations, we demonstrate numerically
that our algorithm is effective in treating various integration densities, and
that it is even very competitive when compared to existing methods for
Uncertainty Quantification.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.01536</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.01536</id><submitter>Ylva Jansson</submitter><version version="v1"><date>Fri, 3 Apr 2020 13:00:35 GMT</date><size>4052kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 21 May 2020 19:04:47 GMT</date><size>4592kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 18 Jun 2020 10:14:16 GMT</date><size>4665kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 29 Jun 2020 13:06:41 GMT</date><size>4665kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 15 Sep 2020 12:32:33 GMT</date><size>8810kb</size><source_type>D</source_type></version><version version="v6"><date>Mon, 12 Apr 2021 09:07:54 GMT</date><size>8810kb</size><source_type>D</source_type></version><version version="v7"><date>Tue, 18 May 2021 09:27:23 GMT</date><size>8810kb</size><source_type>D</source_type></version><title>Exploring the ability of CNNs to generalise to previously unseen scales
  over wide scale ranges</title><authors>Ylva Jansson and Tony Lindeberg</authors><categories>cs.CV</categories><comments>14 pages, 6 figures, 3 tables</comments><journal-ref>Shortened version in International Conference on Pattern
  Recognition (ICPR 2020), pages 1181-1188, Jan 2021</journal-ref><doi>10.1109/ICPR48806.2021.9413276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to handle large scale variations is crucial for many real world
visual tasks. A straightforward approach for handling scale in a deep network
is to process an image at several scales simultaneously in a set of scale
channels. Scale invariance can then, in principle, be achieved by using weight
sharing between the scale channels together with max or average pooling over
the outputs from the scale channels. The ability of such scale channel networks
to generalise to scales not present in the training set over significant scale
ranges has, however, not previously been explored. We, therefore, present a
theoretical analysis of invariance and covariance properties of scale channel
networks and perform an experimental evaluation of the ability of different
types of scale channel networks to generalise to previously unseen scales. We
identify limitations of previous approaches and propose a new type of foveated
scale channel architecture, where the scale channels process increasingly
larger parts of the image with decreasing resolution. Our proposed FovMax and
FovAvg networks perform almost identically over a scale range of 8, also when
training on single scale training data, and do also give improvements in the
small sample regime.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.03512</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.03512</id><submitter>Robert Rehr</submitter><version version="v1"><date>Tue, 7 Apr 2020 16:09:54 GMT</date><size>5789kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 14:04:40 GMT</date><size>4538kb</size><source_type>D</source_type></version><title>SNR-Based Features and Diverse Training Data for Robust DNN-Based Speech
  Enhancement</title><authors>Robert Rehr, Timo Gerkmann</authors><categories>eess.AS cs.LG cs.SD</categories><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing.
  (c) 2021 IEEE</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the generalization of deep neural network (DNN)
based speech enhancement to unseen noise conditions for the case that training
data is limited in size and diversity. To gain more insights, we analyze the
generalization with respect to (1) the size and diversity of the training data,
(2) different network architectures, and (3) the chosen features. To address
(1), we train networks on the Hu noise corpus (limited size), the CHiME 3 noise
corpus (limited diversity) and also propose a large and diverse dataset
collected based on freely available sounds. To address (2), we compare a
fully-connected feed-forward and a long short-term memory (LSTM) architecture.
To address (3), we compare three input features, namely logarithmized noisy
periodograms, noise aware training (NAT) and the proposed signal-to-noise ratio
(SNR) based noise aware training (SNR-NAT). We confirm that rich training data
and improved network architectures help DNNs to generalize. Furthermore, we
show via experimental results and an analysis using t-distributed stochastic
neighbor embedding (t-SNE) that the proposed SNR-NAT features yield robust and
level independent results in unseen noise even with simple network
architectures and when trained on only small datasets, which is the key
contribution of this paper.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.04986</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.04986</id><submitter>Amit Portnoy</submitter><version version="v1"><date>Fri, 10 Apr 2020 10:59:16 GMT</date><size>235kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 08:10:10 GMT</date><size>271kb</size><source_type>D</source_type></version><title>Towards Federated Learning With Byzantine-Robust Client Weighting</title><authors>Amit Portnoy, Yoav Tirosh, and Danny Hendler</authors><categories>cs.LG cs.CR stat.ML</categories><acm-class>I.2.6; I.2.11; C.2.0</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Federated Learning(FL) is a distributed machine learning paradigm where data
is distributed among clients who collaboratively train a model in a computation
process coordinated by a central server. By assigning a weight to each client
based on the proportion of data instances it possesses, the rate of convergence
to an accurate joint model can be greatly accelerated. Some previous works
studied FLin a Byzantine setting, in which a fraction of the clients may send
arbitrary or even malicious information regarding their model. However, these
works either ignore the issue of data unbalancedness altogether or assume that
client weights are apriori known to the server, whereas, in practice, it is
likely that weights will be reported to the server by the clients themselves
and therefore cannot be relied upon. We address this issue for the first time
by proposing a practical weight-truncation-based preprocessing method and
demonstrating empirically that it is able to strike a good balance between
model quality and Byzantine robustness. We also establish analytically that our
method can be applied to a randomly selected sample of client weights.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.07641</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.07641</id><submitter>Lars Lorch</submitter><version version="v1"><date>Wed, 15 Apr 2020 17:18:32 GMT</date><size>2985kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 24 Apr 2020 17:29:32 GMT</date><size>4381kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 28 Aug 2020 13:58:12 GMT</date><size>32468kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 26 Oct 2020 13:36:24 GMT</date><size>11519kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 18 May 2021 13:15:42 GMT</date><size>11229kb</size><source_type>D</source_type></version><title>Quantifying the Effects of Contact Tracing, Testing, and Containment
  Measures in the Presence of Infection Hotspots</title><authors>Lars Lorch, Heiner Kremer, William Trouleau, Stratis Tsirtsis, Aron
  Szanto, Bernhard Sch\&quot;olkopf, and Manuel Gomez-Rodriguez</authors><categories>cs.LG cs.SI physics.soc-ph q-bio.PE stat.ML</categories><comments>Statistical tests for overdispersion of secondary infections; contour
  plots for parameter estimation; corrected experimental results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple lines of evidence strongly suggest that infection hotspots, where a
single individual infects many others, play a key role in the transmission
dynamics of COVID-19. However, most of the existing epidemiological models fail
to capture this aspect by neither representing the sites visited by individuals
explicitly nor characterizing disease transmission as a function of individual
mobility patterns. In this work, we introduce a temporal point process modeling
framework that specifically represents visits to the sites where individuals
get in contact and infect each other. Under our model, the number of infections
caused by an infectious individual naturally emerges to be overdispersed. Using
an efficient sampling algorithm, we demonstrate how to apply Bayesian
optimization with longitudinal case data to estimate the transmission rate of
infectious individuals at the sites they visit and in their households.
Simulations using fine-grained and publicly available demographic data and site
locations from Bern, Switzerland showcase the flexibility of our framework. To
facilitate research and analyses of other cities and regions, we release an
open-source implementation of our framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.09829</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.09829</id><submitter>Jihua Zhu</submitter><version version="v1"><date>Tue, 21 Apr 2020 08:52:38 GMT</date><size>649kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 10 May 2020 09:07:07 GMT</date><size>1005kb</size><source_type>D</source_type></version><title>Robust Motion Averaging under Maximum Correntropy Criterion</title><authors>Jihua Zhu, Jie Hu, Huimin Lu, Badong Chen, Zhongyu Li</authors><categories>cs.CV</categories><journal-ref>IEEE ICRA 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the motion averaging method has been introduced as an effective
means to solve the multi-view registration problem. This method aims to recover
global motions from a set of relative motions, where the original method is
sensitive to outliers due to using the Frobenius norm error in the
optimization. Accordingly, this paper proposes a novel robust motion averaging
method based on the maximum correntropy criterion (MCC). Specifically, the
correntropy measure is used instead of utilizing Frobenius norm error to
improve the robustness of motion averaging against outliers. According to the
half-quadratic technique, the correntropy measure based optimization problem
can be solved by the alternating minimization procedure, which includes
operations of weight assignment and weighted motion averaging. Further, we
design a selection strategy of adaptive kernel width to take advantage of
correntropy. Experimental results on benchmark data sets illustrate that the
new method has superior performance on accuracy and robustness for multi-view
registration.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.10050</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.10050</id><submitter>Xuehe Wang</submitter><version version="v1"><date>Sat, 18 Apr 2020 09:18:46 GMT</date><size>279kb</size></version><version version="v2"><date>Wed, 22 Apr 2020 08:53:20 GMT</date><size>279kb</size></version><version version="v3"><date>Tue, 18 May 2021 02:59:31 GMT</date><size>438kb</size></version><title>Dynamic Pricing and Mean Field Analysis for Controlling Age of
  Information</title><authors>Xuehe Wang and Lingjie Duan</authors><categories>eess.SY cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1904.01185</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today many mobile users in various zones are invited to sense and send back
real-time useful information (e.g., traffic observation and sensor data) to
keep the freshness of the content updates in such zones. However, due to the
sampling cost in sensing and transmission, a user may not have the incentive to
contribute the real-time information to help reduce the age of information
(AoI). We propose dynamic pricing for each zone to offer age-dependent monetary
returns and encourage users to sample information at different rates over time.
This dynamic pricing design problem needs to well balance the monetary payments
as rewards to users and the AoI evolution over time, and is challenging to
solve especially under the incomplete information about users' arrivals and
their private sampling costs. After formulating the problem as a nonlinear
constrained dynamic program, to avoid the curse of dimensionality, we first
propose to approximate the dynamic AoI reduction as a time-average term and
successfully solve the approximate dynamic pricing in closed-form. Further, by
providing the steady-state analysis for an infinite time horizon, we show that
the pricing scheme (though in closed-form) can be further simplified to an
$\varepsilon$-optimal version without recursive computing over time. Finally,
we extend the AoI control from a single zone to many zones with heterogeneous
user arrival rates and initial ages, where each zone cares not only its own AoI
dynamics but also the average AoI of all the zones in a mean field game system
to provide a holistic service. Accordingly, we propose decentralized mean field
pricing for each zone to self-operate by using a mean field term to estimate
the average age dynamics of all the zones, which does not even require many
zones to exchange their local data with each other.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.10715</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.10715</id><submitter>Jithin Jagannath</submitter><version version="v1"><date>Wed, 22 Apr 2020 17:20:00 GMT</date><size>239kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Mar 2021 01:31:17 GMT</date><size>240kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 7 Mar 2021 00:54:10 GMT</date><size>1134kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 17 May 2021 13:14:46 GMT</date><size>1135kb</size><source_type>D</source_type></version><title>Redefining Wireless Communication for 6G: Signal Processing Meets Deep
  Learning with Deep Unfolding</title><authors>Anu Jagannath, Jithin Jagannath, and Tommaso Melodia</authors><categories>cs.NI cs.LG eess.SP</categories><comments>Preprint submitted to IEEE Transactions on Artificial Intelligence</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The year 2019 witnessed the rollout of the 5G standard, which promises to
offer significant data rate improvement over 4G. While 5G is still in its
infancy, there has been an increased shift in the research community for
communication technologies beyond 5G. The recent emergence of machine learning
approaches for enhancing wireless communications and empowering them with
much-desired intelligence holds immense potential for redefining wireless
communication for 6G. The evolving communication systems will be bottlenecked
in terms of latency, throughput, and reliability by the underlying signal
processing at the physical layer. In this position paper, we motivate the need
to redesign iterative signal processing algorithms by leveraging deep unfolding
techniques to fulfill the physical layer requirements for 6G networks. To this
end, we begin by presenting the service requirements and the key challenges
posed by the envisioned 6G communication architecture. We outline the
deficiencies of the traditional algorithmic principles and data-hungry deep
learning (DL) approaches in the context of 6G networks. Specifically, deep
unfolded signal processing is presented by sketching the interplay between
domain knowledge and DL. The deep unfolded approaches reviewed in this article
are positioned explicitly in the context of the requirements imposed by the
next generation of cellular networks. Finally, this article motivates open
research challenges to truly realize hardware-efficient edge intelligence for
future 6G networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.11582</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.11582</id><submitter>Jan Krajicek</submitter><version version="v1"><date>Fri, 24 Apr 2020 07:43:22 GMT</date><size>6kb</size></version><version version="v2"><date>Wed, 23 Dec 2020 09:11:58 GMT</date><size>6kb</size></version><title>Small circuits and dual weak PHP in the universal theory of p-time
  algorithms</title><authors>Jan Krajicek</authors><categories>math.LO cs.CC</categories><comments>Preprint April 2020, revision December 2020</comments><msc-class>03F30, 68Q15</msc-class><acm-class>F.4.1</acm-class><journal-ref>ACM Transactions on Computational Logic, 22, 2, Article 11 (May
  2021)</journal-ref><doi>10.1145/3446207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove, under a computational complexity hypothesis, that it is consistent
with the true universal theory of p-time algorithms that a specific p-time
function extending $n$ bits to $m \geq n^2$ bits violates the dual weak
pigeonhole principle: every string $y$ of length $m$ equals the value of the
function for some $x$ of length $n$. The function is the truth-table function
assigning to a circuit the table of the function it computes and the hypothesis
is that every language in P has circuits of a fixed polynomial size $n^d$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.11678</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.11678</id><submitter>Ylva Jansson</submitter><version version="v1"><date>Fri, 24 Apr 2020 12:20:35 GMT</date><size>2202kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 4 May 2020 08:38:07 GMT</date><size>2195kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 30 Jun 2020 13:29:50 GMT</date><size>2088kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 23 Dec 2020 11:48:25 GMT</date><size>8422kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 18 May 2021 09:14:59 GMT</date><size>8422kb</size><source_type>D</source_type></version><title>Understanding when spatial transformer networks do not support
  invariance, and what to do about it</title><authors>Lukas Finnveden, Ylva Jansson and Tony Lindeberg</authors><categories>cs.CV</categories><comments>13 pages, 7 figures, 6 tables</comments><journal-ref>Shortened version in International Conference on Pattern
  Recognition (ICPR 2020), pages 3427--3434, Jan 2021</journal-ref><doi>10.1109/ICPR48806.2021.9412997</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial transformer networks (STNs) were designed to enable convolutional
neural networks (CNNs) to learn invariance to image transformations. STNs were
originally proposed to transform CNN feature maps as well as input images. This
enables the use of more complex features when predicting transformation
parameters. However, since STNs perform a purely spatial transformation, they
do not, in the general case, have the ability to align the feature maps of a
transformed image with those of its original. STNs are therefore unable to
support invariance when transforming CNN feature maps. We present a simple
proof for this and study the practical implications, showing that this
inability is coupled with decreased classification accuracy. We therefore
investigate alternative STN architectures that make use of complex features. We
find that while deeper localization networks are difficult to train,
localization networks that share parameters with the classification network
remain stable as they grow deeper, which allows for higher classification
accuracy on difficult datasets. Finally, we explore the interaction between
localization network complexity and iterative image alignment.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.11853</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.11853</id><submitter>Moi Hoon Yap</submitter><version version="v1"><date>Fri, 24 Apr 2020 16:56:48 GMT</date><size>3838kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 Aug 2020 16:50:04 GMT</date><size>4021kb</size><source_type>D</source_type></version><title>DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection</title><authors>Bill Cassidy and Neil D. Reeves and Pappachan Joseph and David
  Gillespie and Claire O'Shea and Satyan Rajbhandari and Arun G. Maiya and Eibe
  Frank and Andrew Boulton and David Armstrong and Bijan Najafi and Justina Wu
  and Moi Hoon Yap</authors><categories>cs.CV</categories><comments>16 pages, 8 figures</comments><doi>10.17925/EE.2021.1.1.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every 20 seconds, a limb is amputated somewhere in the world due to diabetes.
This is a global health problem that requires a global solution. The MICCAI
challenge discussed in this paper, which concerns the automated detection of
diabetic foot ulcers using machine learning techniques, will accelerate the
development of innovative healthcare technology to address this unmet medical
need. In an effort to improve patient care and reduce the strain on healthcare
systems, recent research has focused on the creation of cloud-based detection
algorithms. These can be consumed as a service by a mobile app that patients
(or a carer, partner or family member) could use themselves at home to monitor
their condition and to detect the appearance of a diabetic foot ulcer (DFU).
Collaborative work between Manchester Metropolitan University, Lancashire
Teaching Hospital and the Manchester University NHS Foundation Trust has
created a repository of 4,000 DFU images for the purpose of supporting research
toward more advanced methods of DFU detection. Based on a joint effort
involving the lead scientists of the UK, US, India and New Zealand, this
challenge will solicit original work, and promote interactions between
researchers and interdisciplinary collaborations. This paper presents a dataset
description and analysis, assessment methods, benchmark algorithms and initial
evaluation results. It facilitates the challenge by providing useful insights
into state-of-the-art and ongoing research. This grand challenge takes on even
greater urgency in a peri and post-pandemic period, where stresses on resource
utilization will increase the need for technology that allows people to remain
active, healthy and intact in their home.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.12271</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.12271</id><submitter>Prakirt Jhunjhunwala</submitter><version version="v1"><date>Sun, 26 Apr 2020 02:18:25 GMT</date><size>196kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 19:34:52 GMT</date><size>3653kb</size><source_type>D</source_type></version><title>Low-Complexity Switch Scheduling Algorithms: Delay Optimality in Heavy
  Traffic</title><authors>Prakirt Jhunjhunwala and Siva Theja Maguluri</authors><categories>math.PR cs.NI</categories><comments>10 pages paper with 8 page appendix. 4 figures and 1 table. Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in data center networks, in this paper, we study
the problem of scheduling in an input queued switch. While throughput
maximizing algorithms in a switch are well-understood, delay analysis was
developed only recently. It was recently shown that the well-known MaxWeight
algorithm achieves optimal scaling of mean queue lengths in steady state in the
heavy-traffic regime, and is within a factor less than $2$ of a universal lower
bound. However, MaxWeight is not used in practice because of its high time
complexity. In this paper, we study several low complexity algorithms and show
that their heavy-traffic performance is identical to that of MaxWeight. We
first present a negative result that picking a random schedule does not have
optimal heavy-traffic scaling of queue lengths even under uniform traffic. We
then show that if one picks the best among two matchings or modifies a random
matching even a little, using the so-called flip operation, it leads to
MaxWeight like heavy-traffic performance under uniform traffic. We then focus
on the case of non-uniform traffic and show that a large class of low time
complexity algorithms have the same heavy-traffic performance as MaxWeight, as
long as it is ensured that a MaxWeight matching is picked often enough. We also
briefly discuss the performance of these algorithms in the large scale
heavy-traffic regime when the size of the switch increases simultaneously with
the load. Finally, we perform empirical study on a new algorithm to compare its
performance with some existing algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.13318</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.13318</id><submitter>Jiangbin Lyu Dr.</submitter><version version="v1"><date>Tue, 28 Apr 2020 06:08:37 GMT</date><size>1567kb</size></version><version version="v2"><date>Mon, 25 May 2020 09:30:35 GMT</date><size>1567kb</size></version><version version="v3"><date>Tue, 1 Sep 2020 02:33:02 GMT</date><size>1570kb</size></version><version version="v4"><date>Sat, 15 May 2021 11:34:25 GMT</date><size>2825kb</size></version><title>Hybrid Active/Passive Wireless Network Aided by Intelligent Reflecting
  Surface: System Modeling and Performance Analysis</title><authors>Jiangbin Lyu and Rui Zhang</authors><categories>cs.IT cs.NI math.IT stat.AP</categories><comments>To appear in IEEE Trans. Wireless Commun.. First work to model
  large-scale multi-cell hybrid network with distributed active BSs and passive
  IRSs subjected to inter-cell interference, and characterize distributions of
  signal/interference power, SINR and spatial throughput based on stochastic
  geometry. Unveil that an optimal IRS/BS density ratio exists in achieving
  sustainable capacity growth</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent reflecting surface (IRS) is a new and promising paradigm to
substantially improve the spectral and energy efficiency of wireless networks,
by constructing favorable communication channels via tuning massive low-cost
passive reflecting elements. Despite recent advances in the link-level
performance optimization for various IRS-aided wireless systems, it still
remains an open problem whether the large-scale deployment of IRSs in wireless
networks can be a cost-effective solution to achieve their sustainable capacity
growth in the future. To address this problem, we study in this paper a new
hybrid wireless network comprising both active base stations (BSs) and passive
IRSs, and characterize its achievable spatial throughput in the downlink as
well as other pertinent key performance metrics averaged over both channel
fading and random locations of the deployed BSs/IRSs therein based on
stochastic geometry. Compared to prior works on characterizing the performance
of wireless networks with active BSs only, our analysis needs to derive the
power distributions of both the signal and interference reflected by
distributed IRSs in the network under spatially correlated channels, which
exhibit channel hardening effects when the number of IRS elements becomes
large. Extensive numerical results are presented to validate our analysis and
demonstrate the effectiveness of deploying distributed IRSs in enhancing the
hybrid network throughput against the conventional network without IRS, which
significantly boosts the signal power but results in only marginally increased
interference in the network. Moreover, it is unveiled that there exists an
optimal IRS/BS density ratio that maximizes the hybrid network throughput
subject to a total deployment cost given their individual costs, while the
conventional network without IRS is generally suboptimal in terms of throughput
per unit cost.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.13612</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.13612</id><submitter>Calypso Herrera</submitter><version version="v1"><date>Tue, 28 Apr 2020 15:45:21 GMT</date><size>129kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 5 Jun 2020 18:26:24 GMT</date><size>123kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 13:35:22 GMT</date><size>174kb</size><source_type>D</source_type></version><title>Denise: Deep Robust Principal Component Analysis for Positive
  Semidefinite Matrices</title><authors>Calypso Herrera, Florian Krach, Anastasis Kratsios, Pierre Ruyssen,
  Josef Teichmann</authors><categories>stat.ML cs.LG math.OC q-fin.CP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robust PCA of covariance matrices plays an essential role when isolating
key explanatory features. The currently available methods for performing such a
low-rank plus sparse decomposition are matrix specific, meaning, those
algorithms must re-run for every new matrix. Since these algorithms are
computationally expensive, it is preferable to learn and store a function that
instantaneously performs this decomposition when evaluated. Therefore, we
introduce Denise, a deep learning-based algorithm for robust PCA of covariance
matrices, or more generally of symmetric positive semidefinite matrices, which
learns precisely such a function. Theoretical guarantees for Denise are
provided. These include a novel universal approximation theorem adapted to our
geometric deep learning problem, convergence to an optimal solution of the
learning problem and convergence of the training scheme. Our experiments show
that Denise matches state-of-the-art performance in terms of decomposition
quality, while being approximately 2000x faster than the state-of-the-art, PCP,
and 200x faster than the current speed optimized method, fast PCP.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2004.14162</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2004.14162</id><submitter>Pengjie Ren</submitter><version version="v1"><date>Wed, 29 Apr 2020 13:07:53 GMT</date><size>3789kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 06:40:31 GMT</date><size>3877kb</size><source_type>D</source_type></version><title>Conversations with Search Engines: SERP-based Conversational Response
  Generation</title><authors>Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof
  Monz, and Maarten de Rijke</authors><categories>cs.IR cs.AI</categories><comments>published in TOIS 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of answering complex information needs
by conversing conversations with search engines, in the sense that users can
express their queries in natural language, and directly receivethe information
they need from a short system response in a conversational manner. Recently,
there have been some attempts towards a similar goal, e.g., studies on
Conversational Agents (CAs) and Conversational Search (CS). However, they
either do not address complex information needs, or they are limited to the
development of conceptual frameworks and/or laboratory-based user studies.
  We pursue two goals in this paper: (1) the creation of a suitable dataset,
the Search as a Conversation (SaaC) dataset, for the development of pipelines
for conversations with search engines, and (2) the development of
astate-of-the-art pipeline for conversations with search engines, the
Conversations with Search Engines (CaSE), using this dataset. SaaC is built
based on a multi-turn conversational search dataset, where we further employ
workers from a crowdsourcing platform to summarize each relevant passage into a
short, conversational response. CaSE enhances the state-of-the-art by
introducing a supporting token identification module and aprior-aware pointer
generator, which enables us to generate more accurate responses.
  We carry out experiments to show that CaSE is able to outperform strong
baselines. We also conduct extensive analyses on the SaaC dataset to show where
there is room for further improvement beyond CaSE. Finally, we release the SaaC
dataset and the code for CaSE and all models used for comparison to facilitate
future research on this topic.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.01912</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.01912</id><submitter>Leopoldo Sarra</submitter><version version="v1"><date>Mon, 4 May 2020 16:43:49 GMT</date><size>1451kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 4 Jun 2020 10:54:07 GMT</date><size>1451kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 5 Mar 2021 11:20:34 GMT</date><size>6545kb</size><source_type>D</source_type></version><title>Renormalized Mutual Information for Artificial Scientific Discovery</title><authors>Leopoldo Sarra, Andrea Aiello, Florian Marquardt</authors><categories>cs.LG physics.data-an</categories><comments>Added a more detailed introduction and link to code repository.
  Physics-based examples and Feature Extraction section have been updated</comments><doi>10.1103/PhysRevLett.126.200601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a well-defined renormalized version of mutual information that
allows to estimate the dependence between continuous random variables in the
important case when one is deterministically dependent on the other. This is
the situation relevant for feature extraction, where the goal is to produce a
low-dimensional effective description of a high-dimensional system. Our
approach enables the discovery of collective variables in physical systems,
thus adding to the toolbox of artificial scientific discovery, while also
aiding the analysis of information flow in artificial neural networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.02972</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.02972</id><submitter>Jean-Daniel Fekete</submitter><version version="v1"><date>Wed, 6 May 2020 17:26:07 GMT</date><size>2955kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 15:19:40 GMT</date><size>3490kb</size><source_type>D</source_type></version><title>Integrating Prior Knowledge in Mixed Initiative Social Network
  Clustering</title><authors>Alexis Pister, Paolo Buono, Jean-Daniel Fekete, Catherine Plaisant,
  Paola Valdivia</authors><categories>cs.HC cs.SI</categories><acm-class>H.5.2</acm-class><journal-ref>IEEE Transactions on Visualization and Computer Graphics, 2021</journal-ref><doi>10.1109/TVCG.2020.3030347</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a new approach -- called PK-clustering -- to help social
scientists create meaningful clusters in social networks. Many clustering
algorithms exist but most social scientists find them difficult to understand,
and tools do not provide any guidance to choose algorithms, or to evaluate
results taking into account the prior knowledge of the scientists. Our work
introduces a new clustering approach and a visual analytics user interface that
address this issue. It is based on a process that 1) captures the prior
knowledge of the scientists as a set of incomplete clusters, 2) runs multiple
clustering algorithms (similarly to clustering ensemble methods), 3) visualizes
the results of all the algorithms ranked and summarized by how well each
algorithm matches the prior knowledge, 4) evaluates the consensus between
user-selected algorithms, and 5) allows users to review details and iteratively
update the acquired knowledge. We describe our approach using an initial
functional prototype, then provide two examples of use and early feedback from
social scientists. We believe our clustering approach offers a novel
constructive method to iteratively build knowledge while avoiding being overly
influenced by the results of often randomly selected black-box clustering
algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.05846</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.05846</id><submitter>Gary Pui-Tung Choi</submitter><version version="v1"><date>Tue, 12 May 2020 15:07:05 GMT</date><size>7850kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 5 Apr 2021 15:11:23 GMT</date><size>6989kb</size><source_type>D</source_type></version><title>An additive algorithm for origami design</title><authors>Levi H. Dudte, Gary P. T. Choi, L. Mahadevan</authors><categories>cond-mat.soft cs.CG</categories><journal-ref>Proceedings of the National Academy of Sciences, 118(21),
  e2019241118 (2021)</journal-ref><doi>10.1073/pnas.2019241118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the allure of additive fabrication, we pose the problem of
origami design from a new perspective: how can we grow a folded surface in
three dimensions from a seed so that it is guaranteed to be isometric to the
plane? We solve this problem in two steps: by first identifying the geometric
conditions for the compatible completion of two separate folds into a single
developable four-fold vertex, and then showing how this foundation allows us to
grow a geometrically compatible front at the boundary of a given folded seed.
This yields a complete marching, or additive, algorithm for the inverse design
of the complete space of developable quad origami patterns that can be folded
from flat sheets. We illustrate the flexibility of our approach by growing
ordered, disordered, straight and curved folded origami and fitting surfaces of
given curvature with folded approximants. Overall, our simple shift in
perspective from a global search to a local rule has the potential to transform
origami-based meta-structure design.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.08081</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.08081</id><submitter>Fenglin Liu</submitter><version version="v1"><date>Sat, 16 May 2020 20:00:39 GMT</date><size>457kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Jun 2020 10:21:33 GMT</date><size>441kb</size><source_type>D</source_type></version><version version="v3"><date>Wed, 21 Oct 2020 05:58:38 GMT</date><size>1335kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 15 May 2021 08:52:22 GMT</date><size>701kb</size><source_type>D</source_type></version><title>Layer-Wise Multi-View Decoding for Natural Language Generation</title><authors>Fenglin Liu, Xuancheng Ren, Guangxiang Zhao, Xu Sun</authors><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In sequence-to-sequence learning, e.g., natural language generation, the
decoder relies on the attention mechanism to efficiently extract information
from the encoder. While it is common practice to draw information from only the
last encoder layer, recent work has proposed to use representations from
different encoder layers for diversified levels of information. Nonetheless,
the decoder still obtains only a single view of the source sequences, which
might lead to insufficient training of the encoder layer stack due to the
hierarchy bypassing problem. In this work, we propose layer-wise multi-view
decoding, where for each decoder layer, together with the representations from
the last encoder layer, which serve as a global view, those from other encoder
layers are supplemented for a stereoscopic view of the source sequences.
Systematic experiments and analyses show that we successfully address the
hierarchy bypassing problem and substantially improve the performance of
sequence-to-sequence learning with deep representations on diverse tasks, i.e.,
machine translation, abstractive summarization and image captioning. In
particular, our approach surpasses the previous state-of-the-art models on
three benchmark machine translation datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.08612</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.08612</id><submitter>Zolt\'an Kmetty</submitter><version version="v1"><date>Mon, 18 May 2020 11:49:35 GMT</date><size>566kb</size></version><version version="v2"><date>Mon, 17 May 2021 10:51:42 GMT</date><size>379kb</size></version><title>The presence of occupational structure in online texts based on word
  embedding NLP models</title><authors>Zolt\'an Kmetty, Julia Koltai, Tam\'as Rudas</authors><categories>cs.CL</categories><comments>34 pages, 2 figures, 4 tables. Paper presented at IC2S2 2019 and RC28
  summer meeting 2019 (Columbia University)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on social stratification is closely linked to analysing the prestige
associated with different occupations. This research focuses on the positions
of occupations in the semantic space represented by large amounts of textual
data. The results are compared to standard results in social stratification to
see whether the classical results are reproduced and if additional insights can
be gained into the social positions of occupations. The paper gives an
affirmative answer to both questions. The results show fundamental similarity
of the occupational structure obtained from text analysis to the structure
described by prestige and social distance scales. While our research reinforces
many theories and empirical findings of the traditional body of literature on
social stratification and, in particular, occupational hierarchy, it pointed to
the importance of a factor not discussed in the main line of stratification
literature so far: the power and organizational aspect.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2005.13249</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2005.13249</id><submitter>Dani Kiyasseh</submitter><version version="v1"><date>Wed, 27 May 2020 09:25:41 GMT</date><size>2575kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 30 Nov 2020 17:46:42 GMT</date><size>3918kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 13:12:14 GMT</date><size>5550kb</size><source_type>D</source_type></version><title>CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and
  Patients</title><authors>Dani Kiyasseh, Tingting Zhu, David A. Clifton</authors><categories>cs.LG eess.SP stat.ML</categories><comments>Accepted to ICML 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The healthcare industry generates troves of unlabelled physiological data.
This data can be exploited via contrastive learning, a self-supervised
pre-training method that encourages representations of instances to be similar
to one another. We propose a family of contrastive learning methods, CLOCS,
that encourages representations across space, time, \textit{and} patients to be
similar to one another. We show that CLOCS consistently outperforms the
state-of-the-art methods, BYOL and SimCLR, when performing a linear evaluation
of, and fine-tuning on, downstream tasks. We also show that CLOCS achieves
strong generalization performance with only 25\% of labelled training data.
Furthermore, our training procedure naturally generates patient-specific
representations that can be used to quantify patient-similarity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05291</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05291</id><submitter>Mohamed A. Suliman</submitter><version version="v1"><date>Tue, 9 Jun 2020 14:33:55 GMT</date><size>771kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 11:02:52 GMT</date><size>676kb</size><source_type>D</source_type></version><title>Mathematical Theory of Atomic Norm Denoising In Blind Two-Dimensional
  Super-Resolution (Extended Version)</title><authors>Mohamed A. Suliman, Wei Dai</authors><categories>cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a new mathematical framework for denoising in blind
two-dimensional (2D) super-resolution upon using the atomic norm. The framework
denoises a signal that consists of a weighted sum of an unknown number of
time-delayed and frequency-shifted unknown waveforms from its noisy
measurements. Moreover, the framework also provides an approach for estimating
the unknown parameters in the signal. We prove that when the number of the
observed samples satisfies certain lower bound that is a function of the system
parameters, we can estimate the noise-free signal, with very high accuracy,
upon solving a regularized least-squares atomic norm minimization problem. We
derive the theoretical mean-squared error of the estimator, and we show that it
depends on the noise variance, the number of unknown waveforms, the number of
samples, and the dimension of the low-dimensional space where the unknown
waveforms lie. Finally, we verify the theoretical findings of the paper by
using extensive simulation experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.05330</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.05330</id><submitter>Sascha Kurz</submitter><version version="v1"><date>Tue, 9 Jun 2020 15:10:57 GMT</date><size>9kb</size></version><version version="v2"><date>Tue, 18 May 2021 15:30:23 GMT</date><size>10kb</size></version><title>Are weighted games sufficiently good for binary voting?</title><authors>Sascha Kurz</authors><categories>cs.GT econ.TH</categories><comments>7 pages, 2 tables; typos corrected</comments><msc-class>91B12, 91A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary yes-no decisions in a legislative committee or a shareholder meeting
are commonly modeled as a weighted game. However, there are noteworthy
exceptions. E.g., the voting rules of the European Council according to the
Treaty of Lisbon use a more complicated construction. Here we want to study the
question if we loose much from a practical point of view, if we restrict
ourselves to weighted games. To this end, we invoke power indices that measure
the influence of a member in binary decision committees. More precisely, we
compare the achievable power distributions of weighted games with those from a
reasonable superset of weighted games. It turns out that the deviation is
relatively small.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.06142</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.06142</id><submitter>Goran Muric</submitter><version version="v1"><date>Thu, 11 Jun 2020 01:31:00 GMT</date><size>463kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 23 Mar 2021 18:35:59 GMT</date><size>846kb</size></version><version version="v3"><date>Thu, 25 Mar 2021 03:00:19 GMT</date><size>839kb</size></version><title>Gender disparity in the authorship of biomedical research publications
  during the COVID-19 pandemic</title><authors>Goran Muric, Kristina Lerman, Emilio Ferrara</authors><categories>cs.DL physics.soc-ph</categories><doi>10.2196/25379</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preliminary evidence suggests that women, including female researchers, are
disproportionately affected by the COVID-19 pandemic in terms of unequal
distribution of childcare, elderly care and other kinds of domestic and
emotional labor. Sudden lockdowns and abrupt shifts in daily routines have
disproportionate consequences on their productivity, which is reflected by a
sudden drop in research output in biomedical research, consequently affecting
the number of female authors of scientific publications. We investigate the
proportion of male and female researchers who published scientific papers
during the COVID-19 pandemic, using bibliometric data from biomedical preprint
servers and selected Springer-Nature journals. Our findings document a decrease
in the number of publications by female authors in biomedical field during the
global pandemic. This effect is particularly pronounced for papers related to
COVID-19, indicating that women are producing fewer publications related to
COVID-19 research. This sudden increase in the gender gap is persistent across
the ten countries with the highest number of researchers. These results should
be used to inform the scientific community of the worrying trend in COVID-19
research and the disproportionate effect that the pandemic has on female
academics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.07644</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.07644</id><submitter>Lin Bai</submitter><version version="v1"><date>Sat, 13 Jun 2020 14:12:23 GMT</date><size>5865kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 13:59:45 GMT</date><size>14589kb</size><source_type>D</source_type></version><title>RoadNet-RT: High Throughput CNN Architecture and SoC Design for
  Real-Time Road Segmentation</title><authors>Lin Bai, Yecheng Lyu and Xinming Huang</authors><categories>eess.IV cs.CV</categories><journal-ref>in IEEE Transactions on Circuits and Systems I: Regular Papers,
  vol. 68, no. 2, pp. 704-714, Feb. 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, convolutional neural network has gained popularity in many
engineering applications especially for computer vision. In order to achieve
better performance, often more complex structures and advanced operations are
incorporated into the neural networks, which results very long inference time.
For time-critical tasks such as autonomous driving and virtual reality,
real-time processing is fundamental. In order to reach real-time process speed,
a light-weight, high-throughput CNN architecture namely RoadNet-RT is proposed
for road segmentation in this paper. It achieves 90.33% MaxF score on test set
of KITTI road segmentation task and 8 ms per frame when running on GTX 1080
GPU. Comparing to the state-of-the-art network, RoadNet-RT speeds up the
inference time by a factor of 20 at the cost of only 6.2% accuracy loss. For
hardware design optimization, several techniques such as depthwise separable
convolution and non-uniformed kernel size convolution are customized designed
to further reduce the processing time. The proposed CNN architecture has been
successfully implemented on an FPGA ZCU102 MPSoC platform that achieves the
computation capability of 83.05 GOPS. The system throughput reaches 327.9
frames per second with image size 1216x176.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.08152</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.08152</id><submitter>Guillaume Sartoretti</submitter><version version="v1"><date>Mon, 15 Jun 2020 06:13:45 GMT</date><size>2100kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 16:41:11 GMT</date><size>1658kb</size><source_type>D</source_type></version><title>ForMIC: Foraging via Multiagent RL with Implicit Communication</title><authors>Samuel Shaw, Emerson Wenzel, Alexis Walker, Guillaume Sartoretti</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent foraging (MAF) involves distributing a team of agents to search
an environment and extract resources from it. Many foraging algorithms use
biologically-inspired signaling mechanisms, such as pheromones, to help agents
navigate from resources back to a central nest while relying on local sensing
only. However, these approaches often rely on predictable pheromone dynamics
and/or perfect robot localization. In nature, certain environmental factors
(e.g., heat or rain) can disturb or destroy pheromone trails, while imperfect
sensing can lead robots astray. In this work, we propose ForMIC, a distributed
reinforcement learning MAF approach that relies on pheromones as a way to endow
agents with implicit communication abilities via their shared environment.
Specifically, full agents involuntarily lay trails of pheromones as they move;
other agents can then measure the local levels of pheromones to guide their
individual decisions. We show how these stigmergic interactions among agents
can lead to a highly scalable, decentralized MAF policy that is naturally
resilient to common environmental disturbances, such as depleting resources and
sudden pheromone disappearance. We present simulation results that compare our
learning policy against existing state-of-the-art MAF algorithms in a set of
experiments that vary team size, number and placement of resources, and key
environmental disturbances. Our results demonstrate that our learned policy
outperforms these baselines, approaching the performance of a planner with full
observability and centralized agent allocation.
  Preprint of the paper submitted to Springer's Swarm Intelligence in May 2021.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.10972</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.10972</id><submitter>Seunghoon Lee</submitter><version version="v1"><date>Fri, 19 Jun 2020 06:07:34 GMT</date><size>45kb</size></version><version version="v2"><date>Wed, 14 Oct 2020 20:25:37 GMT</date><size>64kb</size></version><version version="v3"><date>Tue, 15 Dec 2020 17:09:50 GMT</date><size>65kb</size></version><version version="v4"><date>Tue, 18 May 2021 17:06:55 GMT</date><size>59kb</size></version><title>On the Security of Proofs of Sequential Work in a Post-Quantum World</title><authors>Jeremiah Blocki, Seunghoon Lee, Samson Zhou</authors><categories>cs.CR</categories><comments>45 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Proof of Sequential Work (PoSW) allows a prover to convince a
resource-bounded verifier that the prover invested a substantial amount of
sequential time to perform some underlying computation. PoSWs have many
applications including time-stamping, blockchain design, and universally
verifiable CPU benchmarks. Mahmoody, Moran, and Vadhan (ITCS 2013) gave the
first construction of a PoSW in the random oracle model though the construction
relied on expensive depth-robust graphs. In a recent breakthrough, Cohen and
Pietrzak (EUROCRYPT 2018) gave an efficient PoSW construction that does not
require expensive depth-robust graphs.
  In the classical parallel random oracle model, it is straightforward to argue
that any successful PoSW attacker must produce a long $\mathcal{H}$-sequence
and that any malicious party running in sequential time $T-1$ will fail to
produce an $\mathcal{H}$-sequence of length $T$ except with negligible
probability. In this paper, we prove that any quantum attacker running in
sequential time $T-1$ will fail to produce an $\mathcal{H}$-sequence except
with negligible probability -- even if the attacker submits a large batch of
quantum queries in each round. The proof is substantially more challenging and
highlights the power of Zhandry's recent compressed oracle technique (CRYPTO
2019). We further extend this result to establish post-quantum security of a
non-interactive PoSW obtained by applying the Fiat-Shamir transform to Cohen
and Pietrzak's efficient construction (EUROCRYPT 2018).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.12169</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.12169</id><submitter>Yao Lu</submitter><version version="v1"><date>Mon, 22 Jun 2020 12:07:29 GMT</date><size>4393kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 16 Oct 2020 21:57:02 GMT</date><size>4357kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 4 May 2021 08:02:46 GMT</date><size>4359kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 18 May 2021 08:20:36 GMT</date><size>4359kb</size><source_type>D</source_type></version><title>Bidirectionally Self-Normalizing Neural Networks</title><authors>Yao Lu, Stephen Gould, Thalaiyasingam Ajanthan</authors><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of vanishing and exploding gradients has been a long-standing
obstacle that hinders the effective training of neural networks. Despite
various tricks and techniques that have been employed to alleviate the problem
in practice, there still lacks satisfactory theories or provable solutions. In
this paper, we address the problem from the perspective of high-dimensional
probability theory. We provide a rigorous result that shows, under mild
conditions, how the vanishing/exploding gradients problem disappears with high
probability if the neural networks have sufficient width. Our main idea is to
constrain both forward and backward signal propagation in a nonlinear neural
network through a new class of activation functions, namely Gaussian-Poincar\'e
normalized functions, and orthogonal weight matrices. Experiments on both
synthetic and real-world data validate our theory and confirm its effectiveness
on very deep neural networks when applied in practice.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.14189</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.14189</id><submitter>Emre Mengi</submitter><version version="v1"><date>Thu, 25 Jun 2020 05:41:21 GMT</date><size>76kb</size></version><version version="v2"><date>Tue, 18 May 2021 17:02:50 GMT</date><size>78kb</size></version><title>Derivative Interpolating Subspace Frameworks for Nonlinear Eigenvalue
  Problems</title><authors>Rifqi Aziz, Emre Mengi, Matthias Voigt</authors><categories>math.NA cs.NA</categories><comments>26 pages, 4 figures</comments><msc-class>65F15, 65D05, 34K17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first consider the problem of approximating a few eigenvalues of a
rational matrix-valued function closest to a prescribed target. It is assumed
that the proper rational part of the rational matrix-valued function is
expressed in the transfer function form $H(s) = C (sI - A)^{-1} B$, where the
middle factor is large, whereas the number of rows of $C$ and the number of
columns of $B$ are equal and small. We propose a subspace framework that
performs two-sided or one-sided projections on the state-space representation
of $H(\cdot)$, commonly employed in model reduction and giving rise to a
reduced transfer function. At every iteration, the projection subspaces are
expanded to attain Hermite interpolation conditions at the eigenvalues of the
reduced transfer function closest to the target, which in turn leads to a new
reduced transfer function. We prove in theory that, when a sequence of
eigenvalues of the reduced transfer functions converges to an eigenvalue of the
full problem, it converges at least at a quadratic rate. In the second part, we
extend the proposed framework to locate the eigenvalues of a general square
large-scale nonlinear meromorphic matrix-valued function $T(\cdot)$, where we
exploit a representation $\mathcal{R}(s) = C(s) A(s)^{-1} B(s) - D(s)$ defined
in terms of the block components of $T(\cdot)$. The numerical experiments
illustrate that the proposed framework is reliable in locating a few
eigenvalues closest to the target point, and that, with respect to runtime, it
is competitive to established methods for nonlinear eigenvalue problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.14352</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.14352</id><submitter>Simon Yusuf Enoch</submitter><version version="v1"><date>Thu, 25 Jun 2020 12:52:10 GMT</date><size>2654kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 15 Jul 2020 03:23:21 GMT</date><size>5079kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 17 Jul 2020 23:00:39 GMT</date><size>5079kb</size><source_type>D</source_type></version><title>HARMer: Cyber-attacks Automation and Evaluation</title><authors>Simon Yusuf Enoch, Zhibin Huang, Chun Yong Moon, Donghwan Lee, Myung
  Kil Ahn, and Dong Seong Kim</authors><categories>cs.CR</categories><comments>19 pages, journal</comments><journal-ref>IEEE Access, 8, 129397-129414 (2020)</journal-ref><doi>10.1109/ACCESS.2020.3009748</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing growth of cyber-attack incidences, it is important to
develop innovative and effective techniques to assess and defend networked
systems against cyber attacks. One of the well-known techniques for this is
performing penetration testing which is carried by a group of security
professionals (i.e, red team). Penetration testing is also known to be
effective to find existing and new vulnerabilities, however, the quality of
security assessment can be depending on the quality of the red team members and
their time and devotion to the penetration testing. In this paper, we propose a
novel automation framework for cyber-attacks generation named `HARMer' to
address the challenges with respect to manual attack execution by the red team.
Our novel proposed framework, design, and implementation is based on a scalable
graphical security model called Hierarchical Attack Representation Model
(HARM). (1) We propose the requirements and the key phases for the automation
framework. (2) We propose security metrics-based attack planning strategies
along with their algorithms. (3) We conduct experiments in a real enterprise
network and Amazon Web Services. The results show how the different phases of
the framework interact to model the attackers' operations. This framework will
allow security administrators to automatically assess the impact of various
threats and attacks in an automated manner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.14472</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.14472</id><submitter>Xiang Yu</submitter><version version="v1"><date>Wed, 24 Jun 2020 14:13:43 GMT</date><size>385kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 12:32:28 GMT</date><size>384kb</size><source_type>D</source_type></version><title>Teamwise Mean Field Competitions</title><authors>Xiang Yu, Yuchong Zhang, Zhou Zhou</authors><categories>cs.GT econ.TH math.OC</categories><comments>Final version, forthcoming in Applied Mathematics and Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies competitions with rank-based reward among a large number
of teams. Within each sizable team, we consider a mean-field contribution game
in which each team member contributes to the jump intensity of a common Poisson
project process; across all teams, a mean field competition game is formulated
on the rank of the completion time, namely the jump time of Poisson project
process, and the reward to each team is paid based on its ranking. On the layer
of teamwise competition game, three optimization problems are introduced when
the team size is determined by: (i) the team manager; (ii) the central planner;
(iii) the team members' voting as partnership. We propose a relative
performance criteria for each team member to share the team's reward and
formulate some special cases of mean field games of mean field games, which are
new to the literature. In all problems with homogeneous parameters, the
equilibrium control of each worker and the equilibrium or optimal team size can
be computed in an explicit manner, allowing us to analytically examine the
impacts of some model parameters and discuss their economic implications. Two
numerical examples are also presented to illustrate the parameter dependence
and comparison between different team size decision making.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.14799</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.14799</id><submitter>Asli Celikyilmaz</submitter><version version="v1"><date>Fri, 26 Jun 2020 04:52:48 GMT</date><size>8430kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 07:04:41 GMT</date><size>8818kb</size><source_type>D</source_type></version><title>Evaluation of Text Generation: A Survey</title><authors>Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao</authors><categories>cs.CL cs.LG</categories><comments>47 pages (revised version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper surveys evaluation methods of natural language generation (NLG)
systems that have been developed in the last few years. We group NLG evaluation
methods into three categories: (1) human-centric evaluation metrics, (2)
automatic metrics that require no training, and (3) machine-learned metrics.
For each category, we discuss the progress that has been made and the
challenges still being faced, with a focus on the evaluation of recently
proposed NLG tasks and neural NLG models. We then present two examples for
task-specific NLG evaluations for automatic text summarization and long text
generation, and conclude the paper by proposing future research directions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2006.15717</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2006.15717</id><submitter>IA Grant Wilson</submitter><version version="v1"><date>Sun, 28 Jun 2020 21:16:22 GMT</date><size>1086kb</size></version><version version="v2"><date>Mon, 17 May 2021 16:39:22 GMT</date><size>1884kb</size></version><title>ESPENI: A method to calculate Great Britains half-hourly electrical
  demand from publicly available data</title><authors>IA Grant Wilson, Shivangi Sharma, Joseph Day, Noah Godfrey</authors><categories>cs.CY</categories><comments>28 pages, 3 Figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A method is presented to combine publicly available electrical generation and
interconnector data to create a timeseries dataset that approximates Great
Britains half-hourly electrical demand for its public distribution system. The
method uses Elexon data for generation plants connected at the transmission
level that are monitored as part of the electrical systems national balancing
mechanism and combines these with estimates for embedded/distributed solar and
wind generation from the system operator National Grid. The resulting
timeseries data therefore has both transmission connected and
embedded/distribution connected generation, which overcomes one of the
limitations of solely using the Elexon data. The data are presented in a raw as
well as a visually checked and cleaned form and have been parsed to provide
both coordinated universal time (UTC) and localtime values in ISO 8601
compatible format, making the data significantly more interoperable with other
data, and more useable by a wider group of researchers and stakeholders. The
method has been developed and adapted over several years as the categories of
the underlying raw data have been changed from the data providers. Publishing
the method and the raw and cleaned data provides other researchers with a
method and data that can be further enhanced or adapted and allows the method
itself to be considered and critiqued.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.00622</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.00622</id><submitter>Weichen Dai</submitter><version version="v1"><date>Wed, 1 Jul 2020 17:11:02 GMT</date><size>7866kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 08:46:02 GMT</date><size>4642kb</size><source_type>D</source_type></version><title>A Multi-spectral Dataset for Evaluating Motion Estimation Systems</title><authors>Weichen Dai, Yu Zhang, Shenzhou Chen, Donglei Sun, Da Kong</authors><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible images have been widely used for motion estimation. Thermal images,
in contrast, are more challenging to be used in motion estimation since they
typically have lower resolution, less texture, and more noise. In this paper, a
novel dataset for evaluating the performance of multi-spectral motion
estimation systems is presented. All the sequences are recorded from a handheld
multi-spectral device. It consists of a standard visible-light camera, a
long-wave infrared camera, an RGB-D camera, and an inertial measurement unit
(IMU). The multi-spectral images, including both color and thermal images in
full sensor resolution (640 x 480), are obtained from a standard and a
long-wave infrared camera at 32Hz with hardware-synchronization. The depth
images are captured by a Microsoft Kinect2 and can have benefits for learning
cross-modalities stereo matching. For trajectory evaluation, accurate
ground-truth camera poses obtained from a motion capture system are provided.
In addition to the sequences with bright illumination, the dataset also
contains dim, varying, and complex illumination scenes. The full dataset,
including raw data and calibration data with detailed data format
specifications, is publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.01442</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.01442</id><submitter>Ronshee Chawla</submitter><version version="v1"><date>Thu, 2 Jul 2020 23:54:56 GMT</date><size>466kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 30 Oct 2020 01:10:50 GMT</date><size>616kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 04:55:24 GMT</date><size>255kb</size><source_type>D</source_type></version><title>Multi-Agent Low-Dimensional Linear Bandits</title><authors>Ronshee Chawla, Abishek Sankararaman and Sanjay Shakkottai</authors><categories>cs.LG cs.DC cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a multi-agent stochastic linear bandit with side information,
parameterized by an unknown vector $\theta^* \in \mathbb{R}^d$. The side
information consists of a finite collection of low-dimensional subspaces, one
of which contains $\theta^*$. In our setting, agents can collaborate to reduce
regret by sending recommendations across a communication graph connecting them.
We present a novel decentralized algorithm, where agents communicate subspace
indices with each other and each agent plays a projected variant of LinUCB on
the corresponding (low-dimensional) subspace. By distributing the search for
the optimal subspace across users and learning of the unknown vector by each
agent in the corresponding low-dimensional subspace, we show that the per-agent
finite-time regret is much smaller than the case when agents do not
communicate. We finally complement these results through simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.03184</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.03184</id><submitter>Yang Fang</submitter><version version="v1"><date>Tue, 7 Jul 2020 03:36:28 GMT</date><size>656kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 09:53:57 GMT</date><size>1378kb</size><source_type>D</source_type></version><title>Pre-Trained Models for Heterogeneous Information Networks</title><authors>Yang Fang, Xiang Zhao, Yifan Chen, Weidong Xiao, Maarten de Rijke</authors><categories>cs.AI cs.CL cs.LG</categories><comments>Submitted to TKDE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network representation learning we learn how to represent heterogeneous
information networks in a low-dimensional space so as to facilitate effective
search, classification, and prediction solutions. Previous network
representation learning methods typically require sufficient task-specific
labeled data to address domain-specific problems. The trained model usually
cannot be transferred to out-of-domain datasets. We propose a self-supervised
pre-training and fine-tuning framework, PF-HIN, to capture the features of a
heterogeneous information network. Unlike traditional network representation
learning models that have to train the entire model all over again for every
downstream task and dataset, PF-HIN only needs to fine-tune the model and a
small number of extra task-specific parameters, thus improving model efficiency
and effectiveness. During pre-training, we first transform the neighborhood of
a given node into a sequence. PF-HIN is pre-trained based on two
self-supervised tasks, masked node modeling and adjacent node prediction. We
adopt deep bi-directional transformer encoders to train the model, and leverage
factorized embedding parameterization and cross-layer parameter sharing to
reduce the parameters. In the fine-tuning stage, we choose four benchmark
downstream tasks, i.e., link prediction, similarity search, node
classification, and node clustering. PF-HIN consistently and significantly
outperforms state-of-the-art alternatives on each of these tasks, on four
datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.03486</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.03486</id><submitter>Simon Yusuf Enoch</submitter><version version="v1"><date>Tue, 7 Jul 2020 14:18:31 GMT</date><size>1447kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 17 Jul 2020 23:44:40 GMT</date><size>7176kb</size><source_type>D</source_type></version><title>Composite Metrics for Network Security Analysis</title><authors>Simon Yusuf Enoch and Jin B. Hong and Mengmeng Ge and Dong Seong Kim</authors><categories>cs.CR</categories><comments>21 pages journal</comments><journal-ref>Software Networking, 2018(1), 137-160</journal-ref><doi>10.13052/jsn2445-9739.2017.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security metrics present the security level of a system or a network in both
qualitative and quantitative ways. In general, security metrics are used to
assess the security level of a system and to achieve security goals. There are
a lot of security metrics for security analysis, but there is no systematic
classification of security metrics that are based on network reachability
information. To address this, we propose a systematic classification of
existing security metrics based on network reachability information. Mainly, we
classify the security metrics into host-based and network-based metrics. The
host-based metrics are classified into metrics ``without probability&quot; and &quot;with
probability&quot;, while the network-based metrics are classified into &quot;path-based&quot;
and &quot;non-path based&quot;. Finally, we present and describe an approach to develop
composite security metrics and it's calculations using a Hierarchical Attack
Representation Model (HARM) via an example network. Our novel classification of
security metrics provides a new methodology to assess the security of a system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.05963</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.05963</id><submitter>Miguel A. Lopez-Carmona</submitter><version version="v1"><date>Sun, 12 Jul 2020 11:37:53 GMT</date><size>3534kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 06:15:53 GMT</date><size>32049kb</size><source_type>D</source_type></version><title>CellEVAC: An adaptive guidance system for crowd evacuation through
  behavioral optimization</title><authors>Miguel A. Lopez-Carmona, Alvaro Paricio Garcia</authors><categories>cs.MA cs.SY eess.SY</categories><comments>47 pages, 26 figures</comments><acm-class>I.6.4</acm-class><doi>10.1016/j.ssci.2021.105215</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A critical aspect of crowds' evacuation processes is the dynamism of
individual decision making. Here, we investigate how to favor a coordinated
group dynamic through optimal exit-choice instructions using behavioral
strategy optimization. We propose and evaluate an adaptive guidance system
(Cell-based Crowd Evacuation, CellEVAC) that dynamically allocates colors to
cells in a cell-based pedestrian positioning infrastructure, to provide
efficient exit-choice indications. The operational module of CellEVAC
implements an optimized discrete-choice model that integrates the influential
factors that would make evacuees adapt their exit choice. To optimize the
model, we used a simulation-optimization modeling framework that integrates
microscopic pedestrian simulation based on the classical Social Force Model. We
paid particular attention to safety by using Pedestrian Fundamental Diagrams
that model the dynamics of the exit gates. CellEVAC has been tested in a
simulated real scenario (Madrid Arena) under different external pedestrian flow
patterns that simulate complex pedestrian interactions. Results showed that
CellEVAC outperforms evacuation processes in which the system is not used, with
an exponential improvement as interactions become complex. We compared our
system with an existing approach based on Cartesian Genetic Programming. Our
system exhibited a better overall performance in terms of safety, evacuation
time, and the number of revisions of exit-choice decisions. Further analyses
also revealed that Cartesian Genetic Programming generates less natural
pedestrian reactions and movements than CellEVAC. The fact that the decision
logic module is built upon a behavioral model seems to favor a more natural and
effective response. We also found that our proposal has a positive influence on
evacuations even for a low compliance rate (40%).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.07203</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.07203</id><submitter>Weihao Gao</submitter><version version="v1"><date>Sun, 12 Jul 2020 06:23:51 GMT</date><size>4549kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 05:45:30 GMT</date><size>5210kb</size><source_type>D</source_type></version><title>Deep Retrieval: Learning A Retrievable Structure for Large-Scale
  Recommendations</title><authors>Weihao Gao, Xiangjun Fan, Chong Wang, Jiankai Sun, Kai Jia, Wenzhi
  Xiao, Ruofan Ding, Xingyan Bin, Hui Yang, Xiaobing Liu</authors><categories>cs.IR cs.LG stat.ML</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the core problems in large-scale recommendations is to retrieve top
relevant candidates accurately and efficiently, preferably in sub-linear time.
Previous approaches are mostly based on a two-step procedure: first learn an
inner-product model, and then use some approximate nearest neighbor (ANN)
search algorithm to find top candidates. In this paper, we present Deep
Retrieval (DR), to learn a retrievable structure directly with user-item
interaction data (e.g. clicks) without resorting to the Euclidean space
assumption in ANN algorithms. DR's structure encodes all candidate items into a
discrete latent space. Those latent codes for the candidates are model
parameters and learnt together with other neural network parameters to maximize
the same objective function. With the model learnt, a beam search over the
structure is performed to retrieve the top candidates for reranking.
Empirically, we first demonstrate that DR, with sub-linear computational
complexity, can achieve almost the same accuracy as the brute-force baseline on
two public datasets. Moreover, we show that, in a live production
recommendation system, a deployed DR approach significantly outperforms a
well-tuned ANN baseline in terms of engagement metrics. To the best of our
knowledge, DR is among the first non-ANN algorithms successfully deployed at
the scale of hundreds of millions of items for industrial recommendation
systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.10824</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.10824</id><submitter>David Harris</submitter><version version="v1"><date>Fri, 17 Jul 2020 11:27:08 GMT</date><size>57kb</size></version><version version="v2"><date>Fri, 9 Oct 2020 13:03:27 GMT</date><size>56kb</size></version><version version="v3"><date>Sat, 30 Jan 2021 20:54:19 GMT</date><size>54kb</size></version><version version="v4"><date>Mon, 17 May 2021 20:41:26 GMT</date><size>57kb</size></version><title>Parameter estimation for Gibbs distributions</title><authors>David G. Harris, Vladimir Kolmogorov</authors><categories>cs.DS cs.CC cs.DM math.PR</categories><comments>This is a significantly extended version of a paper &quot;A Faster
  Approximation Algorithm for the Gibbs Partition Function&quot; (arXiv:1608.04223),
  which was published in COLT 2018. It covers many additional topics; most
  importantly, algorithms to estimate counts and algorithm specialized for
  integer-valued distributions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider \emph{Gibbs distributions}, which are families of probability
distributions over a discrete space $\Omega$ with probability mass function of
the form $\mu^\Omega_\beta(\omega) \propto e^{\beta H(\omega)}$ for $\beta$ in
an interval $[\beta_{\min}, \beta_{\max}]$ and $H( \omega ) \in \{0 \} \cup [1,
n]$. The \emph{partition function} is the normalization factor
$Z(\beta)=\sum_{\omega \in\Omega}e^{\beta H(\omega)}$.
  Two important parameters of these distributions are the log partition ratio
$q = \log \tfrac{Z(\beta_{\max})}{Z(\beta_{\min})}$ and the counts $c_x =
|H^{-1}(x)|$. These are correlated with system parameters in a number of
physical applications and sampling algorithms. Our first main result is to
estimate the counts $c_x$ using roughly $\tilde O( \frac{q}{\varepsilon^2})$
samples for general Gibbs distributions and $\tilde O(
\frac{n^2}{\varepsilon^2} )$ samples for integer-valued distributions (ignoring
some second-order terms and parameters), and we show this is optimal up to
logarithmic factors. We illustrate with improved algorithms for counting
connected subgraphs and perfect matchings in a graph.
  We develop a key subroutine to estimate the partition function $Z$.
Specifically, it generates a data structure to estimate $Z(\beta)$ for
\emph{all} values $\beta$, without further samples. Constructing the data
structure requires $O(\frac{q \log n}{\varepsilon^2})$ samples for general
Gibbs distributions and $O(\frac{n^2 \log n}{\varepsilon^2} + n \log q)$
samples for integer-valued distributions. This improves over a prior algorithm
of Huber (2015) which computes a single point estimate $Z(\beta_\max)$ using
$O( q \log n( \log q + \log \log n + \varepsilon^{-2}))$ samples. We show
matching lower bounds, demonstrating that this complexity is optimal as a
function of $n$ and $q$ up to logarithmic terms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.10868</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.10868</id><submitter>Christoph M\&quot;uller</submitter><version version="v1"><date>Mon, 20 Jul 2020 16:09:07 GMT</date><size>324kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 10:14:05 GMT</date><size>287kb</size><source_type>D</source_type></version><title>Scaling Polyhedral Neural Network Verification on GPUs</title><authors>Christoph M\&quot;uller, Fran\c{c}ois Serre, Gagandeep Singh, Markus
  P\&quot;uschel, Martin Vechev</authors><categories>cs.LG stat.ML</categories><comments>M\&quot;uller and Serre contributed equally to this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certifying the robustness of neural networks against adversarial attacks is
essential to their reliable adoption in safety-critical systems such as
autonomous driving and medical diagnosis. Unfortunately, state-of-the-art
verifiers either do not scale to bigger networks or are too imprecise to prove
robustness, limiting their practical adoption. In this work, we introduce
GPUPoly, a scalable verifier that can prove the robustness of significantly
larger deep neural networks than previously possible. The key technical insight
behind GPUPoly is the design of custom, sound polyhedra algorithms for neural
network verification on a GPU. Our algorithms leverage the available GPU
parallelism and inherent sparsity of the underlying verification task. GPUPoly
scales to large networks: for example, it can prove the robustness of a 1M
neuron, 34-layer deep residual network in approximately 34.5 ms. We believe
GPUPoly is a promising step towards practical verification of real-world neural
networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.12045</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.12045</id><submitter>Alexandre Coulombe</submitter><version version="v1"><date>Thu, 23 Jul 2020 14:48:55 GMT</date><size>872kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 23:11:39 GMT</date><size>872kb</size><source_type>D</source_type></version><title>High Precision Real Time Collision Detection</title><authors>Alexandre Coulombe and Hsiu-Chin Lin</authors><categories>cs.RO</categories><comments>5 pages, 5 figures, Accepted at Robotics Science and Systems: Power
  On and Go workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collision detection and collision avoidance are essential components in these
systems for safe human-robot interactions. Robotics systems that can work
&quot;out-of-the-box&quot; without excessive amount of installation and calibration from
the experts is highly ideal. For this, we propose a generic, high precision,
collision detect system that only requires the unified robot description format
(URDF) and is capable of running in real time. We extended the
Gilbert-Johnson-Keerthi (GJK) algorithm by utilizing a geometrical approach to
determine the distance between each rigid body in the environment and check for
collisions. The proposed system's performance is shown by checking the
self-collision of the KUKA LBR iiwa 7 R800 and the Mecademic Meca500. The
performance is compared to the Flexible Collision Library (FCL).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.12105</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.12105</id><submitter>S{\o}ren Eller Thomsen</submitter><version version="v1"><date>Thu, 23 Jul 2020 16:12:53 GMT</date><size>32kb</size></version><version version="v2"><date>Tue, 18 May 2021 10:37:43 GMT</date><size>38kb</size></version><title>Formalizing Nakamoto-Style Proof of Stake</title><authors>S{\o}ren Eller Thomsen and Bas Spitters</authors><categories>cs.CR cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault-tolerant distributed systems move the trust in a single party to a
majority of parties participating in the protocol. This makes blockchain based
crypto-currencies possible: they allow parties to agree on a total order of
transactions without a trusted third party. To trust a distributed system, the
security of the protocol and the correctness of the implementation must be
indisputable.
  We present the first machine checked proof that guarantees both safety and
liveness for a consensus algorithm. We verify a Proof of Stake (PoS)
Nakamoto-style blockchain (NSB) protocol, using the foundational proof
assistant Coq.
  In particular, we consider a PoS NSB in a synchronous network with a static
set of corrupted parties. We define execution semantics for this setting and
prove chain growth, chain quality, and common prefix which together imply both
safety and liveness.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.12174</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.12174</id><submitter>Freark van der Berg</submitter><version version="v1"><date>Thu, 23 Jul 2020 11:19:22 GMT</date><size>573kb</size></version><version version="v2"><date>Tue, 18 May 2021 16:32:57 GMT</date><size>475kb</size><source_type>D</source_type></version><title>Recursive Variable-Length State Compression for Multi-Core Software
  Model Checking</title><authors>Freark I. van der Berg</authors><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  High-performance multi-core software typically uses concurrent data
structures. Tests for such data structures have significantly smaller state
spaces than the entire software, making it feasible to model check them.
However, dynamic memory allocations on the heap complicate the use of standard
fixed-length state vectors. In this paper, we introduce dtree, a concurrent
compression tree data structure that compactly stores variable-length states
while allowing partial state reconstruction and incremental updates without
concretising states. It supports describing a state as a tree, allowing direct
modeling of the heap. We implemented dtree in DMC, our multi-core model
checker. We show that its performance approaches that of state-of-the-art model
checkers for fixed-length states. For models with variable-length states, dtree
is up to 2.9 times faster.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.12415</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.12415</id><submitter>Xi Li</submitter><version version="v1"><date>Fri, 24 Jul 2020 09:12:37 GMT</date><size>1181kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 13:54:27 GMT</date><size>5739kb</size><source_type>D</source_type></version><title>What and Where: Learn to Plug Adapters via NAS for Multi-Domain Learning</title><authors>Hanbin Zhao, Hao Zeng, Xin Qin, Yongjian Fu, Hui Wang, Bourahla Omar,
  and Xi Li</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an important and challenging problem, multi-domain learning (MDL)
typically seeks for a set of effective lightweight domain-specific adapter
modules plugged into a common domain-agnostic network. Usually, existing ways
of adapter plugging and structure design are handcrafted and fixed for all
domains before model learning, resulting in the learning inflexibility and
computational intensiveness. With this motivation, we propose to learn a
data-driven adapter plugging strategy with Neural Architecture Search (NAS),
which automatically determines where to plug for those adapter modules.
Furthermore, we propose a NAS-adapter module for adapter structure design in a
NAS-driven learning scheme, which automatically discovers effective adapter
module structures for different domains. Experimental results demonstrate the
effectiveness of our MDL model against existing approaches under the conditions
of comparable performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.13310</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.13310</id><submitter>Guo-Jun Qi</submitter><version version="v1"><date>Mon, 27 Jul 2020 04:56:41 GMT</date><size>3242kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 1 Feb 2021 08:00:58 GMT</date><size>3243kb</size><source_type>D</source_type></version><title>K-Shot Contrastive Learning of Visual Features with Multiple Instance
  Augmentations</title><authors>Haohang Xu, Hongkai Xiong, Guo-Jun Qi</authors><categories>cs.CV</categories><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the $K$-Shot Contrastive Learning (KSCL) of visual
features by applying multiple augmentations to investigate the sample
variations within individual instances. It aims to combine the advantages of
inter-instance discrimination by learning discriminative features to
distinguish between different instances, as well as intra-instance variations
by matching queries against the variants of augmented samples over instances.
Particularly, for each instance, it constructs an instance subspace to model
the configuration of how the significant factors of variations in $K$-shot
augmentations can be combined to form the variants of augmentations. Given a
query, the most relevant variant of instances is then retrieved by projecting
the query onto their subspaces to predict the positive instance class. This
generalizes the existing contrastive learning that can be viewed as a special
one-shot case. An eigenvalue decomposition is performed to configure instance
subspaces, and the embedding network can be trained end-to-end through the
differentiable subspace configuration. Experiment results demonstrate the
proposed $K$-shot contrastive learning achieves superior performances to the
state-of-the-art unsupervised methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.14957</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.14957</id><submitter>Feng Huang</submitter><version version="v1"><date>Wed, 29 Jul 2020 17:01:24 GMT</date><size>9079kb</size></version><title>Learning enables adaptation in cooperation for multi-player stochastic
  games</title><authors>Feng Huang, Ming Cao, and Long Wang</authors><categories>q-bio.PE cs.GT cs.MA physics.bio-ph physics.soc-ph</categories><journal-ref>J. R. Soc. Interface 17: 20200639 (2020)</journal-ref><doi>10.1098/rsif.2020.0639</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactions among individuals in natural populations often occur in a
dynamically changing environment. Understanding the role of environmental
variation in population dynamics has long been a central topic in theoretical
ecology and population biology. However, the key question of how individuals,
in the middle of challenging social dilemmas (e.g., the &quot;tragedy of the
commons&quot;), modulate their behaviors to adapt to the fluctuation of the
environment has not yet been addressed satisfactorily. Utilizing evolutionary
game theory and stochastic games, we develop a game-theoretical framework that
incorporates the adaptive mechanism of reinforcement learning to investigate
whether cooperative behaviors can evolve in the ever-changing group interaction
environment. When the action choices of players are just slightly influenced by
past reinforcements, we construct an analytical condition to determine whether
cooperation can be favored over defection. Intuitively, this condition reveals
why and how the environment can mediate cooperative dilemmas. Under our model
architecture, we also compare this learning mechanism with two non-learning
decision rules, and we find that learning significantly improves the propensity
for cooperation in weak social dilemmas, and, in sharp contrast, hinders
cooperation in strong social dilemmas. Our results suggest that in complex
social-ecological dilemmas, learning enables the adaptation of individuals to
varying environments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.15108</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.15108</id><submitter>Saeid Sedighi</submitter><version version="v1"><date>Wed, 29 Jul 2020 21:03:19 GMT</date><size>971kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 9 Apr 2021 11:42:46 GMT</date><size>996kb</size><source_type>D</source_type></version><title>Localization with One-Bit Passive Radars in Narrowband
  Internet-of-Things using Multivariate Polynomial Optimization</title><authors>Saeid Sedighi, Kumar Vijay Mishra, M. R. Bhavani Shankar and Bj\&quot;orn
  Ottersten</authors><categories>eess.SP cs.IR math.OC</categories><comments>16 pages, 11 figures</comments><doi>10.1109/TSP.2021.3072834</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several Internet-of-Things (IoT) applications provide location-based
services, wherein it is critical to obtain accurate position estimates by
aggregating information from individual sensors. In the recently proposed
narrowband IoT (NB-IoT) standard, which trades off bandwidth to gain wide
coverage, the location estimation is compounded by the low sampling rate
receivers and limited-capacity links. We address both of these NB-IoT drawbacks
in the framework of passive sensing devices that receive signals from the
target-of-interest. We consider the limiting case where each node receiver
employs one-bit analog-to-digital-converters and propose a novel low-complexity
nodal delay estimation method using constrained-weighted least squares
minimization. To support the low-capacity links to the fusion center (FC), the
range estimates obtained at individual sensors are then converted to one-bit
data. At the FC, we propose target localization with the aggregated one-bit
range vector using both optimal and sub-optimal techniques. The computationally
expensive former approach is based on Lasserre's method for multivariate
polynomial optimization while the latter employs our less complex iterative
joint r\textit{an}ge-\textit{tar}get location \textit{es}timation (ANTARES)
algorithm. Our overall one-bit framework not only complements the low NB-IoT
bandwidth but also supports the design goal of inexpensive NB-IoT location
sensing. Numerical experiments demonstrate feasibility of the proposed one-bit
approach with a $0.6$\% increase in the normalized localization error for the
small set of $20$-$60$ nodes over the full-precision case. When the number of
nodes is sufficiently large ($&gt;80$), the one-bit methods yield the same
performance as the full precision.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.15121</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.15121</id><submitter>Arjun Roy</submitter><version version="v1"><date>Wed, 29 Jul 2020 21:40:01 GMT</date><size>773kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 17:10:02 GMT</date><size>830kb</size><source_type>D</source_type></version><title>Exploiting stance hierarchies for cost-sensitive stance detection of Web
  documents</title><authors>Arjun Roy, Pavlos Fafalios, Asif Ekbal, Xiaofei Zhu, Stefan Dietze</authors><categories>cs.CL cs.IR cs.LG</categories><comments>This is a pre-print version of the Journal paper published in J
  Intell Inf Syst (2021) (Springer). https://rdcu.be/ckLiC</comments><doi>10.1007/s10844-021-00642-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fact checking is an essential challenge when combating fake news. Identifying
documents that agree or disagree with a particular statement (claim) is a core
task in this process. In this context, stance detection aims at identifying the
position (stance) of a document towards a claim. Most approaches address this
task through a 4-class classification model where the class distribution is
highly imbalanced. Therefore, they are particularly ineffective in detecting
the minority classes (for instance, 'disagree'), even though such instances are
crucial for tasks such as fact-checking by providing evidence for detecting
false claims. In this paper, we exploit the hierarchical nature of stance
classes, which allows us to propose a modular pipeline of cascading binary
classifiers, enabling performance tuning on a per step and class basis. We
implement our approach through a combination of neural and traditional
classification models that highlight the misclassification costs of minority
classes. Evaluation results demonstrate state-of-the-art performance of our
approach and its ability to significantly improve the classification
performance of the important 'disagree' class.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2007.15634</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2007.15634</id><submitter>Ralph Foorthuis</submitter><version version="v1"><date>Thu, 30 Jul 2020 17:55:11 GMT</date><size>2126kb</size></version><version version="v2"><date>Sun, 27 Dec 2020 22:15:50 GMT</date><size>2177kb</size></version><version version="v3"><date>Sat, 15 May 2021 11:45:58 GMT</date><size>2273kb</size></version><title>On the Nature and Types of Anomalies: A Review of Deviations in Data</title><authors>Ralph Foorthuis</authors><categories>cs.DB cs.AI cs.LG stat.OT</categories><comments>39 pages (30 pages content), 10 figures and 3 tables. Preprint;
  comments will be appreciated. Improvements in version 3: Added new anomaly
  subtypes; Tightening of definitions; Additional examples from new literature;
  Various minor additions and improvements</comments><msc-class>62A01</msc-class><acm-class>G.3; I.2.6; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomalies are occurrences in a dataset that are in some way unusual and do
not fit the general patterns. The concept of the anomaly is usually ill-defined
and perceived as vague and domain-dependent. Moreover, despite some 250 years
of publications on the topic, no comprehensive and concrete overviews of the
different types of anomalies have hitherto been published. By means of an
extensive literature review this study therefore offers the first theoretically
principled and domain-independent typology of data anomalies, and presents a
full overview of anomaly types and subtypes. To concretely define the concept
of the anomaly and its different manifestations, the typology employs five
dimensions: data type, cardinality of relationship, anomaly level, data
structure, and data distribution. These fundamental and data-centric dimensions
naturally yield 3 broad groups, 9 basic types and 63 subtypes of anomalies. The
typology facilitates the evaluation of the functional capabilities of anomaly
detection algorithms, contributes to explainable data science, and provides
insights into relevant topics such as local versus global anomalies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.00214</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.00214</id><submitter>Vasileios Kouliaridis</submitter><version version="v1"><date>Sat, 1 Aug 2020 08:39:53 GMT</date><size>303kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 27 Aug 2020 17:50:31 GMT</date><size>85kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 21 Sep 2020 13:45:10 GMT</date><size>88kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 17 May 2021 13:46:41 GMT</date><size>106kb</size><source_type>D</source_type></version><title>Dissecting contact tracing apps in the Android platform</title><authors>Vasileios Kouliaridis, Georgios Kambourakis, Efstratios Chatzoglou,
  Dimitrios Geneiatakis, Hua Wang</authors><categories>cs.CR</categories><comments>revised</comments><journal-ref>PLOS ONE 16(5), 2021</journal-ref><doi>10.1371/journal.pone.0251867</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Contact tracing has historically been used to retard the spread of infectious
diseases, but if it is exercised by hand in large-scale, it is known to be a
resource-intensive and quite deficient process. Nowadays, digital contact
tracing has promptly emerged as an indispensable asset in the global fight
against the coronavirus pandemic. The work at hand offers a meticulous study of
all the official Android contact tracing apps deployed hitherto by European
countries. Each app is closely scrutinized both statically and dynamically by
means of dynamic instrumentation. Depending on the level of examination, static
analysis results are grouped in two axes. The first encompasses permissions,
API calls, and possible connections to external URLs, while the second
concentrates on potential security weaknesses and vulnerabilities, including
the use of trackers, in-depth manifest analysis, shared software analysis, and
taint analysis. Dynamic analysis on the other hand collects data pertaining to
Java classes and network traffic. The results demonstrate that while overall
these apps are well-engineered, they are not free of weaknesses,
vulnerabilities, and misconfigurations that may ultimately put the user
security and privacy at risk.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.00404</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.00404</id><submitter>Yixin Su</submitter><version version="v1"><date>Sun, 2 Aug 2020 06:08:23 GMT</date><size>211kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 18 Sep 2020 04:51:57 GMT</date><size>212kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 26 Sep 2020 02:23:31 GMT</date><size>212kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 2 Jan 2021 06:37:45 GMT</date><size>204kb</size><source_type>D</source_type></version><version version="v5"><date>Tue, 30 Mar 2021 00:47:40 GMT</date><size>287kb</size><source_type>D</source_type></version><version version="v6"><date>Tue, 18 May 2021 11:57:21 GMT</date><size>287kb</size><source_type>D</source_type></version><title>Detecting Beneficial Feature Interactions for Recommender Systems</title><authors>Yixin Su, Rui Zhang, Sarah Erfani, Zhenghua Xu</authors><categories>cs.LG cs.IR stat.ML</categories><comments>14 pages, 7 figures, 5 tables, AAAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature interactions are essential for achieving high accuracy in recommender
systems. Many studies take into account the interaction between every pair of
features. However, this is suboptimal because some feature interactions may not
be that relevant to the recommendation result, and taking them into account may
introduce noise and decrease recommendation accuracy. To make the best out of
feature interactions, we propose a graph neural network approach to effectively
model them, together with a novel technique to automatically detect those
feature interactions that are beneficial in terms of recommendation accuracy.
The automatic feature interaction detection is achieved via edge prediction
with an L0 activation regularization. Our proposed model is proved to be
effective through the information bottleneck principle and statistical
interaction theory. Experimental results show that our model (i) outperforms
existing baselines in terms of accuracy, and (ii) automatically identifies
beneficial feature interactions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.01411</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.01411</id><submitter>Xi Li</submitter><version version="v1"><date>Tue, 4 Aug 2020 08:39:40 GMT</date><size>3555kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 13:32:08 GMT</date><size>5683kb</size><source_type>D</source_type></version><title>Memory Efficient Class-Incremental Learning for Image Classification</title><authors>Hanbin Zhao, Hui Wang, Yongjian Fu, Fei Wu, Xi Li</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the memory-resource-limited constraints, class-incremental learning
(CIL) usually suffers from the &quot;catastrophic forgetting&quot; problem when updating
the joint classification model on the arrival of newly added classes. To cope
with the forgetting problem, many CIL methods transfer the knowledge of old
classes by preserving some exemplar samples into the size-constrained memory
buffer. To utilize the memory buffer more efficiently, we propose to keep more
auxiliary low-fidelity exemplar samples rather than the original real
high-fidelity exemplar samples. Such a memory-efficient exemplar preserving
scheme makes the old-class knowledge transfer more effective. However, the
low-fidelity exemplar samples are often distributed in a different domain away
from that of the original exemplar samples, that is, a domain shift. To
alleviate this problem, we propose a duplet learning scheme that seeks to
construct domain-compatible feature extractors and classifiers, which greatly
narrows down the above domain gap. As a result, these low-fidelity auxiliary
exemplar samples have the ability to moderately replace the original exemplar
samples with a lower memory cost. In addition, we present a robust classifier
adaptation scheme, which further refines the biased classifier (learned with
the samples containing distillation label knowledge about old classes) with the
help of the samples of pure true class labels. Experimental results demonstrate
the effectiveness of this work against the state-of-the-art approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.01742</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.01742</id><submitter>Mayank Mundhra</submitter><version version="v1"><date>Tue, 4 Aug 2020 18:00:51 GMT</date><size>1925kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Feb 2021 06:39:57 GMT</date><size>4553kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 20:10:31 GMT</date><size>1690kb</size><source_type>D</source_type></version><title>SISSLE in consensus-based Ripple: Some Improvements in Speed, Security
  and Last Mile Connectivity</title><authors>Mayank Mundhra, Chester Rebeiro</authors><categories>cs.DC cs.CR</categories><comments>11 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptocurrencies are rapidly finding application in areas such as Real Time
Gross Settlements and Payments. Ripple is a cryptocurrency that has gained
prominence with banks and payment providers. It solves the Byzantine General's
Problem with its Ripple Protocol Consensus Algorithm (RPCA), where each server
maintains a list of servers, called the Unique Node List (UNL), that represents
the network for that server and will not collectively defraud it. The server
believes that the network has come to a consensus when servers on the UNL come
to a consensus on a transaction.
  In this paper we improve Ripple to achieve better speed, security and last
mile connectivity. We implement guidelines for resilience, robustness, improved
security, and efficient information propagation (IP). We enhance the system to
ensure that each server receives information from across the whole network
rather than just from the UNL members. We introduce the paradigm of UNL overlap
as a function of IP and the trust a server assigns to its own UNL. Our design
makes it possible to identify and mitigate some malicious behaviours including
attempts to fraudulently Double Spend or stall the system. We provide
experimental evidence of the benefits of our approach over the current Ripple
scheme. We observe $\geq 99.67\%$ reduction in opportunities for double spend
attacks and censorship, $1.71x$ increase in fault tolerance to $\geq 34.21\%$
malicious nodes, $\geq 4.97x$ and $98.22x$ speedup and success rate for IP
respectively, and $\geq 3.16x$ and $51.70x$ speedup and success rate in
consensus respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.01779</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.01779</id><submitter>Mark Tygert</submitter><version version="v1"><date>Tue, 4 Aug 2020 19:30:02 GMT</date><size>6429kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 26 Feb 2021 22:37:32 GMT</date><size>19647kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 28 Mar 2021 17:26:47 GMT</date><size>19613kb</size><source_type>D</source_type></version><version version="v4"><date>Sun, 16 May 2021 17:20:41 GMT</date><size>19444kb</size><source_type>D</source_type></version><title>Cumulative deviation of a subpopulation from the full population</title><authors>Mark Tygert</authors><categories>stat.ME cs.CY</categories><comments>54 pages, 35 figures; the new versions of the paper merge in most of
  arXiv:2006.02504</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assessing equity in treatment of a subpopulation often involves assigning
numerical &quot;scores&quot; to all individuals in the full population such that similar
individuals get similar scores; matching via propensity scores or appropriate
covariates is common, for example. Given such scores, individuals with similar
scores may or may not attain similar outcomes independent of the individuals'
memberships in the subpopulation. The traditional graphical methods for
visualizing inequities are known as &quot;reliability diagrams&quot; or &quot;calibrations
plots,&quot; which bin the scores into a partition of all possible values, and for
each bin plot both the average outcomes for only individuals in the
subpopulation as well as the average outcomes for all individuals; comparing
the graph for the subpopulation with that for the full population gives some
sense of how the averages for the subpopulation deviate from the averages for
the full population. Unfortunately, real data sets contain only finitely many
observations, limiting the usable resolution of the bins, and so the
conventional methods can obscure important variations due to the binning.
Fortunately, plotting cumulative deviation of the subpopulation from the full
population as proposed in this paper sidesteps the problematic coarse binning.
The cumulative plots encode subpopulation deviation directly as the slopes of
secant lines for the graphs. Slope is easy to perceive even when the constant
offsets of the secant lines are irrelevant. The cumulative approach avoids
binning that smooths over deviations of the subpopulation from the full
population. Such cumulative aggregation furnishes both high-resolution
graphical methods and simple scalar summary statistics (analogous to those of
Kuiper and of Kolmogorov and Smirnov used in statistical significance testing
for comparing probability distributions).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.02100</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.02100</id><submitter>Raghunandan M Rao</submitter><version version="v1"><date>Tue, 4 Aug 2020 02:37:10 GMT</date><size>2522kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 19:54:02 GMT</date><size>7728kb</size><source_type>D</source_type></version><title>Underlay Radar-Massive MIMO Spectrum Sharing: Modeling Fundamentals and
  Performance Analysis</title><authors>Raghunandan M. Rao, Harpreet S. Dhillon, Vuk Marojevic, Jeffrey H.
  Reed</authors><categories>cs.NI eess.SP</categories><comments>This arXiv manuscript subsumes the contents of the conference paper
  presented at the 2019 IEEE Global Communications Conference (Globecom),
  Waikoloa, HI. The conference version is available at arXiv:1907.09536</comments><doi>10.1109/TWC.2021.3081458</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study underlay radar-massive MIMO cellular coexistence in
LoS/near-LoS channels, where both systems have 3D beamforming capabilities.
Using mathematical tools from stochastic geometry, we derive an upper bound on
the average interference power at the radar due to the 3D massive MIMO cellular
downlink under the worst-case `cell-edge beamforming' conditions. To overcome
the technical challenges imposed by asymmetric and arbitrarily large cells, we
devise a novel construction in which each Poisson Voronoi (PV) cell is bounded
by its circumcircle to bound the effect of the random cell shapes on average
interference. Since this model is intractable for further analysis due to the
correlation between adjacent PV cells' shapes and sizes, we propose a tractable
nominal interference model, where we model each PV cell as a circular disk with
an area equal to the average area of the typical cell. We quantify the gap in
the average interference power between these two models and show that the upper
bound is tight for realistic deployment parameters. We also compare them with a
more practical but intractable MU-MIMO scheduling model to show that our
worst-case interference models show the same trends and do not deviate
significantly from realistic scheduler models. Under the nominal interference
model, we characterize the interference distribution using the dominant
interferer approximation by deriving the equi-interference contour expression
when the typical receiver uses 3D beamforming. Finally, we use tractable
expressions for the interference distribution to characterize radar's spatial
probability of false alarm/detection in a quasi-static target tracking
scenario. Our results reveal useful trends in the average interference as a
function of the deployment parameters (BS density, exclusion zone radius,
antenna height, transmit power of each BS, etc.).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.02379</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.02379</id><submitter>Behdad Chalaki</submitter><version version="v1"><date>Wed, 5 Aug 2020 22:03:20 GMT</date><size>3140kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 12:27:57 GMT</date><size>6368kb</size><source_type>D</source_type></version><title>Optimal Control of Connected and Automated Vehicles at Multiple Adjacent
  Intersections</title><authors>Behdad Chalaki and Andreas A. Malikopoulos</authors><categories>math.OC cs.SY eess.SY</categories><comments>14 pages, 9 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish a decentralized optimal control framework for
connected and automated vehicles (CAVs) crossing multiple adjacent, multi-lane
signal-free intersections to minimize energy consumption and improve traffic
throughput. Our framework consists of two layers of planning. In the
upper-level planning, each CAV computes its optimal arrival time at each
intersection recursively along with the optimal lane to improve the traffic
throughput. In the low-level planning, we formulate an energy-optimal control
problem with interior-point constraints, the solution of which yields the
optimal control input (acceleration/deceleration) of each CAV to cross the
intersections at the time specified by the upper-level planning. Moreover, we
extend the results of the proposed bi-level framework to include a bounded
steady-state error in tracking the optimal position of the CAVs. Finally, we
demonstrate the effectiveness of the proposed framework through simulation for
symmetric and asymmetric intersections and comparison with traditional
signalized intersections.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.02397</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.02397</id><submitter>Mohammad Malekzadeh</submitter><version version="v1"><date>Wed, 5 Aug 2020 23:42:03 GMT</date><size>2579kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 16 Nov 2020 12:50:40 GMT</date><size>2575kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 12:47:31 GMT</date><size>4280kb</size><source_type>D</source_type></version><title>DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor
  Data</title><authors>Mohammad Malekzadeh, Richard G. Clegg, Andrea Cavallaro, Hamed Haddadi</authors><categories>cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion sensors embedded in wearable and mobile devices allow for dynamic
selection of sensor streams and sampling rates, enabling useful applications,
e.g. for power management or control of data sharing. While deep neural
networks (DNNs) achieve competitive accuracy in sensor data classification,
current DNN architectures only process data coming from a fixed set of sensors
with a fixed sampling rate, and changes in the dimensions of their inputs cause
considerable accuracy loss, unnecessary computations, or failure in operation.
To address this problem, we introduce a dimension-adaptive pooling (DAP) layer
that makes DNNs flexible and more robust to changes in sampling rate and in
sensor availability. DAP operates on convolutional filter maps of variable
dimensions and produces an input of fixed dimensions suitable for feedforward
and recurrent layers. Further, we propose a dimension-adaptive training (DAT)
procedure for enabling DNNs that use DAP to better generalize over the set of
feasible data dimensions at inference time. DAT comprises the random selection
of dimensions during the forward passes and optimization with accumulated
gradients of several backward passes. Combining DAP and DAT, we show how to
transform existing non-adaptive DNNs into a Dimension-Adaptive Neural
Architecture (DANA), while keeping the same number of parameters. Compared to
the existing approaches, DANA provides better average classification accuracy
over the range of possible data dimensions, and it does not need up-sampling or
imputation, thus reduces unnecessary computations at inference time.
Experimental results, on four benchmark real-world datasets of human activity
recognition as well as three synthetic datasets, show that DANA prevents
significant losses in classification accuracy of the state-of-the-art DNNs and,
compared to baselines, it better captures correlated patterns in sensor data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.03533</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.03533</id><submitter>Tran Thien Dat Nguyen</submitter><version version="v1"><date>Sat, 8 Aug 2020 14:21:15 GMT</date><size>25450kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 24 Nov 2020 14:30:38 GMT</date><size>38837kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 11:45:13 GMT</date><size>19731kb</size><source_type>D</source_type></version><title>How Trustworthy are Performance Evaluations for Basic Vision Tasks?</title><authors>Hamid Rezatofighi, Tran Thien Dat Nguyen, Ba-Ngu Vo, Ba-Tuong Vo,
  Silvio Savarese, Ian Reid</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines performance evaluation criteria for basic vision tasks
involving sets of objects namely, object detection,instance-level segmentation
and multi-object tracking. The rankings of algorithms by an existing criterion
can fluctuate with different choices of parameters,e.g.Intersection over Union
(IoU) threshold, making their evaluations unreliable. More importantly, there
is no means to verify whether we can trust the evaluations of a criterion. This
work suggests a notion of trustworthiness for performance criteria, which
requires (i) robustness to parameters for reliability, (ii) contextual
meaningfulness in sanity tests, and (iii) consistency with mathematical
requirements such as the metric properties. We observe that these requirements
were overlooked by many widely-used criteria, and explore alternative criteria
using metrics for sets of shapes. We also assess all these criteria based on
the suggested requirements for trustworthiness.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.03813</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.03813</id><submitter>Xudong Wang</submitter><version version="v1"><date>Sun, 9 Aug 2020 21:13:13 GMT</date><size>17700kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 Aug 2020 00:00:23 GMT</date><size>23202kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 5 Oct 2020 08:16:51 GMT</date><size>23643kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 3 Dec 2020 06:43:35 GMT</date><size>18193kb</size><source_type>D</source_type></version><version version="v5"><date>Sun, 16 May 2021 03:11:23 GMT</date><size>40711kb</size><source_type>D</source_type></version><title>Unsupervised Feature Learning by Cross-Level Instance-Group
  Discrimination</title><authors>Xudong Wang, Ziwei Liu, Stella X. Yu</authors><categories>cs.CV cs.LG stat.ML</categories><comments>Accepted at CVPR 2021; Project page:
  http://people.eecs.berkeley.edu/~xdwang/projects/CLD/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised feature learning has made great strides with contrastive
learning based on instance discrimination and invariant mapping, as benchmarked
on curated class-balanced datasets. However, natural data could be highly
correlated and long-tail distributed. Natural between-instance similarity
conflicts with the presumed instance distinction, causing unstable training and
poor performance.
  Our idea is to discover and integrate between-instance similarity into
contrastive learning, not directly by instance grouping, but by cross-level
discrimination (CLD) between instances and local instance groups. While
invariant mapping of each instance is imposed by attraction within its
augmented views, between-instance similarity could emerge from common repulsion
against instance groups.
  Our batch-wise and cross-view comparisons also greatly improve the
positive/negative sample ratio of contrastive learning and achieve better
invariant mapping. To effect both grouping and discrimination objectives, we
impose them on features separately derived from a shared representation. In
addition, we propose normalized projection heads and unsupervised
hyper-parameter tuning for the first time.
  Our extensive experimentation demonstrates that CLD is a lean and powerful
add-on to existing methods such as NPID, MoCo, InfoMin, and BYOL on highly
correlated, long-tail, or balanced datasets. It not only achieves new
state-of-the-art on self-supervision, semi-supervision, and transfer learning
benchmarks, but also beats MoCo v2 and SimCLR on every reported performance
attained with a much larger compute. CLD effectively brings unsupervised
learning closer to natural data and real-world applications. Our code is
publicly available at: https://github.com/frank-xwang/CLD-UnsupervisedLearning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.04986</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.04986</id><submitter>Ziran Wang</submitter><version version="v1"><date>Tue, 11 Aug 2020 19:54:15 GMT</date><size>2827kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 Aug 2020 23:26:35 GMT</date><size>2827kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 05:30:37 GMT</date><size>2827kb</size><source_type>D</source_type></version><title>MOVESTAR: An Open-Source Vehicle Fuel and Emission Model based on USEPA
  MOVES</title><authors>Ziran Wang, Guoyuan Wu, and George Scora</authors><categories>eess.SP cs.SY eess.SY</categories><comments>MOVESTAR source code:
  https://github.com/ziranw/MOVESTAR-Fuel-and-Emission-Model</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we introduce an open-source model &quot;MOVESTAR&quot; to calculate the
fuel consumption and pollutant emissions of motor vehicles. This model is
developed based on U.S. Environmental Protection Agency's (EPA) Motor Vehicle
Emission Simulator (MOVES), which provides an accurate estimate of vehicle
emissions under a wide range of user-defined conditions. Originally, MOVES
requires users to specify many parameters through its software, including
vehicle types, time periods, geographical areas, pollutants, vehicle operating
characteristics, and road types. In this paper, MOVESTAR is developed as a
simplified version, which only takes the second-by-second vehicle speed data
and vehicle type as inputs. To enable easy integration of this model, its
source code is provided in various languages, including Python, MATLAB and C++.
A case study is introduced in this paper to illustrate the effectiveness of the
model in the development of advanced vehicle technology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.06317</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.06317</id><submitter>Subhamoy Maitra</submitter><version version="v1"><date>Fri, 14 Aug 2020 12:17:48 GMT</date><size>22kb</size></version><version version="v2"><date>Mon, 17 Aug 2020 06:25:37 GMT</date><size>36kb</size></version><version version="v3"><date>Mon, 21 Sep 2020 18:31:58 GMT</date><size>35kb</size></version><version version="v4"><date>Wed, 10 Mar 2021 09:29:37 GMT</date><size>107kb</size><source_type>D</source_type></version><version version="v5"><date>Sun, 16 May 2021 13:35:55 GMT</date><size>98kb</size></version><title>Exact Quantum Query Algorithms Outperforming Parity -- Beyond The
  Symmetric functions</title><authors>Chandra Sekhar Mukherjee, Subhamoy Maitra</authors><categories>quant-ph cs.CC</categories><comments>22 pages, modified the presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Exact Quantum Query model, almost all of the Boolean functions for which
non-trivial query algorithms exist are symmetric in nature. The most well known
techniques in this domain exploit parity decision trees, in which the parity of
two bits can be obtained by a single query. Thus, exact quantum query
algorithms outperforming parity decision trees are rare. In this paper we first
obtain optimal exact quantum query algorithms ($Q_{algo}(f)$) for a direct sum
based class of $\Omega \left( 2^{\frac{\sqrt{n}}{2}} \right)$ non-symmetric
functions. We construct these algorithms by analyzing the algebraic normal form
together with a novel untangling strategy. Next we obtain the generalized
parity decision tree complexity ($D_{\oplus}(f)$) analysing the Walsh Spectrum.
Finally, we show that query complexity of $Q_{algo}$ is $\lceil \frac{3n}{4}
\rceil$ whereas $D_{\oplus}(f)$ varies between $n-1$ and $\lceil \frac{3n}{4}
\rceil+1$ for different classes, underlining linear separation between the two
measures in many cases. To the best of our knowledge, this is the first family
of algorithms beyond generalized parity (and thus parity) for a large class of
non-symmetric functions. We also implement these techniques for a larger
(doubly exponential in $\frac{n}{4}$) class of Maiorana-McFarland type
functions, but could only obtain partial results using similar algorithmic
techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.07514</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.07514</id><submitter>Yunzhong Hou</submitter><version version="v1"><date>Mon, 17 Aug 2020 17:57:33 GMT</date><size>773kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 07:11:57 GMT</date><size>2241kb</size><source_type>D</source_type></version><title>Source Free Domain Adaptation with Image Translation</title><authors>Yunzhong Hou, Liang Zheng</authors><categories>cs.CV cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effort in releasing large-scale datasets may be compromised by privacy and
intellectual property considerations. A feasible alternative is to release
pre-trained models instead. While these models are strong on their original
task (source domain), their performance might degrade significantly when
deployed directly in a new environment (target domain), which might not contain
labels for training under realistic settings. Domain adaptation (DA) is a known
solution to the domain gap problem, but usually requires labeled source data.
In this paper, we study the problem of source free domain adaptation (SFDA),
whose distinctive feature is that the source domain only provides a pre-trained
model, but no source data. Being source free adds significant challenges to DA,
especially when considering that the target dataset is unlabeled. To solve the
SFDA problem, we propose an image translation approach that transfers the style
of target images to that of unseen source images. To this end, we align the
batch-wise feature statistics of generated images to that stored in batch
normalization layers of the pre-trained model. Compared with directly
classifying target images, higher accuracy is obtained with these style
transferred images using the pre-trained model. On several image classification
datasets, we show that the above-mentioned improvements are consistent and
statistically significant.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.08561</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.08561</id><submitter>Baoliang Chen</submitter><version version="v1"><date>Wed, 19 Aug 2020 17:31:23 GMT</date><size>2352kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 24 Nov 2020 09:44:37 GMT</date><size>3725kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 16:45:10 GMT</date><size>4936kb</size><source_type>D</source_type></version><title>No-reference Screen Content Image Quality Assessment with Unsupervised
  Domain Adaptation</title><authors>Baoliang Chen, Haoliang Li, Hongfei Fan and Shiqi Wang</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we quest the capability of transferring the quality of natural
scene images to the images that are not acquired by optical cameras (e.g.,
screen content images, SCIs), rooted in the widely accepted view that the human
visual system has adapted and evolved through the perception of natural
environment. Here, we develop the first unsupervised domain adaptation based no
reference quality assessment method for SCIs, leveraging rich subjective
ratings of the natural images (NIs). In general, it is a non-trivial task to
directly transfer the quality prediction model from NIs to a new type of
content (i.e., SCIs) that holds dramatically different statistical
characteristics. Inspired by the transferability of pair-wise relationship, the
proposed quality measure operates based on the philosophy of improving the
transferability and discriminability simultaneously. In particular, we
introduce three types of losses which complementarily and explicitly regularize
the feature space of ranking in a progressive manner. Regarding feature
discriminatory capability enhancement, we propose a center based loss to
rectify the classifier and improve its prediction capability not only for
source domain (NI) but also the target domain (SCI). For feature discrepancy
minimization, the maximum mean discrepancy (MMD) is imposed on the extracted
ranking features of NIs and SCIs. Furthermore, to further enhance the feature
diversity, we introduce the correlation penalization between different feature
dimensions, leading to the features with lower rank and higher diversity.
Experiments show that our method can achieve higher performance on different
source-target settings based on a light-weight convolution neural network. The
proposed method also sheds light on learning quality assessment measures for
unseen application-specific content without the cumbersome and costing
subjective evaluations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.11308</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.11308</id><submitter>Karishma Sharma</submitter><version version="v1"><date>Tue, 25 Aug 2020 23:35:01 GMT</date><size>1754kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 08:13:48 GMT</date><size>5825kb</size><source_type>D</source_type></version><title>Identifying Coordinated Accounts on Social Media through Hidden
  Influence and Group Behaviours</title><authors>Karishma Sharma, Yizhou Zhang, Emilio Ferrara, Yan Liu</authors><categories>cs.SI</categories><comments>KDD'2021 (Accepted)</comments><journal-ref>ACM SIGKDD International Conference on Knowledge Discovery &amp; Data
  Mining 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disinformation campaigns on social media, involving coordinated activities
from malicious accounts towards manipulating public opinion, have become
increasingly prevalent. Existing approaches to detect coordinated accounts
either make very strict assumptions about coordinated behaviours, or require
part of the malicious accounts in the coordinated group to be revealed in order
to detect the rest. To address these drawbacks, we propose a generative model,
AMDN-HAGE (Attentive Mixture Density Network with Hidden Account Group
Estimation) which jointly models account activities and hidden group behaviours
based on Temporal Point Processes (TPP) and Gaussian Mixture Model (GMM), to
capture inherent characteristics of coordination which is, accounts that
coordinate must strongly influence each other's activities, and collectively
appear anomalous from normal accounts. To address the challenges of optimizing
the proposed model, we provide a bilevel optimization algorithm with
theoretical guarantee on convergence. We verified the effectiveness of the
proposed method and training algorithm on real-world social network data
collected from Twitter related to coordinated campaigns from Russia's Internet
Research Agency targeting the 2016 U.S. Presidential Elections, and to identify
coordinated campaigns related to the COVID-19 pandemic. Leveraging the learned
model, we find that the average influence between coordinated account pairs is
the highest.On COVID-19, we found coordinated group spreading anti-vaccination,
anti-masks conspiracies that suggest the pandemic is a hoax and political scam.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.13578</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.13578</id><submitter>Yijue Wang</submitter><version version="v1"><date>Fri, 28 Aug 2020 02:15:44 GMT</date><size>2338kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 3 May 2021 05:48:54 GMT</date><size>1542kb</size><source_type>D</source_type></version><title>Against Membership Inference Attack: Pruning is All You Need</title><authors>Yijue Wang, Chenghong Wang, Zigeng Wang, Shanglin Zhou, Hang Liu,
  Jinbo Bi, Caiwen Ding, Sanguthevar Rajasekaran</authors><categories>cs.LG stat.ML</categories><comments>Machine Learning (cs.LG); Cryptography and Security (cs.CR); Machine
  Learning (stat.ML)</comments><journal-ref>IJCAI, 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large model size, high computational operations, and vulnerability
against membership inference attack (MIA) have impeded Deep learning or deep
neural networks (DNNs) popularity, especially on mobile devices. To address the
challenge, we envision that the weight pruning technique will help DNNs against
MIA while reducing model storage and computational operation. In this work, we
propose a pruning algorithm, and we show that the proposed algorithm can find a
subnetwork that can prevent privacy leakage from MIA and achieves competitive
accuracy with the original DNNs. We also verify our theoretical insights with
experiments. Our experimental results illustrate that the attack accuracy using
model compression is up to 13.6\% and 10\% lower than that of the baseline and
Min-Max game, accordingly.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2008.13610</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2008.13610</id><submitter>Carlo A. Furia</submitter><version version="v1"><date>Mon, 31 Aug 2020 13:58:18 GMT</date><size>58kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 17 Dec 2020 10:03:25 GMT</date><size>56kb</size></version><version version="v3"><date>Tue, 18 May 2021 07:52:53 GMT</date><size>58kb</size></version><title>VerifyThis 2019: A Program Verification Competition (Extended Report)</title><authors>Claire Dross, Carlo A. Furia, Marieke Huisman, Rosemary Monahan, Peter
  M\&quot;uller</authors><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VerifyThis is a series of program verification competitions that emphasize
the human aspect: participants tackle the verification of detailed behavioral
properties -- something that lies beyond the capabilities of fully automatic
verification, and requires instead human expertise to suitably encode programs,
specifications, and invariants. This paper describes the 8th edition of
VerifyThis, which took place at ETAPS 2019 in Prague. Thirteen teams entered
the competition, which consisted of three verification challenges and spanned
two days of work. The report analyzes how the participating teams fared on
these challenges, reflects on what makes a verification challenge more or less
suitable for the typical VerifyThis participants, and outlines the difficulties
of comparing the work of teams using wildly different verification approaches
in a competition focused on the human aspect.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.01411</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.01411</id><submitter>Bowen Jing</submitter><version version="v1"><date>Thu, 3 Sep 2020 01:54:25 GMT</date><size>803kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 31 Dec 2020 15:30:18 GMT</date><size>727kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 02:35:25 GMT</date><size>607kb</size><source_type>D</source_type></version><title>Learning from Protein Structure with Geometric Vector Perceptrons</title><authors>Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael J.L. Townshend,
  Ron Dror</authors><categories>q-bio.BM cs.LG stat.ML</categories><comments>Presented at ICLR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning on 3D structures of large biomolecules is emerging as a distinct
area in machine learning, but there has yet to emerge a unifying network
architecture that simultaneously leverages the graph-structured and geometric
aspects of the problem domain. To address this gap, we introduce geometric
vector perceptrons, which extend standard dense layers to operate on
collections of Euclidean vectors. Graph neural networks equipped with such
layers are able to perform both geometric and relational reasoning on efficient
and natural representations of macromolecular structure. We demonstrate our
approach on two important problems in learning from protein structure: model
quality assessment and computational protein design. Our approach improves over
existing classes of architectures, including state-of-the-art graph-based and
voxel-based methods. We release our code at https://github.com/drorlab/gvp.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.01560</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.01560</id><submitter>Cong Sun</submitter><version version="v1"><date>Thu, 3 Sep 2020 10:10:20 GMT</date><size>644kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 07:48:41 GMT</date><size>1161kb</size><source_type>D</source_type></version><title>Biomedical named entity recognition using BERT in the machine reading
  comprehension framework</title><authors>Cong Sun, Zhihao Yang, Lei Wang, Yin Zhang, Hongfei Lin, Jian Wang</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of biomedical entities from literature is a challenging research
focus, which is the foundation for extracting a large amount of biomedical
knowledge existing in unstructured texts into structured formats. Using the
sequence labeling framework to implement biomedical named entity recognition
(BioNER) is currently a conventional method. This method, however, often cannot
take full advantage of the semantic information in the dataset, and the
performance is not always satisfactory. In this work, instead of treating the
BioNER task as a sequence labeling problem, we formulate it as a machine
reading comprehension (MRC) problem. This formulation can introduce more prior
knowledge utilizing well-designed queries, and no longer need decoding
processes such as conditional random fields (CRF). We conduct experiments on
six BioNER datasets, and the experimental results demonstrate the effectiveness
of our method. Our method achieves state-of-the-art (SOTA) performance on the
BC4CHEMD, BC5CDR-Chem, BC5CDR-Disease, NCBI-Disease, BC2GM and JNLPBA datasets,
achieving F1-scores of 92.92%, 94.19%, 87.83%, 90.04%, 85.48% and 78.93%,
respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.02731</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.02731</id><submitter>Nghi D. Q. Bui</submitter><version version="v1"><date>Sun, 6 Sep 2020 13:31:16 GMT</date><size>1365kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 9 Oct 2020 05:38:39 GMT</date><size>1364kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 16 Jan 2021 12:12:51 GMT</date><size>2101kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 19 Jan 2021 11:46:56 GMT</date><size>2100kb</size><source_type>D</source_type></version><version version="v5"><date>Wed, 20 Jan 2021 09:49:11 GMT</date><size>2101kb</size><source_type>D</source_type></version><version version="v6"><date>Sun, 2 May 2021 19:25:45 GMT</date><size>2149kb</size><source_type>D</source_type></version><version version="v7"><date>Mon, 17 May 2021 18:01:27 GMT</date><size>2165kb</size><source_type>D</source_type></version><title>Self-Supervised Contrastive Learning for Code Retrieval and
  Summarization via Semantic-Preserving Transformations</title><authors>Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang</authors><categories>cs.SE cs.AI cs.LG cs.PL</categories><comments>Accepted at SIGRIR'21</comments><doi>10.1145/3404835.3462840</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Corder, a self-supervised contrastive learning framework for
source code model. Corder is designed to alleviate the need of labeled data for
code retrieval and code summarization tasks. The pre-trained model of Corder
can be used in two ways: (1) it can produce vector representation of code which
can be applied to code retrieval tasks that do not have labeled data; (2) it
can be used in a fine-tuning process for tasks that might still require label
data such as code summarization. The key innovation is that we train the source
code model by asking it to recognize similar and dissimilar code snippets
through a contrastive learning objective. To do so, we use a set of
semantic-preserving transformation operators to generate code snippets that are
syntactically diverse but semantically equivalent. Through extensive
experiments, we have shown that the code models pretrained by Corder
substantially outperform the other baselines for code-to-code retrieval,
text-to-code retrieval, and code-to-text summarization tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.04534</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.04534</id><submitter>Swetha Mandava</submitter><version version="v1"><date>Wed, 9 Sep 2020 19:39:15 GMT</date><size>138kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 17 Dec 2020 07:41:58 GMT</date><size>213kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 04:03:34 GMT</date><size>290kb</size><source_type>D</source_type></version><title>Pay Attention when Required</title><authors>Swetha Mandava, Szymon Migacz, Alex Fit Florea</authors><categories>cs.LG cs.CL</categories><comments>9 pages, 5 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformer-based models consist of interleaved feed-forward blocks - that
capture content meaning, and relatively more expensive self-attention blocks -
that capture context meaning. In this paper, we explored trade-offs and
ordering of the blocks to improve upon the current Transformer architecture and
proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL
achieved by replacing ~63% of the self-attention blocks with feed-forward
blocks, and retains the perplexity on WikiText-103 language modelling
benchmark. We further validated our results on text8 and enwiki8 datasets, as
well as on the BERT model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.05104</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.05104</id><submitter>Henry Charlesworth</submitter><version version="v1"><date>Wed, 9 Sep 2020 13:49:52 GMT</date><size>1094kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 19:32:26 GMT</date><size>1186kb</size><source_type>D</source_type></version><title>Solving Challenging Dexterous Manipulation Tasks With Trajectory
  Optimisation and Reinforcement Learning</title><authors>Henry Charlesworth and Giovanni Montana</authors><categories>cs.RO cs.AI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training agents to autonomously learn how to use anthropomorphic robotic
hands has the potential to lead to systems capable of performing a multitude of
complex manipulation tasks in unstructured and uncertain environments. In this
work, we first introduce a suite of challenging simulated manipulation tasks
that current reinforcement learning and trajectory optimisation techniques find
difficult. These include environments where two simulated hands have to pass or
throw objects between each other, as well as an environment where the agent
must learn to spin a long pen between its fingers. We then introduce a simple
trajectory optimisation that performs significantly better than existing
methods on these environments. Finally, on the challenging PenSpin task we
combine sub-optimal demonstrations generated through trajectory optimisation
with off-policy reinforcement learning, obtaining performance that far exceeds
either of these approaches individually, effectively solving the environment.
Videos of all of our results are available at:
https://dexterous-manipulation.github.io/
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.07724</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.07724</id><submitter>Colorado J Reed</submitter><version version="v1"><date>Wed, 16 Sep 2020 14:49:03 GMT</date><size>218kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 24 Nov 2020 17:53:51 GMT</date><size>802kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 15:11:06 GMT</date><size>854kb</size><source_type>D</source_type></version><title>SelfAugment: Automatic Augmentation Policies for Self-Supervised
  Learning</title><authors>Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, Kurt
  Keutzer</authors><categories>cs.CV cs.LG</categories><comments>Computer Vision and Pattern Recognition (CVPR), 2021</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  A common practice in unsupervised representation learning is to use labeled
data to evaluate the quality of the learned representations. This supervised
evaluation is then used to guide critical aspects of the training process such
as selecting the data augmentation policy. However, guiding an unsupervised
training process through supervised evaluations is not possible for real-world
data that does not actually contain labels (which may be the case, for example,
in privacy sensitive fields such as medical imaging). Therefore, in this work
we show that evaluating the learned representations with a self-supervised
image rotation task is highly correlated with a standard set of supervised
evaluations (rank correlation $&gt; 0.94$). We establish this correlation across
hundreds of augmentation policies, training settings, and network architectures
and provide an algorithm (SelfAugment) to automatically and efficiently select
augmentation policies without using supervised evaluations. Despite not using
any labeled data, the learned augmentation policies perform comparably with
augmentation policies that were determined using exhaustive supervised
evaluations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.07799</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.07799</id><submitter>Qianxiao Li</submitter><version version="v1"><date>Wed, 16 Sep 2020 16:48:28 GMT</date><size>122kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 03:36:21 GMT</date><size>5714kb</size><source_type>D</source_type></version><title>On the Curse of Memory in Recurrent Neural Networks: Approximation and
  Optimization Analysis</title><authors>Zhong Li, Jiequn Han, Weinan E, Qianxiao Li</authors><categories>cs.LG math.OC stat.ML</categories><comments>Published version</comments><msc-class>68W25, 68T07, 37M10</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximation properties and optimization dynamics of recurrent
neural networks (RNNs) when applied to learn input-output relationships in
temporal data. We consider the simple but representative setting of using
continuous-time linear RNNs to learn from data generated by linear
relationships. Mathematically, the latter can be understood as a sequence of
linear functionals. We prove a universal approximation theorem of such linear
functionals, and characterize the approximation rate and its relation with
memory. Moreover, we perform a fine-grained dynamical analysis of training
linear RNNs, which further reveal the intricate interactions between memory and
learning. A unifying theme uncovered is the non-trivial effect of memory, a
notion that can be made precise in our framework, on approximation and
optimization: when there is long term memory in the target, it takes a large
number of neurons to approximate it. Moreover, the training process will suffer
from slow downs. In particular, both of these effects become exponentially more
pronounced with memory - a phenomenon we call the &quot;curse of memory&quot;. These
analyses represent a basic step towards a concrete mathematical understanding
of new phenomenon that may arise in learning temporal relationships using
recurrent architectures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08319</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08319</id><submitter>Michael Laskin</submitter><version version="v1"><date>Mon, 14 Sep 2020 19:11:13 GMT</date><size>1548kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 30 Sep 2020 16:35:40 GMT</date><size>1548kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 20:44:18 GMT</date><size>1764kb</size><source_type>D</source_type></version><title>Decoupling Representation Learning from Reinforcement Learning</title><authors>Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin</authors><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Improved related works and fixed code hyperlink</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an effort to overcome limitations of reward-driven feature learning in
deep reinforcement learning (RL) from images, we propose decoupling
representation learning from policy learning. To this end, we introduce a new
unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC),
which trains a convolutional encoder to associate pairs of observations
separated by a short time difference, under image augmentations and using a
contrastive loss. In online RL experiments, we show that training the encoder
exclusively using ATC matches or outperforms end-to-end RL in most
environments. Additionally, we benchmark several leading UL algorithms by
pre-training encoders on expert demonstrations and using them, with weights
frozen, in RL agents; we find that agents using ATC-trained encoders outperform
all others. We also train multi-task encoders on data from multiple
environments and show generalization to different downstream RL tasks. Finally,
we ablate components of ATC, and introduce a new data augmentation to enable
replay of (compressed) latent images from pre-trained encoders when RL requires
augmentation. Our experiments span visually diverse RL benchmarks in DeepMind
Control, DeepMind Lab, and Atari, and our complete code is available at
https://github.com/astooke/rlpyt/tree/master/rlpyt/ul.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.08697</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.08697</id><submitter>Shangwei Guo</submitter><version version="v1"><date>Fri, 18 Sep 2020 09:14:54 GMT</date><size>6049kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 06:23:29 GMT</date><size>398kb</size><source_type>D</source_type></version><title>Fine-tuning Is Not Enough: A Simple yet Effective Watermark Removal
  Attack for DNN Models</title><authors>Shangwei Guo, Tianwei Zhang, Han Qiu, Yi Zeng, Tao Xiang, and Yang Liu</authors><categories>cs.CR cs.LG stat.ML</categories><comments>7 pages, 4 figures, accpeted by IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Watermarking has become the tendency in protecting the intellectual property
of DNN models. Recent works, from the adversary's perspective, attempted to
subvert watermarking mechanisms by designing watermark removal attacks.
However, these attacks mainly adopted sophisticated fine-tuning techniques,
which have certain fatal drawbacks or unrealistic assumptions. In this paper,
we propose a novel watermark removal attack from a different perspective.
Instead of just fine-tuning the watermarked models, we design a simple yet
powerful transformation algorithm by combining imperceptible pattern embedding
and spatial-level transformations, which can effectively and blindly destroy
the memorization of watermarked models to the watermark samples. We also
introduce a lightweight fine-tuning strategy to preserve the model performance.
Our solution requires much less resource or knowledge about the watermarking
scheme than prior works. Extensive experimental results indicate that our
attack can bypass state-of-the-art watermarking solutions with very high
success rates. Based on our attack, we propose watermark augmentation
techniques to enhance the robustness of existing watermarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.09680</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.09680</id><submitter>Haoyu Song</submitter><version version="v1"><date>Mon, 21 Sep 2020 08:38:23 GMT</date><size>370kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 30 Sep 2020 23:36:07 GMT</date><size>370kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 9 Nov 2020 05:55:46 GMT</date><size>370kb</size><source_type>D</source_type></version><version version="v4"><date>Wed, 31 Mar 2021 07:09:47 GMT</date><size>527kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 17 May 2021 03:13:50 GMT</date><size>358kb</size><source_type>D</source_type></version><title>Profile Consistency Identification for Open-domain Dialogue Agents</title><authors>Haoyu Song, Yan Wang, Wei-Nan Zhang, Zhengyu Zhao, Ting Liu, Xiaojiang
  Liu</authors><categories>cs.CL</categories><comments>EMNLP20</comments><doi>10.18653/v1/2020.emnlp-main.539</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maintaining a consistent attribute profile is crucial for dialogue agents to
naturally converse with humans. Existing studies on improving attribute
consistency mainly explored how to incorporate attribute information in the
responses, but few efforts have been made to identify the consistency relations
between response and attribute profile. To facilitate the study of profile
consistency identification, we create a large-scale human-annotated dataset
with over 110K single-turn conversations and their key-value attribute
profiles. Explicit relation between response and profile is manually labeled.
We also propose a key-value structure information enriched BERT model to
identify the profile consistency, and it gained improvements over strong
baselines. Further evaluations on downstream tasks demonstrate that the profile
consistency identification model is conducive for improving dialogue
consistency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.11037</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.11037</id><submitter>Kentaro Wada</submitter><version version="v1"><date>Wed, 23 Sep 2020 10:15:16 GMT</date><size>3705kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 24 Sep 2020 07:31:37 GMT</date><size>3724kb</size><source_type>D</source_type></version><title>A new look at departure time choice equilibrium models with
  heterogeneous users</title><authors>Takashi Akamatsu, Kentaro Wada, Takamasa Iryo, Shunsuke Hayashi</authors><categories>math.OC cs.GT</categories><comments>42 pages, 10 figures</comments><doi>10.1016/j.trb.2021.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a systematic approach for analyzing the departure-time
choice equilibrium (DTCE) problem of a single bottleneck with heterogeneous
commuters. The approach is based on the fact that the DTCE is equivalently
represented as a linear programming problem with a special structure, which can
be analytically solved by exploiting the theory of optimal transport combined
with a decomposition technique. By applying the proposed approach to several
types of models with heterogeneous commuters, it is shown that (i) the
essential condition for emerging equilibrium &quot;sorting patterns,&quot; which have
been known in the literature, is that the schedule delay functions have the
&quot;Monge property,&quot; (ii) the equilibrium problems with the Monge property can be
solved analytically, and (iii) the proposed approach can be applied to a more
general problem with more than two types of heterogeneities.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.11330</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.11330</id><submitter>Farzana Beente Yusuf</submitter><version version="v1"><date>Wed, 23 Sep 2020 18:26:48 GMT</date><size>80kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 25 Sep 2020 04:46:15 GMT</date><size>80kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 16:52:38 GMT</date><size>791kb</size><source_type>D</source_type></version><title>Cache Replacement as a MAB with Delayed Feedback and Decaying Costs</title><authors>Farzana Beente Yusuf, Vitalii Stebliankin, Giuseppe Vietri, Giri
  Narasimhan</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the cache replacement problem, we propose and solve a new variant
of the well-known multi-armed bandit (MAB), thus providing a solution for
improving existing state-of-the-art cache management methods. Each arm (or
expert) represents a distinct cache replacement policy, which advises on the
page to evict from the cache when needed. Feedback on the eviction comes in the
form of a &quot;miss&quot;, but at an indeterminate time after the action is taken, and
the cost of the eviction is set to be inversely proportional to the response
time. The feedback is ignored if it comes after a threshold value for the
delay, which we set to be equal to the size of the page eviction history. Thus,
for delays beyond the threshold, its cost is assumed to be zero. Consequently,
we call this problem with delayed feedback and decaying costs. We introduce an
adaptive reinforcement learning algorithm EXP4-DFDC that provides a solution to
the problem. We derive an optimal learning rate for EXP4-DFDC that defines the
balance between exploration and exploitation and proves theoretically that the
expected regret of our algorithm is a vanishing quantity as a function of time.
As an application, we show that LeCaR, a recent top-performing machine learning
algorithm for cache replacement, can be enhanced with adaptive learning using
our formulations. We present an improved adaptive versionofLeCaR, calledOLeCaR,
with the learning rate set as determined by the theoretical derivation
presented here to minimize regret for EXP4-DFDC. It then follows that LeCaR and
OLeCaR are theoretically guaranteed to have vanishing regret over time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.12976</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.12976</id><submitter>Ankit Pensia</submitter><version version="v1"><date>Sun, 27 Sep 2020 22:48:48 GMT</date><size>50kb</size></version><version version="v2"><date>Mon, 17 May 2021 16:40:45 GMT</date><size>421kb</size><source_type>D</source_type></version><title>Robust regression with covariate filtering: Heavy tails and adversarial
  contamination</title><authors>Ankit Pensia, Varun Jog, Po-Ling Loh</authors><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>V2: Adds new results for unknown covariance matrix (Theorem 3.13),
  Gaussian design (Remark 3.12), and Simulations (Section 7)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of linear regression where both covariates and responses
are potentially (i) heavy-tailed and (ii) adversarially contaminated. Several
computationally efficient estimators have been proposed for the simpler setting
where the covariates are sub-Gaussian and uncontaminated; however, these
estimators may fail when the covariates are either heavy-tailed or contain
outliers. In this work, we show how to modify the Huber regression, least
trimmed squares, and least absolute deviation estimators to obtain estimators
which are simultaneously computationally and statistically efficient in the
stronger contamination model. Our approach is quite simple, and consists of
applying a filtering algorithm to the covariates, and then applying the
classical robust regression estimators to the remaining data. We show that the
Huber regression estimator achieves near-optimal error rates in this setting,
whereas the least trimmed squares and least absolute deviation estimators can
be made to achieve near-optimal error after applying a postprocessing step.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.13104</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.13104</id><submitter>Debasis Mishra</submitter><version version="v1"><date>Mon, 28 Sep 2020 07:24:49 GMT</date><size>14kb</size></version><version version="v2"><date>Sun, 16 May 2021 11:12:22 GMT</date><size>14kb</size></version><title>Ordinal Bayesian incentive compatibility in random assignment model</title><authors>Sulagna Dasgupta and Debasis Mishra</authors><categories>econ.TH cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the consequences of weakening the notion of incentive
compatibility from strategy-proofness to ordinal Bayesian incentive
compatibility (OBIC) in the random assignment model. If the common prior of the
agents is a uniform prior, then a large class of random mechanisms are OBIC
with respect to this prior -- this includes the probabilistic serial mechanism.
We then introduce a robust version of OBIC: a mechanism is locally robust OBIC
if it is OBIC with respect all independent priors in some neighborhood of a
given independent prior. We show that every locally robust OBIC mechanism
satisfying a mild property called elementary monotonicity is strategy-proof.
This leads to a strengthening of the impossibility result in Bogomolnaia and
Moulin (2001): if there are at least four agents, there is no locally robust
OBIC and ordinally efficient mechanism satisfying equal treatment of equals.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.13290</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.13290</id><submitter>Hang Du</submitter><version version="v1"><date>Mon, 28 Sep 2020 13:02:17 GMT</date><size>12045kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 08:54:13 GMT</date><size>11906kb</size><source_type>D</source_type></version><title>The Elements of End-to-end Deep Face Recognition: A Survey of Recent
  Advances</title><authors>Hang Du, Hailin Shi, Dan Zeng, Xiaoping Zhang, and Tao Mei</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition is one of the most fundamental and long-standing topics in
computer vision community. With the recent developments of deep convolutional
neural networks and large-scale datasets, deep face recognition has made
remarkable progress and been widely used in the real-world applications. Given
a natural image or video frame as input, an end-to-end deep face recognition
system outputs the face feature for recognition. To achieve this, the whole
system is generally built with three key elements: face detection, face
alignment, and face representation. The face detection locates faces in the
image or frame. Then, the face alignment is proceeded to calibrate the faces to
a canonical view and crop them to a normalized pixel size. Finally, in the
stage of face representation, the discriminative features are extracted from
the preprocessed faces for recognition. All of the three elements are fulfilled
by deep convolutional neural networks. In this paper, we present a
comprehensive survey about the recent advances of every element of the
end-to-end deep face recognition, since the thriving deep learning techniques
have greatly improved the capability of them. To start with, we introduce an
overview of the end-to-end deep face recognition, which, as mentioned above,
includes face detection, face alignment, and face representation. Then, we
review the deep learning based advances of each element, respectively, covering
many aspects such as the up-to-date algorithm designs, evaluation metrics,
datasets, performance comparison, existing challenges, and promising directions
for future research. We hope this survey could bring helpful thoughts to one
for better understanding of the big picture of end-to-end face recognition and
deeper exploration in a systematic way.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.13605</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.13605</id><submitter>Manuel Beyeler</submitter><version version="v1"><date>Mon, 28 Sep 2020 20:13:27 GMT</date><size>50kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 14:46:50 GMT</date><size>119kb</size><source_type>D</source_type></version><title>iMLCA: Machine Learning-powered Iterative Combinatorial Auctions with
  Interval Bidding</title><authors>Manuel Beyeler, Gianluca Brero, Benjamin Lubin, Sven Seuken</authors><categories>cs.GT</categories><comments>22 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of iterative combinatorial auctions for domains with a
large number of items. In such domains, preference elicitation is a major
challenge because the bundle space grows exponentially in the number of items.
To keep preference elicitation manageable, recent work has employed machine
learning (ML) algorithms that identify a small set of bundles to query from
each bidder. However, a major limitation of this prior work is that bidders
must submit exact values for the queried bundles, which can be quite costly for
them. To address this, we propose iMLCA, a new ML-powered auction with interval
bidding (i.e., where bidders submit upper and lower bounds for the queried
bundles). To steer the auction towards an efficient allocation, we introduce a
new price-based activity rule, asking bidders to tighten bounds on relevant
bundles only. The activity rule is designed such that the auctioneer receives
enough information about bidders' preferences to achieve high efficiency and
good incentives, while minimizing elicitation costs. Our experiments show that
iMLCA, despite only eliciting interval bids, achieves almost the same
allocative efficiency as the prior auction design that required bidders to
submit exact values. Finally, we show that iMLCA beats the well-known
combinatorial clock auction in a realistically-sized domain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.13732</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.13732</id><submitter>Alex LaGrassa</submitter><version version="v1"><date>Tue, 29 Sep 2020 02:26:54 GMT</date><size>6790kb</size><source_type>D</source_type></version><title>Learning Skills to Patch Plans Based on Inaccurate Models</title><authors>Alex LaGrassa, Steven Lee, Oliver Kroemer</authors><categories>cs.RO</categories><comments>8 pages, 10 figures, accepted to Intelligent Robots and Systems
  (IROS) 2020</comments><journal-ref>International Conference on Intelligent Robots and Systems (IROS).
  9441-9448. Sept. 2020</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planners using accurate models can be effective for accomplishing
manipulation tasks in the real world, but are typically highly specialized and
require significant fine-tuning to be reliable. Meanwhile, learning is useful
for adaptation, but can require a substantial amount of data collection. In
this paper, we propose a method that improves the efficiency of sub-optimal
planners with approximate but simple and fast models by switching to a
model-free policy when unexpected transitions are observed. Unlike previous
work, our method specifically addresses when the planner fails due to
transition model error by patching with a local policy only where needed.
First, we use a sub-optimal model-based planner to perform a task until model
failure is detected. Next, we learn a local model-free policy from expert
demonstrations to complete the task in regions where the model failed. To show
the efficacy of our method, we perform experiments with a shape insertion
puzzle and compare our results to both pure planning and imitation learning
approaches. We then apply our method to a door opening task. Our experiments
demonstrate that our patch-enhanced planner performs more reliably than pure
planning and with lower overall sample complexity than pure imitation learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.14094</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.14094</id><submitter>Daniel Schuster</submitter><version version="v1"><date>Tue, 29 Sep 2020 15:24:42 GMT</date><size>1897kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 5 Oct 2020 15:40:22 GMT</date><size>1097kb</size><source_type>D</source_type></version><title>Alignment Approximation for Process Trees</title><authors>Daniel Schuster and Sebastiaan van Zelst and Wil M. P. van der Aalst</authors><categories>cs.DB</categories><doi>10.1007/978-3-030-72693-5_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparing observed behavior (event data generated during process executions)
with modeled behavior (process models), is an essential step in process mining
analyses. Alignments are the de-facto standard technique for calculating
conformance checking statistics. However, the calculation of alignments is
computationally complex since a shortest path problem must be solved on a state
space which grows non-linearly with the size of the model and the observed
behavior, leading to the well-known state space explosion problem. In this
paper, we present a novel framework to approximate alignments on process trees
by exploiting their hierarchical structure. Process trees are an important
process model formalism used by state-of-the-art process mining techniques such
as the inductive mining approaches. Our approach exploits structural properties
of a given process tree and splits the alignment computation problem into
smaller sub-problems. Finally, sub-results are composed to obtain an alignment.
Our experiments show that our approach provides a good balance between accuracy
and computation time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2009.14332</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2009.14332</id><submitter>Guangtao Wang</submitter><version version="v1"><date>Tue, 29 Sep 2020 22:41:19 GMT</date><size>865kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 1 Oct 2020 00:45:07 GMT</date><size>865kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 2 Oct 2020 04:51:02 GMT</date><size>865kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 15 May 2021 18:31:04 GMT</date><size>1055kb</size><source_type>D</source_type></version><title>Multi-hop Attention based Graph Neural Network</title><authors>Guangtao Wang, Rex Ying, Jing Huang, Jure Leskovec</authors><categories>cs.LG stat.ML</categories><comments>Accepted by IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-attention mechanism in graph neural networks (GNNs) led to
state-of-the-art performance on many graph representation learning tasks.
Currently, at every layer, attention is computed between connected pairs of
nodes and depends solely on the representation of the two nodes. However, such
attention mechanism does not account for nodes that are not directly connected
but provide important network context. Here we propose Multi-hop Attention
Graph Neural Network (MAGNA), a principled way to incorporate multi-hop context
information into every layer of attention computation. MAGNA diffuses the
attention scores across the network, which increases the receptive field for
every layer of the GNN. Unlike previous approaches, MAGNA uses a diffusion
prior on attention values, to efficiently account for all paths between the
pair of disconnected nodes. We demonstrate in theory and experiments that MAGNA
captures large-scale structural information in every layer, and has a low-pass
effect that eliminates noisy high-frequency information from graph data.
Experimental results on node classification as well as the knowledge graph
completion benchmarks show that MAGNA achieves state-of-the-art results: MAGNA
achieves up to 5.7 percent relative error reduction over the previous
state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains the best
performance on a large-scale Open Graph Benchmark dataset. On knowledge graph
completion MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four
different performance metrics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.01809</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.01809</id><submitter>Xudong Wang</submitter><version version="v1"><date>Mon, 5 Oct 2020 06:53:44 GMT</date><size>2369kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 7 Apr 2021 06:37:20 GMT</date><size>1944kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 23:05:59 GMT</date><size>1814kb</size><source_type>D</source_type></version><title>Long-tailed Recognition by Routing Diverse Distribution-Aware Experts</title><authors>Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella X. Yu</authors><categories>cs.CV</categories><comments>Accepted at ICLR 2021 (Spotlight); Update the bias-variance
  decomposition section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural data are often long-tail distributed over semantic classes. Existing
recognition methods tackle this imbalanced classification by placing more
emphasis on the tail data, through class re-balancing/re-weighting or
ensembling over different data groups, resulting in increased tail accuracies
but reduced head accuracies.
  We take a dynamic view of the training data and provide a principled model
bias and variance analysis as the training data fluctuates: Existing long-tail
classifiers invariably increase the model variance and the head-tail model bias
gap remains large, due to more and larger confusion with hard negatives for the
tail.
  We propose a new long-tailed classifier called RoutIng Diverse Experts
(RIDE). It reduces the model variance with multiple experts, reduces the model
bias with a distribution-aware diversity loss, reduces the computational cost
with a dynamic expert routing module. RIDE outperforms the state-of-the-art by
5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is
also a universal framework that is applicable to various backbone networks,
long-tailed algorithms, and training mechanisms for consistent performance
gains. Our code is available at:
https://github.com/frank-xwang/RIDE-LongTailRecognition.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.02392</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.02392</id><submitter>Karl Willis</submitter><version version="v1"><date>Mon, 5 Oct 2020 23:18:21 GMT</date><size>5142kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 03:58:04 GMT</date><size>18106kb</size><source_type>D</source_type></version><title>Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD
  Construction from Human Design Sequences</title><authors>Karl D.D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G.
  Lambourne, Armando Solar-Lezama, Wojciech Matusik</authors><categories>cs.LG cs.CV cs.GR</categories><comments>Accepted to SIGGRAPH 2021; data/code available at
  https://github.com/AutodeskAILab/Fusion360GalleryDataset</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametric computer-aided design (CAD) is a standard paradigm used to design
manufactured objects, where a 3D shape is represented as a program supported by
the CAD software. Despite the pervasiveness of parametric CAD and a growing
interest from the research community, currently there does not exist a dataset
of realistic CAD models in a concise programmatic form. In this paper we
present the Fusion 360 Gallery, consisting of a simple language with just the
sketch and extrude modeling operations, and a dataset of 8,625 human design
sequences expressed in this language. We also present an interactive
environment called the Fusion 360 Gym, which exposes the sequential
construction of a CAD program as a Markov decision process, making it amendable
to machine learning approaches. As a use case for our dataset and environment,
we define the CAD reconstruction task of recovering a CAD program from a target
geometry. We report results of applying state-of-the-art methods of program
synthesis with neurally guided search on this task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.03649</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.03649</id><submitter>Daniel Seidl</submitter><version version="v1"><date>Wed, 7 Oct 2020 20:57:15 GMT</date><size>3812kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Dec 2020 21:07:28 GMT</date><size>3165kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 16:05:21 GMT</date><size>7170kb</size><source_type>D</source_type></version><title>Calibration of Elastoplastic Constitutive Model Parameters from
  Full-field Data with Automatic Differentiation-based Sensitivities</title><authors>Daniel Thomas Seidl and Brian Neal Granzow</authors><categories>cs.CE</categories><report-no>SAND2020-10986 O</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for calibration of parameters in elastoplastic
constitutive models that is based on the use of automatic differentiation (AD).
The model calibration problem is posed as a partial differential
equation-constrained optimization problem where a finite element (FE) model of
the coupled equilibrium equation and constitutive model evolution equations
serves as the constraint. The objective function quantifies the mismatch
between the displacement predicted by the FE model and full-field digital image
correlation data, and the optimization problem is solved using gradient-based
optimization algorithms. Forward and adjoint sensitivities are used to compute
the gradient at considerably less cost than its calculation from finite
difference approximations. Through the use of AD, we need only to write the
constraints in terms of AD objects, where all of the derivatives required for
the forward and inverse problems are obtained by appropriately seeding and
evaluating these quantities. We present three numerical examples that verify
the correctness of the gradient, demonstrate the AD approach's parallel
computation capabilities via application to a large-scale FE model, and
highlight the formulation's ease of extensibility to other classes of
constitutive models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.04700</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.04700</id><submitter>Jamie Alnasir PhD</submitter><version version="v1"><date>Fri, 9 Oct 2020 17:40:11 GMT</date><size>151kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 3 Nov 2020 18:37:15 GMT</date><size>152kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 15:10:47 GMT</date><size>127kb</size></version><title>Distributed Computing in a Pandemic: A Review of Technologies Available
  for Tackling COVID-19</title><authors>Jamie J Alnasir</authors><categories>cs.DC</categories><comments>20 pages (14 excl. refs), 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The current COVID-19 global pandemic caused by the SARS-CoV-2 betacoronavirus
has resulted in over a million deaths and is having a grave socio-economic
impact, hence there is an urgency to find solutions to key research challenges.
Much of this COVID-19 research depends on distributed computing. In this
article, I review distributed architectures -- various types of clusters, grids
and clouds -- that can be leveraged to perform these tasks at scale, at
high-throughput, with a high degree of parallelism, and which can also be used
to work collaboratively. High-performance computing (HPC) clusters will be used
to carry out much of this work. Several bigdata processing tasks used in
reducing the spread of SARS-CoV-2 require high-throughput approaches, and a
variety of tools, which Hadoop and Spark offer, even using commodity hardware.
Extremely large-scale COVID-19 research has also utilised some of the world's
fastest supercomputers, such as IBM's SUMMIT -- for ensemble docking
high-throughput screening against SARS-CoV-2 targets for drug-repurposing, and
high-throughput gene analysis -- and Sentinel, an XPE-Cray based system used to
explore natural products. Grid computing has facilitated the formation of the
world's first Exascale grid computer. This has accelerated COVID-19 research in
molecular dynamics simulations of SARS-CoV-2 spike protein interactions through
massively-parallel computation and was performed with over 1 million volunteer
computing devices using the Folding@home platform. Grids and clouds both can
also be used for international collaboration by enabling access to important
datasets and providing services that allow researchers to focus on research
rather than on time-consuming data-management tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.05006</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.05006</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Sat, 10 Oct 2020 14:03:20 GMT</date><size>248kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Dec 2020 06:30:35 GMT</date><size>250kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 11:15:40 GMT</date><size>156kb</size><source_type>D</source_type></version><title>Automated Concatenation of Embeddings for Structured Prediction</title><authors>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
  Huang, Kewei Tu</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to Proceedings of ACL-IJCNLP 2021. Submission Version. 17
  pages</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Pretrained contextualized embeddings are powerful word representations for
structured prediction tasks. Recent work found that better word representations
can be obtained by concatenating different types of embeddings. However, the
selection of embeddings to form the best concatenated representation usually
varies depending on the task and the collection of candidate embeddings, and
the ever-increasing number of embedding types makes it a more difficult
problem. In this paper, we propose Automated Concatenation of Embeddings (ACE)
to automate the process of finding better concatenations of embeddings for
structured prediction tasks, based on a formulation inspired by recent progress
on neural architecture search. Specifically, a controller alternately samples a
concatenation of embeddings, according to its current belief of the
effectiveness of individual embedding types in consideration for a task, and
updates the belief based on a reward. We follow strategies in reinforcement
learning to optimize the parameters of the controller and compute the reward
based on the accuracy of a task model, which is fed with the sampled
concatenation as input and trained on a task dataset. Empirical results on 6
tasks and 21 datasets show that our approach outperforms strong baselines and
achieves state-of-the-art performance with fine-tuned embeddings in all the
evaluations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.05010</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.05010</id><submitter>Xinyu Wang</submitter><version version="v1"><date>Sat, 10 Oct 2020 14:19:25 GMT</date><size>244kb</size></version><version version="v2"><date>Tue, 18 May 2021 12:07:25 GMT</date><size>60kb</size></version><title>Structural Knowledge Distillation: Tractably Distilling Information for
  Structured Predictor</title><authors>Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia, Nguyen Bach, Tao Wang,
  Zhongqiang Huang, Fei Huang, Kewei Tu</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to Proceedings of ACL-IJCNLP 2021. Submission version. 15
  pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Knowledge distillation is a critical technique to transfer knowledge between
models, typically from a large model (the teacher) to a more fine-grained one
(the student). The objective function of knowledge distillation is typically
the cross-entropy between the teacher and the student's output distributions.
However, for structured prediction problems, the output space is exponential in
size; therefore, the cross-entropy objective becomes intractable to compute and
optimize directly. In this paper, we derive a factorized form of the knowledge
distillation objective for structured prediction, which is tractable for many
typical choices of the teacher and student models. In particular, we show the
tractability and empirical effectiveness of structural knowledge distillation
between sequence labeling and dependency parsing models under four different
scenarios: 1) the teacher and student share the same factorization form of the
output structure scoring function; 2) the student factorization produces more
fine-grained substructures than the teacher factorization; 3) the teacher
factorization produces more fine-grained substructures than the student
factorization; 4) the factorization forms from the teacher and the student are
incompatible.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.05352</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.05352</id><submitter>Pranav Rajpurkar</submitter><version version="v1"><date>Sun, 11 Oct 2020 21:42:10 GMT</date><size>2235kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 21 Feb 2021 02:54:06 GMT</date><size>2114kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 20:39:42 GMT</date><size>4952kb</size><source_type>D</source_type></version><title>MoCo-CXR: MoCo Pretraining Improves Representation and Transferability
  of Chest X-ray Models</title><authors>Hari Sowrirajan, Jingbo Yang, Andrew Y. Ng, Pranav Rajpurkar</authors><categories>cs.CV cs.AI cs.LG</categories><comments>Accepted at Medical Imaging with Deep Learning (MIDL) Conference 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Contrastive learning is a form of self-supervision that can leverage
unlabeled data to produce pretrained models. While contrastive learning has
demonstrated promising results on natural image classification tasks, its
application to medical imaging tasks like chest X-ray interpretation has been
limited. In this work, we propose MoCo-CXR, which is an adaptation of the
contrastive learning method Momentum Contrast (MoCo), to produce models with
better representations and initializations for the detection of pathologies in
chest X-rays. In detecting pleural effusion, we find that linear models trained
on MoCo-CXR-pretrained representations outperform those without
MoCo-CXR-pretrained representations, indicating that MoCo-CXR-pretrained
representations are of higher-quality. End-to-end fine-tuning experiments
reveal that a model initialized via MoCo-CXR-pretraining outperforms its
non-MoCo-CXR-pretrained counterpart. We find that MoCo-CXR-pretraining provides
the most benefit with limited labeled training data. Finally, we demonstrate
similar results on a target Tuberculosis dataset unseen during pretraining,
indicating that MoCo-CXR-pretraining endows models with representations and
transferability that can be applied across chest X-ray datasets and tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.06259</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.06259</id><submitter>Marios Constantinides</submitter><version version="v1"><date>Tue, 13 Oct 2020 09:48:34 GMT</date><size>523kb</size><source_type>D</source_type></version><title>MeetCues: Supporting Online Meetings Experience</title><authors>Bon Adriel Aseniero, Marios Constantinides, Sagar Joglekar, Ke Zhou,
  Daniele Quercia</authors><categories>cs.HC</categories><comments>5 pages, 2 figures, 1 table</comments><doi>10.1109/VIS47514.2020.00054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remote work ecosystem is transforming patterns of communication between
teams and individuals located at distance. Particularly, the absence of certain
subtle cues in current communication tools may hinder an online's meeting
outcome by negatively impacting attendees' overall experience and, often, make
them feeling disconnected. The problem here might be due to the fact that
current tools fall short in capturing it. To partly address this, we developed
an online platform-MeetCues-with the aim of supporting online communication
during meetings. MeetCues is a companion platform for a commercial
communication tool with interactive and visual UI features that support
back-channels of communications. It allows attendees to be more engaged during
a meeting, and reflect in real-time or post-meeting. We evaluated our platform
in a diverse set of five, real-world corporate meetings, and we found that, not
only people were more engaged and aware during their meetings, but they also
felt more connected. These findings suggest promise in the design of new
communications tools, and reinforce the role of InfoVis in augmenting and
enriching online meetings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.06270</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.06270</id><submitter>Sergey Kirgizov S.</submitter><version version="v1"><date>Tue, 13 Oct 2020 10:28:42 GMT</date><size>28kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 2 Apr 2021 17:36:27 GMT</date><size>33kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 22 Apr 2021 18:28:52 GMT</date><size>34kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 17 May 2021 20:55:37 GMT</date><size>34kb</size><source_type>D</source_type></version><title>Pattern statistics in faro words and permutations</title><authors>Jean-Luc Baril, Alexander Burstein and Sergey Kirgizov</authors><categories>math.CO cs.DM</categories><comments>25 pages, 4 figures</comments><msc-class>05A05 (Primary) 05A15, 05A19, 68R15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the distribution and the popularity of some patterns in $k$-ary faro
words, i.e. words over the alphabet $\{1, 2, \ldots, k\}$ obtained by
interlacing the letters of two nondecreasing words of lengths differing by at
most one. We present a bijection between these words and dispersed Dyck paths
(i.e. Motzkin paths with all level steps on the $x$-axis) with a given number
of peaks. We show how the bijection maps statistics of consecutive patterns of
faro words into linear combinations of other pattern statistics on paths. Then,
we deduce enumerative results by providing multivariate generating functions
for the distribution and the popularity of patterns of length at most three.
Finally, we consider some interesting subclasses of faro words that are
permutations, involutions, derangements, or subexcedent words.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.06349</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.06349</id><submitter>Zongxin Yang</submitter><version version="v1"><date>Tue, 13 Oct 2020 13:06:10 GMT</date><size>8023kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 11:21:08 GMT</date><size>8244kb</size><source_type>D</source_type></version><title>Collaborative Video Object Segmentation by Multi-Scale
  Foreground-Background Integration</title><authors>Zongxin Yang, Yunchao Wei, Yi Yang</authors><categories>cs.CV</categories><comments>Accepted by TPAMI; Journal extension of arXiv:2003.08333 (ECCV 2020,
  Spotlight)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the principles of embedding learning to tackle the
challenging semi-supervised video object segmentation. Unlike previous
practices that focus on exploring the embedding learning of foreground object
(s), we consider background should be equally treated. Thus, we propose a
Collaborative video object segmentation by Foreground-Background Integration
(CFBI) approach. CFBI separates the feature embedding into the foreground
object region and its corresponding background region, implicitly promoting
them to be more contrastive and improving the segmentation results accordingly.
Moreover, CFBI performs both pixel-level matching processes and instance-level
attention mechanisms between the reference and the predicted sequence, making
CFBI robust to various object scales. Based on CFBI, we introduce a multi-scale
matching structure and propose an Atrous Matching strategy, resulting in a more
robust and efficient framework, CFBI+. We conduct extensive experiments on two
popular benchmarks, i.e., DAVIS and YouTube-VOS. Without applying any simulated
data for pre-training, our CFBI+ achieves the performance (J&amp;F) of 82.9% and
82.8%, outperforming all the other state-of-the-art methods. Code:
https://github.com/z-x-yang/CFBI.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.06697</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.06697</id><submitter>Hao Zhou</submitter><version version="v1"><date>Fri, 9 Oct 2020 05:16:07 GMT</date><size>10934kb</size><source_type>D</source_type></version><title>Accelerated computational micromechanics</title><authors>Hao Zhou, Kaushik Bhattacharya</authors><categories>cs.CE</categories><comments>36 pages, 15 figures, Journal article, submitted to computational
  methods in applied mechanics and engineering</comments><doi>10.1016/j.jmps.2021.104470</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to solving problems in micromechanics that is amenable
to massively parallel calculations through the use of graphical processing
units and other accelerators. The problems lead to nonlinear differential
equations that are typically second order in space and first order in time.
This combination of nonlinearity and nonlocality makes such problems difficult
to solve in parallel. However, this combination is a result of collapsing
nonlocal, but linear and universal physical laws (kinematic compatibility,
balance laws), and nonlinear but local constitutive relations. We propose an
operator-splitting scheme inspired by this structure. The governing equations
are formulated as (incremental) variational problems, the differential
constraints like compatibility are introduced using an augmented Lagrangian,
and the resulting incremental variational principle is solved by the
alternating direction method of multipliers. The resulting algorithm has a
natural connection to physical principles, and also enables massively parallel
implementation on structured grids. We present this method and use it to study
two examples. The first concerns the long wavelength instability of finite
elasticity, and allows us to verify the approach against previous numerical
simulations. We also use this example to study convergence and parallel
performance. The second example concerns microstructure evolution in liquid
crystal elastomers and provides new insights into some counter-intuitive
properties of these materials. We use this example to validate the model and
the approach against experimental observations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.06847</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.06847</id><submitter>Andreas Selmar Hauptmann</submitter><version version="v1"><date>Wed, 14 Oct 2020 07:19:10 GMT</date><size>2710kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 07:09:02 GMT</date><size>4402kb</size><source_type>D</source_type></version><title>Fusing electrical and elasticity imaging</title><authors>Andreas Hauptmann and Danny Smyl</authors><categories>physics.med-ph cs.CE cs.NA eess.SP math.NA math.OC</categories><journal-ref>Philosophical Transactions of the Royal Society A, 379(2200),
  20200194 (2021)</journal-ref><doi>10.1098/rsta.2020.0194</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electrical and elasticity imaging are promising modalities for a suite of
different applications including medical tomography, non-destructive testing,
and structural health monitoring. These emerging modalities are capable of
providing remote, non-invasive, and low cost opportunities. Unfortunately, both
modalities are severely ill-posed nonlinear inverse problems, susceptive to
noise and modelling errors. Nevertheless, the ability to incorporate
complimentary data sets obtained simultaneously offers mutually-beneficial
information. By fusing electrical and elastic modalities as a joint problem we
are afforded the possibility to stabilise the inversion process via the
utilisation of auxiliary information from both modalities as well as joint
structural operators. In this study, we will discuss a possible approach to
combine electrical and elasticity imaging in a joint reconstruction problem
giving rise to novel multi-modality applications for use in both medical and
structural engineering.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.07057</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.07057</id><submitter>Alisa Pankova</submitter><version version="v1"><date>Wed, 14 Oct 2020 13:02:44 GMT</date><size>54kb</size></version><version version="v2"><date>Mon, 17 May 2021 13:28:11 GMT</date><size>66kb</size></version><title>PrivaLog: a privacy-aware logic programming language</title><authors>Joosep J\&quot;a\&quot;ager and Alisa Pankova</authors><categories>cs.CR</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic Programming (LP) is a subcategory of declarative programming that is
considered to be relatively simple for non-programmers. LP developers focus on
describing facts and rules of a logical derivation, and do not need to think
about the algorithms actually implementing the derivation.
  Secure multiparty computation (MPC) is a cryptographic technology that allows
to perform computation on private data without actually seeing the data. In
this paper, we bring together the notions of MPC and LP, allowing users to
write privacy-preserving applications in logic programming language.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.08423</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.08423</id><submitter>Suhas Thejaswi</submitter><version version="v1"><date>Fri, 16 Oct 2020 14:40:26 GMT</date><size>27670kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 22 Nov 2020 21:05:27 GMT</date><size>21323kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 11:06:14 GMT</date><size>21276kb</size><source_type>D</source_type></version><title>Restless reachability problems in temporal graphs</title><authors>Suhas Thejaswi, Juho Lauri, Aristides Gionis</authors><categories>cs.DS cs.DC</categories><comments>The paper is updated with more illustrations for improving
  readability and directed towards broader audience (non-expert group)</comments><acm-class>F.2.2; G.4; F.2.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We study a family of reachability problems under waiting-time restrictions in
temporal and vertex-colored temporal graphs. Given a temporal graph and a set
of source vertices, we find the set of vertices that are reachable from a
source via a time-respecting path, where the difference in timestamps between
consecutive edges is at most a resting time. Given a vertex-colored temporal
graph and a multiset query of colors, we find the set of vertices reachable
from a source via a time-respecting path such that the vertex colors of the
path agree with the multiset query and the difference in timestamps between
consecutive edges is at most a resting time. These kind of problems have
several applications in understanding the spread of a disease in a network,
tracing contacts in epidemic outbreaks, finding signaling pathways in the brain
network, and recommending tours for tourists.
  We present an algebraic algorithmic framework based on constrained
multilinear sieving for solving the restless reachability problems we propose.
In particular, parameterized by the length of a path $k$ sought, we show the
problems can be solved in $O(2^k k m \Delta)$ time and $O(n \tau)$ space, where
$n$ is the number of vertices, $m$ the number of edges, $\Delta$ the maximum
resting time and $\tau$ the maximum timestamp of an input temporal graph. In
addition, we prove that the algorithms presented for the restless reachability
problems in vertex-colored temporal graphs are optimal under plausible
complexity-theoretic assumptions. Finally, with an open-source implementation,
we demonstrate that our algorithm scales to large graphs with up to one billion
temporal edges, despite the problems being NP-hard. Specifically, we present
extensive experiments to evaluate our scalability claims both on synthetic and
real-world graphs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.08488</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.08488</id><submitter>Takuo Matsubara</submitter><version version="v1"><date>Fri, 16 Oct 2020 16:39:45 GMT</date><size>7632kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 09:42:34 GMT</date><size>6893kb</size><source_type>D</source_type></version><title>The Ridgelet Prior: A Covariance Function Approach to Prior
  Specification for Bayesian Neural Networks</title><authors>Takuo Matsubara and Chris J. Oates and Fran\c{c}ois-Xavier Briol</authors><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian neural networks attempt to combine the strong predictive performance
of neural networks with formal quantification of uncertainty associated with
the predictive output in the Bayesian framework. However, it remains unclear
how to endow the parameters of the network with a prior distribution that is
meaningful when lifted into the output space of the network. A possible
solution is proposed that enables the user to posit an appropriate Gaussian
process covariance function for the task at hand. Our approach constructs a
prior distribution for the parameters of the network, called a ridgelet prior,
that approximates the posited Gaussian process in the output space of the
network. In contrast to existing work on the connection between neural networks
and Gaussian processes, our analysis is non-asymptotic, with finite sample-size
error bounds provided. This establishes the universality property that a
Bayesian neural network can approximate any Gaussian process whose covariance
function is sufficiently regular. Our experimental assessment is limited to a
proof-of-concept, where we demonstrate that the ridgelet prior can out-perform
an unstructured prior on regression problems for which a suitable Gaussian
process prior can be provided.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.08506</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.08506</id><submitter>Taylor Howell</submitter><version version="v1"><date>Fri, 16 Oct 2020 17:01:32 GMT</date><size>1713kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 14 Jan 2021 21:21:00 GMT</date><size>821kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 22:45:34 GMT</date><size>694kb</size><source_type>D</source_type></version><title>Direct Policy Optimization using Deterministic Sampling and Collocation</title><authors>Taylor A. Howell, Chunjiang Fu, and Zachary Manchester</authors><categories>cs.RO</categories><comments>final revisions for RA-L</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for approximately solving discrete-time stochastic
optimal-control problems by combining direct trajectory optimization,
deterministic sampling, and policy optimization. Our feedback motion-planning
algorithm uses a quasi-Newton method to simultaneously optimize a reference
trajectory, a set of deterministically chosen sample trajectories, and a
parameterized policy. We demonstrate that this approach exactly recovers LQR
policies in the case of linear dynamics, quadratic objective, and Gaussian
disturbances. We also demonstrate the algorithm on several nonlinear,
underactuated robotic systems to highlight its performance and ability to
handle control limits, safely avoid obstacles, and generate robust plans in the
presence of unmodeled dynamics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.08895</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.08895</id><submitter>Zongyi Li</submitter><version version="v1"><date>Sun, 18 Oct 2020 00:34:21 GMT</date><size>3182kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 20 Mar 2021 19:58:28 GMT</date><size>2817kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 03:12:33 GMT</date><size>2817kb</size><source_type>D</source_type></version><title>Fourier Neural Operator for Parametric Partial Differential Equations</title><authors>Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu,
  Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar</authors><categories>cs.LG cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical development of neural networks has primarily focused on
learning mappings between finite-dimensional Euclidean spaces. Recently, this
has been generalized to neural operators that learn mappings between function
spaces. For partial differential equations (PDEs), neural operators directly
learn the mapping from any functional parametric dependence to the solution.
Thus, they learn an entire family of PDEs, in contrast to classical methods
which solve one instance of the equation. In this work, we formulate a new
neural operator by parameterizing the integral kernel directly in Fourier
space, allowing for an expressive and efficient architecture. We perform
experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The
Fourier neural operator is the first ML-based method to successfully model
turbulent flows with zero-shot super-resolution. It is up to three orders of
magnitude faster compared to traditional PDE solvers. Additionally, it achieves
superior accuracy compared to previous learning-based solvers under fixed
resolution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09312</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09312</id><submitter>Takuya Tsuchiya</submitter><version version="v1"><date>Mon, 19 Oct 2020 08:45:26 GMT</date><size>58kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 10:47:36 GMT</date><size>58kb</size><source_type>D</source_type></version><title>A robust discontinuous Galerkin scheme on anisotropic meshes</title><authors>Takahito Kashiwabara, Takuya Tsuchiya</authors><categories>math.NA cs.NA</categories><comments>to appear in Japan Journal of Industrial and Applied Mathematics</comments><msc-class>65N30, 65N50</msc-class><doi>10.1007/s13160-021-00474-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discontinuous Galerkin (DG) methods are extensions of the usual Galerkin
finite element methods. Although there are vast amount of studies on DG
methods, most of them have assumed shape-regularity conditions on meshes for
both theoretical error analysis and practical computations. In this paper, we
present a new symmetric interior penalty DG scheme with a modified penalty
term. We show that, without imposing the shape-regularity condition on the
meshes, the new DG scheme inherits all of the good properties of standard DG
methods, and is thus robust on anisotropic meshes. Numerical experiments
confirm the theoretical error estimates obtained.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09526</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09526</id><submitter>Sebastian Fuchs</submitter><version version="v1"><date>Mon, 19 Oct 2020 13:57:41 GMT</date><size>5834kb</size></version><version version="v2"><date>Tue, 18 May 2021 08:06:48 GMT</date><size>6426kb</size></version><title>A novel smoothed particle hydrodynamics and finite element coupling
  scheme for fluid-structure interaction: the sliding boundary particle
  approach</title><authors>Sebastian L. Fuchs, Christoph Meier, Wolfgang A. Wall, Christian J.
  Cyron</authors><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel numerical formulation for solving fluid-structure interaction (FSI)
problems is proposed where the fluid field is spatially discretized using
smoothed particle hydrodynamics (SPH) and the structural field using the finite
element method (FEM). As compared to fully mesh- or grid-based FSI frameworks,
due to the Lagrangian nature of SPH this framework can be easily extended to
account for more complex fluids consisting of multiple phases and dynamic phase
transitions. Moreover, this approach facilitates the handling of large
deformations of the fluid domain respectively the fluid-structure interface
without additional methodological and computational efforts. In particular, to
achieve an accurate representation of interaction forces between fluid
particles and structural elements also for strongly curved interface
geometries, the novel sliding boundary particle approach is proposed to ensure
full support of SPH particles close to the interface. The coupling of the fluid
and the structural field is based on a Dirichlet-Neumann partitioned approach,
where the fluid field is the Dirichlet partition with prescribed interface
displacements and the structural field is the Neumann partition subject to
interface forces. To overcome instabilities inherent to weakly coupled schemes
an iterative fixed-point coupling scheme is employed. Several numerical
examples in form of well-known benchmark tests are considered to validate the
accuracy, stability, and robustness of the proposed formulation. Finally, the
filling process of a highly flexible thin-walled balloon-like container is
studied, representing a model problem close to potential application scenarios
of the proposed scheme in the field of biomechanics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.09780</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.09780</id><submitter>Wenhao Yu</submitter><version version="v1"><date>Mon, 19 Oct 2020 18:39:30 GMT</date><size>2354kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 05:00:18 GMT</date><size>2342kb</size><source_type>D</source_type></version><title>Technical Question Answering across Tasks and Domains</title><authors>Wenhao Yu, Lingfei Wu, Yu Deng, Qingkai Zeng, Ruchi Mahindru, Sinem
  Guven, Meng Jiang</authors><categories>cs.CL cs.AI</categories><comments>NAACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Building automatic technical support system is an important yet challenge
task. Conceptually, to answer a user question on a technical forum, a human
expert has to first retrieve relevant documents, and then read them carefully
to identify the answer snippet. Despite huge success the researchers have
achieved in coping with general domain question answering (QA), much less
attentions have been paid for investigating technical QA. Specifically,
existing methods suffer from several unique challenges (i) the question and
answer rarely overlaps substantially and (ii) very limited data size. In this
paper, we propose a novel framework of deep transfer learning to effectively
address technical QA across tasks and domains. To this end, we present an
adjustable joint learning approach for document retrieval and reading
comprehension tasks. Our experiments on the TechQA demonstrates superior
performance compared with state-of-the-art methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10124</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10124</id><submitter>Dominik Stallmann</submitter><version version="v1"><date>Tue, 20 Oct 2020 08:36:51 GMT</date><size>2071kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 08:51:12 GMT</date><size>2568kb</size><source_type>D</source_type></version><title>Towards an Automatic Analysis of CHO-K1 Suspension Growth in
  Microfluidic Single-cell Cultivation</title><authors>Dominik Stallmann and Jan P. G\&quot;opfert and Julian Schmitz and
  Alexander Gr\&quot;unberger and Barbara Hammer</authors><categories>cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Innovative microfluidic systems carry the promise to greatly
facilitate spatio-temporal analysis of single cells under well-defined
environmental conditions, allowing novel insights into population heterogeneity
and opening new opportunities for fundamental and applied biotechnology.
Microfluidics experiments, however, are accompanied by vast amounts of data,
such as time series of microscopic images, for which manual evaluation is
infeasible due to the sheer number of samples. While classical image processing
technologies do not lead to satisfactory results in this domain, modern deep
learning technologies such as convolutional networks can be sufficiently
versatile for diverse tasks, including automatic cell tracking and counting as
well as the extraction of critical parameters, such as growth rate. However,
for successful training, current supervised deep learning requires label
information, such as the number or positions of cells for each image in a
series; obtaining these annotations is very costly in this setting. Results: We
propose a novel Machine Learning architecture together with a specialized
training procedure, which allows us to infuse a deep neural network with
human-powered abstraction on the level of data, leading to a high-performing
regression model that requires only a very small amount of labeled data.
Specifically, we train a generative model simultaneously on natural and
synthetic data, so that it learns a shared representation, from which a target
variable, such as the cell count, can be reliably estimated.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10464</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10464</id><submitter>Lakshmi Natarajan Dr</submitter><version version="v1"><date>Tue, 20 Oct 2020 17:24:47 GMT</date><size>132kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 14:40:35 GMT</date><size>1292kb</size><source_type>D</source_type></version><title>Blind Updates in Coded Caching</title><authors>Suman Ghosh, Prasad Krishnan and Lakshmi Prasad Natarajan</authors><categories>cs.IT math.IT</categories><comments>Shorter version was accepted and presented in ITW 2020, Riva del
  Garda, Italy. Changes with respect to arXiv:2010.10464v1 -- improved
  presentation and corrected minor errors. Keywords: blind update, broadcast
  channel, coded caching, communication cost, placement delivery array</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the centralized coded caching system where a library of files is
available at the server and their subfiles are cached at the clients as
prescribed by a placement delivery array (PDA). We are interested in the
problem where a specific file in the library is replaced with a new file at the
server, the contents of which are correlated with the file being replaced, and
this change needs to be communicated to the caches. Upon replacement, the
server has access only to the updated file and is unaware of its differences
with the original, while each cache has access to specific subfiles of the
original file as dictated by the PDA. We model the correlation between the two
files by assuming that they differ in at the most $\epsilon$ subfiles, and aim
to reduce the number of bits broadcast by the server to update the caches. We
design a new elegant coded transmission strategy for the server to update the
caches blindly, and also identify a simple scheme that is based on MDS codes.
We then derive converse bounds on the minimum communication cost $\ell^*$ among
all linear strategies. For two well-known families of PDAs -- Maddah-Ali &amp;
Niesen's caching scheme and a PDA by Tang &amp; Ramamoorthy and Yan et al. -- our
new scheme has cost $\ell^*(1 + o(1))$ when the updates are sufficiently
sparse, while the scheme using MDS codes has order-optimal cost when the
updates are dense.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10783</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10783</id><submitter>Jiancan Wu</submitter><version version="v1"><date>Wed, 21 Oct 2020 06:35:26 GMT</date><size>1767kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 29 Apr 2021 06:58:32 GMT</date><size>1430kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 02:15:00 GMT</date><size>3634kb</size><source_type>D</source_type></version><title>Self-supervised Graph Learning for Recommendation</title><authors>Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun
  Lian, and Xing Xie</authors><categories>cs.IR cs.LG</categories><comments>10 pages, 7 figures, 5 tables. Accepted by SIGIR 2021</comments><doi>10.1145/3404835.3462862</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Representation learning on user-item graph for recommendation has evolved
from using single ID or interaction history to exploiting higher-order
neighbors. This leads to the success of graph convolution networks (GCNs) for
recommendation such as PinSage and LightGCN. Despite effectiveness, we argue
that they suffer from two limitations: (1) high-degree nodes exert larger
impact on the representation learning, deteriorating the recommendations of
low-degree (long-tail) items; and (2) representations are vulnerable to noisy
interactions, as the neighborhood aggregation scheme further enlarges the
impact of observed edges.
  In this work, we explore self-supervised learning on user-item graph, so as
to improve the accuracy and robustness of GCNs for recommendation. The idea is
to supplement the classical supervised task of recommendation with an auxiliary
self-supervised task, which reinforces node representation learning via
self-discrimination. Specifically, we generate multiple views of a node,
maximizing the agreement between different views of the same node compared to
that of other nodes. We devise three operators to generate the views -- node
dropout, edge dropout, and random walk -- that change the graph structure in
different manners. We term this new learning paradigm as
\textit{Self-supervised Graph Learning} (SGL), implementing it on the
state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL
has the ability of automatically mining hard negatives. Empirical studies on
three benchmark datasets demonstrate the effectiveness of SGL, which improves
the recommendation accuracy, especially on long-tail items, and the robustness
against interaction noises. Our implementations are available at
\url{https://github.com/wujcan/SGL}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.10888</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.10888</id><submitter>Tobias Alt</submitter><version version="v1"><date>Wed, 21 Oct 2020 10:50:29 GMT</date><size>516kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 09:44:13 GMT</date><size>410kb</size><source_type>D</source_type></version><title>Learning Integrodifferential Models for Image Denoising</title><authors>Tobias Alt, Joachim Weickert</authors><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an integrodifferential extension of the edge-enhancing
anisotropic diffusion model for image denoising. By accumulating weighted
structural information on multiple scales, our model is the first to create
anisotropy through multiscale integration. It follows the philosophy of
combining the advantages of model-based and data-driven approaches within
compact, insightful, and mathematically well-founded models with improved
performance. We explore trained results of scale-adaptive weighting and
contrast parameters to obtain an explicit modelling by smooth functions. This
leads to a transparent model with only three parameters, without significantly
decreasing its denoising performance. Experiments demonstrate that it
outperforms its diffusion-based predecessors. We show that both multiscale
information and anisotropy are crucial for its success.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.11638</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.11638</id><submitter>Kostantinos Papadamou Mr</submitter><version version="v1"><date>Thu, 22 Oct 2020 12:20:01 GMT</date><size>8044kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 3 Nov 2020 10:37:30 GMT</date><size>8044kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 17 Jan 2021 16:46:07 GMT</date><size>7711kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 18 May 2021 13:48:56 GMT</date><size>9174kb</size><source_type>D</source_type></version><title>&quot;It is just a flu&quot;: Assessing the Effect of Watch History on YouTube's
  Pseudoscientific Video Recommendations</title><authors>Kostantinos Papadamou and Savvas Zannettou and Jeremy Blackburn and
  Emiliano De Cristofaro and Gianluca Stringhini and Michael Sirivianos</authors><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role played by YouTube's recommendation algorithm in unwittingly
promoting misinformation and conspiracy theories is not entirely understood.
Yet, this can have dire real-world consequences, especially when
pseudoscientific content is promoted to users at critical times, such as the
COVID-19 pandemic. In this paper, we set out to characterize and detect
pseudoscientific misinformation on YouTube. We collect 6.6K videos related to
COVID-19, the Flat Earth theory, as well as the anti-vaccination and anti-mask
movements. Using crowdsourcing, we annotate them as pseudoscience, legitimate
science, or irrelevant and train a deep learning classifier to detect
pseudoscientific videos with an accuracy of 0.79.
  We quantify user exposure to this content on various parts of the platform
and how this exposure changes based on the user's watch history. We find that
YouTube suggests more pseudoscientific content regarding traditional
pseudoscientific topics (e.g., flat earth, anti-vaccination) than for emerging
ones (like COVID-19). At the same time, these recommendations are more common
on the search results page than on a user's homepage or in the recommendation
section when actively watching videos. Finally, we shed light on how a user's
watch history substantially affects the type of recommended videos.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.11762</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.11762</id><submitter>Tobias Reinhard</submitter><version version="v1"><date>Thu, 22 Oct 2020 14:33:35 GMT</date><size>92kb</size></version><version version="v2"><date>Tue, 18 May 2021 10:28:18 GMT</date><size>125kb</size></version><title>Ghost Signals: Verifying Termination of Busy-Waiting (Extended Version)</title><authors>Tobias Reinhard, Bart Jacobs</authors><categories>cs.LO cs.PL</categories><comments>68 pages; 42 figures; Simplified logic by removing permissions and
  updated soundness proof in appendix. Simplified the logic's presentation.
  Added case studies to appendix.; This is the extended version of a paper
  which is to be published at CAV21</comments><acm-class>F.3.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programs for multiprocessor machines commonly perform busy waiting for
synchronization. We propose the first separation logic for modularly verifying
termination of such programs under fair scheduling. Our logic requires the
proof author to associate a ghost signal with each busy-waiting loop and allows
such loops to iterate while their corresponding signal $s$ is not set. The
proof author further has to define a well-founded order on signals and to prove
that if the looping thread holds an obligation to set a signal $s^\prime$, then
$s^\prime$ is ordered above $s$. By using conventional shared state invariants
to associate the state of ghost signals with the state of data structures,
programs busy-waiting for arbitrary conditions over arbitrary data structures
can be verified.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.11871</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.11871</id><submitter>Hideyuki Tachibana</submitter><version version="v1"><date>Thu, 22 Oct 2020 17:08:17 GMT</date><size>3192kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 13:40:26 GMT</date><size>3528kb</size><source_type>D</source_type></version><title>Towards Listening to 10 People Simultaneously: An Efficient Permutation
  Invariant Training of Audio Source Separation Using Sinkhorn's Algorithm</title><authors>Hideyuki Tachibana</authors><categories>cs.SD cs.LG eess.AS</categories><comments>5 pages, 8 figures, IEEE ICASSP 2021</comments><journal-ref>Proc. ICASSP (2021)</journal-ref><doi>10.1109/ICASSP39728.2021.9414508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In neural network-based monaural speech separation techniques, it has been
recently common to evaluate the loss using the permutation invariant training
(PIT) loss. However, the ordinary PIT requires to try all $N!$ permutations
between $N$ ground truths and $N$ estimates. Since the factorial complexity
explodes very rapidly as $N$ increases, a PIT-based training works only when
the number of source signals is small, such as $N = 2$ or $3$. To overcome this
limitation, this paper proposes a SinkPIT, a novel variant of the PIT losses,
which is much more efficient than the ordinary PIT loss when $N$ is large. The
SinkPIT is based on Sinkhorn's matrix balancing algorithm, which efficiently
finds a doubly stochastic matrix which approximates the best permutation in a
differentiable manner. The author conducted an experiment to train a neural
network model to decompose a single-channel mixture into 10 sources using the
SinkPIT, and obtained promising results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.12294</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.12294</id><submitter>Bastian Sch\&quot;afermeier</submitter><version version="v1"><date>Fri, 23 Oct 2020 10:53:42 GMT</date><size>967kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 26 Oct 2020 14:11:19 GMT</date><size>976kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 12:09:47 GMT</date><size>1030kb</size><source_type>D</source_type></version><title>Topic Space Trajectories: A case study on machine learning literature</title><authors>Bastian Sch\&quot;afermeier and Gerd Stumme and Tom Hanika</authors><categories>cs.LG cs.DL</categories><comments>41 pages, 8 figures</comments><acm-class>I.2.7; I.2.6; H.3.1; H.3.7</acm-class><journal-ref>Scientometrics (2021)</journal-ref><doi>10.1007/s11192-021-03931-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The annual number of publications at scientific venues, for example,
conferences and journals, is growing quickly. Hence, even for researchers it
becomes harder and harder to keep track of research topics and their progress.
In this task, researchers can be supported by automated publication analysis.
Yet, many such methods result in uninterpretable, purely numerical
representations. As an attempt to support human analysts, we present topic
space trajectories, a structure that allows for the comprehensible tracking of
research topics. We demonstrate how these trajectories can be interpreted based
on eight different analysis approaches. To obtain comprehensible results, we
employ non-negative matrix factorization as well as suitable visualization
techniques. We show the applicability of our approach on a publication corpus
spanning 50 years of machine learning research from 32 publication venues. Our
novel analysis method may be employed for paper classification, for the
prediction of future research topics, and for the recommendation of fitting
conferences and journals for submitting unpublished work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.12868</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.12868</id><submitter>Yongchang Hao</submitter><version version="v1"><date>Sat, 24 Oct 2020 11:00:58 GMT</date><size>7169kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 07:24:55 GMT</date><size>98kb</size><source_type>D</source_type></version><title>Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine
  Translation</title><authors>Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu and
  Xing Wang</authors><categories>cs.CL</categories><comments>NAACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Non-Autoregressive machine Translation (NAT) models have demonstrated
significant inference speedup but suffer from inferior translation accuracy.
The common practice to tackle the problem is transferring the Autoregressive
machine Translation (AT) knowledge to NAT models, e.g., with knowledge
distillation. In this work, we hypothesize and empirically verify that AT and
NAT encoders capture different linguistic properties of source sentences.
Therefore, we propose to adopt Multi-Task learning to transfer the AT knowledge
to NAT models through encoder sharing. Specifically, we take the AT model as an
auxiliary task to enhance NAT model performance. Experimental results on WMT14
English-German and WMT16 English-Romanian datasets show that the proposed
Multi-Task NAT achieves significant improvements over the baseline NAT models.
Furthermore, the performance on large-scale WMT19 and WMT20 English-German
datasets confirm the consistency of our proposed method. In addition,
experimental results demonstrate that our Multi-Task NAT is complementary to
knowledge distillation, the standard knowledge transfer method for NAT.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.13686</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.13686</id><submitter>Gaetan Laurens</submitter><version version="v1"><date>Mon, 26 Oct 2020 16:02:04 GMT</date><size>962kb</size><source_type>D</source_type></version><title>Infrared spectra of neutral polycyclic aromatic hydrocarbons by machine
  learning</title><authors>Ga\'etan Laurens and Malalatiana Rabary and Julien Lam and Daniel
  Pel\'aez and Abdul-Rahman Allouche</authors><categories>physics.chem-ph cs.LG physics.comp-ph</categories><comments>13 pages, 6 figures</comments><doi>10.1007/s00214-021-02773-6</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Interest in polycyclic aromatic hydrocarbons (PAHs) spans numerous fields
and infrared spectroscopy is usually the method of choice to disentangle their
molecular structure. In order to compute vibrational frequencies, numerous
theoretical studies employ either quantum calculation methods, or empirical
potentials, but it remains difficult to combine the accuracy of the first
approach with the computational cost of the second. In this work, we employed
Machine Learning techniques to develop a potential energy surface and a dipole
mapping based on an artificial neural network (ANN) architecture. Altogether,
while trained on only 11 small PAH molecules, the obtained ANNs are able to
retrieve the infrared spectra of those small molecules, but more importantly of
8 large PAHs different from the training set, thus demonstrating the
transferability of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.14782</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.14782</id><submitter>Xin Ding</submitter><version version="v1"><date>Wed, 28 Oct 2020 06:19:30 GMT</date><size>878kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 29 Oct 2020 20:35:45 GMT</date><size>878kb</size><source_type>D</source_type></version><title>Classification Beats Regression: Counting of Cells from Greyscale
  Microscopic Images based on Annotation-free Training Samples</title><authors>Xin Ding, Qiong Zhang, William J. Welch</authors><categories>eess.IV cs.CV</categories><journal-ref>The CAAI International Conference on Artificial Intelligence
  (CICAI 2021)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern methods often formulate the counting of cells from microscopic images
as a regression problem and more or less rely on expensive, manually annotated
training images (e.g., dot annotations indicating the centroids of cells or
segmentation masks identifying the contours of cells). This work proposes a
supervised learning framework based on classification-oriented convolutional
neural networks (CNNs) to count cells from greyscale microscopic images without
using annotated training images. In this framework, we formulate the cell
counting task as an image classification problem, where the cell counts are
taken as class labels. This formulation has its limitation when some cell
counts in the test stage do not appear in the training data. Moreover, the
ordinal relation among cell counts is not utilized. To deal with these
limitations, we propose a simple but effective data augmentation (DA) method to
synthesize images for the unseen cell counts. We also introduce an ensemble
method, which can not only moderate the influence of unseen cell counts but
also utilize the ordinal information to improve the prediction accuracy. This
framework outperforms many modern cell counting methods and won the data
analysis competition (Case Study 1: Counting Cells From Microscopic Images
https://ssc.ca/en/case-study/case-study-1-counting-cells-microscopic-images) of
the 47th Annual Meeting of the Statistical Society of Canada (SSC). Our code is
available at https://github.com/anno2020/CellCount_TinyBBBC005.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.15920</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.15920</id><submitter>Ashwin Balakrishna</submitter><version version="v1"><date>Thu, 29 Oct 2020 20:10:02 GMT</date><size>20727kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 21:20:48 GMT</date><size>12517kb</size><source_type>D</source_type></version><title>Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones</title><authors>Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo,
  Krishnan Srinivasan, Minho Hwang, Joseph E. Gonzalez, Julian Ibarz, Chelsea
  Finn, Ken Goldberg</authors><categories>cs.LG cs.AI cs.RO</categories><comments>RA-L and ICRA 2021. First two authors contributed equally</comments><journal-ref>Robotics and Automation Letters (RA-L) and International
  Conference on Robotics and Automation (ICRA) 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safety remains a central obstacle preventing widespread use of RL in the real
world: learning new tasks in uncertain environments requires extensive
exploration, but safety requires limiting exploration. We propose Recovery RL,
an algorithm which navigates this tradeoff by (1) leveraging offline data to
learn about constraint violating zones before policy learning and (2)
separating the goals of improving task performance and constraint satisfaction
across two policies: a task policy that only optimizes the task reward and a
recovery policy that guides the agent to safety when constraint violation is
likely. We evaluate Recovery RL on 6 simulation domains, including two
contact-rich manipulation tasks and an image-based navigation task, and an
image-based obstacle avoidance task on a physical robot. We compare Recovery RL
to 5 prior safe RL methods which jointly optimize for task performance and
safety via constrained optimization or reward shaping and find that Recovery RL
outperforms the next best prior method across all domains. Results suggest that
Recovery RL trades off constraint violations and task successes 2 - 20 times
more efficiently in simulation domains and 3 times more efficiently in physical
experiments. See https://tinyurl.com/rl-recovery for videos and supplementary
material.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.15965</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.15965</id><submitter>Dhruv Guliani</submitter><version version="v1"><date>Thu, 29 Oct 2020 22:01:37 GMT</date><size>103kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 18:49:19 GMT</date><size>176kb</size><source_type>D</source_type></version><title>Training Speech Recognition Models with Federated Learning: A
  Quality/Cost Framework</title><authors>Dhruv Guliani, Francoise Beaufays, Giovanni Motta</authors><categories>cs.LG cs.DC</categories><comments>Paper published at ICASSP 2021</comments><msc-class>68T10</msc-class><acm-class>I.2.7</acm-class><journal-ref>ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), 2021, pp. 3080-3084</journal-ref><doi>10.1109/ICASSP39728.2021.9413397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose using federated learning, a decentralized on-device learning
paradigm, to train speech recognition models. By performing epochs of training
on a per-user basis, federated learning must incur the cost of dealing with
non-IID data distributions, which are expected to negatively affect the quality
of the trained model. We propose a framework by which the degree of
non-IID-ness can be varied, consequently illustrating a trade-off between model
quality and the computational cost of federated training, which we capture
through a novel metric. Finally, we demonstrate that hyper-parameter
optimization and appropriate use of variational noise are sufficient to
compensate for the quality impact of non-IID distributions, while decreasing
the cost.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.16143</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.16143</id><submitter>Yudong Zhu</submitter><version version="v1"><date>Fri, 30 Oct 2020 09:30:54 GMT</date><size>896kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 08:58:20 GMT</date><size>251kb</size><source_type>D</source_type></version><title>HyperText: Endowing FastText with Hyperbolic Geometry</title><authors>Yudong Zhu, Di Zhou, Jinghui Xiao, Xin Jiang, Xiao Chen, Qun Liu</authors><categories>cs.CL cs.LG</categories><comments>Findings of EMNLP 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language data exhibit tree-like hierarchical structures such as the
hypernym-hyponym relations in WordNet. FastText, as the state-of-the-art text
classifier based on shallow neural network in Euclidean space, may not model
such hierarchies precisely with limited representation capacity. Considering
that hyperbolic space is naturally suitable for modeling tree-like hierarchical
data, we propose a new model named HyperText for efficient text classification
by endowing FastText with hyperbolic geometry. Empirically, we show that
HyperText outperforms FastText on a range of text classification tasks with
much reduced parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.16223</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.16223</id><submitter>Nicolas Gillis</submitter><version version="v1"><date>Fri, 30 Oct 2020 12:31:35 GMT</date><size>3207kb</size><source_type>D</source_type></version><title>Multiplicative Updates for NMF with $\beta$-Divergences under Disjoint
  Equality Constraints</title><authors>Valentin Leplat, Nicolas Gillis, J\'er\^ome Idier</authors><categories>cs.LG cs.NA eess.SP math.NA math.OC stat.ML</categories><comments>23 pages + 6 pages of supplementary materials</comments><journal-ref>SIAM J. on Matrix Analysis and its Applications 42 (2), 730-752,
  2021</journal-ref><doi>10.1137/20M1377278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) is the problem of approximating an
input nonnegative matrix, $V$, as the product of two smaller nonnegative
matrices, $W$ and $H$. In this paper, we introduce a general framework to
design multiplicative updates (MU) for NMF based on $\beta$-divergences
($\beta$-NMF) with disjoint equality constraints, and with penalty terms in the
objective function. By disjoint, we mean that each variable appears in at most
one equality constraint. Our MU satisfy the set of constraints after each
update of the variables during the optimization process, while guaranteeing
that the objective function decreases monotonically. We showcase this framework
on three NMF models, and show that it competes favorably the state of the art:
(1)~$\beta$-NMF with sum-to-one constraints on the columns of $H$, (2)
minimum-volume $\beta$-NMF with sum-to-one constraints on the columns of $W$,
and (3) sparse $\beta$-NMF with $\ell_2$-norm constraints on the columns of
$W$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2010.16295</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2010.16295</id><submitter>Luca Ganassali</submitter><version version="v1"><date>Fri, 30 Oct 2020 14:42:17 GMT</date><size>287kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 14:46:09 GMT</date><size>72kb</size><source_type>D</source_type></version><title>Sharp threshold for alignment of graph databases with Gaussian weights</title><authors>Luca Ganassali</authors><categories>stat.ML cs.LG math.PR</categories><comments>18 pages, 2 figures. Revised version: Remark 3 updated, typos
  corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental limits for reconstruction in weighted graph (or
matrix) database alignment. We consider a model of two graphs where $\pi^*$ is
a planted uniform permutation and all pairs of edge weights $(A_{i,j},
B_{\pi^*(i),\pi^*(j)})_{1 \leq i&lt;j \leq n}$ are i.i.d. pairs of Gaussian
variables with zero mean, unit variance and correlation parameter $\rho \in
[0,1]$. We prove that there is a sharp threshold for exact recovery of $\pi^*$:
if $n \rho^2 \geq (4+\epsilon) \log n + \omega(1)$ for some $\epsilon&gt;0$, there
is an estimator $\hat{\pi}$ -- namely the MAP estimator -- based on the
observation of databases $A,B$ that achieves exact reconstruction with high
probability. Conversely, if $n \rho^2 \leq 4 \log n - \log \log n - \omega(1)$,
then any estimator $\hat{\pi}$ verifies $\hat{\pi}=\pi$ with probability
$o(1)$.
  This result shows that the information-theoretic threshold for exact recovery
is the same as the one obtained for detection in a recent work by Wu et al.
(2020): in other words, for Gaussian weighted graph alignment, the problem of
reconstruction is not more difficult than that of detection. Though the
reconstruction task was already well understood for vector-shaped database
alignment (that is taking signal of the form $(u_i, v_{\pi^*(i)})_{1 \leq i\leq
n}$ where $(u_i, v_{\pi^*(i)})$ are i.i.d. pairs in $\mathbb{R}^{d_u} \times
\mathbb{R}^{d_v}$), its formulation for graph (or matrix) databases brings a
drastically different problem for which the hard phase is conjectured to be
wide.
  The proofs build upon the analysis of the MAP estimator and the second moment
method, together with the study of the correlation structure of energies of
permutations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.00079</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.00079</id><submitter>Jan Zur</submitter><version version="v1"><date>Fri, 30 Oct 2020 20:00:28 GMT</date><size>6161kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 14:04:37 GMT</date><size>6171kb</size><source_type>D</source_type></version><title>The transport of images method: computing all zeros of harmonic mappings
  by continuation</title><authors>Olivier S\`ete, Jan Zur</authors><categories>math.NA cs.NA math.CV</categories><msc-class>65H20, 31A05, 30C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a continuation method to compute all zeros of a harmonic mapping
$f$ in the complex plane. Our method works without any prior knowledge of the
number of zeros or their approximate location. We start by computing all
solution of $f(z) = \eta$ with $|\eta|$ sufficiently large and then track all
solutions as $\eta$ tends to $0$ to finally obtain all zeros of $f$. Using
theoretical results on harmonic mappings we analyze where and how the number of
solutions of $f(z) = \eta$ changes and incorporate this into the method. We
prove that our method is guaranteed to compute all zeros, as long as none of
them is singular. In our numerical example the method always terminates with
the correct number of zeros, is very fast compared to general purpose root
finders and is highly accurate in terms of the residual. An easy-to-use MATLAB
implementation is freely available online.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.00459</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.00459</id><submitter>Martin Servin</submitter><version version="v1"><date>Sun, 1 Nov 2020 09:51:07 GMT</date><size>47762kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 21:12:33 GMT</date><size>47764kb</size><source_type>D</source_type></version><title>A multiscale model of terrain dynamics for real-time earthmoving
  simulation</title><authors>Martin Servin, Tomas Berglund and Samuel Nystedt</authors><categories>cs.CE</categories><comments>35 pages, 24 figures</comments><journal-ref>Advanced Modeling and Simulation in Engineering Sciences 8, 11
  (2021)</journal-ref><doi>10.1186/s40323-021-00196-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiscale model for real-time simulation of terrain dynamics is explored.
To represent the dynamics on different scales the model combines the
description of soil as a continuous solid, as distinct particles and as rigid
multibodies. The models are dynamically coupled to each other and to the
earthmoving equipment. Agitated soil is represented by a hybrid of contacting
particles and continuum solid, with the moving equipment and resting soil as
geometric boundaries. Each zone of active soil is aggregated into distinct
bodies, with the proper mass, momentum and frictional-cohesive properties,
which constrain the equipment's multibody dynamics. The particle model
parameters are pre-calibrated to the bulk mechanical parameters for a wide
range of different soils. The result is a computationally efficient model for
earthmoving operations that resolve the motion of the soil, using a fast
iterative solver, and provide realistic forces and dynamic for the equipment,
using a direct solver for high numerical precision. Numerical simulations of
excavation and bulldozing operations are performed to validate the model and
measure the computational performance. Reference data is produced using coupled
discrete element and multibody dynamics simulations at relatively high
resolution. The digging resistance and soil displacements with the real-time
multiscale model agree with the reference model up to 10-25%, and run more than
three orders of magnitude faster.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.01509</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.01509</id><submitter>Fangtian Zhong</submitter><version version="v1"><date>Tue, 3 Nov 2020 06:59:09 GMT</date><size>13422kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 08:03:31 GMT</date><size>16659kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 01:21:48 GMT</date><size>16650kb</size><source_type>D</source_type></version><title>MalFox: Camouflaged Adversarial Malware Example Generation Based on
  Conv-GANs Against Black-Box Detectors</title><authors>Fangtian Zhong and Xiuzhen Cheng and Dongxiao Yu and Bei Gong and
  Shuaiwen Song and Jiguo Yu</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning is a thriving field currently stuffed with many practical
applications and active research topics. It allows computers to learn from
experience and to understand the world in terms of a hierarchy of concepts,
with each being defined through its relations to simpler concepts. Relying on
the strong learning capabilities of deep learning, we propose a convolutional
generative adversarial network-based (C-GAN) framework titled MalFox, targeting
adversarial malware example generation against third-party black-box detectors.
MalFox adopts a novel approach to confrontationally produce perturbation paths,
with each formed by up to three methods (namely Obfusmal, Stealmal, and
Hollowmal) to generate adversarial malware examples via changing the process of
program execution in our implementation. To demonstrate the effectiveness of
MalFox, we collect a large dataset consisting of both malware and benignware,
and investigate the performance of MalFox in terms of accuracy, detection rate,
and evasive rate of the generated adversarial malware examples. Our evaluation
indicates that the accuracy can be as high as 99.01% which significantly
outperforms the other 6 well-known learning models. Furthermore, the detection
rate is dramatically decreased by 44.3% on average, and the average evasive
rate is noticeably improved by up to 55.3%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02223</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02223</id><submitter>Kieran Greer Dr</submitter><version version="v1"><date>Wed, 4 Nov 2020 10:59:01 GMT</date><size>728kb</size></version><version version="v2"><date>Sat, 15 May 2021 23:43:49 GMT</date><size>511kb</size></version><title>New Ideas for Brain Modelling 7</title><authors>Kieran Greer</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper updates the cognitive model, firstly by creating two systems and
then unifying them over the same structure. It represents information at the
semantic level only, where labelled patterns are aggregated into a
'type-set-match' form. It is described that the aggregations can be used to
match across regions with potentially different functionality and therefore
give the structure a required amount of flexibility. The theory is that if the
model stores information which can be transposed in consistent ways, then that
will result in knowledge and some level of intelligence. As part of the design,
patterns have to become distinct and that is realised by unique paths through
shared aggregated structures. An ensemble-hierarchy relation also helps to
define uniqueness through local feedback that may even be an action potential.
The earlier models are still consistent in terms of their proposed
functionality, but some of the architecture boundaries have been moved to match
them up more closely. After pattern optimisation and tree-like aggregations,
the two main models differ only in their upper, more intelligent level. One
provides a propositional logic for mutually inclusive or exclusive pattern
groups and sequences, while the other provides a behaviour script that is
constructed from node types. It can be seen that these two views are
complimentary and would allow some control over behaviours, as well as
memories, that might get selected.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02635</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02635</id><submitter>Jinglun Feng</submitter><version version="v1"><date>Thu, 5 Nov 2020 03:26:01 GMT</date><size>7991kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 5 May 2021 21:01:49 GMT</date><size>7512kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 16:06:31 GMT</date><size>7512kb</size><source_type>D</source_type></version><title>GPR-based Model Reconstruction System for Underground Utilities Using
  GPRNet</title><authors>Jinglun Feng, Liang Yang, Ejup Hoxha, Diar Sanakov, Stanislav
  Sotnikov, Jizhong Xiao</authors><categories>cs.CV cs.RO eess.IV</categories><comments>Accepted by ICRA 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ground Penetrating Radar (GPR) is one of the most important non-destructive
evaluation (NDE) instruments to detect and locate underground objects (i.e.,
rebars, utility pipes). Many previous researches focus on GPR image-based
feature detection only, and none can process sparse GPR measurements to
successfully reconstruct a very fine and detailed 3D model of underground
objects for better visualization. To address this problem, this paper presents
a novel robotic system to collect GPR data, localize the underground utilities,
and reconstruct the underground objects' dense point cloud model. This system
is composed of three modules: 1) visual-inertial-based GPR data collection
module, which tags the GPR measurements with positioning information provided
by an omnidirectional robot; 2) a deep neural network (DNN) migration module to
interpret the raw GPR B-scan image into a cross-section of object model; 3) a
DNN-based 3D reconstruction module, i.e., GPRNet, to generate underground
utility model with the fine 3D point cloud. In this paper, both the
quantitative and qualitative experiment results verify our method that can
generate a dense and complete point cloud model of pipe-shaped utilities based
on a sparse input, i.e., GPR raw data incompleteness and various noise. The
experiment results on synthetic data and field test data further support the
effectiveness of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02881</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02881</id><submitter>Hai Shu</submitter><version version="v1"><date>Wed, 4 Nov 2020 05:55:06 GMT</date><size>6183kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 28 Nov 2020 06:48:15 GMT</date><size>4121kb</size><source_type>D</source_type></version><title>A Two-Stage Cascade Model with Variational Autoencoders and Attention
  Gates for MRI Brain Tumor Segmentation</title><authors>Chenggang Lyu, Hai Shu</authors><categories>eess.IV cs.CV cs.LG</categories><journal-ref>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
  Brain Injuries (BrainLes 2020)</journal-ref><doi>10.1007/978-3-030-72084-1_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic MRI brain tumor segmentation is of vital importance for the disease
diagnosis, monitoring, and treatment planning. In this paper, we propose a
two-stage encoder-decoder based model for brain tumor subregional segmentation.
Variational autoencoder regularization is utilized in both stages to prevent
the overfitting issue. The second-stage network adopts attention gates and is
trained additionally using an expanded dataset formed by the first-stage
outputs. On the BraTS 2020 validation dataset, the proposed method achieves the
mean Dice score of 0.9041, 0.8350, and 0.7958, and Hausdorff distance (95%) of
4.953, 6.299, and 23.608 for the whole tumor, tumor core, and enhancing tumor,
respectively. The corresponding results on the BraTS 2020 testing dataset are
0.8729, 0.8357, and 0.8205 for Dice score, and 11.4288, 19.9690, and 15.6711
for Hausdorff distance. The code is publicly available at
https://github.com/shu-hai/two-stage-VAE-Attention-gate-BraTS2020.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02887</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02887</id><submitter>Diego Kozlowski</submitter><version version="v1"><date>Thu, 5 Nov 2020 14:57:41 GMT</date><size>29373kb</size><source_type>D</source_type></version><title>Semantic and Relational Spaces in Science of Science: Deep Learning
  Models for Article Vectorisation</title><authors>Diego Kozlowski, Jennifer Dusdal, Jun Pang and Andreas Zilian</authors><categories>cs.SI cs.LG physics.soc-ph</categories><doi>10.1007/s11192-021-03984-1</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Over the last century, we observe a steady and exponentially growth of
scientific publications globally. The overwhelming amount of available
literature makes a holistic analysis of the research within a field and between
fields based on manual inspection impossible. Automatic techniques to support
the process of literature review are required to find the epistemic and social
patterns that are embedded in scientific publications. In computer sciences,
new tools have been developed to deal with large volumes of data. In
particular, deep learning techniques open the possibility of automated
end-to-end models to project observations to a new, low-dimensional space where
the most relevant information of each observation is highlighted. Using deep
learning to build new representations of scientific publications is a growing
but still emerging field of research. The aim of this paper is to discuss the
potential and limits of deep learning for gathering insights about scientific
research articles. We focus on document-level embeddings based on the semantic
and relational aspects of articles, using Natural Language Processing (NLP) and
Graph Neural Networks (GNNs). We explore the different outcomes generated by
those techniques. Our results show that using NLP we can encode a semantic
space of articles, while with GNN we are able to build a relational space where
the social practices of a research community are also encoded.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.02947</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.02947</id><submitter>Zheng Yuan</submitter><version version="v1"><date>Thu, 5 Nov 2020 16:16:49 GMT</date><size>566kb</size></version><version version="v2"><date>Mon, 17 May 2021 03:39:55 GMT</date><size>5856kb</size></version><version version="v3"><date>Tue, 18 May 2021 00:46:29 GMT</date><size>6507kb</size></version><title>CODER: Knowledge infused cross-lingual medical term embedding for term
  normalization</title><authors>Zheng Yuan and Zhengyun Zhao and Haixia Sun and Jiao Li and Fei Wang
  and Sheng Yu</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes CODER: contrastive learning on knowledge graphs for
cross-lingual medical term representation. CODER is designed for medical term
normalization by providing close vector representations for different terms
that represent the same or similar medical concepts with cross-lingual support.
We train CODER via contrastive learning on a medical knowledge graph (KG) named
the Unified Medical Language System, where similarities are calculated
utilizing both terms and relation triplets from KG. Training with relations
injects medical knowledge into embeddings and aims to provide potentially
better machine learning features. We evaluate CODER in zero-shot term
normalization, semantic similarity, and relation classification benchmarks,
which show that CODERoutperforms various state-of-the-art biomedical word
embedding, concept embeddings, and contextual embeddings. Our codes and models
are available at https://github.com/GanjinZero/CODER.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.03194</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.03194</id><submitter>Manuel Torres</submitter><version version="v1"><date>Fri, 6 Nov 2020 05:14:27 GMT</date><size>51kb</size></version><version version="v2"><date>Mon, 17 May 2021 21:54:10 GMT</date><size>57kb</size></version><title>Fast Approximation Algorithms for Bounded Degree and Crossing Spanning
  Tree Problems</title><authors>Chandra Chekuri, Kent Quanrud and Manuel R. Torres</authors><categories>cs.DS</categories><comments>Updated to reflect overlap in results with arXiv:1811.07464</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop fast approximation algorithms for the minimum-cost version of the
Bounded-Degree MST problem (BD-MST) and its generalization the Crossing
Spanning Tree problem (Crossing-ST). We solve the underlying LP to within a
$(1+\epsilon)$ approximation factor in near-linear time via the multiplicative
weight update (MWU) technique. This yields, in particular, a near-linear time
algorithm that outputs an estimate $B$ such that $B \le B^* \le \lceil
(1+\epsilon)B \rceil +1$ where $B^*$ is the minimum-degree of a spanning tree
of a given graph. To round the fractional solution, in our main technical
contribution, we describe a fast near-linear time implementation of
swap-rounding in the spanning tree polytope of a graph. The fractional solution
can also be used to sparsify the input graph that can in turn be used to speed
up existing combinatorial algorithms. Together, these ideas lead to
significantly faster approximation algorithms than known before for the two
problems of interest. In addition, a fast algorithm for swap rounding in the
graphic matroid is a generic tool that has other applications, including to TSP
and submodular function maximization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.03451</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.03451</id><submitter>Rongcheng Tu</submitter><version version="v1"><date>Fri, 6 Nov 2020 16:02:35 GMT</date><size>566kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 15:02:27 GMT</date><size>1978kb</size><source_type>D</source_type></version><title>Deep Cross-modal Hashing via Margin-dynamic-softmax Loss</title><authors>Rong-Cheng Tu, Xian-Ling Mao, Rongxin Tu, Binbin Bian, Wei Wei, Heyan
  Huang</authors><categories>cs.CV cs.IR cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to their high retrieval efficiency and low storage cost for cross-modal
search task, cross-modal hashing methods have attracted considerable attention.
For the supervised cross-modal hashing methods, how to make the learned hash
codes preserve semantic information sufficiently contained in the label of
datapoints is the key to further enhance the retrieval performance. Hence,
almost all supervised cross-modal hashing methods usually depends on defining a
similarity between datapoints with the label information to guide the hashing
model learning fully or partly. However, the defined similarity between
datapoints can only capture the label information of datapoints partially and
misses abundant semantic information, then hinders the further improvement of
retrieval performance. Thus, in this paper, different from previous works, we
propose a novel cross-modal hashing method without defining the similarity
between datapoints, called Deep Cross-modal Hashing via
\textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains
a proxy hashing network to transform each category information of a dataset
into a semantic discriminative hash code, called proxy hash code. Each proxy
hash code can preserve the semantic information of its corresponding category
well. Next, without defining the similarity between datapoints to supervise the
training process of the modality-specific hashing networks , we propose a novel
\textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing
codes as supervised information. Finally, by minimizing the novel
\textit{margin-dynamic-softmax loss}, the modality-specific hashing networks
can be trained to generate hash codes which can simultaneously preserve the
cross-modal similarity and abundant semantic information well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.03616</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.03616</id><submitter>Niranjan Hasabnis</submitter><version version="v1"><date>Fri, 6 Nov 2020 22:19:05 GMT</date><size>185kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 7 Dec 2020 06:55:50 GMT</date><size>186kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 5 Jan 2021 00:44:27 GMT</date><size>187kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 13 May 2021 21:02:03 GMT</date><size>301kb</size><source_type>D</source_type></version><version version="v5"><date>Mon, 17 May 2021 16:22:04 GMT</date><size>340kb</size><source_type>D</source_type></version><title>ControlFlag: A Self-Supervised Idiosyncratic Pattern Detection System
  for Software Control Structures</title><authors>Niranjan Hasabnis and Justin Gottschlich</authors><categories>cs.SE cs.AI cs.PL</categories><comments>To appear in Proceedings of the 5th ACM SIGPLAN International
  Symposium on Machine Programming (MAPS '21)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software debugging has been shown to utilize upwards of half of developers'
time. Yet, machine programming (MP), the field concerned with the automation of
software (and hardware) development, has recently made strides in both research
and production-quality automated debugging systems. In this paper we present
ControlFlag, a self-supervised MP system that aims to improve debugging by
attempting to detect idiosyncratic pattern violations in software control
structures. ControlFlag also suggests possible corrections in the event an
anomalous pattern is detected. We present ControlFlag's design and provide an
experimental evaluation and analysis of its efficacy in identifying potential
programming errors in production-quality software. As a first concrete evidence
towards improving software quality, ControlFlag has already found an anomaly in
CURL that has been acknowledged and fixed by its developers. We also discuss
future extensions of ControlFlag.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.03798</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.03798</id><submitter>Linlin Chao</submitter><version version="v1"><date>Sat, 7 Nov 2020 16:09:03 GMT</date><size>401kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 5 Feb 2021 09:14:46 GMT</date><size>5754kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 13:06:26 GMT</date><size>5754kb</size><source_type>D</source_type></version><title>PairRE: Knowledge Graph Embeddings via Paired Relation Vectors</title><authors>Linlin Chao, Jianshan He, Taifeng Wang, Wei Chu</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance based knowledge graph embedding methods show promising results on
link prediction task, on which two topics have been widely studied: one is the
ability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the
other is to encode various relation patterns, such as symmetry/antisymmetry.
However, the existing methods fail to solve these two problems at the same
time, which leads to unsatisfactory results. To mitigate this problem, we
propose PairRE, a model with paired vectors for each relation representation.
The paired vectors enable an adaptive adjustment of the margin in loss function
to fit for complex relations. Besides, PairRE is capable of encoding three
important relation patterns, symmetry/antisymmetry, inverse and composition.
Given simple constraints on relation representations, PairRE can encode
subrelation further. Experiments on link prediction benchmarks demonstrate the
proposed key capabilities of PairRE. Moreover, We set a new state-of-the-art on
two knowledge graph datasets of the challenging Open Graph Benchmark.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.03925</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.03925</id><submitter>Irene Guessarian</submitter><version version="v1"><date>Sun, 8 Nov 2020 08:48:51 GMT</date><size>27kb</size></version><version version="v2"><date>Fri, 5 Mar 2021 18:37:49 GMT</date><size>27kb</size></version><version version="v3"><date>Fri, 14 May 2021 21:03:26 GMT</date><size>15kb</size></version><title>The algebra of binary trees is affine complete</title><authors>Andre Arnold, Patrick Cegielski, Serge Grigorieff and Irene Guessarian</authors><categories>cs.FL</categories><comments>9 pages, 1 figure</comments><msc-class>06A99, 08A30, 08B20</msc-class><journal-ref>DMTCS vol 23:2, 2021,#1</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function on an algebra is congruence preserving if, for any congruence, it
maps pairs of congruent elements onto pairs of congruent elements. We show that
on the algebra of binary trees whose leaves are labeled by letters of an
alphabet containing at least three letters, a function is congruence preserving
if and only if it is polynomial.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.04464</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.04464</id><submitter>\'Angel F. Garc\'ia-Fern\'andez</submitter><version version="v1"><date>Mon, 9 Nov 2020 14:41:40 GMT</date><size>52kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 06:28:22 GMT</date><size>54kb</size><source_type>D</source_type></version><title>A Poisson multi-Bernoulli mixture filter for coexisting point and
  extended targets</title><authors>\'Angel F. Garc\'ia-Fern\'andez, Jason L. Williams, Lennart Svensson,
  Yuxuan Xia</authors><categories>stat.ME cs.CV stat.AP</categories><comments>Matlab files can be found at
  https://github.com/Agarciafernandez/Coexisting-point-extended-target-PMBM-filter
  and
  https://github.com/yuhsuansia/Coexisting-point-extended-target-PMBM-filter. A
  relevant multi-object tracking course can be found at
  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw</comments><journal-ref>in IEEE Transactions on Signal Processing, vol. 69, pp. 2600-2610,
  2021</journal-ref><doi>10.1109/TSP.2021.3072006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a Poisson multi-Bernoulli mixture (PMBM) filter for
coexisting point and extended targets, i.e., for scenarios where there may be
simultaneous point and extended targets. The PMBM filter provides a recursion
to compute the multi-target filtering posterior based on probabilistic
information on data associations, and single-target predictions and updates. In
this paper, we first derive the PMBM filter update for a generalised
measurement model, which can include measurements originated from point and
extended targets. Second, we propose a single-target space that accommodates
both point and extended targets and derive the filtering recursion that
propagates Gaussian densities for point targets and gamma Gaussian inverse
Wishart densities for extended targets. As a computationally efficient
approximation of the PMBM filter, we also develop a Poisson multi-Bernoulli
(PMB) filter for coexisting point and extended targets. The resulting filters
are analysed via numerical simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05473</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05473</id><submitter>Kirk M. Soodhalter</submitter><version version="v1"><date>Wed, 11 Nov 2020 00:16:45 GMT</date><size>4215kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 13:32:21 GMT</date><size>5603kb</size><source_type>D</source_type></version><title>Subspace Recycling-based Regularization Methods</title><authors>Ronny Ramlau and Kirk M. Soodhalter and Victoria Hutterer</authors><categories>math.NA cs.NA</categories><comments>27 pages, 10 figures, in revision with journal</comments><msc-class>65F22, 65F50, 65J20, 65J22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace recycling techniques have been used quite successfully for the
acceleration of iterative methods for solving large-scale linear systems. These
methods often work by augmenting a solution subspace generated iteratively by a
known algorithm with a fixed subspace of vectors which are ``useful'' for
solving the problem. Often, this has the effect of inducing a projected version
of the original linear system to which the known iterative method is then
applied, and this projection can act as a deflation preconditioner,
accelerating convergence. Most often, these methods have been applied for the
solution of well-posed problems. However, they have also begun to be considered
for the solution of ill-posed problems.
  In this paper, we consider subspace augmentation-type iterative schemes
applied to linear ill-posed problems in a continuous Hilbert space setting,
based on a recently developed framework describing these methods. We show that
under suitable assumptions, a recycling method satisfies the formal definition
of a regularization, as long as the underlying scheme is itself a
regularization. We then develop an augmented subspace version of the gradient
descent method and demonstrate its effectiveness, both on an academic Gaussian
blur model and on problems arising from the adaptive optics community for the
resolution of large sky images by ground-based extremely large telescopes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.05733</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.05733</id><submitter>Yupan Liu</submitter><version version="v1"><date>Wed, 11 Nov 2020 12:30:42 GMT</date><size>36kb</size></version><version version="v2"><date>Mon, 17 May 2021 12:59:06 GMT</date><size>37kb</size></version><title>StoqMA meets distribution testing</title><authors>Yupan Liu</authors><categories>quant-ph cs.CC</categories><comments>24 pages. To appear in TQC 2021. v2: mostly adds corrections and
  clarifications</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  $\mathsf{StoqMA}$ captures the computational hardness of approximating the
ground energy of local Hamiltonians that do not suffer the so-called sign
problem. We provide a novel connection between $\mathsf{StoqMA}$ and
distribution testing via reversible circuits. First, we prove that easy-witness
$\mathsf{StoqMA}$ (viz. $\mathsf{eStoqMA}$, a sub-class of $\mathsf{StoqMA}$)
is contained in $\mathsf{MA}$. Easy witness is a generalization of a subset
state such that the associated set's membership can be efficiently verifiable,
and all non-zero coordinates are not necessarily uniform. This sub-class
$\mathsf{eStoqMA}$ contains $\mathsf{StoqMA}$ with perfect completeness
($\mathsf{StoqMA}_1$), which further signifies a simplified proof for
$\mathsf{StoqMA}_1 \subseteq \mathsf{MA}$ [BBT06, BT10]. Second, by showing
distinguishing reversible circuits with ancillary random bits is
$\mathsf{StoqMA}$-complete (as a comparison, distinguishing quantum circuits is
$\mathsf{QMA}$-complete [JWB05]), we construct soundness error reduction of
$\mathsf{StoqMA}$. Additionally, we show that both variants of
$\mathsf{StoqMA}$ that without any ancillary random bit and with perfect
soundness are contained in $\mathsf{NP}$. Our results make a step towards
collapsing the hierarchy $\mathsf{MA} \subseteq \mathsf{StoqMA} \subseteq
\mathsf{SBP}$ [BBT06], in which all classes are contained in $\mathsf{AM}$ and
collapse to $\mathsf{NP}$ under derandomization assumptions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.06878</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.06878</id><submitter>Giulia Cisotto</submitter><version version="v1"><date>Fri, 13 Nov 2020 12:26:54 GMT</date><size>252kb</size><source_type>D</source_type></version><title>REPAC: Reliable estimation of phase-amplitude coupling in brain networks</title><authors>Giulia Cisotto</authors><categories>eess.SP cs.CV cs.LG q-bio.NC q-bio.QM</categories><journal-ref>2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP39728.2021.9414749</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent evidence has revealed cross-frequency coupling and, particularly,
phase-amplitude coupling (PAC) as an important strategy for the brain to
accomplish a variety of high-level cognitive and sensory functions. However,
decoding PAC is still challenging. This contribution presents REPAC, a reliable
and robust algorithm for modeling and detecting PAC events in EEG signals.
First, we explain the synthesis of PAC-like EEG signals, with special attention
to the most critical parameters that characterize PAC, i.e., SNR, modulation
index, duration of coupling. Second, REPAC is introduced in detail. We use
computer simulations to generate a set of random PAC-like EEG signals and test
the performance of REPAC with regard to a baseline method. REPAC is shown to
outperform the baseline method even with realistic values of SNR, e.g., -10 dB.
They both reach accuracy levels around 99%, but REPAC leads to a significant
improvement of sensitivity, from 20.11% to 65.21%, with comparable specificity
(around 99%). REPAC is also applied to a real EEG signal showing preliminary
encouraging results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.07738</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.07738</id><submitter>Akshay Mete</submitter><version version="v1"><date>Mon, 16 Nov 2020 06:09:56 GMT</date><size>26kb</size></version><version version="v2"><date>Sun, 22 Nov 2020 04:24:38 GMT</date><size>26kb</size></version><version version="v3"><date>Sat, 15 May 2021 20:47:58 GMT</date><size>2189kb</size><source_type>D</source_type></version><title>Reward Biased Maximum Likelihood Estimation for Reinforcement Learning</title><authors>Akshay Mete, Rahul Singh, Xi Liu and P. R. Kumar</authors><categories>cs.LG cs.SY eess.SY</categories><comments>3rd Annual Learning for Dynamics &amp; Control Conference (L4DC 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Reward-Biased Maximum Likelihood Estimate (RBMLE) for adaptive control of
Markov chains was proposed to overcome the central obstacle of what is
variously called the fundamental &quot;closed-identifiability problem&quot; of adaptive
control, the &quot;dual control problem&quot;, or, contemporaneously, the &quot;exploration
vs. exploitation problem&quot;. It exploited the key observation that since the
maximum likelihood parameter estimator can asymptotically identify the
closed-transition probabilities under a certainty equivalent approach, the
limiting parameter estimates must necessarily have an optimal reward that is
less than the optimal reward attainable for the true but unknown system. Hence
it proposed a counteracting reverse bias in favor of parameters with larger
optimal rewards, providing a solution to the fundamental problem alluded to
above. It thereby proposed an optimistic approach of favoring parameters with
larger optimal rewards, now known as &quot;optimism in the face of uncertainty&quot;. The
RBMLE approach has been proved to be long-term average reward optimal in a
variety of contexts. However, modern attention is focused on the much finer
notion of &quot;regret&quot;, or finite-time performance. Recent analysis of RBMLE for
multi-armed stochastic bandits and linear contextual bandits has shown that it
not only has state-of-the-art regret, but it also exhibits empirical
performance comparable to or better than the best current contenders, and leads
to strikingly simple index policies. Motivated by this, we examine the
finite-time performance of RBMLE for reinforcement learning tasks that involve
the general problem of optimal control of unknown Markov Decision Processes. We
show that it has a regret of $\mathcal{O}( \log T)$ over a time horizon of $T$
steps, similar to state-of-the-art algorithms. Simulation studies show that
RBMLE outperforms other algorithms such as UCRL2 and Thompson Sampling.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.08332</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.08332</id><submitter>Yang Jiao</submitter><version version="v1"><date>Mon, 16 Nov 2020 23:28:22 GMT</date><size>3036kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 29 Mar 2021 16:23:55 GMT</date><size>3078kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 03:45:42 GMT</date><size>3079kb</size><source_type>D</source_type></version><title>EffiScene: Efficient Per-Pixel Rigidity Inference for Unsupervised Joint
  Learning of Optical Flow, Depth, Camera Pose and Motion Segmentation</title><authors>Yang Jiao, Trac D. Tran and Guangming Shi</authors><categories>cs.CV</categories><comments>Accpeted by IEEE Conf. on Computer Vision and Pattern Recognition
  (CVPR) 2021; v1 - original submit v2 - camera ready version v3 - correct
  small bugs in Equation(2)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the challenging unsupervised scene flow estimation
problem by jointly learning four low-level vision sub-tasks: optical flow
$\textbf{F}$, stereo-depth $\textbf{D}$, camera pose $\textbf{P}$ and motion
segmentation $\textbf{S}$. Our key insight is that the rigidity of the scene
shares the same inherent geometrical structure with object movements and scene
depth. Hence, rigidity from $\textbf{S}$ can be inferred by jointly coupling
$\textbf{F}$, $\textbf{D}$ and $\textbf{P}$ to achieve more robust estimation.
To this end, we propose a novel scene flow framework named EffiScene with
efficient joint rigidity learning, going beyond the existing pipeline with
independent auxiliary structures. In EffiScene, we first estimate optical flow
and depth at the coarse level and then compute camera pose by
Perspective-$n$-Points method. To jointly learn local rigidity, we design a
novel Rigidity From Motion (RfM) layer with three principal components:
\emph{}{(i)} correlation extraction; \emph{}{(ii)} boundary learning; and
\emph{}{(iii)} outlier exclusion. Final outputs are fused based on the rigid
map $M_R$ from RfM at finer levels. To efficiently train EffiScene, two new
losses $\mathcal{L}_{bnd}$ and $\mathcal{L}_{unc}$ are designed to prevent
trivial solutions and to regularize the flow boundary discontinuity. Extensive
experiments on scene flow benchmark KITTI show that our method is effective and
significantly improves the state-of-the-art approaches for all sub-tasks, i.e.
optical flow ($5.19 \rightarrow 4.20$), depth estimation ($3.78 \rightarrow
3.46$), visual odometry ($0.012 \rightarrow 0.011$) and motion segmentation
($0.57 \rightarrow 0.62$).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.08472</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.08472</id><submitter>Amr Alanwar</submitter><version version="v1"><date>Tue, 17 Nov 2020 06:48:23 GMT</date><size>603kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 09:07:54 GMT</date><size>760kb</size><source_type>D</source_type></version><title>Data-Driven Reachability Analysis Using Matrix Zonotopes</title><authors>Amr Alanwar, Anne Koch, Frank Allg\&quot;ower, Karl Henrik Johansson</authors><categories>eess.SY cs.LG cs.SY</categories><comments>Accepted at 3rd Annual Learning for Dynamics &amp; Control Conference
  (L4DC), 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a data-driven reachability analysis approach for
unknown system dynamics. Reachability analysis is an essential tool for
guaranteeing safety properties. However, most current reachability analysis
heavily relies on the existence of a suitable system model, which is often not
directly available in practice. We instead propose a data-driven reachability
analysis approach from noisy data. More specifically, we first provide an
algorithm for over-approximating the reachable set of a linear time-invariant
system using matrix zonotopes. Then we introduce an extension for Lipschitz
nonlinear systems. We provide theoretical guarantees in both cases. Numerical
examples show the potential and applicability of the introduced methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.08517</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.08517</id><submitter>Frederik Hagelskjaer</submitter><version version="v1"><date>Tue, 17 Nov 2020 09:12:11 GMT</date><size>8926kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 13:59:33 GMT</date><size>10865kb</size><source_type>D</source_type></version><title>Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based
  Domain Randomization</title><authors>Frederik Hagelskjaer and Anders Glent Buch</authors><categories>cs.CV</categories><comments>10 pages, 5 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the introduction of modern deep learning methods for object pose
estimation, test accuracy and efficiency has increased significantly. For
training, however, large amounts of annotated training data are required for
good performance. While the use of synthetic training data prevents the need
for manual annotation, there is currently a large performance gap between
methods trained on real and synthetic data. This paper introduces a new method,
which bridges this gap.
  Most methods trained on synthetic data use 2D images, as domain randomization
in 2D is more developed. To obtain precise poses, many of these methods perform
a final refinement using 3D data. Our method integrates the 3D data into the
network to increase the accuracy of the pose estimation. To allow for domain
randomization in 3D, a sensor-based data augmentation has been developed.
Additionally, we introduce the SparseEdge feature, which uses a wider search
space during point cloud propagation to avoid relying on specific features
without increasing run-time.
  Experiments on three large pose estimation benchmarks show that the presented
method outperforms previous methods trained on synthetic data and achieves
comparable results to existing methods trained on real data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.08724</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.08724</id><submitter>Yu Yan</submitter><version version="v1"><date>Tue, 17 Nov 2020 15:54:18 GMT</date><size>509kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 07:36:43 GMT</date><size>989kb</size></version><title>Multi-SQL: An extensible multi-model data query language</title><authors>Yu Yan, Nan Jiang, Hongzhi Wang, Yutong Wang, Chang Liu, Yuzhuo Wang</authors><categories>cs.DB</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Big data management aims to establish data hubs that support data in multiple
models and types in an all-around way. Thus, the multi-model database system is
a promising architecture for building such a multi-model data store. For an
integrated data hub, a unified and flexible query language is incredibly
necessary. In this paper, an extensible and practical query language--Multi-SQL
is proposed to realize the unified management of multi-model data considering
the co-processing of multi-model data. To the best of our knowledge, Multi-SQL
is the first query language based on various data models. Multi-SQL can also be
expanded to suit more complicated scenarios as it is flexible to support other
data models. Moreover, we provide a formal semantic definition of the core
features of Multi-SQL, including the multi-model definition, multi-model
filters, multi-model joins, etc. Furthermore, we propose a two-level query
implementation method to totally exploit the existing query optimization
capabilities of the underlying engines which could largely improve the query
excution efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.08742</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.08742</id><submitter>Ra\'ul Pardo</submitter><version version="v1"><date>Tue, 17 Nov 2020 16:19:43 GMT</date><size>894kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 10 Dec 2020 17:40:32 GMT</date><size>828kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 4 Apr 2021 13:37:46 GMT</date><size>1142kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 15 May 2021 16:34:25 GMT</date><size>1203kb</size><source_type>D</source_type></version><title>Privug: Quantifying Leakage using Probabilistic Programming for Privacy
  Risk Analysis</title><authors>Ra\'ul Pardo, Willard Rafnsson, Christian Probst, Andrzej W\k{a}sowski</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disclosure of data analytics results has important scientific and commercial
justifications. However, no data shall be disclosed without a diligent
investigation of risks for privacy of subjects. Privug is a tool-supported
method to explore information leakage properties of data analytics and
anonymization programs. In Privug, we reinterpret a program probabilistically,
using off-the-shelf tools for Bayesian inference to perform
information-theoretic analysis of the information flow. For privacy
researchers, Privug provides a fast, lightweight way to experiment with privacy
protection measures and mechanisms. We show that Privug is accurate, scalable,
and applicable to a range of leakage analysis scenarios.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09144</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09144</id><submitter>Siamak Layeghy</submitter><version version="v1"><date>Wed, 18 Nov 2020 07:50:38 GMT</date><size>4552kb</size><source_type>D</source_type></version><title>NetFlow Datasets for Machine Learning-based Network Intrusion Detection
  Systems</title><authors>Mohanad Sarhan, Siamak Layeghy, Nour Moustafa, Marius Portmann</authors><categories>cs.NI</categories><journal-ref>BDTA 2020</journal-ref><doi>10.1007/978-3-030-72802-1_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning (ML)-based Network Intrusion Detection Systems (NIDSs) have
proven to become a reliable intelligence tool to protect networks against
cyberattacks. Network data features has a great impact on the performances of
ML-based NIDSs. However, evaluating ML models often are not reliable, as each
ML-enabled NIDS is trained and validated using different data features that may
do not contain security events. Therefore, a common ground feature set from
multiple datasets is required to evaluate an ML model's detection accuracy and
its ability to generalise across datasets. This paper presents NetFlow features
from four benchmark NIDS datasets known as UNSW-NB15, BoT-IoT, ToN-IoT, and
CSE-CIC-IDS2018 using their publicly available packet capture files. In a
real-world scenario, NetFlow features are relatively easier to extract from
network traffic compared to the complex features used in the original datasets,
as they are usually extracted from packet headers. The generated Netflow
datasets have been labelled for solving binary- and multiclass-based learning
challenges. Preliminary results indicate that NetFlow features lead to similar
binary-class results and lower multi-class classification results amongst the
four datasets compared to their respective original features datasets. The
NetFlow datasets are named NF-UNSW-NB15, NF-BoT-IoT, NF-ToN-IoT,
NF-CSE-CIC-IDS2018 and NF-UQ-NIDS are published at
http://staff.itee.uq.edu.au/marius/NIDS_datasets/ for research purposes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09161</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09161</id><submitter>Yuanjun Xiong</submitter><version version="v1"><date>Wed, 18 Nov 2020 09:00:44 GMT</date><size>2732kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 20 Nov 2020 17:00:29 GMT</date><size>2732kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 20:10:51 GMT</date><size>2356kb</size><source_type>D</source_type></version><title>Positive-Congruent Training: Towards Regression-Free Model Updates</title><authors>Sijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng
  Wang, Wei Xia, Stefano Soatto</authors><categories>cs.CV cs.LG</categories><comments>Accepted to CVPR 2021 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reducing inconsistencies in the behavior of different versions of an AI
system can be as important in practice as reducing its overall error. In image
classification, sample-wise inconsistencies appear as &quot;negative flips&quot;: A new
model incorrectly predicts the output for a test sample that was correctly
classified by the old (reference) model. Positive-congruent (PC) training aims
at reducing error rate while at the same time reducing negative flips, thus
maximizing congruency with the reference model only on positive predictions,
unlike model distillation. We propose a simple approach for PC training, Focal
Distillation, which enforces congruence with the reference model by giving more
weights to samples that were correctly classified. We also found that, if the
reference model itself can be chosen as an ensemble of multiple deep neural
networks, negative flips can be further reduced without affecting the new
model's accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.09772</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.09772</id><submitter>Daeun Song</submitter><version version="v1"><date>Thu, 19 Nov 2020 11:12:51 GMT</date><size>6482kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 21:23:48 GMT</date><size>6776kb</size><source_type>D</source_type></version><title>Solving Footstep Planning as a Feasibility Problem using L1-norm
  Minimization (Extended Version)</title><authors>Daeun Song, Pierre Fernbach, Thomas Flayols, Andrea Del Prete, Nicolas
  Mansard, Steve Tonneau, Young J. Kim</authors><categories>cs.RO</categories><comments>Extended version of the paper to be published in IEEE Robotics and
  Automation Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One challenge of legged locomotion on uneven terrains is to deal with both
the discrete problem of selecting a contact surface for each footstep and the
continuous problem of placing each footstep on the selected surface.
Consequently, footstep planning can be addressed with a Mixed Integer Program
(MIP), an elegant but computationally-demanding method, which can make it
unsuitable for online planning. We reformulate the MIP into a cardinality
problem, then approximate it as a computationally efficient l1-norm
minimisation, called SL1M. Moreover, we improve the performance and convergence
of SL1M by combining it with a sampling-based root trajectory planner to prune
irrelevant surface candidates. Our tests on the humanoid Talos in four
representative scenarios show that SL1M always converges faster than MIP. For
scenarios when the combinatorial complexity is small (&lt; 10 surfaces per step),
SL1M converges at least two times faster than MIP with no need for pruning. In
more complex cases, SL1M converges up to 100 times faster than MIP with the
help of pruning. Moreover, pruning can also improve the MIP computation time.
The versatility of the framework is shown with additional tests on the
quadruped robot ANYmal.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10243</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10243</id><submitter>Jia Peng</submitter><version version="v1"><date>Fri, 20 Nov 2020 07:26:02 GMT</date><size>3681kb</size></version><version version="v2"><date>Tue, 18 May 2021 11:48:57 GMT</date><size>6220kb</size></version><title>Point Spread Function Estimation for Wide Field Small Aperture
  Telescopes with Deep Neural Networks and Calibration Data</title><authors>Peng Jia, Xuebo Wu, Zhengyang Li, Bo Li, Weihua Wang, Qiang Liu, Adam
  Popowicz</authors><categories>astro-ph.IM astro-ph.GA astro-ph.SR cs.CV</categories><comments>Accepted by the MNRAS</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The point spread function (PSF) reflects states of a telescope and plays an
important role in development of data processing methods, such as PSF based
astrometry, photometry and image restoration. However, for wide field small
aperture telescopes (WFSATs), estimating PSF in any position of the whole field
of view is hard, because aberrations induced by the optical system are quite
complex and the signal to noise ratio of star images is often too low for PSF
estimation. In this paper, we further develop our deep neural network (DNN)
based PSF modelling method and show its applications in PSF estimation. During
the telescope alignment and testing stage, our method collects system
calibration data through modification of optical elements within engineering
tolerances (tilting and decentering). Then we use these data to train a DNN
(Tel--Net). After training, the Tel--Net can estimate PSF in any field of view
from several discretely sampled star images. We use both simulated and
experimental data to test performance of our method. The results show that the
Tel--Net can successfully reconstruct PSFs of WFSATs of any states and in any
positions of the FoV. Its results are significantly more precise than results
obtained by the compared classic method - Inverse Distance Weight (IDW)
interpolation. Our method provides foundations for developing of deep neural
network based data processing methods for WFSATs, which require strong prior
information of PSFs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10687</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10687</id><submitter>Gowri Somanath</submitter><version version="v1"><date>Sat, 21 Nov 2020 01:01:53 GMT</date><size>44966kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 26 Mar 2021 19:32:32 GMT</date><size>34980kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 14 May 2021 21:32:49 GMT</date><size>34979kb</size><source_type>D</source_type></version><title>HDR Environment Map Estimation for Real-Time Augmented Reality</title><authors>Gowri Somanath and Daniel Kurz</authors><categories>cs.CV cs.AI cs.GR cs.LG</categories><comments>Supplementary video at
  https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4
  Accepted to CVPR 2021</comments><acm-class>I.2.10; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10890</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10890</id><submitter>Tianrong Chen</submitter><version version="v1"><date>Sat, 21 Nov 2020 23:00:50 GMT</date><size>4512kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 18:42:42 GMT</date><size>2198kb</size><source_type>D</source_type></version><title>Large-Scale Multi-Agent Deep FBSDEs</title><authors>Tianrong Chen, Ziyi Wang, Ioannis Exarchos, Evangelos A. Theodorou</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a scalable deep learning framework for finding
Markovian Nash Equilibria in multi-agent stochastic games using fictitious
play. The motivation is inspired by theoretical analysis of Forward Backward
Stochastic Differential Equations (FBSDE) and their implementation in a deep
learning setting, which is the source of our algorithm's sample efficiency
improvement. By taking advantage of the permutation-invariant property of
agents in symmetric games, the scalability and performance is further enhanced
significantly. We showcase superior performance of our framework over the
state-of-the-art deep fictitious play algorithm on an inter-bank
lending/borrowing problem in terms of multiple metrics. More importantly, our
approach scales up to 3000 agents in simulation, a scale which, to the best of
our knowledge, represents a new state-of-the-art. We also demonstrate the
applicability of our framework in robotics on a belief space autonomous racing
problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.10896</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.10896</id><submitter>Masudul Quraishi</submitter><version version="v1"><date>Sun, 22 Nov 2020 00:25:55 GMT</date><size>3646kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 12 Feb 2021 04:27:01 GMT</date><size>4192kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 00:56:23 GMT</date><size>3393kb</size><source_type>D</source_type></version><title>HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for
  Enabling Hardware-agnostic Programming with True Performance Portability for
  Heterogeneous HPC</title><authors>Michael Riera, Erfan Bank Tavakoli, Masudul Hassan Quraishi, Fengbo
  Ren</authors><categories>cs.DC cs.CL cs.PF</categories><comments>13 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper presents HALO 1.0, an open-ended extensible multi-agent software
framework that implements a set of proposed hardware-agnostic accelerator
orchestration (HALO) principles. HALO implements a novel compute-centric
message passing interface (C^2MPI) specification for enabling the
performance-portable execution of a hardware-agnostic host application across
heterogeneous accelerators. The experiment results of evaluating eight widely
used HPC subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs,
and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified
control flow for host programs to run across all the computing devices with a
consistently top performance portability score, which is up to five orders of
magnitude higher than the OpenCL-based solution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.11021</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.11021</id><submitter>Adem Kaya</submitter><version version="v1"><date>Sun, 22 Nov 2020 14:15:32 GMT</date><size>2603kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 23 Feb 2021 20:58:54 GMT</date><size>9127kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 00:05:09 GMT</date><size>12882kb</size><source_type>D</source_type></version><title>Application of adapted-bubbles to the Helmholtz equation with large wave
  numbers in 2d</title><authors>Adem Kaya</authors><categories>math.NA cs.NA</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  An adapted bubble approach which is a modifiation of the residual-free
bubbles (RFB) method, is proposed for the Helmhotz problem in 2D. A new
two-level finite element method is introduced for the approximations of the
bubble functions. Unlike the other equations such as the advection-diffusion
equation, RFB method when applied to the Helmholtz equation, does not depend on
another stabilized method to obtain approximations to the solutions of the
sub-problems. Adapted bubbles (AB) are obtained by a simple modification of the
sub-problems. This modification increases the accuracy of the numerical
solution impressively. The AB method is able to solve the Helmholtz equation
efficiently in 2D up to ch = 3.5 where c is the wave number and h is the mesh
size. We provide analysis to show how the AB method mitigates the pollution
error.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.12600</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.12600</id><submitter>Jean-Simon Lemay</submitter><version version="v1"><date>Wed, 25 Nov 2020 09:22:02 GMT</date><size>36kb</size></version><version version="v2"><date>Fri, 14 May 2021 06:29:32 GMT</date><size>41kb</size></version><version version="v3"><date>Mon, 17 May 2021 07:46:14 GMT</date><size>41kb</size></version><title>Cartesian Difference Categories</title><authors>Mario Alvarez-Picallo and Jean-Simon Pacaud Lemay</authors><categories>math.CT cs.LO</categories><comments>This paper is an extended version of a conference paper
  [arXiv:2002.01091] in Foundations of Software Science and Computation
  Structures: 23rd International Conference (FOSSACS 2020). This paper has been
  submitted to a special issue of Logical Methods in Computer Science (LMCS)
  devoted to FOSSACS 2020 papers</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cartesian differential categories are categories equipped with a differential
combinator which axiomatizes the directional derivative. Important models of
Cartesian differential categories include classical differential calculus of
smooth functions and categorical models of the differential $\lambda$-calculus.
However, Cartesian differential categories cannot account for other interesting
notions of differentiation of a more discrete nature such as the calculus of
finite differences. On the other hand, change action models have been shown to
capture these examples as well as more &quot;exotic&quot; examples of differentiation.
But change action models are very general and do not share the nice properties
of Cartesian differential categories. In this paper, we introduce Cartesian
difference categories as a bridge between Cartesian differential categories and
change action models. We show that every Cartesian differential category is a
Cartesian difference category, and how certain well-behaved change action
models are Cartesian difference categories. In particular, Cartesian difference
categories model both the differential calculus of smooth functions and the
calculus of finite differences. Furthermore, every Cartesian difference
category comes equipped with a tangent bundle monad whose Kleisli category is
again a Cartesian difference category.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.12672</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.12672</id><submitter>Mattia Segu</submitter><version version="v1"><date>Wed, 25 Nov 2020 12:02:57 GMT</date><size>2390kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 26 Nov 2020 17:05:19 GMT</date><size>2390kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 09:58:12 GMT</date><size>2388kb</size><source_type>D</source_type></version><title>Batch Normalization Embeddings for Deep Domain Generalization</title><authors>Mattia Segu, Alessio Tonioni, Federico Tombari</authors><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain generalization aims at training machine learning models to perform
robustly across different and unseen domains. Several recent methods use
multiple datasets to train models to extract domain-invariant features, hoping
to generalize to unseen domains. Instead, first we explicitly train
domain-dependant representations by using ad-hoc batch normalization layers to
collect independent domain's statistics. Then, we propose to use these
statistics to map domains in a shared latent space, where membership to a
domain can be measured by means of a distance function. At test time, we
project samples from an unknown domain into the same space and infer properties
of their domain as a linear combination of the known ones. We apply the same
mapping strategy at training and test time, learning both a latent
representation and a powerful but lightweight ensemble model. We show a
significant increase in classification accuracy over current state-of-the-art
techniques on popular domain generalization benchmarks: PACS, Office-31 and
Office-Caltech.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.12745</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.12745</id><submitter>Yue Qian</submitter><version version="v1"><date>Wed, 25 Nov 2020 14:00:18 GMT</date><size>10259kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 07:44:11 GMT</date><size>7196kb</size><source_type>D</source_type></version><title>Deep Magnification-Flexible Upsampling over 3D Point Clouds</title><authors>Yue Qian, Junhui Hou, Sam Kwong and Ying He</authors><categories>cs.CV</categories><comments>13 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the problem of generating dense point clouds from given
sparse point clouds to model the underlying geometric structures of
objects/scenes. To tackle this challenging issue, we propose a novel end-to-end
learning-based framework. Specifically, by taking advantage of the linear
approximation theorem, we first formulate the problem explicitly, which boils
down to determining the interpolation weights and high-order approximation
errors. Then, we design a lightweight neural network to adaptively learn
unified and sorted interpolation weights as well as the high-order refinements,
by analyzing the local geometry of the input point cloud. The proposed method
can be interpreted by the explicit formulation, and thus is more
memory-efficient than existing ones. In sharp contrast to the existing methods
that work only for a pre-defined and fixed upsampling factor, the proposed
framework only requires a single neural network with one-time training to
handle various upsampling factors, which is highly desired in real-world
applications. In addition, we propose a simple yet effective training strategy
to drive such a flexible ability. In addition, our method can handle
non-uniformly distributed and noisy data well. Extensive experiments on both
synthetic and real-world data demonstrate the superiority of the proposed
method over state-of-the-art methods both quantitatively and qualitatively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.13171</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.13171</id><submitter>Dexter Kozen</submitter><version version="v1"><date>Thu, 26 Nov 2020 08:14:57 GMT</date><size>27kb</size></version><version version="v2"><date>Fri, 14 May 2021 19:36:56 GMT</date><size>27kb</size></version><title>Universal Semantics for the Stochastic Lambda-Calculus</title><authors>Pedro Amorim and Dexter Kozen and Radu Mardare and Prakash Panangaden
  and Michael Roberts</authors><categories>cs.LO</categories><comments>14 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We define sound and adequate denotational and operational semantics for the
stochastic lambda calculus. These two semantic approaches build on previous
work that used similar techniques to reason about higher-order probabilistic
programs, but for the first time admit an adequacy theorem relating the
operational and denotational views. This resolves the main issue left open in
(Bacci et al. 2018).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.13388</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.13388</id><submitter>Mattia Segu</submitter><version version="v1"><date>Thu, 26 Nov 2020 16:59:12 GMT</date><size>14521kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 30 Nov 2020 08:45:55 GMT</date><size>14521kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 14 Jan 2021 09:29:34 GMT</date><size>14521kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 18 May 2021 09:17:13 GMT</date><size>14374kb</size><source_type>D</source_type></version><title>3DSNet: Unsupervised Shape-to-Shape 3D Style Transfer</title><authors>Mattia Segu, Margarita Grinvald, Roland Siegwart, Federico Tombari</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transferring the style from one image onto another is a popular and widely
studied task in computer vision. Yet, style transfer in the 3D setting remains
a largely unexplored problem. To our knowledge, we propose the first
learning-based approach for style transfer between 3D objects based on
disentangled content and style representations. The proposed method can
synthesize new 3D shapes both in the form of point clouds and meshes, combining
the content and style of a source and target 3D model to generate a novel shape
that resembles in style the target while retaining the source content.
Furthermore, we extend our technique to implicitly learn the multimodal style
distribution of the chosen domains. By sampling style codes from the learned
distributions, we increase the variety of styles that our model can confer to
an input shape. Experimental results validate the effectiveness of the proposed
3D style transfer method on a number of benchmarks. The implementation of our
framework will be released upon acceptance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2011.14958</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2011.14958</id><submitter>Mohammad Reza Jafari Harandi</submitter><version version="v1"><date>Mon, 30 Nov 2020 16:33:26 GMT</date><size>157kb</size></version><version version="v2"><date>Mon, 17 May 2021 06:28:05 GMT</date><size>150kb</size></version><title>On the Matching Equations of Kinetic Energy Shaping in IDA-PBC</title><authors>M. Reza J. Harandi and Hamid D. Taghirad</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnection and damping assignment passivity-based control scheme has
been used to stabilize many physical systems such as underactuated mechanical
systems through total energy shaping. In this method, some partial differential
equations (PDEs) arisen by kinetic and potential energy shaping, shall be
solved analytically. Finding a suitable desired inertia matrix as the solution
of nonlinear PDEs related to kinetic energy shaping is a challenging problem.
  In this paper, a systematic approach to solve this matching equation for
systems with one degree of underactuation is proposed. A special structure for
desired inertia matrix is proposed to simplify the solution of the
corresponding PDE. It is shown that the proposed method is more general than
that of some reported methods in the literature. In order to derive a suitable
desired inertia matrix, a necessary condition is also derived. The proposed
method is applied to three examples, including VTOL aircraft, pendubot and 2D
SpiderCrane system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00685</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00685</id><submitter>Dario Marvin</submitter><version version="v1"><date>Wed, 14 Oct 2020 09:35:48 GMT</date><size>3250kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 09:45:11 GMT</date><size>5410kb</size><source_type>D</source_type></version><title>A data-driven approach to the forecasting of ground-level ozone
  concentration</title><authors>Dario Marvin, Lorenzo Nespoli, Davide Strepparava and Vasco Medici</authors><categories>physics.ao-ph cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to forecast the concentration of air pollutants in an urban
region is crucial for decision-makers wishing to reduce the impact of pollution
on public health through active measures (e.g. temporary traffic closures). In
this study, we present a machine learning approach applied to the forecast of
the day-ahead maximum value of the ozone concentration for several geographical
locations in southern Switzerland. Due to the low density of measurement
stations and to the complex orography of the use case terrain, we adopted
feature selection methods instead of explicitly restricting relevant features
to a neighbourhood of the prediction sites, as common in spatio-temporal
forecasting methods. We then used Shapley values to assess the explainability
of the learned models in terms of feature importance and feature interactions
in relation to ozone predictions; our analysis suggests that the trained models
effectively learned explanatory cross-dependencies among atmospheric variables.
Finally, we show how weighting observations helps in increasing the accuracy of
the forecasts for specific ranges of ozone's daily peak values.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.00704</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.00704</id><submitter>Pingan Cheng</submitter><version version="v1"><date>Tue, 1 Dec 2020 18:10:12 GMT</date><size>865kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 2 Dec 2020 22:14:21 GMT</date><size>864kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 12:55:52 GMT</date><size>879kb</size><source_type>D</source_type></version><title>Lower Bounds for Semialgebraic Range Searching and Stabbing Problems</title><authors>Peyman Afshani, Pingan Cheng</authors><categories>cs.CG</categories><comments>Accepted by SoCG'21 (Best Paper); Submitted to JACM; improved some
  results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the semialgebraic range searching problem, we are to preprocess $n$ points
in $\mathbb{R}^d$ s.t. for any query range from a family of constant complexity
semialgebraic sets, all the points intersecting the range can be reported or
counted efficiently. When the ranges are composed of simplices, the problem can
be solved using $S(n)$ space and with $Q(n)$ query time with $S(n)Q^d(n) =
\tilde{O}(n^d)$ and this trade-off is almost tight. Consequently, there exists
low space structures that use $\tilde{O}(n)$ space with $O(n^{1-1/d})$ query
time and fast query structures that use $O(n^d)$ space with $O(\log^{d} n)$
query time. However, for the general semialgebraic ranges, only low space
solutions are known, but the best solutions match the same trade-off curve as
the simplex queries. It has been conjectured that the same could be done for
the fast query case but this open problem has stayed unresolved.
  Here, we disprove this conjecture. We give the first nontrivial lower bounds
for semilagebraic range searching and related problems. We show that any data
structure for reporting the points between two concentric circles with $Q(n)$
query time must use $S(n)=\Omega(n^{3-o(1)}/Q(n)^5)$ space, meaning, for
$Q(n)=O(\log^{O(1)}n)$, $\Omega(n^{3-o(1)})$ space must be used. We also study
the problem of reporting the points between two polynomials of form
$Y=\sum_{i=0}^\Delta a_i X^i$ where $a_0, \cdots, a_\Delta$ are given at the
query time. We show $S(n)=\Omega(n^{\Delta+1-o(1)}/Q(n)^{\Delta^2+\Delta})$. So
for $Q(n)=O(\log^{O(1)}n)$, we must use $\Omega(n^{\Delta+1-o(1)})$ space. For
the dual semialgebraic stabbing problems, we show that in linear space, any
data structure that solves 2D ring stabbing must use $\Omega(n^{2/3})$ query
time. This almost matches the linearization upper bound. For general
semialgebraic slab stabbing problems, again, we show an almost tight lower
bounds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.01918</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.01918</id><submitter>Haijin Zeng</submitter><version version="v1"><date>Thu, 3 Dec 2020 13:57:00 GMT</date><size>11310kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 07:13:54 GMT</date><size>16019kb</size><source_type>D</source_type></version><title>Multi-mode Core Tensor Factorization based Low-Rankness and Its
  Applications to Tensor Completion</title><authors>Haijin Zeng</authors><categories>cs.CV cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank tensor completion has been widely used in computer vision and
machine learning. This paper develops a kind of multi-modal core tensor
factorization (MCTF) method together with a tensor low-rankness measure and a
better nonconvex relaxation form of it (NonMCTF). The proposed models encode
low-rank insights for general tensors provided by Tucker and T-SVD, and thus
are expected to simultaneously model spectral low-rankness in multiple
orientations and accurately restore the data of intrinsic low-rank structure
based on few observed entries. Furthermore, we study the MCTF and NonMCTF
regularization minimization problem, and design an effective BSUM algorithm to
solve them. This efficient solver can extend MCTF to various tasks, such as
tensor completion. A series of experiments, including hyperspectral image
(HSI), video and MRI completion, confirm the superior performance of the
proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.02670</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.02670</id><submitter>Dario Pasquini</submitter><version version="v1"><date>Fri, 4 Dec 2020 15:41:00 GMT</date><size>2050kb</size></version><version version="v2"><date>Thu, 21 Jan 2021 12:58:49 GMT</date><size>3300kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 14 May 2021 19:08:20 GMT</date><size>3882kb</size><source_type>D</source_type></version><title>Unleashing the Tiger: Inference Attacks on Split Learning</title><authors>Dario Pasquini, Giuseppe Ateniese and Massimo Bernaschi</authors><categories>cs.CR cs.LG</categories><comments>To appear in the proceedings of: ACM Conference on Computer and
  Communications Security 2021 (CCS21)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the security of Split Learning -- a novel collaborative
machine learning framework that enables peak performance by requiring minimal
resources consumption. In the present paper, we expose vulnerabilities of the
protocol and demonstrate its inherent insecurity by introducing general attack
strategies targeting the reconstruction of clients' private training sets. More
prominently, we show that a malicious server can actively hijack the learning
process of the distributed model and bring it into an insecure state that
enables inference attacks on clients' data. We implement different adaptations
of the attack and test them on various datasets as well as within realistic
threat scenarios. We demonstrate that our attack is able to overcome recently
proposed defensive techniques aimed at enhancing the security of the split
learning protocol. Finally, we also illustrate the protocol's insecurity
against malicious clients by extending previously devised attacks for Federated
Learning. To make our results reproducible, we made our code available at
https://github.com/pasquini-dario/SplitNN_FSHA.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03433</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03433</id><submitter>Jiansheng Fang</submitter><version version="v1"><date>Mon, 7 Dec 2020 03:05:47 GMT</date><size>1063kb</size><source_type>D</source_type></version><title>Probabilistic Latent Factor Model for Collaborative Filtering with
  Bayesian Inference</title><authors>Jiansheng Fang, Xiaoqing Zhang, Yan Hu, Yanwu Xu, Ming Yang, Jiang Liu</authors><categories>cs.IR</categories><comments>8 pages, 5 figures, ICPR2020 conference</comments><doi>10.1109/ICPR48806.2021.9412376</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Latent Factor Model (LFM) is one of the most successful methods for
Collaborative filtering (CF) in the recommendation system, in which both users
and items are projected into a joint latent factor space. Base on matrix
factorization applied usually in pattern recognition, LFM models user-item
interactions as inner products of factor vectors of user and item in that space
and can be efficiently solved by least square methods with optimal estimation.
However, such optimal estimation methods are prone to overfitting due to the
extreme sparsity of user-item interactions. In this paper, we propose a
Bayesian treatment for LFM, named Bayesian Latent Factor Model (BLFM). Based on
observed user-item interactions, we build a probabilistic factor model in which
the regularization is introduced via placing prior constraint on latent
factors, and the likelihood function is established over observations and
parameters. Then we draw samples of latent factors from the posterior
distribution with Variational Inference (VI) to predict expected value. We
further make an extension to BLFM, called BLFMBias, incorporating
user-dependent and item-dependent biases into the model for enhancing
performance. Extensive experiments on the movie rating dataset show the
effectiveness of our proposed models by compared with several strong baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.03619</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.03619</id><submitter>Satya Almasian</submitter><version version="v1"><date>Mon, 7 Dec 2020 12:09:37 GMT</date><size>7617kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 11:38:28 GMT</date><size>680kb</size><source_type>D</source_type></version><title>Structural Text Segmentation of Legal Documents</title><authors>Dennis Aumiller, Satya Almasian, Sebastian Lackner and Michael Gertz</authors><categories>cs.CL</categories><doi>10.1145/3462757.3466085</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The growing complexity of legal cases has lead to an increasing interest in
legal information retrieval systems that can effectively satisfy user-specific
information needs. However, such downstream systems typically require documents
to be properly formatted and segmented, which is often done with relatively
simple pre-processing steps, disregarding topical coherence of segments.
Systems generally rely on representations of individual sentences or
paragraphs, which may lack crucial context, or document-level representations,
which are too long for meaningful search results. To address this issue, we
propose a segmentation system that can predict topical coherence of sequential
text segments spanning several paragraphs, effectively segmenting a document
and providing a more balanced representation for downstream applications. We
build our model on top of popular transformer networks and formulate structural
text segmentation as topical change detection, by performing a series of
independent classifications that allow for efficient fine-tuning on
task-specific data. We crawl a novel dataset consisting of roughly $74,000$
online Terms-of-Service documents, including hierarchical topic annotations,
which we use for training. Results show that our proposed system significantly
outperforms baselines, and adapts well to structural peculiarities of legal
documents. We release both data and trained models to the research community
for future work.https://github.com/dennlinger/TopicalChange
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04211</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04211</id><submitter>Guangsheng Ma</submitter><version version="v1"><date>Tue, 8 Dec 2020 04:54:02 GMT</date><size>21kb</size></version><version version="v2"><date>Wed, 3 Feb 2021 13:57:41 GMT</date><size>97kb</size></version><version version="v3"><date>Wed, 7 Apr 2021 10:52:17 GMT</date><size>235kb</size></version><version version="v4"><date>Sun, 11 Apr 2021 10:43:30 GMT</date><size>235kb</size></version><version version="v5"><date>Tue, 18 May 2021 04:49:30 GMT</date><size>243kb</size></version><title>Quantum Fully Homomorphic Encryption with neither Clifford Gate
  Decomposition nor Real Representation</title><authors>Guangsheng Ma and Hongbo Li</authors><categories>quant-ph cs.CC cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a novel quantum fully homomorphic encryption (QFHE) scheme, which
allows to perform conditional rotations with the control bit in encrypted form.
In our scheme, any quantum circuit can be directly evaluated with no need to
decompose into Clifford/non-Clifford gates, nor to be transformed into real
representation. Our scheme is able to evaluate quantum Fourier transform
algorithm faster than previous QFHE schemes in the worst case.
  The security of our scheme relies on the hardness of the underlying quantum
capable FHE scheme, and the latter sets its security on the learning with
errors problem and the circular security assumption.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04358</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04358</id><submitter>Markus P\&quot;uschel</submitter><version version="v1"><date>Tue, 8 Dec 2020 11:09:29 GMT</date><size>2630kb</size><source_type>D</source_type></version><title>Discrete Signal Processing on Meet/Join Lattices</title><authors>Markus P\&quot;uschel, Bastian Seifert, Chris Wendler</authors><categories>cs.IT cs.SI eess.SP math.IT</categories><comments>13 pages, submitted for publication</comments><doi>10.1109/TSP.2021.3081036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lattice is a partially ordered set supporting a meet (or join) operation
that returns the largest lower bound (smallest upper bound) of two elements.
Just like graphs, lattices are a fundamental structure that occurs across
domains including social data analysis, natural language processing,
computational chemistry and biology, and database theory. In this paper we
introduce discrete-lattice signal processing (DLSP), an SP framework for data,
or signals, indexed by such lattices. We use the meet (or join) to define a
shift operation and derive associated notions of filtering, Fourier basis and
transform, and frequency response. We show that the spectrum of a lattice
signal inherits the lattice structure of the signal domain and derive a
sampling theorem. Finally, we show two prototypical applications: spectral
analysis of formal concept lattices in social science and sampling and Wiener
filtering of multiset lattices in combinatorial auctions. Formal concept
lattices are a compressed representation of relations between objects and
attributes. Since relations are equivalent to bipartite graphs and hypergraphs,
DLSP offers a form of Fourier analysis for these structures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.04764</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.04764</id><submitter>Mohammad Havaei</submitter><version version="v1"><date>Tue, 8 Dec 2020 22:10:04 GMT</date><size>32615kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 02:35:19 GMT</date><size>40280kb</size><source_type>D</source_type></version><title>Conditional Generation of Medical Images via Disentangled Adversarial
  Inference</title><authors>Mohammad Havaei, Ximeng Mao, Yiping Wang, Qicheng Lao</authors><categories>eess.IV cs.CV</categories><comments>Accepted by Medical Image Analysis</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Synthetic medical image generation has a huge potential for improving
healthcare through many applications, from data augmentation for training
machine learning systems to preserving patient privacy. Conditional Adversarial
Generative Networks (cGANs) use a conditioning factor to generate images and
have shown great success in recent years. Intuitively, the information in an
image can be divided into two parts: 1) content which is presented through the
conditioning vector and 2) style which is the undiscovered information missing
from the conditioning vector. Current practices in using cGANs for medical
image generation, only use a single variable for image generation (i.e.,
content) and therefore, do not provide much flexibility nor control over the
generated image. In this work we propose a methodology to learn from the image
itself, disentangled representations of style and content, and use this
information to impose control over the generation process. In this framework,
style is learned in a fully unsupervised manner, while content is learned
through both supervised learning (using the conditioning vector) and
unsupervised learning (with the inference mechanism). We undergo two novel
regularization steps to ensure content-style disentanglement. First, we
minimize the shared information between content and style by introducing a
novel application of the gradient reverse layer (GRL); second, we introduce a
self-supervised regularization method to further separate information in the
content and style variables. We show that in general, two latent variable
models achieve better performance and give more control over the generated
image. We also show that our proposed model (DRAI) achieves the best
disentanglement score and has the best overall performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.05745</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.05745</id><submitter>Andre Mastmeyer</submitter><version version="v1"><date>Thu, 10 Dec 2020 15:28:34 GMT</date><size>2281kb</size><source_type>D</source_type></version><title>3D Bounding Box Detection in Volumetric Medical Image Data: A Systematic
  Literature Review</title><authors>Daria Kern, Andre Mastmeyer</authors><categories>eess.IV cs.CV cs.LG</categories><comments>10 pages, 5 figures, 1 table</comments><journal-ref>IEEE ICIEA / JOIG 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses current methods and trends for 3D bounding box detection
in volumetric medical image data. For this purpose, an overview of relevant
papers from recent years is given. 2D and 3D implementations are discussed and
compared. Multiple identified approaches for localizing anatomical structures
are presented. The results show that most research recently focuses on Deep
Learning methods, such as Convolutional Neural Networks vs. methods with manual
feature engineering, e.g. Random-Regression-Forests. An overview of bounding
box detection options is presented and helps researchers to select the most
promising approach for their target objects.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.06244</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.06244</id><submitter>Bohan Wang</submitter><version version="v1"><date>Fri, 11 Dec 2020 11:15:32 GMT</date><size>46kb</size></version><version version="v2"><date>Mon, 17 May 2021 02:38:47 GMT</date><size>992kb</size><source_type>D</source_type></version><title>The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous
  Neural Networks</title><authors>Bohan Wang, Qi Meng, Wei Chen</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite their overwhelming capacity to overfit, deep neural networks trained
by specific optimization algorithms tend to generalize well to unseen data.
Recently, researchers explained it by investigating the implicit regularization
effect of optimization algorithms. A remarkable progress is the work (Lyu&amp;Li,
2019), which proves gradient descent (GD) maximizes the margin of homogeneous
deep neural networks. Except GD, adaptive algorithms such as AdaGrad, RMSProp
and Adam are popular owing to their rapid training process. However,
theoretical guarantee for the generalization of adaptive optimization
algorithms is still lacking. In this paper, we study the implicit
regularization of adaptive optimization algorithms when they are optimizing the
logistic loss on homogeneous deep neural networks. We prove that adaptive
algorithms that adopt exponential moving average strategy in conditioner (such
as Adam and RMSProp) can maximize the margin of the neural network, while
AdaGrad that directly sums historical squared gradients in conditioner can not.
It indicates superiority on generalization of exponential moving average
strategy in the design of the conditioner. Technically, we provide a unified
framework to analyze convergent direction of adaptive optimization algorithms
by constructing novel adaptive gradient flow and surrogate margin. Our
experiments can well support the theoretical findings on convergent direction
of adaptive optimization algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.07162</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.07162</id><submitter>Chi Chen</submitter><version version="v1"><date>Sun, 13 Dec 2020 21:44:29 GMT</date><size>273kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 06:14:58 GMT</date><size>998kb</size><source_type>D</source_type></version><title>Mask-Align: Self-Supervised Neural Word Alignment</title><authors>Chi Chen, Maosong Sun, and Yang Liu</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word alignment, which aims to align translationally equivalent words between
source and target sentences, plays an important role in many natural language
processing tasks. Current unsupervised neural alignment methods focus on
inducing alignments from neural machine translation models, which does not
leverage the full context in the target sequence. In this paper, we propose
Mask-Align, a self-supervised word alignment model that takes advantage of the
full context on the target side. Our model masks out each target token and
predicts it conditioned on both source and the remaining target tokens. This
two-step process is based on the assumption that the source token contributing
most to recovering the masked target token should be aligned. We also introduce
an attention variant called leaky attention, which alleviates the problem of
unexpected high cross-attention weights on special tokens such as periods.
Experiments on four language pairs show that our model outperforms previous
unsupervised neural aligners and obtains new state-of-the-art results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.07291</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.07291</id><submitter>Yi Luo</submitter><version version="v1"><date>Mon, 14 Dec 2020 06:57:58 GMT</date><size>226kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 05:11:52 GMT</date><size>237kb</size><source_type>D</source_type></version><title>Group Communication with Context Codec for Lightweight Source Separation</title><authors>Yi Luo, Cong Han, Nima Mesgarani</authors><categories>eess.AS cs.AI cs.LG cs.SD</categories><comments>IEEE/ACM Transactions on Audio, Speech, and Language Processing
  (TASLP)</comments><doi>10.1109/TASLP.2021.3078640</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Despite the recent progress on neural network architectures for speech
separation, the balance between the model size, model complexity and model
performance is still an important and challenging problem for the deployment of
such models to low-resource platforms. In this paper, we propose two simple
modules, group communication and context codec, that can be easily applied to a
wide range of architectures to jointly decrease the model size and complexity
without sacrificing the performance. A group communication module splits a
high-dimensional feature into groups of low-dimensional features and captures
the inter-group dependency. A separation module with a significantly smaller
model size can then be shared by all the groups. A context codec module,
containing a context encoder and a context decoder, is designed as a learnable
downsampling and upsampling module to decrease the length of a sequential
feature processed by the separation module. The combination of the group
communication and the context codec modules is referred to as the GC3 design.
Experimental results show that applying GC3 on multiple network architectures
for speech separation can achieve on-par or better performance with as small as
2.5% model size and 17.6% model complexity, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.07580</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.07580</id><submitter>Zied Bouraoui</submitter><version version="v1"><date>Fri, 4 Dec 2020 14:03:03 GMT</date><size>1425kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 15:00:19 GMT</date><size>1664kb</size><source_type>D</source_type></version><title>Modelling General Properties of Nouns by Selectively Averaging
  Contextualised Embeddings</title><authors>Na Li, Zied Bouraoui, Jose Camacho Collados, Luis Espinosa-Anke, Qing
  Gu, Steven Schockaert</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the success of pre-trained language models has largely eliminated the
need for high-quality static word vectors in many NLP applications, such
vectors continue to play an important role in tasks where words need to be
modelled in the absence of linguistic context. In this paper, we explore how
the contextualised embeddings predicted by BERT can be used to produce
high-quality word vectors for such domains, in particular related to knowledge
base completion, where our focus is on capturing the semantic properties of
nouns. We find that a simple strategy of averaging the contextualised
embeddings of masked word mentions leads to vectors that outperform the static
word vectors learned by BERT, as well as those from standard word embedding
models, in property induction tasks. We notice in particular that masking
target words is critical to achieve this strong performance, as the resulting
vectors focus less on idiosyncratic properties and more on general semantic
properties. Inspired by this view, we propose a filtering strategy which is
aimed at removing the most idiosyncratic mention vectors, allowing us to obtain
further performance gains in property induction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.08213</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.08213</id><submitter>Hiroaki Nishikawa</submitter><version version="v1"><date>Tue, 15 Dec 2020 11:02:11 GMT</date><size>7199kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 13:47:52 GMT</date><size>7672kb</size><source_type>D</source_type></version><title>Economically High-Order Unstructured-Grid Methods: Clarification and
  Efficient FSR Schemes</title><authors>Hiroaki Nishikawa</authors><categories>math.NA cs.NA physics.comp-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we clarify reconstruction-based discretization schemes for
unstructured grids and discuss their economically high-order versions, which
can achieve high-order accuracy under certain conditions at little extra cost.
The clarification leads to one of the most economical approaches: the
flux-and-solution-reconstruction (FSR) approach, where highly economical
schemes can be constructed based on an extended kappa-scheme combined with
economical flux reconstruction formulas, achieving up to fifth-order accuracy
(sixth-order with zero dissipation) when a grid is regular. Various economical
FSR schemes are presented and their formal orders of accuracy are verified by
numerical experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09289</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09289</id><submitter>Mauricio Delbracio</submitter><version version="v1"><date>Wed, 16 Dec 2020 22:13:03 GMT</date><size>27805kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 15:31:44 GMT</date><size>28244kb</size><source_type>D</source_type></version><title>Projected Distribution Loss for Image Enhancement</title><authors>Mauricio Delbracio, Hossein Talebi, Peyman Milanfar</authors><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Features obtained from object recognition CNNs have been widely used for
measuring perceptual similarities between images. Such differentiable metrics
can be used as perceptual learning losses to train image enhancement models.
However, the choice of the distance function between input and target features
may have a consequential impact on the performance of the trained model. While
using the norm of the difference between extracted features leads to limited
hallucination of details, measuring the distance between distributions of
features may generate more textures; yet also more unrealistic details and
artifacts. In this paper, we demonstrate that aggregating 1D-Wasserstein
distances between CNN activations is more reliable than the existing
approaches, and it can significantly improve the perceptual performance of
enhancement models. More explicitly, we show that in imaging applications such
as denoising, super-resolution, demosaicing, deblurring and JPEG artifact
removal, the proposed learning loss outperforms the current state-of-the-art on
reference-based perceptual losses. This means that the proposed learning loss
can be plugged into different imaging frameworks and produce perceptually
realistic results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09322</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09322</id><submitter>Mauricio Delbracio</submitter><version version="v1"><date>Wed, 16 Dec 2020 23:38:39 GMT</date><size>27983kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 16:38:48 GMT</date><size>41574kb</size><source_type>D</source_type></version><title>Polyblur: Removing mild blur by polynomial reblurring</title><authors>Mauricio Delbracio, Ignacio Garcia-Dorado, Sungjoon Choi, Damien
  Kelly, Peyman Milanfar</authors><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a highly efficient blind restoration method to remove mild blur in
natural images. Contrary to the mainstream, we focus on removing slight blur
that is often present, damaging image quality and commonly generated by small
out-of-focus, lens blur, or slight camera motion. The proposed algorithm first
estimates image blur and then compensates for it by combining multiple
applications of the estimated blur in a principled way. To estimate blur we
introduce a simple yet robust algorithm based on empirical observations about
the distribution of the gradient in sharp natural images. Our experiments show
that, in the context of mild blur, the proposed method outperforms traditional
and modern blind deblurring methods and runs in a fraction of the time. Our
method can be used to blindly correct blur before applying off-the-shelf deep
super-resolution methods leading to superior results than other highly complex
and computationally demanding techniques. The proposed method estimates and
removes mild blur from a 12MP image on a modern mobile phone in a fraction of a
second.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09590</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09590</id><submitter>Daniel Graziotin</submitter><version version="v1"><date>Wed, 16 Dec 2020 14:27:45 GMT</date><size>257kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 10 Feb 2021 12:52:32 GMT</date><size>189kb</size><source_type>D</source_type></version><title>The Mind Is a Powerful Place: How Showing Code Comprehensibility Metrics
  Influences Code Understanding</title><authors>Marvin Wyrich, Andreas Preikschat, Daniel Graziotin, Stefan Wagner</authors><categories>cs.SE cs.CY cs.PL</categories><comments>To appear in: Proceedings of the 43rd International Conference on
  Software Engineering (ICSE '21), Madrid, Spain, 12 pages. 12 pages, 1 figure.
  Postprint, after peer review</comments><journal-ref>2021 IEEE/ACM 43rd International Conference on Software
  Engineering (ICSE), 2021 pp. 512-523</journal-ref><doi>10.1109/ICSE43902.2021.00055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static code analysis tools and integrated development environments present
developers with quality-related software metrics, some of which describe the
understandability of source code. Software metrics influence overarching
strategic decisions that impact the future of companies and the prioritization
of everyday software development tasks. Several software metrics, however, lack
in validation: we just choose to trust that they reflect what they are supposed
to measure. Some of them were even shown to not measure the quality aspects
they intend to measure. Yet, they influence us through biases in our
cognitive-driven actions. In particular, they might anchor us in our decisions.
Whether the anchoring effect exists with software metrics has not been studied
yet. We conducted a randomized and double-blind experiment to investigate the
extent to which a displayed metric value for source code comprehensibility
anchors developers in their subjective rating of source code comprehensibility,
whether performance is affected by the anchoring effect when working on
comprehension tasks, and which individual characteristics might play a role in
the anchoring effect. We found that the displayed value of a comprehensibility
metric has a significant and large anchoring effect on a developer's code
comprehensibility rating. The effect does not seem to affect the time or
correctness when working on comprehension questions related to the code
snippets under study. Since the anchoring effect is one of the most robust
cognitive biases, and we have limited understanding of the consequences of the
demonstrated manipulation of developers by non-validated metrics, we call for
an increased awareness of the responsibility in code quality reporting and for
corresponding tools to be based on scientific evidence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.09688</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.09688</id><submitter>Meng-Hao Guo</submitter><version version="v1"><date>Thu, 17 Dec 2020 15:55:17 GMT</date><size>4246kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 11 Mar 2021 11:12:08 GMT</date><size>2454kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 27 Mar 2021 04:36:37 GMT</date><size>2454kb</size><source_type>D</source_type></version><title>Pct: Point cloud transformer</title><authors>Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R.
  Martin and Shi-Min Hu</authors><categories>cs.CV</categories><comments>11 pages, 5 figures</comments><journal-ref>Computational Visual Media, 2021, Vol. 7, No. 2, Pages: 187 - 199</journal-ref><doi>10.1007/s41095-021-0229-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The irregular domain and lack of ordering make it challenging to design deep
neural networks for point cloud processing. This paper presents a novel
framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is
based on Transformer, which achieves huge success in natural language
processing and displays great potential in image processing. It is inherently
permutation invariant for processing a sequence of points, making it
well-suited for point cloud learning. To better capture local context within
the point cloud, we enhance input embedding with the support of farthest point
sampling and nearest neighbor search. Extensive experiments demonstrate that
the PCT achieves the state-of-the-art performance on shape classification, part
segmentation and normal estimation tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.10294</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.10294</id><submitter>Martin Dyrba</submitter><version version="v1"><date>Fri, 18 Dec 2020 15:16:50 GMT</date><size>1378kb</size></version><version version="v2"><date>Mon, 1 Mar 2021 17:57:38 GMT</date><size>1718kb</size></version><version version="v3"><date>Wed, 3 Mar 2021 16:52:24 GMT</date><size>1730kb</size></version><version version="v4"><date>Mon, 17 May 2021 13:41:03 GMT</date><size>2348kb</size></version><title>Improving 3D convolutional neural network comprehensibility via
  interactive visualization of relevance maps: Evaluation in Alzheimer's
  disease</title><authors>Martin Dyrba, Moritz Hanzig, Slawek Altenstein, Sebastian Bader,
  Tommaso Ballarini, Frederic Brosseron, Katharina Buerger, Daniel Cantr\'e,
  Peter Dechent, Laura Dobisch, Emrah D\&quot;uzel, Michael Ewers, Klaus Fliessbach,
  Wenzel Glanz, John D. Haynes, Michael T. Heneka, Daniel Janowitz, Deniz Baris
  Keles, Ingo Kilimann, Christoph Laske, Franziska Maier, Coraline D. Metzger,
  Matthias H. Munk, Robert Perneczky, Oliver Peters, Lukas Preis, Josef
  Priller, Boris Rauchmann, Nina Roy, Klaus Scheffler, Anja Schneider, Bj\&quot;orn
  H. Schott, Annika Spottke, Eike J. Spruth, Marc-Andr\'e Weber, Birgit
  Ertl-Wagner, Michael Wagner, Jens Wiltfang, Frank Jessen, Stefan J. Teipel</authors><categories>eess.IV cs.CV</categories><comments>24 pages, 9 figures/tables, supplementary material, source code
  available on GitHub</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Although convolutional neural networks (CNN) achieve high
diagnostic accuracy for detecting Alzheimer's disease (AD) dementia based on
magnetic resonance imaging (MRI) scans, they are not yet applied in clinical
routine. One important reason for this is a lack of model comprehensibility.
Recently developed visualization methods for deriving CNN relevance maps may
help to fill this gap. We investigated whether models with higher accuracy also
rely more on discriminative brain regions predefined by prior knowledge.
  Methods: We trained a CNN for the detection of AD in N=663 T1-weighted MRI
scans of patients with dementia and amnestic mild cognitive impairment (MCI)
and verified the accuracy of the models via cross-validation and in three
independent samples including N=1655 cases. We evaluated the association of
relevance scores and hippocampus volume to validate the clinical utility of
this approach. To improve model comprehensibility, we implemented an
interactive visualization of 3D CNN relevance maps.
  Results: Across three independent datasets, group separation showed high
accuracy for AD dementia vs. controls (AUC$\geq$0.92) and moderate accuracy for
MCI vs. controls (AUC$\approx$0.75). Relevance maps indicated that hippocampal
atrophy was considered as the most informative factor for AD detection, with
additional contributions from atrophy in other cortical and subcortical
regions. Relevance scores within the hippocampus were highly correlated with
hippocampal volumes (Pearson's r$\approx$-0.86, p&lt;0.001).
  Conclusion: The relevance maps highlighted atrophy in regions that we had
hypothesized a priori. This strengthens the comprehensibility of the CNN
models, which were trained in a purely data-driven manner based on the scans
and diagnosis labels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.11655</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.11655</id><submitter>Hyojin Park</submitter><version version="v1"><date>Mon, 21 Dec 2020 19:40:17 GMT</date><size>6401kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 4 Apr 2021 11:25:26 GMT</date><size>6401kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 11:54:06 GMT</date><size>6397kb</size><source_type>D</source_type></version><title>Learning Dynamic Network Using a Reuse Gate Function in Semi-supervised
  Video Object Segmentation</title><authors>Hyojin Park, Jayeon Yoo, Seohyeong Jeong, Ganesh Venkatesh, Nojun Kwak</authors><categories>cs.CV</categories><comments>CVPR2021, code: https://github.com/HYOJINPARK/Reuse_VOS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current state-of-the-art approaches for Semi-supervised Video Object
Segmentation (Semi-VOS) propagates information from previous frames to generate
segmentation mask for the current frame. This results in high-quality
segmentation across challenging scenarios such as changes in appearance and
occlusion. But it also leads to unnecessary computations for stationary or
slow-moving objects where the change across frames is minimal. In this work, we
exploit this observation by using temporal information to quickly identify
frames with minimal change and skip the heavyweight mask generation step. To
realize this efficiency, we propose a novel dynamic network that estimates
change across frames and decides which path -- computing a full network or
reusing previous frame's feature -- to choose depending on the expected
similarity. Experimental results show that our approach significantly improves
inference speed without much accuracy degradation on challenging Semi-VOS
datasets -- DAVIS 16, DAVIS 17, and YouTube-VOS. Furthermore, our approach can
be applied to multiple Semi-VOS methods demonstrating its generality. The code
is available in https://github.com/HYOJINPARK/Reuse_VOS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.12418</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.12418</id><submitter>Xingyi Yang</submitter><version version="v1"><date>Tue, 22 Dec 2020 23:48:42 GMT</date><size>1059kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 01:53:12 GMT</date><size>1060kb</size><source_type>D</source_type></version><title>Stochastic Gradient Variance Reduction by Solving a Filtering Problem</title><authors>Xingyi Yang</authors><categories>cs.LG cs.CV stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep neural networks (DNN) are typically optimized using stochastic gradient
descent (SGD). However, the estimation of the gradient using stochastic samples
tends to be noisy and unreliable, resulting in large gradient variance and bad
convergence. In this paper, we propose \textbf{Filter Gradient Decent}~(FGD),
an efficient stochastic optimization algorithm that makes the consistent
estimation of the local gradient by solving an adaptive filtering problem with
different design of filters. Our method reduces variance in stochastic gradient
descent by incorporating the historical states to enhance the current
estimation. It is able to correct noisy gradient direction as well as to
accelerate the convergence of learning. We demonstrate the effectiveness of the
proposed Filter Gradient Descent on numerical optimization and training neural
networks, where it achieves superior and robust performance compared with
traditional momentum-based methods. To the best of our knowledge, we are the
first to provide a practical solution that integrates filtering into gradient
estimation by making the analogy between gradient estimation and filtering
problems in signal processing. (The code is provided in
https://github.com/Adamdad/Filter-Gradient-Decent)
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15733</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15733</id><submitter>Sai Munikoti</submitter><version version="v1"><date>Sat, 26 Dec 2020 07:22:29 GMT</date><size>468kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 15:40:12 GMT</date><size>926kb</size><source_type>D</source_type></version><title>Bayesian Graph Neural Network for Fast identification of critical nodes
  in Uncertain Complex Networks</title><authors>Sai Munikoti, Laya Das and Balasubramaniam Natarajan</authors><categories>cs.SI cs.LG</categories><comments>6 pages, 2 figures, 3 Tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the quest to improve efficiency, interdependence and complexity are
becoming defining characteristics of modern complex networks representing
engineered and natural systems. Graph theory is a widely used framework for
modeling such complex networks and to evaluate their robustness to disruptions.
Particularly, identification of critical nodes/links in a graph can facilitate
the enhancement of graph (system) robustness and characterize crucial factors
of system performance. Most existing methods of critical node identification
are based on an iterative approach that explores each node/link of a graph.
These methods suffer from high computational complexity and the resulting
analysis is network specific. Additionally, uncertainty associated with the
underlying graphical model further limits the potential value of these
traditional approaches. To overcome these challenges, we propose a Bayesian
graph neural network based node classification framework that is
computationally efficient and systematically incorporates uncertainties.
Instead of utilizing the observed graph for training the model, a MAP estimate
of the graph is computed based on the observed topology and node target labels.
Further, a Monte-Carlo (MC) dropout algorithm is incorporated to account for
the epistemic uncertainty. The fidelity and the gain in computational
complexity offered by the Bayesian framework is illustrated using simulation
results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15762</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15762</id><submitter>Michael Whittaker</submitter><version version="v1"><date>Thu, 31 Dec 2020 17:38:39 GMT</date><size>193kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 1 Jan 2021 19:29:17 GMT</date><size>193kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 17:48:04 GMT</date><size>208kb</size><source_type>D</source_type></version><title>Scaling Replicated State Machines with Compartmentalization [Technical
  Report]</title><authors>Michael Whittaker, Ailidani Ailijiang, Aleksey Charapko, Murat
  Demirbas, Neil Giridharan, Joseph M. Hellerstein, Heidi Howard, Ion Stoica,
  Adriana Szekeres</authors><categories>cs.DC</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State machine replication protocols, like MultiPaxos and Raft, are a critical
component of many distributed systems and databases. However, these protocols
offer relatively low throughput due to several bottlenecked components.
Numerous existing protocols fix different bottlenecks in isolation but fall
short of a complete solution. When you fix one bottleneck, another arises. In
this paper, we introduce compartmentalization, the first comprehensive
technique to eliminate state machine replication bottlenecks.
Compartmentalization involves decoupling individual bottlenecks into distinct
components and scaling these components independently. Compartmentalization has
two key strengths. First, compartmentalization leads to strong performance. In
this paper, we demonstrate how to compartmentalize MultiPaxos to increase its
throughput by 6x on a write-only workload and 16x on a mixed read-write
workload. Unlike other approaches, we achieve this performance without the need
for specialized hardware. Second, compartmentalization is a technique, not a
protocol. Industry practitioners can apply compartmentalization to their
protocols incrementally without having to adopt a completely new protocol.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2012.15815</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2012.15815</id><submitter>Brett Lopez</submitter><version version="v1"><date>Thu, 31 Dec 2020 18:36:15 GMT</date><size>1292kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 9 Apr 2021 03:53:46 GMT</date><size>1302kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 15:12:23 GMT</date><size>1349kb</size><source_type>D</source_type></version><title>Universal Adaptive Control of Nonlinear Systems</title><authors>Brett T. Lopez and Jean-Jacques E. Slotine</authors><categories>eess.SY cs.RO cs.SY</categories><comments>Streamlined proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-performance feedback control requires an accurate model of the
underlying dynamical system which is often difficult, expensive, or
time-consuming to obtain. Online model learning is an attractive approach that
can handle model variations while achieving the desired level of performance.
However, most model learning methods developed within adaptive nonlinear
control are limited to certain types of uncertainties, called matched
uncertainties, because the certainty equivalency principle can be employed in
the design phase. This work develops a universal adaptive control framework
that extends the certainty equivalence principle to nonlinear systems with
unmatched uncertainties through two key innovations. The first is introducing
parameter-dependent storage functions that guarantee closed-loop tracking of a
desired trajectory generated by an adapting reference model. The second is
modulating the learning rate so the closed-loop system remains stable during
the learning transients. The analysis is first presented under the lens of
contraction theory, and then expanded to general Lyapunov functions which can
be synthesized via feedback linearization, backstepping, or optimization-based
techniques. The proposed approach is more general than existing methods as the
uncertainties can be unmatched and the system only needs to be stabilizable.
The developed algorithm can be combined with learned feedback policies,
facilitating transfer learning and bridging the sim-to-real gap. Simulation
results showcase the method
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00124</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00124</id><submitter>I-Hung Hsu</submitter><version version="v1"><date>Fri, 1 Jan 2021 00:52:53 GMT</date><size>7611kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 07:33:34 GMT</date><size>8133kb</size><source_type>D</source_type></version><title>MrGCN: Mirror Graph Convolution Network for Relation Extraction with
  Long-Term Dependencies</title><authors>I-Hung Hsu, Xiao Guo, Wael AbdAlmageed, Premkumar Natarajan, Nanyun
  Peng</authors><categories>cs.CL cs.AI cs.LG</categories><comments>13 pages, page 11-13 appendix, 7 figures. The first two authors
  contribute equally</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ability to capture complex linguistic structures and long-term
dependencies among words in the passage is essential for relation extraction
(RE) tasks. Graph neural networks (GNNs), one of the means to encode dependency
graphs, have been shown to be effective in prior works. However, relatively
little attention has been paid to receptive fields of GNNs, which can be
crucial for tasks with extremely long text that requires discourse
understanding. In this work, we leverage the idea of graph pooling and propose
the Mirror Graph Convolution Network, a GNN model with a pooling-unpooling
structure tailored to RE tasks. The pooling branch reduces the graph size and
enables the GNN to obtain larger receptive fields within fewer layers; the
unpooling branch restores the pooled graph to its original resolution for
token-level RE tasks. Experiments on two discourse-level relation extraction
datasets demonstrate the effectiveness of our method, showing significant
improvements over prior methods especially when modeling long-term dependencies
is necessary. Moreover, we propose Clause Matching (CM), a novel graph pooling
method that merges nodes based on dependency relations in graph. CM can largely
reduce the graph size while retaining the main semantics of the input text.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00157</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00157</id><submitter>Jing Lin</submitter><version version="v1"><date>Fri, 1 Jan 2021 03:43:36 GMT</date><size>1368kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 24 Mar 2021 01:07:29 GMT</date><size>1383kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 20:06:13 GMT</date><size>1378kb</size><source_type>D</source_type></version><title>Active Learning Under Malicious Mislabeling and Poisoning Attacks</title><authors>Jing Lin, Ryan Luley, and Kaiqi Xiong</authors><categories>cs.LG cs.CR stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Deep neural networks usually require large labeled datasets for training to
achieve the start-of-the-art performance in many tasks, such as image
classification and natural language processing. Though a lot of data is created
each day by active Internet users through various distributed systems across
the world, most of these data are unlabeled and are vulnerable to data
poisoning attacks. In this paper, we develop an efficient active learning
method that requires fewer labeled instances and incorporates the technique of
adversarial retraining in which additional labeled artificial data are
generated without increasing the labeling budget. The generated adversarial
examples also provide a way to measure the vulnerability of the model. To check
the performance of the proposed method under an adversarial setting, i.e.,
malicious mislabeling and data poisoning attacks, we perform an extensive
evaluation on the reduced CIFAR-10 dataset, which contains only two classes:
'airplane' and 'frog' by using the private cloud on campus. Our experimental
results demonstrate that the proposed active learning method is efficient for
defending against malicious mislabeling and data poisoning attacks.
Specifically, whereas the baseline active learning method based on the random
sampling strategy performs poorly (about 50%) under a malicious mislabeling
attack, the proposed active learning method can achieve the desired accuracy of
89% using only one-third of the dataset on average.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.00314</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.00314</id><submitter>Otmar Ertl</submitter><version version="v1"><date>Fri, 1 Jan 2021 20:14:33 GMT</date><size>11004kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 07:23:31 GMT</date><size>23450kb</size><source_type>D</source_type></version><title>SetSketch: Filling the Gap between MinHash and HyperLogLog</title><authors>Otmar Ertl</authors><categories>cs.DS cs.DB</categories><comments>extended version of major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MinHash and HyperLogLog are sketching algorithms that have become
indispensable for set summaries in big data applications. While HyperLogLog
allows counting different elements with very little space, MinHash is suitable
for the fast comparison of sets as it allows estimating the Jaccard similarity
and other joint quantities. This work presents a new data structure called
SetSketch that is able to continuously fill the gap between both use cases. Its
commutative and idempotent insert operation and its mergeable state make it
suitable for distributed environments. Fast, robust, and easy-to-implement
estimators for cardinality and joint quantities, as well as the ability to use
SetSketch for similarity search, enable versatile applications. The presented
joint estimator can also be applied to other data structures such as MinHash,
HyperLogLog, or HyperMinHash, where it even performs better than the
corresponding state-of-the-art estimators in many cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.02047</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.02047</id><submitter>Mohammad Mahmudul Alam</submitter><version version="v1"><date>Wed, 6 Jan 2021 14:05:13 GMT</date><size>22584kb</size></version><version version="v2"><date>Mon, 17 May 2021 23:53:55 GMT</date><size>22584kb</size></version><title>Unified Learning Approach for Egocentric Hand Gesture Recognition and
  Fingertip Detection</title><authors>Mohammad Mahmudul Alam, Mohammad Tariqul Islam and S. M. Mahbubur
  Rahman</authors><categories>cs.CV</categories><comments>19 pages, 7 figures, Manuscript Submitted to the Pattern Recognition,
  Elsevier Science Publishers, 2020</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In human-computer interaction or sign language interpretation, recognizing
hand gestures and detecting fingertips become ubiquitous in computer vision
research. In this paper, a unified approach of convolutional neural network for
both hand gesture recognition and fingertip detection is introduced. The
proposed algorithm uses a single network to predict the probabilities of finger
class and positions of fingertips in one forward propagation of the network.
Instead of directly regressing the positions of fingertips from the fully
connected layer, the ensemble of the position of fingertips is regressed from
the fully convolutional network. Subsequently, the ensemble average is taken to
regress the final position of fingertips. Since the whole pipeline uses a
single network, it is significantly fast in computation. The proposed method
results in remarkably less pixel error as compared to that in the direct
regression approach and it outperforms the existing fingertip detection
approaches including the Heatmap-based framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.02782</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.02782</id><submitter>Zoran Cenev</submitter><version version="v1"><date>Wed, 30 Dec 2020 13:08:42 GMT</date><size>681kb</size></version><version version="v2"><date>Tue, 18 May 2021 12:39:32 GMT</date><size>664kb</size></version><title>Ferrofluidic Manipulator: Automatic Manipulation of Non-magnetic
  Microparticles at Air-Ferrofluid Interface</title><authors>Zoran Cenev, P.A. Diluka Harischandra, Seppo Nurmi, Mika Latikka,
  Ville Hynninen, Robin H. A. Ras, Jaakko V. I. Timonen and Quan Zhou</authors><categories>cs.RO cond-mat.soft</categories><comments>9 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Manipulation of small-scale matter is a fundamental topic in micro- and
nanorobotics. Numerous magnetic robotic systems have been developed for the
manipulation of microparticles in an ambient environment, liquid as well as on
the air-liquid interface. These systems move intrinsically magnetic or
magnetically tagged objects by inducing a magnetic torque or force. However,
most of the materials found in nature are non-magnetic. Here, we report a
ferrofluidic manipulator for automatic two-dimensional manipulation of
non-magnetic objects floating on top of a ferrofluid. The manipulation system
employs eight centimeter-scale solenoids, which can move non-magnetic particles
by deforming the air-ferrofluid interface. Using linear programming, we can
control the motion of the non-magnetic particles with a predefined trajectory
of a line, square, and circle with a precision of 25.1+/-19.5 um, 34.4+/-28.4
um and 33.4+/-26.6 um, respectively. The ferrofluidic manipulator is versatile
with the materials and the shapes of the objects under manipulation. We have
successfully manipulated particles made of polyethylene, polystyrene, a silicon
chip, and poppy and sesame seeds. This work shows a promising venue for the
manipulation of living and non-living matter at the air-liquid interface.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.03617</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.03617</id><submitter>Harsh Kohli Mr.</submitter><version version="v1"><date>Sun, 10 Jan 2021 19:56:39 GMT</date><size>342kb</size><source_type>D</source_type></version><title>Transfer Learning and Augmentation for Word Sense Disambiguation</title><authors>Harsh Kohli</authors><categories>cs.IR</categories><comments>10 pages, 3 figures. Accepted at the 43rd European Conference on
  Information Retrieval (ECIR) 2021</comments><doi>10.1007/978-3-030-72240-1_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many downstream NLP tasks have shown significant improvement through
continual pre-training, transfer learning and multi-task learning.
State-of-the-art approaches in Word Sense Disambiguation today benefit from
some of these approaches in conjunction with information sources such as
semantic relationships and gloss definitions contained within WordNet. Our work
builds upon these systems and uses data augmentation along with extensive
pre-training on various different NLP tasks and datasets. Our transfer learning
and augmentation pipeline achieves state-of-the-art single model performance in
WSD and is at par with the best ensemble results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.03618</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.03618</id><submitter>Sergio G\'omez</submitter><version version="v1"><date>Sun, 10 Jan 2021 20:05:25 GMT</date><size>723kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 25 Mar 2021 19:15:13 GMT</date><size>759kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 10:05:31 GMT</date><size>737kb</size><source_type>D</source_type></version><title>Network clique cover approximation to analyze complex contagions through
  group interactions</title><authors>Giulio Burgio, Alex Arenas, Sergio G\'omez, Joan T. Matamalas</authors><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>26 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contagion processes have been proven to fundamentally depend on the
structural properties of the interaction networks conveying them. Many real
networked systems are characterized by clustered substructures representing
either collections of all-to-all pair-wise interactions (cliques) and/or group
interactions, involving many of their members at once. In this work, focusing
on interaction structures represented as simplicial complexes, we present a
discrete-time microscopic model of complex contagion for a
susceptible-infected-susceptible dynamics. Introducing a particular edge clique
cover and a heuristic to find it, the model accounts for the higher-order
dynamical correlations among the members of the substructures
(cliques/simplices). The analytical computation of the critical point reveals
that higher-order correlations are responsible for its dependence on the
higher-order couplings. While such dependence eludes any mean-field model, the
possibility of a bi-stable region is extended to structured populations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.03934</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.03934</id><submitter>Alexander Temerev</submitter><version version="v1"><date>Mon, 11 Jan 2021 14:55:25 GMT</date><size>6410kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 16 Jan 2021 20:32:55 GMT</date><size>6445kb</size><source_type>D</source_type></version><version version="v3"><date>Thu, 28 Jan 2021 21:54:48 GMT</date><size>6767kb</size><source_type>D</source_type></version><version version="v4"><date>Tue, 18 May 2021 17:24:41 GMT</date><size>5167kb</size><source_type>D</source_type></version><title>A stochastic geospatial epidemic model and simulation using an event
  modulated Gillespie algorithm</title><authors>Alexander Temerev, Liudmila Rozanova, Olivia Keiser, Janne Estill</authors><categories>q-bio.PE cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We developed a model and a software package for stochastic simulations of
transmission of COVID-19 and other similar infectious diseases, that takes into
account contact network structures and geographical distribution of population
density, detailed up to a level of location of individuals. Our analysis
framework includes a surrogate model optimization process for quick fitting of
the model's parameters to the observed epidemic curves for cases,
hospitalizations and deaths. This set of instruments (the model, the simulation
code, and the optimizer) is a useful tool for policymakers and epidemic
response teams who can use it to forecast epidemic development scenarios in
local environments (on the scale from towns to large countries) and design
optimal response strategies. The simulation code also includes a geospatial
visualization subsystem, presenting detailed views of epidemic scenarios
directly on population density maps. We used the developed framework to draw
predictions for COVID-19 spreading in the canton of Geneva, Switzerland.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.04428</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.04428</id><submitter>Suhan Shetty</submitter><version version="v1"><date>Tue, 12 Jan 2021 11:58:32 GMT</date><size>7668kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 19:08:16 GMT</date><size>22740kb</size><source_type>D</source_type></version><title>Ergodic Exploration using Tensor Train: Applications in Insertion Tasks</title><authors>Suhan Shetty, Jo\~ao Silv\'erio, and Sylvain Calinon</authors><categories>cs.RO cs.SY eess.SY math.DS math.OC stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In robotics, ergodic control extends the tracking principle by specifying a
probability distribution over an area to cover instead of a trajectory to
track. The original problem is formulated as a spectral multiscale coverage
problem, typically requiring the spatial distribution to be decomposed as
Fourier series. This approach does not scale well to control problems requiring
exploration in search space of more than 2 dimensions. To address this issue,
we propose the use of tensor trains, a recent low-rank tensor decomposition
technique from the field of multilinear algebra. The proposed solution is
efficient, both computationally and storage-wise, hence making it suitable for
its online implementation in robotic systems. The approach is applied to a
peg-in-hole insertion task requiring full 6D end-effector poses, implemented
with a 7-axis Franka Emika Panda robot. In this experiment, ergodic exploration
allows the task to be achieved without requiring the use of force/torque
sensors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.05890</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.05890</id><submitter>Arnab Dey</submitter><version version="v1"><date>Thu, 14 Jan 2021 22:09:11 GMT</date><size>8115kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 20:33:00 GMT</date><size>8840kb</size><source_type>D</source_type></version><title>Service Guarantees Countering Renewable Generation Uncertainty in
  Multi-microgrids</title><authors>Arnab Dey, Vivek Khatana, Ankur Mani, Murti V. Salapaka</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With increased penetration of Renewable Energy Systems (RES), the
conventional distribution grid is advancing towards Interconnected
multi-microgrid systems (IMMG) supervised by a Distribution Network Operator
(DNO). However, the inherent uncertainty of RES poses a challenge in meeting
the power demand of critical infrastructures in the microgrids unless
sufficient battery storage is maintained. Yet, maintaining expensive battery
storage increases the operating cost. In this article, we propose a dynamic
energy resource allocation strategy to optimize the battery reserve requirement
while ensuring that critical demand is met with a provable guarantee. Our
solution is built upon stochastic control techniques. Under our proposed
scheme, the DNO responds to the evolving uncertainty by dynamically balancing
the RES and battery resources and eliminates the risk of over or
underproduction. We derive battery reserve allocation strategy for
multi-microgrid systems in two settings: when microgrids can (interconnected)
and, cannot (individualized) share power amongst each other. We present
numerical simulations under different scenarios with detailed comparison of the
performance of the proposed algorithm for individualized and shared settings.
The simulation results demonstrate the efficacy of our algorithm. In
particular, it quantifies value of connecting microgrids through savings in
battery requirements under IMMG over individualized microgrids.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.05995</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.05995</id><submitter>Rui Tian</submitter><version version="v1"><date>Fri, 15 Jan 2021 07:21:24 GMT</date><size>2915kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 01:21:20 GMT</date><size>2766kb</size><source_type>D</source_type></version><title>Accurate and Robust Scale Recovery for Monocular Visual Odometry Based
  on Plane Geometry</title><authors>Rui Tian, Yunzhou Zhang, Delong Zhu, Shiwen Liang, Sonya Coleman,
  Dermot Kerr</authors><categories>cs.CV</categories><comments>Submitting to IEEE International Conference on Robotics and
  Automation 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Scale ambiguity is a fundamental problem in monocular visual odometry.
Typical solutions include loop closure detection and environment information
mining. For applications like self-driving cars, loop closure is not always
available, hence mining prior knowledge from the environment becomes a more
promising approach. In this paper, with the assumption of a constant height of
the camera above the ground, we develop a light-weight scale recovery framework
leveraging an accurate and robust estimation of the ground plane. The framework
includes a ground point extraction algorithm for selecting high-quality points
on the ground plane, and a ground point aggregation algorithm for joining the
extracted ground points in a local sliding window. Based on the aggregated
data, the scale is finally recovered by solving a least-squares problem using a
RANSAC-based optimizer. Sufficient data and robust optimizer enable a highly
accurate scale recovery. Experiments on the KITTI dataset show that the
proposed framework can achieve state-of-the-art accuracy in terms of
translation errors, while maintaining competitive performance on the rotation
error. Due to the light-weight design, our framework also demonstrates a high
frequency of 20Hz on the dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06112</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06112</id><submitter>Luciana Oliveira</submitter><version version="v1"><date>Wed, 6 Jan 2021 11:36:58 GMT</date><size>421kb</size></version><title>Context, input and process as critical elements for successful Emergency
  Remote Learning</title><authors>Luciana Oliveira, Anabela Mesquita, Arminda Sequeira, Adriana Oliveira
  and Paulino Silva</authors><categories>cs.CY</categories><comments>10 pages, 1 figure, 1 table</comments><msc-class>K.4.0</msc-class><acm-class>K.4.0</acm-class><doi>10.1007/978-3-030-72660-7_9</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In Spring 2020, the world moved from traditional classes to what was coined
as ERL (Emergency Remote Teaching, Learning, Instruction), posing real
challenges to all actors involved, requiring an immediate, unprecedented, and
unplanned devising of mitigation strategies. The impacts of this transition
cannot, however, be studied only at the educational level, as it consists of a
broader social shift with multidomain repercussions. In this paper, we use the
CIPP model (Context, Input, Process and Product evaluations) to further
investigate interrelations among the context, input and process elements of ERL
during the first wave of COVID-19, as the second wave presses towards
reconfining. A correlation analysis of 46 variables, based students responses
(N=360) to a closed-ended questionnaire shows the crucial importance of
motivation and engagement in online classes, as learning enablers or
constrainers. These also shape the students perception of the role that online
classes play in helping them to stay more positive during ERL.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06543</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06543</id><submitter>Yun Chen</submitter><version version="v1"><date>Sat, 16 Jan 2021 23:00:33 GMT</date><size>25408kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 28 Apr 2021 16:06:21 GMT</date><size>19692kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 05:19:19 GMT</date><size>24612kb</size><source_type>D</source_type></version><title>GeoSim: Realistic Video Simulation via Geometry-Aware Composition for
  Self-Driving</title><authors>Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan,
  Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, Raquel Urtasun</authors><categories>cs.CV cs.AI cs.GR cs.RO</categories><comments>Accepted by CVPR 2021 as Oral</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable sensor simulation is an important yet challenging open problem for
safety-critical domains such as self-driving. Current works in image simulation
either fail to be photorealistic or do not model the 3D environment and the
dynamic objects within, losing high-level control and physical realism. In this
paper, we present GeoSim, a geometry-aware image composition process which
synthesizes novel urban driving scenarios by augmenting existing images with
dynamic objects extracted from other scenes and rendered at novel poses.
Towards this goal, we first build a diverse bank of 3D objects with both
realistic geometry and appearance from sensor data. During simulation, we
perform a novel geometry-aware simulation-by-composition procedure which 1)
proposes plausible and realistic object placements into a given scene, 2)
render novel views of dynamic objects from the asset bank, and 3) composes and
blends the rendered image segments. The resulting synthetic images are
realistic, traffic-aware, and geometrically consistent, allowing our approach
to scale to complex use cases. We demonstrate two such important applications:
long-range realistic video simulation across multiple camera sensors, and
synthetic data generation for data augmentation on downstream segmentation
tasks. Please check https://tmux.top/publication/geosim/ for high-resolution
video results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.06848</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.06848</id><submitter>Isaac Sledge</submitter><version version="v1"><date>Mon, 18 Jan 2021 02:30:13 GMT</date><size>4089kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 5 Feb 2021 07:03:20 GMT</date><size>4302kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 21:52:47 GMT</date><size>28693kb</size><source_type>D</source_type></version><title>Faster Convergence in Deep-Predictive-Coding Networks to Learn Deeper
  Representations</title><authors>Isaac J. Sledge and Jose C. Principe</authors><categories>cs.AI cs.CV cs.NE</categories><comments>Submitted to IEEE TNNLS</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep-predictive-coding networks (DPCNs) are hierarchical, generative models.
They rely on feed-forward and feed-back connections to modulate latent feature
representations of stimuli in a dynamic and context-sensitive manner. A crucial
element of DPCNs is a forward-backward inference procedure to uncover sparse,
invariant features. However, this inference is a major computational
bottleneck. It severely limits the network depth due to learning stagnation.
Here, we prove why this bottleneck occurs. We then propose a new
forward-inference strategy based on accelerated proximal gradients. This
strategy has faster theoretical convergence guarantees than the one used for
DPCNs. It overcomes learning stagnation. We also demonstrate that it permits
constructing deep and wide predictive-coding networks. Such convolutional
networks implement receptive fields that capture well the entire classes of
objects on which the networks are trained. This improves the feature
representations compared with our lab's previous non-convolutional and
convolutional DPCNs. It yields unsupervised object recognition that surpass
convolutional autoencoders and are on par with convolutional networks trained
in a supervised manner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.07714</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.07714</id><submitter>Ashish Sharma</submitter><version version="v1"><date>Tue, 19 Jan 2021 16:37:58 GMT</date><size>1836kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 24 Apr 2021 21:30:22 GMT</date><size>1469kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 17:58:38 GMT</date><size>2294kb</size><source_type>D</source_type></version><title>Towards Facilitating Empathic Conversations in Online Mental Health
  Support: A Reinforcement Learning Approach</title><authors>Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, Tim
  Althoff</authors><categories>cs.CL cs.SI</categories><comments>Published at WWW 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online peer-to-peer support platforms enable conversations between millions
of people who seek and provide mental health support. If successful, web-based
mental health conversations could improve access to treatment and reduce the
global disease burden. Psychologists have repeatedly demonstrated that empathy,
the ability to understand and feel the emotions and experiences of others, is a
key component leading to positive outcomes in supportive conversations.
However, recent studies have shown that highly empathic conversations are rare
in online mental health platforms.
  In this paper, we work towards improving empathy in online mental health
support conversations. We introduce a new task of empathic rewriting which aims
to transform low-empathy conversational posts to higher empathy. Learning such
transformations is challenging and requires a deep understanding of empathy
while maintaining conversation quality through text fluency and specificity to
the conversational context. Here we propose PARTNER, a deep reinforcement
learning agent that learns to make sentence-level edits to posts in order to
increase the expressed level of empathy while maintaining conversation quality.
Our RL agent leverages a policy network, based on a transformer language model
adapted from GPT-2, which performs the dual task of generating candidate
empathic sentences and adding those sentences at appropriate positions. During
training, we reward transformations that increase empathy in posts while
maintaining text fluency, context specificity and diversity. Through a
combination of automatic and human evaluation, we demonstrate that PARTNER
successfully generates more empathic, specific, and diverse responses and
outperforms NLP methods from related tasks like style transfer and empathic
dialogue generation. Our work has direct implications for facilitating empathic
conversations on web-based platforms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.07756</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.07756</id><submitter>Ziran Wang</submitter><version version="v1"><date>Tue, 19 Jan 2021 17:56:02 GMT</date><size>14932kb</size><source_type>D</source_type></version><title>Motion Estimation of Connected and Automated Vehicles under
  Communication Delay and Packet Loss of V2X Communications</title><authors>Ziran Wang and Kyungtae Han and Prashant Han</authors><categories>eess.SY cs.SY</categories><comments>Manuscript accepted as SAE technical paper</comments><doi>10.4271/2021-01-0107</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The emergence of the connected and automated vehicle (CAV) technology enables
numerous advanced applications in our transportation system, benefiting our
daily travels in terms of safety, mobility, and sustainability. However,
vehicular communication technologies such as Dedicated Short-Range
Communications (DSRC) or Cellular-Based Vehicle-to-Everything (C-V2X)
communications unavoidably introduce issues like communication delay and packet
loss, which will downgrade the performances of any CAV applications. In this
study, we propose a consensus-based motion estimation methodology to estimate
the vehicle motion when the vehicular communication environment is not ideal.
This methodology is developed based on the consensus-based feedforward/feedback
motion control algorithm, estimating the position and speed of a CAV in the
presence of communication delay and packet loss. The simulation study is
conducted in a traffic scenario of unsignalized intersections, where CAVs
coordinate with each other through V2X communications and cross intersections
without any full stop. Game engine-based human-in-the-loop simulation results
shows the proposed motion estimation methodology can cap the position
estimation error to 0.5 m during periodic packet loss and time-variant
communication delay.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08387</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08387</id><submitter>Yongquan Yang</submitter><version version="v1"><date>Thu, 21 Jan 2021 01:33:23 GMT</date><size>606kb</size></version><version version="v2"><date>Fri, 16 Apr 2021 03:28:11 GMT</date><size>754kb</size></version><version version="v3"><date>Tue, 18 May 2021 03:47:12 GMT</date><size>874kb</size></version><title>A Survey on Ensemble Learning under the Era of Deep Learning</title><authors>Yongquan Yang, Haijun Lv, Ning Chen</authors><categories>cs.LG cs.AI</categories><comments>39 pages, 9 figures, 15 tables</comments><acm-class>A.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the dominant position of deep learning (mostly deep neural networks)
in various artificial intelligence applications, recently, ensemble learning
based on deep neural networks (ensemble deep learning) has shown significant
performances in improving the generalization of learning system. However, since
modern deep neural networks usually have millions to billions of parameters,
the time and space overheads for training multiple base deep learners and
testing with the ensemble deep learner are far greater than that of traditional
ensemble learning. Though several algorithms of fast ensemble deep learning
have been proposed to promote the deployment of ensemble deep learning in some
applications, further advances still need to be made for many applications in
specific fields, where the developing time and computing resources are usually
restricted or the data to be processed is of large dimensionality. An urgent
problem needs to be solved is how to take the significant advantages of
ensemble deep learning while reduce the required time and space overheads so
that many more applications in specific fields can benefit from it. For the
alleviation of this problem, it is essential to know about how ensemble
learning has developed under the era of deep learning. Thus, in this article,
we present discussion focusing on data analyses of published works, the
methodology, recent works and unattainability of traditional ensemble learning,
and recent developments of ensemble deep learning. We hope this article will be
helpful to realize the technical challenges faced by future developments of
ensemble learning under the era of deep learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.08970</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.08970</id><submitter>Arman Sharififar</submitter><version version="v1"><date>Fri, 22 Jan 2021 07:10:41 GMT</date><size>17kb</size></version><version version="v2"><date>Mon, 17 May 2021 00:52:18 GMT</date><size>133kb</size><source_type>D</source_type></version><title>Update-based Maximum Column Distance Coding Scheme for Index Coding
  Problem</title><authors>Arman Sharififar, Neda Aboutorab, Parastoo Sadeghi</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new scalar linear coding scheme for the index
coding problem called update-based maxi-mum column distance (UMCD) coding
scheme. The central idea in each transmission is to code messages such that one
of the receivers with the minimum size of side information is instantaneously
eliminated from unsatisfied receivers. One main contribution of the paper is to
prove that the other satisfied receivers can be identified after each
transmission, using a polynomial-time algorithm solving the well-known maximum
cardinality matching problem in graph theory. This leads to determining the
total number of transmissions without knowing the coding coefficients. Once
this number and what messages to transmit in each round is found, we then
propose a method to determine all coding coefficients from a sufficiently large
finite field. We provide concrete instances where the proposed UMCD scheme has
a better broadcast performance compared to the most efficient existing linear
coding schemes, including the recursive scheme (Arbabjolfaei and Kim, 2014) and
the interlinked-cycle cover scheme (Thapaet al., 2017).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.09436</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.09436</id><submitter>Xudong Sun</submitter><version version="v1"><date>Sat, 23 Jan 2021 07:09:59 GMT</date><size>578kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 22 Feb 2021 19:00:48 GMT</date><size>1726kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 27 Feb 2021 13:35:03 GMT</date><size>2179kb</size><source_type>D</source_type></version><version version="v4"><date>Thu, 6 May 2021 17:36:04 GMT</date><size>2849kb</size><source_type>D</source_type></version><version version="v5"><date>Fri, 14 May 2021 20:51:15 GMT</date><size>2163kb</size><source_type>D</source_type></version><title>Hierarchical Variational Auto-Encoding for Unsupervised Domain
  Generalization</title><authors>Xudong Sun, Florian Buettner</authors><categories>cs.LG cs.CV stat.ML</categories><comments>Presented at ICLR 2021 RobustML Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the task of domain generalization, where the goal is to train a
predictive model such that it is able to generalize to a new, previously unseen
domain. We choose a hierarchical generative approach within the framework of
variational autoencoders and propose a domain-unsupervised algorithm that is
able to generalize to new domains without domain supervision. We show that our
method is able to learn representations that disentangle domain-specific
information from class-label specific information even in complex settings
where domain structure is not observed during training. Our interpretable
method outperforms previously proposed generative algorithms for domain
generalization as well as other non-generative state-of-the-art approaches in
several hierarchical domain settings including sequential overlapped near
continuous domain shift. It also achieves competitive performance on the
standard domain generalization benchmark dataset PACS compared to
state-of-the-art approaches which rely on observing domain-specific information
during training, as well as another domain unsupervised method. Additionally,
we proposed model selection purely based on Evidence Lower Bound (ELBO) and
also proposed weak domain supervision where implicit domain information can be
added into the algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10269</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10269</id><submitter>Grigorii Trofimiuk</submitter><version version="v1"><date>Mon, 25 Jan 2021 17:51:11 GMT</date><size>99kb</size></version><version version="v2"><date>Tue, 2 Feb 2021 14:25:13 GMT</date><size>100kb</size></version><version version="v3"><date>Mon, 17 May 2021 12:45:00 GMT</date><size>99kb</size></version><title>A Search Method for Large Polarization Kernels</title><authors>Grigorii Trofimiuk</authors><categories>cs.IT math.IT</categories><comments>Final version to appear at ISIT 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A new search method for large polarization kernels is proposed. The algorithm
produces a kernel with given partial distances by employing depth-first search
combined with some methods which reduce the search space. Using the proposed
method, we improved almost all existing lower bounds on the maximum rate of
polarization for kernels of size from 17 to 27. We also obtained kernels which
admit low complexity processing by the recently proposed recursive trellis
algorithm. Numerical results demonstrate the advantage of polar codes with the
proposed kernels compared with shortened polar codes and polar codes with small
kernels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.10495</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.10495</id><submitter>Jayam Umesh Patel</submitter><version version="v1"><date>Tue, 26 Jan 2021 00:13:58 GMT</date><size>4064kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 20:05:03 GMT</date><size>2028kb</size><source_type>D</source_type></version><title>Transparency in Multi-Human Multi-Robot Interaction</title><authors>Jayam Patel, Tyagaraja Ramaswamy, Zhi Li, and Carlo Pinciroli</authors><categories>cs.RO cs.HC cs.MA</categories><comments>8 pages, submitted to IEEE Robotics and Automation Letters</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transparency is a key factor in improving the performance of human-robot
interaction. A transparent interface allows humans to be aware of the state of
a robot and to assess the progress of the tasks at hand. When multi-robot
systems are involved, transparency is an even greater challenge, due to the
larger number of variables affecting the behavior of the robots as a whole.
Significant effort has been devoted to studying transparency when single
operators interact with multiple robots. However, studies on transparency that
focus on multiple human operators interacting with a multi-robot systems are
limited. This paper aims to fill this gap by presenting a human-swarm
interaction interface with graphical elements that can be enabled and disabled.
Through this interface, we study which graphical elements are contribute to
transparency by comparing four &quot;transparency modes&quot;: (i) no transparency (no
operator receives information from the robots), (ii) central transparency (the
operators receive information only relevant to their personal task), (iii)
peripheral transparency (the operators share information on each others'
tasks), and (iv) mixed transparency (both central and peripheral). We report
the results in terms of awareness, trust, and workload of a user study
involving 18 participants engaged in a complex multi-robot task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.11315</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.11315</id><submitter>Siamak Layeghy</submitter><version version="v1"><date>Wed, 27 Jan 2021 11:00:55 GMT</date><size>294kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 01:53:46 GMT</date><size>294kb</size><source_type>D</source_type></version><title>Towards a Standard Feature Set for Network Intrusion Detection System
  Datasets</title><authors>Mohanad Sarhan, Siamak Layeghy, Marius Portmann</authors><categories>cs.NI</categories><comments>13 pages, 4 figures, 13 tables. arXiv admin note: substantial text
  overlap with arXiv:2011.09144</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Network Intrusion Detection Systems (NIDSs) are important tools for the
protection of computer networks against increasingly frequent and sophisticated
cyber attacks. Recently, a lot of research effort has been dedicated to the
development of Machine Learning (ML) based NIDSs. As in any ML-based
application, the availability of high-quality datasets is critical for the
training and evaluation of ML-based NIDS. One of the key problems with the
currently available datasets is the lack of a standard feature set. The use of
a unique and proprietary set of features for each of the publicly available
datasets makes it virtually impossible to compare the performance of ML-based
traffic classifiers on different datasets, and hence to evaluate the ability of
these systems to generalise across different network scenarios. To address that
limitation, this paper proposes and evaluates standard NIDS feature sets based
on the NetFlow network meta-data collection protocol and system. We evaluate
and compare two NetFlow-based feature set variants, a version with 12 features,
and another one with 43 features.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2101.11490</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2101.11490</id><submitter>Ioannis Papoutsidakis</submitter><version version="v1"><date>Wed, 27 Jan 2021 15:40:10 GMT</date><size>440kb</size></version><version version="v2"><date>Thu, 28 Jan 2021 11:04:16 GMT</date><size>440kb</size></version><version version="v3"><date>Sun, 16 May 2021 17:47:29 GMT</date><size>463kb</size></version><title>Non-Asymptotic Converse Bounds Via Auxiliary Channels</title><authors>Ioannis Papoutsidakis, Robert J. Piechocki, and Angela Doufexi</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new derivation method of converse bounds on the
non-asymptotic achievable rate of memoryless discrete channels. It is based on
the finite blocklength statistics of the channel, where with the use of an
auxiliary channel the converse bound is produced. This methodology is general
and initially presented for an arbitrary channel. Afterwards, the main result
is specialized for the $q$-ary erasure (QEC), binary symmetric (BSC), and Z
channels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.00277</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.00277</id><submitter>Rajvir Kaur</submitter><version version="v1"><date>Sat, 30 Jan 2021 17:56:40 GMT</date><size>805kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 3 Feb 2021 13:00:30 GMT</date><size>805kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 05:12:53 GMT</date><size>919kb</size><source_type>D</source_type></version><title>Estimating galaxy masses from kinematics of globular cluster systems: a
  new method based on deep learning</title><authors>Rajvir Kaur, Kenji Bekki, Ghulam Mubashar Hassan, Amitava Datta</authors><categories>astro-ph.CO cs.CV</categories><comments>Accepted by MNRAS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method by which the total masses of galaxies including dark
matter can be estimated from the kinematics of their globular cluster systems
(GCSs). In the proposed method, we apply the convolutional neural networks
(CNNs) to the two-dimensional (2D) maps of line-of-sight-velocities ($V$) and
velocity dispersions ($\sigma$) of GCSs predicted from numerical simulations of
disk and elliptical galaxies. In this method, we first train the CNN using
either only a larger number ($\sim 200,000$) of the synthesized 2D maps of
$\sigma$ (&quot;one-channel&quot;) or those of both $\sigma$ and $V$ (&quot;two-channel&quot;).
Then we use the CNN to predict the total masses of galaxies (i.e., test the
CNN) for the totally unknown dataset that is not used in training the CNN. The
principal results show that overall accuracy for one-channel and two-channel
data is 97.6\% and 97.8\% respectively, which suggests that the new method is
promising. The mean absolute errors (MAEs) for one-channel and two-channel data
are 0.288 and 0.275 respectively, and the value of root mean square errors
(RMSEs) are 0.539 and 0.51 for one-channel and two-channel respectively. These
smaller MAEs and RMSEs for two-channel data (i.e., better performance) suggest
that the new method can properly consider the global rotation of GCSs in the
mass estimation. We also applied our proposed method to real data collected
from observations of NGC 3115 to compare the total mass predicted by our
proposed method and other popular methods from the literature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.00630</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.00630</id><submitter>Aaditya Ramdas</submitter><version version="v1"><date>Mon, 1 Feb 2021 04:31:44 GMT</date><size>161kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 23:31:09 GMT</date><size>954kb</size><source_type>D</source_type></version><title>How can one test if a binary sequence is exchangeable? Fork-convex
  hulls, supermartingales, and Snell envelopes</title><authors>Aaditya Ramdas, Johannes Ruf, Martin Larsson, Wouter Koolen</authors><categories>math.ST cs.IT math.IT math.PR stat.ME stat.TH</categories><comments>33 pages, 7 figures, submitted to the International Journal of
  Approximate Reasoning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we observe an infinite series of coin flips $X_1,X_2,\ldots$, and
wish to sequentially test the null that these binary random variables are
exchangeable. Nonnegative supermartingales (NSMs) are a workhorse of sequential
inference, but we prove that they are powerless for this problem. First,
utilizing a geometric concept called fork-convexity (a sequential analog of
convexity), we show that any process that is an NSM under two distributions, is
also necessarily an NSM under their &quot;fork-convex hull&quot;. Second, we demonstrate
that the fork-convex hull of the exchangeable null consists of all possible
laws over binary sequences; this implies that any NSM under exchangeability is
necessarily nonincreasing, hence always yields a powerless test for any
alternative. Since testing arbitrary deviations from exchangeability is
information theoretically impossible, we focus on Markovian alternatives. We
combine ideas from universal inference and the method of mixtures to derive a
&quot;safe e-value&quot;, which is a nonnegative process with expectation at most one
under the null at any stopping time, and is upper bounded by a martingale, but
is not itself an NSM. This in turn yields a level $\alpha$ sequential test that
is consistent; regret bounds from universal coding also demonstrate
rate-optimal power. We present ways to extend these results to any finite
alphabet and to Markovian alternatives of any order using a &quot;double mixture&quot;
approach. We provide a wide array of simulations, and give general approaches
based on betting for unstructured or ill-specified alternatives. Finally,
inspired by Shafer, Vovk, and Ville, we provide game-theoretic interpretations
of our e-values and pathwise results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01372</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01372</id><submitter>B.Sundar Rajan</submitter><version version="v1"><date>Tue, 2 Feb 2021 07:53:10 GMT</date><size>312kb</size></version><version version="v2"><date>Sat, 15 May 2021 06:07:34 GMT</date><size>314kb</size></version><title>Improved Multi-access Coded Caching Schemes from Cross Resolvable
  Designs</title><authors>Pooja Nayak Muralidhar, Digvijay Katyal and B. Sundar Rajan</authors><categories>cs.IT math.IT</categories><comments>21 pages, 5 figures and 1 table. Section IV added with two
  illustrative examples. arXiv admin note: substantial text overlap with
  arXiv:2005.13731; text overlap with arXiv:2102.00400</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently multi-access coded caching schemes with number of users different
from the number of caches obtained from a special case of resolvable designs
called Cross Resolvable Designs (CRDs) have been reported and a new performance
metric called rate-per-user has been introduced \cite{KNRarXiv}. In this paper
we present a generalization of this work resulting in multi-access coded
caching schemes with improved rate-per-user.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.01749</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.01749</id><submitter>Ali Rahimpour Jounghani</submitter><version version="v1"><date>Tue, 2 Feb 2021 20:48:19 GMT</date><size>614kb</size></version><version version="v2"><date>Tue, 9 Mar 2021 03:06:01 GMT</date><size>615kb</size></version><version version="v3"><date>Sun, 16 May 2021 18:39:08 GMT</date><size>616kb</size></version><title>Vehicle trajectory prediction in top-view image sequences based on deep
  learning method</title><authors>Zahra Salahshoori Nejad, Hamed Heravi, Ali Rahimpour Jounghani,
  Abdollah Shahrezaie, Afshin Ebrahimi</authors><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Annually, a large number of injuries and deaths around the world are related
to motor vehicle accidents. This value has recently been reduced to some
extent, via the use of driver-assistance systems. Developing driver-assistance
systems (i.e., automated driving systems) can play a crucial role in reducing
this number. Estimating and predicting surrounding vehicles' movement is
essential for an automated vehicle and advanced safety systems. Moreover,
predicting the trajectory is influenced by numerous factors, such as drivers'
behavior during accidents, history of the vehicle's movement and the
surrounding vehicles, and their position on the traffic scene. The vehicle must
move over a safe path in traffic and react to other drivers' unpredictable
behaviors in the shortest time. Herein, to predict automated vehicles' path, a
model with low computational complexity is proposed, which is trained by images
taken from the road's aerial image. Our method is based on an encoder-decoder
model that utilizes a social tensor to model the effect of the surrounding
vehicles' movement on the target vehicle. The proposed model can predict the
vehicle's future path in any freeway only by viewing the images related to the
history of the target vehicle's movement and its neighbors. Deep learning was
used as a tool for extracting the features of these images. Using the HighD
database, an image dataset of the road's aerial image was created, and the
model's performance was evaluated on this new database. We achieved the RMSE of
1.91 for the next 5 seconds and found that the proposed method had less error
than the best path-prediction methods in previous studies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02035</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02035</id><submitter>Koen Bertels</submitter><version version="v1"><date>Wed, 3 Feb 2021 12:33:12 GMT</date><size>6502kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 21 Mar 2021 08:26:53 GMT</date><size>6509kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 28 Mar 2021 20:11:57 GMT</date><size>6509kb</size><source_type>D</source_type></version><version version="v4"><date>Mon, 17 May 2021 12:17:47 GMT</date><size>6510kb</size><source_type>D</source_type></version><title>Quantum Accelerator Stack: A Research Roadmap</title><authors>K. Bertels, A. Sarkar, A. Krol, R. Budhrani, J. Samadi, E. Geoffroy,
  J. Matos, R. Abreu, G. Gielen, I. Ashraf</authors><categories>quant-ph cs.AR</categories><comments>39 pages. arXiv admin note: text overlap with arXiv:1903.09575</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents the definition and implementation of a quantum computer
architecture to enable creating a new computational device - a quantum computer
as an accelerator In this paper, we present explicitly the idea of a quantum
accelerator which contains the full stack of the layers of an accelerator. Such
a stack starts at the highest level describing the target application of the
accelerator. Important to realise is that qubits are defined as perfect qubits,
implying they do not decohere and perform good quantum gate operations. The
next layer abstracts the quantum logic outlining the algorithm that is to be
executed on the quantum accelerator. In our case, the logic is expressed in the
universal quantum-classical hybrid computation language developed in the group,
called OpenQL. We also have to start thinking about how to verify, validate and
test the quantum software such that the compiler generates a correct version of
the quantum circuit. The OpenQL compiler translates the program to a common
assembly language, called cQASM. We need to develop a quantum operating system
that manages all the hardware of the micro-architecture. The layer below the
micro-architecture is responsible of the mapping and routing of the qubits on
the topology such that the nearest-neighbour-constraint can be be respected. At
any moment in the future when we are capable of generating multiple good
qubits, the compiler can convert the cQASM to generate the eQASM, which is
executable on a particular experimental device incorporating the
platform-specific parameters. This way, we are able to distinguish clearly the
experimental research towards better qubits, and the industrial and societal
applications that need to be developed and executed on a quantum device.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02669</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02669</id><submitter>Xiaoyu Zhang</submitter><version version="v1"><date>Wed, 3 Feb 2021 07:34:29 GMT</date><size>679kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 15:45:00 GMT</date><size>7395kb</size><source_type>D</source_type></version><title>OmiEmbed: a unified multi-task deep learning framework for multi-omics
  data</title><authors>Xiaoyu Zhang, Yuting Xing, Kai Sun, Yike Guo</authors><categories>q-bio.GN cs.LG</categories><comments>14 pages, 8 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-dimensional omics data contains intrinsic biomedical information that is
crucial for personalised medicine. Nevertheless, it is challenging to capture
them from the genome-wide data due to the large number of molecular features
and small number of available samples, which is also called 'the curse of
dimensionality' in machine learning. To tackle this problem and pave the way
for machine learning aided precision medicine, we proposed a unified multi-task
deep learning framework named OmiEmbed to capture biomedical information from
high-dimensional omics data with the deep embedding and downstream task
modules. The deep embedding module learnt an omics embedding that mapped
multiple omics data types into a latent space with lower dimensionality. Based
on the new representation of multi-omics data, different downstream task
modules were trained simultaneously and efficiently with the multi-task
strategy to predict the comprehensive phenotype profile of each sample.
OmiEmbed support multiple tasks for omics data including dimensionality
reduction, tumour type classification, multi-omics integration, demographic and
clinical feature reconstruction, and survival prediction. The framework
outperformed other methods on all three types of downstream tasks and achieved
better performance with the multi-task strategy comparing to training them
individually. OmiEmbed is a powerful and unified framework that can be widely
adapted to various application of high-dimensional omics data and has a great
potential to facilitate more accurate and personalised clinical decision
making.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.02754</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.02754</id><submitter>Yuval Alaluf</submitter><version version="v1"><date>Thu, 4 Feb 2021 17:33:28 GMT</date><size>56509kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 17:31:01 GMT</date><size>38110kb</size><source_type>D</source_type></version><title>Only a Matter of Style: Age Transformation Using a Style-Based
  Regression Model</title><authors>Yuval Alaluf, Or Patashnik, Daniel Cohen-Or</authors><categories>cs.CV</categories><comments>Accepted to SIGGRAPH 2021, project page available at
  https://yuval-alaluf.github.io/SAM/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of age transformation illustrates the change of an individual's
appearance over time. Accurately modeling this complex transformation over an
input facial image is extremely challenging as it requires making convincing,
possibly large changes to facial features and head shape, while still
preserving the input identity. In this work, we present an image-to-image
translation method that learns to directly encode real facial images into the
latent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a
given aging shift. We employ a pre-trained age regression network to explicitly
guide the encoder in generating the latent codes corresponding to the desired
age. In this formulation, our method approaches the continuous aging process as
a regression task between the input age and desired target age, providing
fine-grained control over the generated image. Moreover, unlike approaches that
operate solely in the latent space using a prior on the path controlling age,
our method learns a more disentangled, non-linear path. Finally, we demonstrate
that the end-to-end nature of our approach, coupled with the rich semantic
latent space of StyleGAN, allows for further editing of the generated images.
Qualitative and quantitative evaluations show the advantages of our method
compared to state-of-the-art approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03041</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03041</id><submitter>Bangti Jin</submitter><version version="v1"><date>Fri, 5 Feb 2021 07:55:06 GMT</date><size>2688kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 06:31:32 GMT</date><size>2688kb</size><source_type>D</source_type></version><title>Reconstruction of a Space-Time Dependent Source in Subdiffusion Models
  via a Perturbation Approach</title><authors>Bangti Jin, Yavar Kian, Zhi Zhou</authors><categories>math.NA cs.NA math.AP</categories><comments>26 pages, 5 figures, to appear at SIAM Journal of Mathematical
  Analysis</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article we study inverse problems of recovering a space-time
dependent source component from the lateral boundary observation in a
subidffusion model. The mathematical model involves a Djrbashian-Caputo
fractional derivative of order $\alpha\in(0,1)$ in time, and a second-order
elliptic operator with time-dependent coefficients. We establish a
well-posedness and a conditional stability result for the inverse problems
using a novel perturbation argument and refined regularity estimates of the
associated direct problem. Further, we present an algorithm for efficiently and
accurately reconstructing the source component, and provide several
two-dimensional numerical results showing the feasibility of the recovery.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03150</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03150</id><submitter>Kristof Sch\&quot;utt</submitter><version version="v1"><date>Fri, 5 Feb 2021 13:00:12 GMT</date><size>790kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 8 Feb 2021 09:27:45 GMT</date><size>790kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 13:09:50 GMT</date><size>794kb</size><source_type>D</source_type></version><title>Equivariant message passing for the prediction of tensorial properties
  and molecular spectra</title><authors>Kristof T. Sch\&quot;utt, Oliver T. Unke, Michael Gastegger</authors><categories>cs.LG physics.chem-ph</categories><comments>Accepted at ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Message passing neural networks have become a method of choice for learning
on graphs, in particular the prediction of chemical properties and the
acceleration of molecular dynamics studies. While they readily scale to large
training data sets, previous approaches have proven to be less data efficient
than kernel methods. We identify limitations of invariant representations as a
major reason and extend the message passing formulation to rotationally
equivariant representations. On this basis, we propose the polarizable atom
interaction neural network (PaiNN) and improve on common molecule benchmarks
over previous networks, while reducing model size and inference time. We
leverage the equivariant atomwise representations obtained by PaiNN for the
prediction of tensorial properties. Finally, we apply this to the simulation of
molecular spectra, achieving speedups of 4-5 orders of magnitude compared to
the electronic structure reference.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03156</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03156</id><submitter>Quentin Bouniot</submitter><version version="v1"><date>Fri, 5 Feb 2021 13:24:36 GMT</date><size>2420kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 30 Mar 2021 08:06:59 GMT</date><size>2419kb</size><source_type>D</source_type></version><title>Optimal Transport as a Defense Against Adversarial Attacks</title><authors>Quentin Bouniot, Romaric Audigier, Ang\'elique Loesch</authors><categories>cs.LG cs.CV stat.ML</categories><comments>Accepted at ICPR2020. Code is available at
  https://github.com/CEA-LIST/adv-sat</comments><doi>10.1109/ICPR48806.2021.9413327</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning classifiers are now known to have flaws in the representations
of their class. Adversarial attacks can find a human-imperceptible perturbation
for a given image that will mislead a trained model. The most effective methods
to defend against such attacks trains on generated adversarial examples to
learn their distribution. Previous work aimed to align original and adversarial
image representations in the same way as domain adaptation to improve
robustness. Yet, they partially align the representations using approaches that
do not reflect the geometry of space and distribution. In addition, it is
difficult to accurately compare robustness between defended models. Until now,
they have been evaluated using a fixed perturbation size. However, defended
models may react differently to variations of this perturbation size. In this
paper, the analogy of domain adaptation is taken a step further by exploiting
optimal transport theory. We propose to use a loss between distributions that
faithfully reflect the ground distance. This leads to SAT (Sinkhorn Adversarial
Training), a more robust defense against adversarial attacks. Then, we propose
to quantify more precisely the robustness of a model to adversarial attacks
over a wide range of perturbation sizes using a different metric, the Area
Under the Accuracy Curve (AUAC). We perform extensive experiments on both
CIFAR-10 and CIFAR-100 datasets and show that our defense is globally more
robust than the state-of-the-art.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03253</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03253</id><submitter>Pietro  Hiram Guzzi</submitter><version version="v1"><date>Fri, 5 Feb 2021 15:57:48 GMT</date><size>2260kb</size><source_type>D</source_type></version><title>Analyzing Host-Viral Interactome of SARS-CoV-2 for Identifying
  Vulnerable Host Proteins during COVID-19 Pathogenesis</title><authors>Jayanta Kumar Das, Swarup Roy, Pietro Hiram Guzzi</authors><categories>q-bio.BM cs.LG</categories><doi>10.1016/j.meegid.2021.104921</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The development of therapeutic targets for COVID-19 treatment is based on the
understanding of the molecular mechanism of pathogenesis. The identification of
genes and proteins involved in the infection mechanism is the key to shed out
light into the complex molecular mechanisms. The combined effort of many
laboratories distributed throughout the world has produced the accumulation of
both protein and genetic interactions. In this work we integrate these
available results and we obtain an host protein-protein interaction network
composed by 1432 human proteins. We calculate network centrality measures to
identify key proteins. Then we perform functional enrichment of central
proteins. We observed that the identified proteins are mostly associated with
several crucial pathways, including cellular process, signalling transduction,
neurodegenerative disease. Finally, we focused on proteins involved in causing
disease in the human respiratory tract. We conclude that COVID19 is a complex
disease, and we highlighted many potential therapeutic targets including RBX1,
HSPA5, ITCH, RAB7A, RAB5A, RAB8A, PSMC5, CAPZB, CANX, IGF2R, HSPA1A, which are
central and also associated with multiple diseases
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03351</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03351</id><submitter>Florenc Demrozi Dr.</submitter><version version="v1"><date>Sat, 30 Jan 2021 09:54:31 GMT</date><size>1387kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 11:37:02 GMT</date><size>5123kb</size><source_type>D</source_type></version><title>Estimating indoor occupancy through low-cost BLE devices</title><authors>Florenc Demrozi, Cristian Turetta, Fabio Chiarani, Philipp H. Kindt,
  and Graziano Pravadelli</authors><categories>eess.SP cs.LG</categories><comments>7 Tabels, 2 Figures, 9 Pages</comments><doi>10.1109/JSEN.2021.3080632</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting the presence of persons and estimating their quantity in an indoor
environment has grown in importance recently. For example, the information if a
room is unoccupied can be used for automatically switching off the light, air
conditioning, and ventilation, thereby saving significant amounts of energy in
public buildings. Most existing solutions rely on dedicated hardware
installations, which involve presence sensors, video cameras, and carbon
dioxide sensors. Unfortunately, such approaches are costly, are subject to
privacy concerns, have high computational requirements, and lack
ubiquitousness. The work presented in this article addresses these limitations
by proposing a low-cost occupancy detection system. Our approach builds upon
detecting variations in Bluetooth Low Energy (BLE) signals related to the
presence of humans. The effectiveness of this approach is evaluated by
performing comprehensive tests on five different datasets. We apply several
pattern recognition models and compare our methodology with systems building
upon IEEE 802.11 (WiFi). On average, in multifarious environments, we can
correctly classify the occupancy with an accuracy of 97.97%. When estimating
the number of people in a room, on average, the estimated number of subjects
differs from the actual one by 0.32 persons. We conclude that our system's
performance is comparable to that of existing ones based on WiFi, while
significantly reducing cost and installation effort. Hence, our approach makes
occupancy detection practical for real-world deployments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03430</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03430</id><submitter>Johannes Gerster</submitter><version version="v1"><date>Fri, 5 Feb 2021 21:58:28 GMT</date><size>386kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 17:55:01 GMT</date><size>386kb</size><source_type>D</source_type></version><title>Pointing out the Convolution Problem of Stochastic Aggregation Methods
  for the Determination of Flexibility Potentials at Vertical System
  Interconnections</title><authors>Johannes Gerster, Marcel Sarstedt, Eric MSP Veith, Sebastian Lehnhoff
  and Lutz Hofmann</authors><categories>eess.SY cs.SY</categories><comments>Accepted by the The Eleventh International Conference on Smart Grids,
  Green Communications and IT Energy-aware Technologies ENERGY 2021, 7 pages, 4
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increase of generation capacity in the area of responsibility of the
distribution system operator (DSO) requires strengthening of coordination
between transmission system operator (TSO) and DSO in order to prevent
conflicting or counteracting use of flexibility options. For this purpose,
methods for the standardized description and identification of the aggregated
flexibility potential of distribution grids (DGs) are developed. Approaches for
identifying the feasible operation region (FOR) of DGs can be categorized into
two main classes: Data-driven/stochastic approaches and optimization based
approaches. While the latter have the advantage of working in real-world
scenarios where no full grid models exist, when relying on naive sampling
strategies, they suffer from poor coverage of the edges of the FOR. To underpin
the need for improved sampling strategies for data-driven approaches, in this
paper we point out and analyse the shortcomings of naive sampling strategies
with focus on the problem of leptocurtic distribution of resulting
interconnection power flows (IPFs). We refer to this problem as convolution
problem, as it can be traced back to the fact that the probability density
function (PDF) of the sum of two or more independent random variables is the
convolution of their respective PDFs. To demonstrate the convolution problem,
we construct a series of synthetic 0.4 kV feeders, which are characterized by
an increasing number of nodes and apply a sampling strategy to them that draws
set-values for the controllable distributed energy resources (DERs) from
independent uniform distributions. By calculating the power flow for each
sample in each feeder, we end up with a collapsing IPF point cloud clearly
indicating the convolution problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.03814</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.03814</id><submitter>Theerawit Wilaiprasitporn</submitter><version version="v1"><date>Sun, 7 Feb 2021 15:20:23 GMT</date><size>12258kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 08:03:59 GMT</date><size>9464kb</size><source_type>D</source_type></version><title>MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor
  Imagery EEG Classification</title><authors>Phairot Autthasan, Rattanaphon Chaisaen, Thapanun Sudhawiyangkul,
  Phurin Rangpong, Suktipol Kiatthaveephong, Nat Dilokthanakul, Gun
  Bhakdisongkhram, Huy Phan, Cuntai Guan and Theerawit Wilaiprasitporn</authors><categories>eess.SP cs.AI cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)
allow control of several applications by decoding neurophysiological phenomena,
which are usually recorded by electroencephalography (EEG) using a non-invasive
technique. Despite great advances in MI-based BCI, EEG rhythms are specific to
a subject and various changes over time. These issues point to significant
challenges to enhance the classification performance, especially in a
subject-independent manner. To overcome these challenges, we propose MIN2Net, a
novel end-to-end multi-task learning to tackle this task. We integrate deep
metric learning into a multi-task autoencoder to learn a compact and
discriminative latent representation from EEG and perform classification
simultaneously. This approach reduces the complexity in pre-processing, results
in significant performance improvement on EEG classification. Experimental
results in a subject-independent manner show that MIN2Net outperforms the
state-of-the-art techniques, achieving an F1-score improvement of 6.72%, and
2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that
MIN2Net improves discriminative information in the latent representation. This
study indicates the possibility and practicality of using this model to develop
MI-based BCI applications for new users without the need for calibration.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.04525</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.04525</id><submitter>Michael Yeung</submitter><version version="v1"><date>Mon, 8 Feb 2021 20:47:38 GMT</date><size>4154kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 08:49:30 GMT</date><size>7087kb</size><source_type>D</source_type></version><title>Generalised Focal Loss: Unifying Dice and Cross Entropy-based Losses to
  Handle Class Imbalanced Medical Image Segmentation</title><authors>Michael Yeung, Evis Sala, Carola-Bibiane Sch\&quot;onlieb, Leonardo Rundo</authors><categories>eess.IV cs.CV cs.LG</categories><acm-class>I.4.6; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic segmentation methods are an important advancement in medical
imaging analysis. Machine learning techniques, and deep neural networks in
particular, are the state-of-the-art for most medical image segmentation tasks.
Issues with class imbalance pose a significant challenge in medical datasets,
with lesions often occupying a considerably smaller volume relative to the
background. Loss functions used in the training of deep learning algorithms
differ in their robustness to class imbalance, with direct consequences for
model convergence. The most commonly used loss functions for segmentation are
based on either the cross entropy loss, Dice loss or a combination of the two.
We propose a Generalised Focal loss, a new framework that unifies Dice and
cross entropy-based losses for handling class imbalance. We evaluate our
proposed loss function on three highly class imbalanced, publicly available
medical imaging datasets: Breast Ultrasound 2017 (BUS2017), Brain Tumour
Segmentation 2020 (BraTS20) and Kidney Tumour Segmentation 2019 (KiTS19). We
compare our loss function performance to six Dice or cross entropy-based loss
functions, and demonstrate that our proposed loss function is robust to class
imbalance, outperforming the other loss functions across datasets. Finally, we
use the Generalised Focal loss together with deep supervision to achieve
state-of-the-art results without modification of the original U-Net
architecture.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.04685</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.04685</id><submitter>Songlin He</submitter><version version="v1"><date>Tue, 9 Feb 2021 07:22:48 GMT</date><size>1824kb</size></version><version version="v2"><date>Wed, 12 May 2021 13:58:06 GMT</date><size>1824kb</size></version><version version="v3"><date>Fri, 14 May 2021 22:39:21 GMT</date><size>1713kb</size></version><title>Fair Peer-to-Peer Content Delivery via Blockchain</title><authors>Songlin He, Yuan Lu, Qiang Tang, Guiling Wang, Chase Qishi Wu</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-peer (p2p) content delivery is promising to provide benefits like
cost-saving and scalable peak-demand handling in comparison with conventional
content delivery networks (CDNs) and complement the decentralized storage
networks such as Filecoin. However, reliable p2p delivery requires proper
enforcement of delivery fairness, i.e., the deliverers should be rewarded
according to their in-time delivery. Unfortunately, most existing studies on
delivery fairness are based on non-cooperative game-theoretic assumptions that
are arguably unrealistic in the ad-hoc p2p setting. We for the first time put
forth the expressive yet still minimalist securities for p2p content delivery,
and give two efficient solutions FairDownload and FairStream via the blockchain
for p2p downloading and p2p streaming scenarios, respectively. Our designs not
only guarantee delivery fairness to ensure deliverers be paid (nearly)
proportional to his in-time delivery, but also ensure the content consumers and
content providers to be fairly treated. The fairness of each party can be
guaranteed when the other two parties collude to arbitrarily misbehave.
Moreover, the systems are efficient in the sense of attaining asymptotically
optimal on-chain costs and optimal deliverer communication. We implement the
protocols to build the prototype systems atop the Ethereum Ropsten network.
Extensive experiments done in LAN and WAN settings showcase their high
practicality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05884</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05884</id><submitter>Glenn Dawson</submitter><version version="v1"><date>Thu, 11 Feb 2021 08:12:44 GMT</date><size>347kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 03:59:53 GMT</date><size>346kb</size><source_type>D</source_type></version><title>OpinionRank: Extracting Ground Truth Labels from Unreliable Expert
  Opinions with Graph-Based Spectral Ranking</title><authors>Glenn Dawson and Robi Polikar</authors><categories>cs.LG</categories><comments>8 pages, 5 figures, accepted at IJCNN 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As larger and more comprehensive datasets become standard in contemporary
machine learning, it becomes increasingly more difficult to obtain reliable,
trustworthy label information with which to train sophisticated models. To
address this problem, crowdsourcing has emerged as a popular, inexpensive, and
efficient data mining solution for performing distributed label collection.
However, crowdsourced annotations are inherently untrustworthy, as the labels
are provided by anonymous volunteers who may have varying, unreliable
expertise. Worse yet, some participants on commonly used platforms such as
Amazon Mechanical Turk may be adversarial, and provide intentionally incorrect
label information without the end user's knowledge. We discuss three
conventional models of the label generation process, describing their
parameterizations and the model-based approaches used to solve them. We then
propose OpinionRank, a model-free, interpretable, graph-based spectral
algorithm for integrating crowdsourced annotations into reliable labels for
performing supervised or semi-supervised learning. Our experiments show that
OpinionRank performs favorably when compared against more highly parameterized
algorithms. We also show that OpinionRank is scalable to very large datasets
and numbers of label sources, and requires considerably fewer computational
resources than previous approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.05963</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.05963</id><submitter>Alejandro Sztrajman</submitter><version version="v1"><date>Thu, 11 Feb 2021 12:00:24 GMT</date><size>46246kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 12 Feb 2021 12:38:18 GMT</date><size>46241kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 14 May 2021 21:57:47 GMT</date><size>44354kb</size><source_type>D</source_type></version><title>Neural BRDF Representation and Importance Sampling</title><authors>Alejandro Sztrajman, Gilles Rainer, Tobias Ritschel, Tim Weyrich</authors><categories>cs.GR cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlled capture of real-world material appearance yields tabulated sets of
highly realistic reflectance data. In practice, however, its high memory
footprint requires compressing into a representation that can be used
efficiently in rendering while remaining faithful to the original. Previous
works in appearance encoding often prioritised one of these requirements at the
expense of the other, by either applying high-fidelity array compression
strategies not suited for efficient queries during rendering, or by fitting a
compact analytic model that lacks expressiveness. We present a compact neural
network-based representation of BRDF data that combines high-accuracy
reconstruction with efficient practical rendering via built-in interpolation of
reflectance. We encode BRDFs as lightweight networks, and propose a training
scheme with adaptive angular sampling, critical for the accurate reconstruction
of specular highlights. Additionally, we propose a novel approach to make our
representation amenable to importance sampling: rather than inverting the
trained networks, we learn to encode them in a more compact embedding that can
be mapped to parameters of an analytic BRDF for which importance sampling is
known. We evaluate encoding results on isotropic and anisotropic BRDFs from
multiple real-world datasets, and importance sampling performance for isotropic
BRDFs mapped to two different analytic models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06454</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06454</id><submitter>Guillaume Carbajal</submitter><version version="v1"><date>Fri, 12 Feb 2021 11:32:48 GMT</date><size>258kb</size><source_type>D</source_type></version><title>Guided Variational Autoencoder for Speech Enhancement With a Supervised
  Classifier</title><authors>Guillaume Carbajal, Julius Richter, Timo Gerkmann</authors><categories>eess.AS cs.LG cs.SD</categories><journal-ref>ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP39728.2021.9414363</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, variational autoencoders have been successfully used to learn a
probabilistic prior over speech signals, which is then used to perform speech
enhancement. However, variational autoencoders are trained on clean speech
only, which results in a limited ability of extracting the speech signal from
noisy speech compared to supervised approaches. In this paper, we propose to
guide the variational autoencoder with a supervised classifier separately
trained on noisy speech. The estimated label is a high-level categorical
variable describing the speech signal (e.g. speech activity) allowing for a
more informed latent distribution compared to the standard variational
autoencoder. We evaluate our method with different types of labels on real
recordings of different noisy environments. Provided that the label better
informs the latent distribution and that the classifier achieves good
performance, the proposed approach outperforms the standard variational
autoencoder and a conventional neural network-based supervised approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06520</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06520</id><submitter>Carsten Scherer</submitter><version version="v1"><date>Fri, 12 Feb 2021 13:45:18 GMT</date><size>759kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 10:31:40 GMT</date><size>763kb</size><source_type>D</source_type></version><title>Convex Synthesis of Accelerated Gradient Algorithms</title><authors>Carsten Scherer and Christian Ebenbauer</authors><categories>math.OC cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present a convex solution for the design of generalized accelerated
gradient algorithms for strongly convex objective functions with Lipschitz
continuous gradients. We utilize integral quadratic constraints and the Youla
parameterization from robust control theory to formulate a solution of the
algorithm design problem as a convex semi-definite program. We establish
explicit formulas for the optimal convergence rates and extend the proposed
synthesis solution to extremum control problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06652</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06652</id><submitter>Philipp Reichenbach</submitter><version version="v1"><date>Fri, 12 Feb 2021 17:47:18 GMT</date><size>826kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 16:43:56 GMT</date><size>80kb</size><source_type>D</source_type></version><title>Barriers for recent methods in geodesic optimization</title><authors>Cole Franks and Philipp Reichenbach</authors><categories>cs.CC math.CO math.OC</categories><comments>worked in referee comments, added references and acknowledgments</comments><msc-class>Primary: 68Q17, Secondary: 05Dxx, 05E10, 14Q20, 20G05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of optimization problems including matrix scaling, matrix
balancing, multidimensional array scaling, operator scaling, and tensor scaling
that arise frequently in theory and in practice. Some of these problems, such
as matrix and array scaling, are convex in the Euclidean sense, but others such
as operator scaling and tensor scaling are geodesically convex on a different
Riemannian manifold. Trust region methods, which include box-constrained
Newton's method, are known to produce high precision solutions very quickly for
matrix scaling and matrix balancing (Cohen et. al., FOCS 2017, Allen-Zhu et.
al. FOCS 2017), and result in polynomial time algorithms for some geodesically
convex problems like operator scaling (Garg et. al. STOC 2018, B\&quot;urgisser et.
al. FOCS 2019). One is led to ask whether these guarantees also hold for
multidimensional array scaling and tensor scaling.
  We show that this is not the case by exhibiting instances with exponential
diameter bound: we construct polynomial-size instances of 3-dimensional array
scaling and 3-tensor scaling whose approximate solutions all have doubly
exponential condition number. Moreover, we study convex-geometric notions of
complexity known as margin and gap, which are used to bound the running times
of all existing optimization algorithms for such problems. We show that margin
and gap are exponentially small for several problems including array scaling,
tensor scaling and polynomial scaling. Our results suggest that it is
impossible to prove polynomial running time bounds for tensor scaling based on
diameter bounds alone. Therefore, our work motivates the search for analogues
of more sophisticated algorithms, such as interior point methods, for
geodesically convex optimization that do not rely on polynomial diameter
bounds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06860</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06860</id><submitter>Clara Lacroce</submitter><version version="v1"><date>Sat, 13 Feb 2021 04:54:46 GMT</date><size>30kb</size></version><version version="v2"><date>Mon, 15 Mar 2021 14:38:09 GMT</date><size>31kb</size></version><version version="v3"><date>Mon, 17 May 2021 15:01:28 GMT</date><size>32kb</size></version><title>Optimal Spectral-Norm Approximate Minimization of Weighted Finite
  Automata</title><authors>Borja Balle, Clara Lacroce, Prakash Panangaden, Doina Precup,
  Guillaume Rabusseau</authors><categories>cs.FL</categories><comments>Full version of ICALP2021 paper, authors are listed in alphabetical
  order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the approximate minimization problem for weighted finite automata
(WFAs) with weights in $\mathbb{R}$, over a one-letter alphabet: to compute the
best possible approximation of a WFA given a bound on the number of states.
This work is grounded in Adamyan-Arov-Krein Approximation theory, a remarkable
collection of results on the approximation of Hankel operators. In addition to
its intrinsic mathematical relevance, this theory has proven to be very
effective for model reduction. We adapt these results to the framework of
weighted automata over a one-letter alphabet. We provide theoretical guarantees
and bounds on the quality of the approximation in the spectral and $\ell^2$
norm. We develop an algorithm that, based on the properties of Hankel
operators, returns the optimal approximation in the spectral norm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06904</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06904</id><submitter>Marcin Bienkowski</submitter><version version="v1"><date>Sat, 13 Feb 2021 11:31:41 GMT</date><size>410kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 20:57:39 GMT</date><size>367kb</size><source_type>D</source_type></version><title>Traveling Repairperson, Unrelated Machines, and Other Stories About
  Average Completion Times</title><authors>Marcin Bienkowski, Artur Kraska, Hsiang-Hsuan Liu</authors><categories>cs.DS</categories><comments>ICALP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unified framework for minimizing average completion time for
many seemingly disparate online scheduling problems, such as the traveling
repairperson problems (TRP), dial-a-ride problems (DARP), and scheduling on
unrelated machines.
  We construct a simple algorithm that handles all these scheduling problems,
by computing and later executing auxiliary schedules, each optimizing a certain
function on already seen prefix of the input. The optimized function resembles
a prize-collecting variant of the original scheduling problem. By a careful
analysis of the interplay between these auxiliary schedules, and later
employing the resulting inequalities in a factor-revealing linear program, we
obtain improved bounds on the competitive ratio for all these scheduling
problems.
  In particular, our techniques yield a $4$-competitive deterministic algorithm
for all previously studied variants of online TRP and DARP, and a
$3$-competitive one for the scheduling on unrelated machines (also with
precedence constraints). This improves over currently best ratios for these
problems that are $5.14$ and $4$, respectively. We also show how to use
randomization to further reduce the competitive ratios to $1+2/\ln 3 &lt; 2.821$
and $1+1/\ln 2 &lt; 2.443$, respectively. The randomized bounds also substantially
improve the current state of the art. Our upper bound for DARP contradicts the
lower bound of 3 given by Fink et al. (Inf. Process. Lett. 2009); we pinpoint a
flaw in their proof.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.06933</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.06933</id><submitter>Lijun Zhang</submitter><version version="v1"><date>Sat, 13 Feb 2021 14:15:55 GMT</date><size>26kb</size></version><version version="v2"><date>Mon, 29 Mar 2021 05:59:58 GMT</date><size>28kb</size></version><version version="v3"><date>Tue, 18 May 2021 09:03:07 GMT</date><size>30kb</size></version><title>Revisiting Smoothed Online Learning</title><authors>Lijun Zhang, Wei Jiang, Shiyin Lu, Tianbao Yang</authors><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the problem of smoothed online learning, in which
the online learner suffers both a hitting cost and a switching cost, and target
two performance metrics: competitive ratio and dynamic regret with switching
cost.
  To bound the competitive ratio, we assume the hitting cost is known to the
learner in each round, and investigate the simple idea of balancing the two
costs by an optimization problem. Surprisingly, we find that minimizing the
hitting cost alone is $\max(1, \frac{2}{\alpha})$-competitive for
$\alpha$-polyhedral functions and $1 + \frac{4}{\lambda}$-competitive for
$\lambda$-quadratic growth functions, both of which improve state-of-the-art
results significantly. Moreover, when the hitting cost is both convex and
$\lambda$-quadratic growth, we reduce the competitive ratio to $1 +
\frac{2}{\sqrt{\lambda}}$ by minimizing the weighted sum of the hitting cost
and the switching cost.
  To bound the dynamic regret with switching cost, we follow the standard
setting of online convex optimization, in which the hitting cost is convex but
hidden from the learner before making predictions. We modify Ader, an existing
algorithm designed for dynamic regret, slightly to take into account the
switching cost when measuring the performance. The proposed algorithm, named as
Smoothed Ader, attains an optimal $O(\sqrt{T(1+P_T)})$ bound for dynamic regret
with switching cost, where $P_T$ is the path-length of the comparator sequence.
Furthermore, if the hitting cost is accessible in the beginning of each round,
we obtain a similar guarantee without the bounded gradient condition, and
establish an $\Omega(\sqrt{T(1+P_T)})$ lower bound to confirm the optimality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.07771</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.07771</id><submitter>Cyrus Mostajeran Dr</submitter><version version="v1"><date>Mon, 15 Feb 2021 17:30:11 GMT</date><size>676kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 16:30:10 GMT</date><size>675kb</size><source_type>D</source_type></version><title>Online learning of Riemannian hidden Markov models in homogeneous
  Hadamard spaces</title><authors>Quinten Tupker, Salem Said, Cyrus Mostajeran</authors><categories>cs.LG math.DG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov models with observations in a Euclidean space play an important
role in signal and image processing. Previous work extending to models where
observations lie in Riemannian manifolds based on the Baum-Welch algorithm
suffered from high memory usage and slow speed. Here we present an algorithm
that is online, more accurate, and offers dramatic improvements in speed and
efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08173</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08173</id><submitter>Giorgos Stamatelatos</submitter><version version="v1"><date>Tue, 16 Feb 2021 14:22:27 GMT</date><size>386kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 13:26:57 GMT</date><size>1171kb</size><source_type>D</source_type></version><title>About Weighted Random Sampling in Preferential Attachment Models</title><authors>Giorgos Stamatelatos and Pavlos S. Efraimidis</authors><categories>cs.DS cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Barab\'asi-Albert model is a popular scheme for creating scale-free
graphs but has been previously shown to have ambiguities in its definition. In
this paper we discuss a new ambiguity in the definition of the BA model by
identifying the tight relation between the preferential attachment process and
unequal probability random sampling. While the probability that each individual
vertex is selected is set to be proportional to their degree, the model does
not specify the joint probabilities that any tuple of $m$ vertices is selected
together for $m&gt;1$. We demonstrate the consequences using analytical,
experimental, and empirical analyses and propose a concise definition of the
model that addresses this ambiguity. Using the connection with unequal
probability random sampling, we also highlight a confusion about the process
via which nodes are selected on each time step, for which -- despite being
implicitly indicated in the original paper -- current literature appears
fragmented.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08372</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08372</id><submitter>Ali Shokri</submitter><version version="v1"><date>Tue, 16 Feb 2021 18:58:02 GMT</date><size>23005kb</size><source_type>D</source_type></version><title>ArCode: Facilitating the Use of Application Frameworks to Implement
  Tactics and Patterns</title><authors>Ali Shokri, Joanna C. S. Santos, Mehdi Mirakhorli</authors><categories>cs.SE</categories><comments>This paper has been accepted in the main track of 2021 IEEE
  International Conference on Software Architecture (ICSA 2021) and is going to
  be published. Please feel free to cite it</comments><doi>10.1109/ICSA51549.2021.00021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software designers and developers are increasingly relying on application
frameworks as first-class design concepts. They instantiate the services that
frameworks provide to implement various architectural tactics and patterns. One
of the challenges in using frameworks for such tasks is the difficulty of
learning and correctly using frameworks' APIs. This paper introduces a
learning-based approach called ArCode to help novice programmers correctly use
frameworks' APIs to implement architectural tactics and patterns. ArCode has
several novel components: a graph-based approach for learning specification of
a framework from a limited number of training software, a program analysis
algorithm to eliminate erroneous training data, and a recommender module to
help programmers use APIs correctly and identify API misuses in their programs.
We evaluated our technique across two popular frameworks: JAAS security
framework used for authentication and authorization tactic and Java RMI
framework used to enable remote method invocation between client and server and
other object-oriented patterns. Our evaluation results show (i) the feasibility
of using ArCode to learn the specification of a framework; (ii) ArCode
generates accurate recommendations for finding the next API call to implement
an architectural tactic/pattern based on the context of the programmer's code;
(iii) it accurately detects API misuses in the code that implements a
tactic/pattern and provides fix recommendations. Comparison of ArCode with two
prior techniques (MAPO and GrouMiner) on API recommendation and misuse
detection shows that ArCode outperforms these approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08583</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08583</id><submitter>Donghwan Lee</submitter><version version="v1"><date>Wed, 17 Feb 2021 05:32:07 GMT</date><size>405kb</size></version><version version="v2"><date>Fri, 19 Feb 2021 15:52:19 GMT</date><size>405kb</size></version><version version="v3"><date>Mon, 22 Feb 2021 05:11:23 GMT</date><size>406kb</size></version><version version="v4"><date>Sat, 15 May 2021 15:49:52 GMT</date><size>405kb</size></version><version version="v5"><date>Tue, 18 May 2021 14:26:32 GMT</date><size>405kb</size></version><title>A Discrete-Time Switching System Analysis of Q-learning</title><authors>Donghwan Lee, Jianghai Hu, Niao He</authors><categories>math.OC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a novel control-theoretic framework to analyze the
non-asymptotic convergence of Q-learning. We show that the dynamics of
asynchronous Q-learning with a constant step-size can be naturally formulated
as a discrete-time stochastic affine switching system. Moreover, the evolution
of the Q-learning estimation error is over- and underestimated by trajectories
of two simpler dynamical systems. Based on these two systems, we derive a new
finite-time error bound of asynchronous Q-learning when a constant stepsize is
used. Our analysis also sheds light on the overestimation phenomenon of
Q-learning. We further illustrate and validate the analysis through numerical
simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08703</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08703</id><submitter>Darya Melnyk</submitter><version version="v1"><date>Wed, 17 Feb 2021 11:18:10 GMT</date><size>229kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 22 Mar 2021 11:07:13 GMT</date><size>230kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 10:47:21 GMT</date><size>191kb</size><source_type>D</source_type></version><title>Local Mending</title><authors>Alkida Balliu, Juho Hirvonen, Darya Melnyk, Dennis Olivetti, Joel
  Rybicki, and Jukka Suomela</authors><categories>cs.DC cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce the graph-theoretic notion of mendability: for each
locally checkable graph problem we can define its mending radius, which
captures the idea of how far one needs to modify a partial solution in order to
&quot;patch a hole.&quot;
  We explore how mendability is connected to the existence of efficient
algorithms, especially in distributed, parallel, and fault-tolerant settings.
It is easy to see that $O(1)$-mendable problems are also solvable in $O(\log^*
n)$ rounds in the LOCAL model of distributed computing. One of the surprises is
that in paths and cycles, a converse also holds in the following sense: if a
problem $\Pi$ can be solved in $O(\log^* n)$, there is always a restriction
$\Pi' \subseteq \Pi$ that is still efficiently solvable but that is also
$O(1)$-mendable.
  We also explore the structure of the landscape of mendability. For example,
we show that in trees, the mending radius of any locally checkable problem is
$O(1)$, $\Theta(\log n)$, or $\Theta(n)$, while in general graphs the structure
is much more diverse.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08706</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08706</id><submitter>Huajian Fang</submitter><version version="v1"><date>Wed, 17 Feb 2021 11:40:42 GMT</date><size>236kb</size><source_type>D</source_type></version><title>Variational Autoencoder for Speech Enhancement with a Noise-Aware
  Encoder</title><authors>Huajian Fang, Guillaume Carbajal, Stefan Wermter, Timo Gerkmann</authors><categories>eess.AS cs.LG cs.SD</categories><comments>ICASSP 2021. (c) 2021 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses, in any
  current or future media, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work in other works</comments><journal-ref>ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP39728.2021.9414060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a generative variational autoencoder (VAE) has been proposed for
speech enhancement to model speech statistics. However, this approach only uses
clean speech in the training phase, making the estimation particularly
sensitive to noise presence, especially in low signal-to-noise ratios (SNRs).
To increase the robustness of the VAE, we propose to include noise information
in the training phase by using a noise-aware encoder trained on noisy-clean
speech pairs. We evaluate our approach on real recordings of different noisy
environments and acoustic conditions using two different noise datasets. We
show that our proposed noise-aware VAE outperforms the standard VAE in terms of
overall distortion without increasing the number of model parameters. At the
same time, we demonstrate that our model is capable of generalizing to unseen
noise conditions better than a supervised feedforward deep neural network
(DNN). Furthermore, we demonstrate the robustness of the model performance to a
reduction of the noisy-clean speech training data size.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08823</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08823</id><submitter>Akaki Mamageishvili</submitter><version version="v1"><date>Wed, 17 Feb 2021 15:32:32 GMT</date><size>821kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 11:58:53 GMT</date><size>1705kb</size><source_type>D</source_type></version><title>Vote Delegation and Misbehavior</title><authors>Hans Gersbach, Akaki Mamageishvili, Manvir Schneider</authors><categories>cs.GT econ.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study vote delegation with &quot;well-behaving&quot; and &quot;misbehaving&quot; agents and
compare it with conventional voting. Typical examples for vote delegation are
validation or governance tasks on blockchains. There is a majority of
well-behaving agents, but they may abstain or delegate their vote to other
agents since voting is costly. Misbehaving agents always vote. We compare
conventional voting allowing for abstention with vote delegation. Preferences
of voters are private information and a positive outcome is achieved if
well-behaving agents win. We illustrate that vote delegation leads to quite
different outcomes than conventional voting with abstention. In particular, we
obtain three insights: First, if the number of misbehaving voters, denoted by f
, is high, both voting methods fail to deliver a positive outcome. Second, if f
takes an intermediate value, conventional voting delivers a positive outcome,
while vote delegation fails with probability one. Third, if f is low,
delegation delivers a positive outcome with higher probability than
conventional voting. Finally, our results characterize worst-case outcomes that
can happen in a liquid democracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.08835</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.08835</id><submitter>Akaki Mamageishvili</submitter><version version="v1"><date>Wed, 17 Feb 2021 15:44:38 GMT</date><size>219kb</size></version><version version="v2"><date>Sat, 15 May 2021 12:27:41 GMT</date><size>190kb</size></version><title>Vote Delegation with Uncertain Number of Voters</title><authors>Hans Gersbach, Akaki Mamageishvili, Manvir Schneider</authors><categories>cs.GT econ.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine vote delegation when delegators do not know the preferences of
representatives. We show that free delegation favors minorities, that is,
alternatives that have a lower chance of winning ex-ante. The same--but to a
lesser degree--occurs if the number of voting rights actual voters can have is
capped. However, when the number of delegators increases, the probability that
the ex-ante minority wins under free and capped delegation converges to the one
under conventional voting--albeit non-monotonically. Finally, when the total
number of voters is converging to infinity with a fixed fraction of the
majority, all three probabilities converge to one, no matter the number of
delegators. Therefore, vote delegation is safe on a large scale.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09149</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09149</id><submitter>Takashi Yamakawa</submitter><version version="v1"><date>Thu, 18 Feb 2021 04:10:00 GMT</date><size>4895kb</size></version><version version="v2"><date>Sun, 16 May 2021 09:12:34 GMT</date><size>73kb</size></version><title>Classically Verifiable (Dual-Mode) NIZK for QMA with Preprocessing</title><authors>Tomoyuki Morimae and Takashi Yamakawa</authors><categories>quant-ph cs.CC cs.CR</categories><comments>46 pages This is a major update version of arXiv:2003.10712</comments><report-no>YITP-21-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose three constructions of classically verifiable non-interactive
proofs (CV-NIP) and non-interactive zero-knowledge proofs and arguments
(CV-NIZK) for QMA in various preprocessing models.
  - We construct an information theoretically sound CV-NIP for QMA in the
secret parameter model where a trusted party generates a quantum proving key
and classical verification key and gives them to the corresponding parties
while keeping it secret from the other party. Alternatively, we can think of
the protocol as one in a model where the verifier sends an instance-independent
quantum message to the prover as preprocessing.
  - We construct a CV-NIZK for QMA in the secret parameter model. It is
information theoretically sound and zero-knowledge.
  - Assuming the quantum hardness of the leaning with errors problem, we
construct a CV-NIZK for QMA in a model where a trusted party generates a CRS
and the verifier sends an instance-independent quantum message to the prover as
preprocessing. This model is the same as one considered in the recent work by
Coladangelo, Vidick, and Zhang (CRYPTO '20). Our construction has the so-called
dual-mode property, which means that there are two computationally
indistinguishable modes of generating CRS, and we have information theoretical
soundness in one mode and information theoretical zero-knowledge property in
the other. This answers an open problem left by Coladangelo et al, which is to
achieve either of soundness or zero-knowledge information theoretically. To the
best of our knowledge, ours is the first dual-mode NIZK for QMA in any kind of
model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09211</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09211</id><submitter>Jianxun Lian</submitter><version version="v1"><date>Thu, 18 Feb 2021 08:24:14 GMT</date><size>1001kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 4 Mar 2021 05:01:09 GMT</date><size>1001kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 08:45:03 GMT</date><size>1001kb</size><source_type>D</source_type></version><title>Multi-Interest-Aware User Modeling for Large-Scale Sequential
  Recommendations</title><authors>Jianxun Lian, Iyad Batal, Zheng Liu, Akshay Soni, Eun Yong Kang, Yajun
  Wang, Xing Xie</authors><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Precise user modeling is critical for online personalized recommendation
services. Generally, users' interests are diverse and are not limited to a
single aspect, which is particularly evident when their behaviors are observed
for a longer time. For example, a user may demonstrate interests in cats/dogs,
dancing and food \&amp; delights when browsing short videos on Tik Tok; the same
user may show interests in real estate and women's wear in her web browsing
behaviors. Traditional models tend to encode a user's behaviors into a single
embedding vector, which do not have enough capacity to effectively capture her
diverse interests.
  This paper proposes a Sequential User Matrix (SUM) to accurately and
efficiently capture users' diverse interests. SUM models user behavior with a
multi-channel network, with each channel representing a different aspect of the
user's interests. User states in different channels are updated by an
\emph{erase-and-add} paradigm with interest- and instance-level attention. We
further propose a local proximity debuff component and a highway connection
component to make the model more robust and accurate. SUM can be maintained and
updated incrementally, making it feasible to be deployed for large-scale online
serving. We conduct extensive experiments on two datasets. Results demonstrate
that SUM consistently outperforms state-of-the-art baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09688</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09688</id><submitter>Raghavendra Ramesh</submitter><version version="v1"><date>Fri, 19 Feb 2021 00:20:35 GMT</date><size>97kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 23:09:43 GMT</date><size>166kb</size><source_type>D</source_type></version><title>Algorithm for Cross-shard Cross-EE Atomic User-level ETH Transfer in
  Ethereum</title><authors>Raghavendra Ramesh</authors><categories>cs.DC</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharding is a way to address scalability problem in blockchain technologies.
Ethereum, a prominent blockchain technology, has included sharding in its
roadmap to increase its throughput. The plan is also to include multiple
execution environments.
  We address the problem of atomic cross shard value transfer in the presence
of multiple execution environments. We leverage on the proposed Ethereum
architecture, more specificially on Beacon chain and crosslinks, and propose a
solution on top of the netted-balance approach that was proposed for EE-level
atomic \eth transfers. We split a cross-shard transfer into two transactions: a
debit and a credit. First, the debit transaction is processed at the source
shard. The corresponding credit transaction is processed at the destination
shard in a subsequent block. We use {\em netted} shard states as channels to
communicate pending credits and pending reverts. We discuss various scenarios
of debit failures and credit failures, and show our approach ensures atomicity
even in the presence of a Byzantine Block proposer.
  The benefits of our approach are that we do not use any locks nor impose any
constraints on the Block Proposer to select specific transactions. However we
inherit the limitation of an expensive operation from the netted-balance
approach of querying partial states from all other shards. We also show a bound
on the size of such inter-shard state reads.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.09964</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.09964</id><submitter>Adrien Corenflos</submitter><version version="v1"><date>Fri, 19 Feb 2021 14:57:17 GMT</date><size>396kb</size><source_type>AD</source_type></version><version version="v2"><date>Mon, 22 Feb 2021 07:30:40 GMT</date><size>396kb</size><source_type>AD</source_type></version><version version="v3"><date>Wed, 10 Mar 2021 19:36:16 GMT</date><size>239kb</size><source_type>AD</source_type></version><version version="v4"><date>Mon, 17 May 2021 07:23:31 GMT</date><size>191kb</size><source_type>D</source_type></version><title>Temporal Gaussian Process Regression in Logarithmic Time</title><authors>Adrien Corenflos, Zheng Zhao, Simo S\&quot;arkk\&quot;a</authors><categories>cs.LG stat.CO stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The aim of this article is to present a novel parallelization method for
temporal Gaussian process (GP) regression problems. The method allows for
solving GP regression problems in logarithmic O(log N) time, where N is the
number of time steps. Our approach uses the state-space representation of GPs
which in its original form allows for linear O(N) time GP regression by
leveraging the Kalman filtering and smoothing methods. By using a recently
proposed parallelization method for Bayesian filters and smoothers, we are able
to reduce the linear computational complexity of the temporal GP regression
problems into logarithmic span complexity. This ensures logarithmic time
complexity when run on parallel hardware such as a graphics processing unit
(GPU). We experimentally demonstrate the computational benefits on simulated
and real datasets via our open-source implementation leveraging the GPflow
framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.10283</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.10283</id><submitter>Sho Sakaino Prof.</submitter><version version="v1"><date>Sat, 20 Feb 2021 08:14:25 GMT</date><size>3025kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 10:04:40 GMT</date><size>3194kb</size><source_type>D</source_type></version><title>Imitation Learning for Variable Speed Object Manipulation</title><authors>Sho Sakaino, Kazuki Fujimoto, Yuki Saigusa, Toshiaki Tsuji</authors><categories>cs.RO cs.SY eess.SY</categories><comments>10 pages, 14 figures, submitted for IEEE Access</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To operate in a real-world environment, robots have several requirements
including environmental adaptability. Moreover, the desired success rate for
the completion of tasks must be achieved. In this regard, end-to-end learning
for autonomous operation is currently being investigated. However, the issue of
operating speed has not been investigated in detail. Therefore, in this paper,
we propose a method for generating variable operating speeds while adapting to
perturbations in the environment. When the work speed changes, there is a
nonlinear relationship between the operating speed and force (e.g., inertial
and frictional forces). However, the proposed method can be adapted to
nonlinearities by utilizing small amount of motion data. We experimentally
evaluated the proposed method for erasing a line using an eraser fixed to the
tip of a robot. Furthermore, the proposed method enables a robot to perform a
task faster than a human operator
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.10484</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.10484</id><submitter>Pranav Rajpurkar</submitter><version version="v1"><date>Sun, 21 Feb 2021 00:47:30 GMT</date><size>1032kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 07:02:56 GMT</date><size>1427kb</size><source_type>D</source_type></version><title>CheXseg: Combining Expert Annotations with DNN-generated Saliency Maps
  for X-ray Segmentation</title><authors>Soham Gadgil, Mark Endo, Emily Wen, Andrew Y. Ng, Pranav Rajpurkar</authors><categories>cs.CV cs.AI cs.LG</categories><comments>Accepted to Medical Imaging with Deep Learning (MIDL) Conference 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Medical image segmentation models are typically supervised by expert
annotations at the pixel-level, which can be expensive to acquire. In this
work, we propose a method that combines the high quality of pixel-level expert
annotations with the scale of coarse DNN-generated saliency maps for training
multi-label semantic segmentation models. We demonstrate the application of our
semi-supervised method, which we call CheXseg, on multi-label chest X-ray
interpretation. We find that CheXseg improves upon the performance (mIoU) of
fully-supervised methods that use only pixel-level expert annotations by 9.7%
and weakly-supervised methods that use only DNN-generated saliency maps by
73.1%. Our best method is able to match radiologist agreement on three out of
ten pathologies and reduces the overall performance gap by 57.2% as compared to
weakly-supervised methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.11502</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.11502</id><submitter>Hu Wang</submitter><version version="v1"><date>Tue, 23 Feb 2021 05:33:55 GMT</date><size>30291kb</size></version><version version="v2"><date>Sun, 16 May 2021 06:37:27 GMT</date><size>30294kb</size></version><title>Oriole: Thwarting Privacy against Trustworthy Deep Learning Models</title><authors>Liuqiao Chen, Hu Wang, Benjamin Zi Hao Zhao, Minhui Xue and Haifeng
  Qian</authors><categories>cs.CR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Networks have achieved unprecedented success in the field of face
recognition such that any individual can crawl the data of others from the
Internet without their explicit permission for the purpose of training
high-precision face recognition models, creating a serious violation of
privacy. Recently, a well-known system named Fawkes (published in USENIX
Security 2020) claimed this privacy threat can be neutralized by uploading
cloaked user images instead of their original images. In this paper, we present
Oriole, a system that combines the advantages of data poisoning attacks and
evasion attacks, to thwart the protection offered by Fawkes, by training the
attacker face recognition model with multi-cloaked images generated by Oriole.
Consequently, the face recognition accuracy of the attack model is maintained
and the weaknesses of Fawkes are revealed. Experimental results show that our
proposed Oriole system is able to effectively interfere with the performance of
the Fawkes system to achieve promising attacking results. Our ablation study
highlights multiple principal factors that affect the performance of the Oriole
system, including the DSSIM perturbation budget, the ratio of leaked clean user
images, and the numbers of multi-cloaks for each uncloaked image. We also
identify and discuss at length the vulnerabilities of Fawkes. We hope that the
new methodology presented in this paper will inform the security community of a
need to design more robust privacy-preserving deep learning models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.11660</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.11660</id><submitter>Davin Choo</submitter><version version="v1"><date>Tue, 23 Feb 2021 12:26:52 GMT</date><size>55kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 14:36:15 GMT</date><size>33kb</size><source_type>D</source_type></version><title>Massively Parallel Correlation Clustering in Bounded Arboricity Graphs</title><authors>M\'elanie Cambus, Davin Choo, Havu Miikonen, Jara Uitto</authors><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying clusters of similar elements in a set is a common task in data
analysis. With the immense growth of data and physical limitations on single
processor speed, it is necessary to find efficient parallel algorithms for
clustering tasks. In this paper, we study the problem of correlation clustering
in bounded arboricity graphs with respect to the Massively Parallel Computation
(MPC) model. More specifically, we are given a complete graph where the edges
are either positive or negative, indicating whether pairs of vertices are
similar or dissimilar. The task is to partition the vertices into clusters with
as few disagreements as possible. That is, we want to minimize the number of
positive inter-cluster edges and negative intra-cluster edges.
  Consider an input graph $G$ on $n$ vertices such that the positive edges
induce a $\lambda$-arboric graph. Our main result is a 3-approximation
($\textit{in expectation}$) algorithm that runs in $\mathcal{O}(\log \lambda
\cdot \textrm{poly}(\log \log n))$ MPC rounds in the $\textit{strongly
sublinear memory regime}$. This is obtained by combining structural properties
of correlation clustering on bounded arboricity graphs with the insights of
Fischer and Noever (SODA '18) on randomized greedy MIS and the $\texttt{PIVOT}$
algorithm of Ailon, Charikar, and Newman (STOC '05). Combined with known graph
matching algorithms, our structural property also implies an exact algorithm
and algorithms with $\textit{worst case}$ $(1+\epsilon)$-approximation
guarantees in the special case of forests, where $\lambda=1$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.12220</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.12220</id><submitter>Yuanxin Wu</submitter><version version="v1"><date>Wed, 24 Feb 2021 11:21:03 GMT</date><size>826kb</size></version><version version="v2"><date>Sun, 16 May 2021 08:07:48 GMT</date><size>1161kb</size></version><title>A Trident Quaternion Framework for Inertial-based Navigation Part II:
  Error Models and Application to Initial Alignment</title><authors>Wei Ouyang, Yuanxin Wu</authors><categories>cs.RO cs.SY eess.SY</categories><comments>17 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with error models for trident quaternion framework proposed
in the companion paper (Part I) and further uses them to investigate the
odometer-aided static/in-motion inertial navigation attitude alignment for land
vehicles. By linearizing the trident quaternion kinematic equation, the left
and right trident quaternion error models are obtained, which are found to be
equivalent to those derived from profound group affine. The two error models
are used to design their corresponding extended Kalman filters (EKF), namely,
the left-quaternion EKF (LQEKF) and the right-quaternion EKF (RQEKF).
Simulations and field tests are conducted to evaluate their actual
performances. Owing to the high estimation consistency, the L/RQEKF converge
much faster in the static alignment than the traditional error model-based EKF,
even under arbitrary large heading initialization. For the in-motion alignment,
the L/RQEKF possess much larger convergence region than the traditional EKF
does, although they still require the aid of attitude initialization so as to
avoid large initial attitude errors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.12293</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.12293</id><submitter>Florent Chatelain</submitter><version version="v1"><date>Wed, 24 Feb 2021 14:01:58 GMT</date><size>1507kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 25 Feb 2021 16:16:06 GMT</date><size>1517kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 06:58:23 GMT</date><size>1898kb</size><source_type>D</source_type></version><title>Two-way kernel matrix puncturing: towards resource-efficient PCA and
  spectral clustering</title><authors>Romain Couillet and Florent Chatelain and Nicolas Le Bihan</authors><categories>cs.LG stat.ML</categories><comments>24 pages (10 for the core paper, 14 for the proofs in supplementary
  materials) , 10 figures. Final version to be published in ICML 2021
  proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article introduces an elementary cost and storage reduction method for
spectral clustering and principal component analysis. The method consists in
randomly &quot;puncturing&quot; both the data matrix $X\in\mathbb{C}^{p\times n}$ (or
$\mathbb{R}^{p\times n}$) and its corresponding kernel (Gram) matrix $K$
through Bernoulli masks: $S\in\{0,1\}^{p\times n}$ for $X$ and
$B\in\{0,1\}^{n\times n}$ for $K$. The resulting &quot;two-way punctured&quot; kernel is
thus given by $K=\frac{1}{p}[(X \odot S)^{\sf H} (X \odot S)] \odot B$. We
demonstrate that, for $X$ composed of independent columns drawn from a Gaussian
mixture model, as $n,p\to\infty$ with $p/n\to c_0\in(0,\infty)$, the spectral
behavior of $K$ -- its limiting eigenvalue distribution, as well as its
isolated eigenvalues and eigenvectors -- is fully tractable and exhibits a
series of counter-intuitive phenomena. We notably prove, and empirically
confirm on GAN-generated image databases, that it is possible to drastically
puncture the data, thereby providing possibly huge computational and storage
gains, for a virtually constant (clustering of PCA) performance. This
preliminary study opens as such the path towards rethinking, from a large
dimensional standpoint, computational and storage costs in elementary machine
learning models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2102.12787</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2102.12787</id><submitter>Yuval Emek</submitter><version version="v1"><date>Thu, 25 Feb 2021 11:19:03 GMT</date><size>69kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 09:53:04 GMT</date><size>69kb</size><source_type>D</source_type></version><title>A Thin Self-Stabilizing Asynchronous Unison Algorithm with Applications
  to Fault Tolerant Biological Networks</title><authors>Yuval Emek and Eyal Keren</authors><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Introduced by Emek and Wattenhofer (PODC 2013), the \emph{stone age (SA)}
model provides an abstraction for network algorithms distributed over
randomized finite state machines. This model, designed to resemble the dynamics
of biological processes in cellular networks, assumes a weak communication
scheme that is built upon the nodes' ability to sense their vicinity in an
asynchronous manner. Recent works demonstrate that the weak computation and
communication capabilities of the SA model suffice for efficient solutions to
some core tasks in distributed computing, but they do so under the (somewhat
less realistic) assumption of fault free computations. In this paper, we
initiate the study of \emph{self-stabilizing} SA algorithms that are guaranteed
to recover from any combination of transient faults. Specifically, we develop
efficient self-stabilizing SA algorithms for the \emph{leader election} and
\emph{maximal independent set} tasks in bounded diameter graphs subject to an
asynchronous scheduler. These algorithms rely on a novel efficient
self-stabilizing \emph{asynchronous unison (AU)} algorithm that is &quot;thin&quot; in
terms of its state space: the number of states used by the AU algorithm is
linear in the graph's diameter bound, irrespective of the number of nodes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00699</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00699</id><submitter>Yang Zhou</submitter><version version="v1"><date>Mon, 1 Mar 2021 02:20:50 GMT</date><size>818kb</size></version><version version="v2"><date>Fri, 12 Mar 2021 05:17:57 GMT</date><size>0kb</size><source_type>I</source_type></version><version version="v3"><date>Sun, 16 May 2021 03:09:33 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Infrastructure Assisted Constrained Connected Automated Vehicle
  Trajectory Optimization on Curved Roads: A Spatial Formulation on a
  Curvilinear Coordinate</title><authors>Ran Yi, Yang Zhou, Xin Wang, Zhiyuan Liu, Xiaotian Li, Bin Ran</authors><categories>eess.SY cs.SY</categories><comments>Some wrong mathematical derivations. Results needs to be re-verified</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle trajectory optimization is essential to ensure vehicles travel
efficiently and safely. This paper presents an infrastructure assisted
constrained connected automated vehicles (CAVs) trajectory optimization method
on curved roads. This paper systematically formulates the problem based on a
curvilinear coordinate which is flexible to model complex road geometries.
Further, to deal with the spatial varying road obstacles, traffic regulations,
and geometric characteristics, two-dimensional vehicle kinematics is given in a
spatial formulation with exact road information provided by the infrastructure.
Consequently, we applied a multi-objective model predictive control (MPC)
approach to optimize the trajectories in a rolling horizon while satisfying the
collision avoidances and vehicle kinematics constraints. To verify the
efficiency of our method, a numerical simulation is conducted. As the results
suggest, the proposed method can provide smooth vehicular trajectories, avoid
road obstacles, and simultaneously follow traffic regulations, which is robust
to road geometries and disturbances.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00854</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00854</id><submitter>Rajaswa Patil</submitter><version version="v1"><date>Mon, 1 Mar 2021 09:07:58 GMT</date><size>5388kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 15:34:26 GMT</date><size>5390kb</size><source_type>D</source_type></version><title>Vy\=akarana: A Colorless Green Benchmark for Syntactic Evaluation in
  Indic Languages</title><authors>Rajaswa Patil, Jasleen Dhillon, Siddhant Mahurkar, Saumitra Kulkarni,
  Manav Malhotra and Veeky Baths</authors><categories>cs.CL</categories><comments>Accepted at ACL-IJCNLP SRW 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While there has been significant progress towards developing NLU resources
for Indic languages, syntactic evaluation has been relatively less explored.
Unlike English, Indic languages have rich morphosyntax, grammatical genders,
free linear word-order, and highly inflectional morphology. In this paper, we
introduce Vy\=akarana: a benchmark of gender-balanced Colorless Green sentences
in Indic languages for syntactic evaluation of multilingual language models.
The benchmark comprises four syntax-related tasks: PoS Tagging, Syntax
Tree-depth Prediction, Grammatical Case Marking, and Subject-Verb Agreement. We
use the datasets from the evaluation tasks to probe five multilingual language
models of varying architectures for syntax in Indic languages. Due to its
prevalence, we also include a code-switching setting in our experiments. Our
results show that the token-level and sentence-level representations from the
Indic language models (IndicBERT and MuRIL) do not capture the syntax in Indic
languages as efficiently as the other highly multilingual language models.
Further, our layer-wise probing experiments reveal that while mBERT,
DistilmBERT, and XLM-R localize the syntax in middle layers, the Indic language
models do not show such syntactic localization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.00947</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.00947</id><submitter>Yunshuang Li</submitter><version version="v1"><date>Mon, 1 Mar 2021 12:22:11 GMT</date><size>2996kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 08:59:58 GMT</date><size>3275kb</size><source_type>D</source_type></version><title>Collaborative Recognition of Feasible Region with Aerial and Ground
  Robots through DPCN</title><authors>Yunshuang Li, Zheyuan Huang, Zexi chen, Yue Wang and Rong Xiong</authors><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ground robots always get collision in that only if they get close to the
obstacles, can they sense the danger and take actions, which is usually too
late to avoid the crash, causing severe damage to the robots. To address this
issue, we present collaboration of aerial and ground robots in recognition of
feasible region. Taking the aerial robots' advantages of having large scale
variance of view points of the same route which the ground robots is on, the
collaboration work provides global information of road segmentation for the
ground robot, thus enabling it to obtain feasible region and adjust its pose
ahead of time. Under normal circumstance, the transformation between these two
devices can be obtained by GPS yet with much error, directly causing inferior
influence on recognition of feasible region. Thereby, we utilize the
state-of-the-art research achievements in matching heterogeneous sensor
measurements called deep phase correlation network(DPCN), which has excellent
performance on heterogeneous mapping, to refine the transformation. The network
is light-weighted and promising for better generalization. We use Aero-Ground
dataset which consists of heterogeneous sensor images and aerial road
segmentation images. The results show that our collaborative system has great
accuracy, speed and stability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.01301</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.01301</id><submitter>Iana Polonskaia</submitter><version version="v1"><date>Mon, 1 Mar 2021 20:45:24 GMT</date><size>3482kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 21:49:55 GMT</date><size>2062kb</size><source_type>D</source_type></version><title>Multi-Objective Evolutionary Design of Composite Data-Driven Models</title><authors>Iana S. Polonskaia, Nikolay O. Nikitin, Ilia Revin, Pavel Vychuzhanin,
  Anna V. Kalyuzhnaya</authors><categories>cs.NE cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a multi-objective approach for the design of composite
data-driven mathematical models is proposed. It allows automating the
identification of graph-based heterogeneous pipelines that consist of different
blocks: machine learning models, data preprocessing blocks, etc. The
implemented approach is based on a parameter-free genetic algorithm (GA) for
model design called GPComp@Free. It is developed to be part of automated
machine learning solutions and to increase the efficiency of the modeling
pipeline automation. A set of experiments was conducted to verify the
correctness and efficiency of the proposed approach and substantiate the
selected solutions. The experimental results confirm that a multi-objective
approach to the model design allows achieving better diversity and quality of
obtained models. The implemented approach is available as a part of the
open-source AutoML framework FEDOT.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.01880</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.01880</id><submitter>Jeffrey Carver</submitter><version version="v1"><date>Tue, 2 Mar 2021 17:24:45 GMT</date><size>7kb</size></version><title>Sustaining Research Software via Research Software Engineers and
  Professional Associations</title><authors>Jeffrey C. Carver and Ian A. Cosden and Chris Hill and Sandra Gesing
  and Daniel S. Katz</authors><categories>cs.SE</categories><comments>Extended abstract for 1st International Workshop on the Body of
  Knowledge for Software Sustainability (BoKSS'21)</comments><doi>10.1109/BoKSS52540.2021.00016</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Research software is a class of software developed to support research. Today
a wealth of such software is created daily in universities, government, and
commercial research enterprises worldwide. The sustainability of this software
faces particular challenges due, at least in part, to the type of people who
develop it. These Research Software Engineers (RSEs) face challenges in
developing and sustaining software that differ from those faced by the
developers of traditional software. As a result, professional associations have
begun to provide support, advocacy, and resources for RSEs. These benefits are
critical to sustaining RSEs, especially in environments where their
contributions are often undervalued and not rewarded. This paper focuses on how
professional associations, such as the United States Research Software Engineer
Association (US-RSE), can provide this.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.01983</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.01983</id><submitter>Steven Rodriguez</submitter><version version="v1"><date>Tue, 2 Mar 2021 19:04:17 GMT</date><size>4897kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 15:20:56 GMT</date><size>4897kb</size><source_type>D</source_type></version><title>Projection-tree reduced order modeling for fast N-body computations</title><authors>Steven N. Rodriguez, Athanasios P. Iliopoulos, Kevin T. Carlberg,
  Steven L. Brunton, John C. Steuben, John G. Michopoulos</authors><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a data-driven reduced-order modeling framework to
accelerate the computations of $N$-body dynamical systems and their pair-wise
interactions. The proposed framework differs from traditional acceleration
methods, like the Barnes-Hut method, which requires online tree building of the
state space, or the fast-multipole method, which requires rigorous $a$ $priori$
analysis of governing kernels and online tree building. Our approach combines
Barnes-Hut hierarchical decomposition, dimensional compression via the
least-squares Petrov-Galerkin (LSPG) projection, and hyper-reduction by way of
the Gauss-Newton with approximated tensor (GNAT) approach. The resulting
$projection-tree$ reduced order model (PTROM) enables a drastic reduction in
operational count complexity by constructing sparse hyper-reduced pairwise
interactions of the $N$-body dynamical system. As a result, the presented
framework is capable of achieving an operational count complexity that is
independent of $N$, the number of bodies in the numerical domain. Capabilities
of the PTROM method are demonstrated on the two-dimensional fluid-dynamic
Biot-Savart kernel within a parametric and reproductive setting. Results show
the PTROM is capable of achieving over 2000$\times$ wall-time speed-up with
respect to the full-order model, where the speed-up increases with $N$. The
resulting solution delivers quantities of interest with errors that are less
than 0.1$\%$ with respect to full-order model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.02002</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.02002</id><submitter>Pierre Baudot</submitter><version version="v1"><date>Tue, 2 Mar 2021 19:51:40 GMT</date><size>1354kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 08:02:26 GMT</date><size>1242kb</size><source_type>D</source_type></version><title>On Information Links</title><authors>Pierre Baudot</authors><categories>q-bio.NC cs.IT math.AT math.IT</categories><msc-class>94A15</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In a joint work with D. Bennequin, we suggested that the (negative) minima of
the 3-way multivariate mutual information correspond to Borromean links, paving
the way for providing probabilistic analogs of linking numbers. This short note
generalizes the correspondence of the minima of k multivariate interaction
information with k Brunnian links in the binary variable case. Following
Jakulin and Bratsko, we also note that the negativity of the associated K-L
divergence of the joint probability law with its Kirkwood approximation implies
their contextuality in the sens of Abramsky. Those negative k-interactions
links, that cannot be captured in lower dimensions then k, provide a
straightforward definition of collective emergence in complex k-body
interacting systems or dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.02125</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.02125</id><submitter>Jan Valdman</submitter><version version="v1"><date>Wed, 3 Mar 2021 01:55:43 GMT</date><size>527kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 20:32:33 GMT</date><size>420kb</size><source_type>D</source_type></version><title>Minimization of p-Laplacian via the Finite Element Method in MATLAB</title><authors>Ctirad Matonoha, Alexej Moskovka, Jan Valdman</authors><categories>math.NA cs.NA math.OC</categories><comments>8 pages, 3 figures, accepted proceeding of conference LSSC 2021,
  Sozopol, Bulgaria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimization of energy functionals is based on a discretization by the finite
element method and optimization by the trust-region method. A key tool is a
local evaluation of the approximated gradients together with sparsity of the
resulting Hessian matrix. We describe a vectorized MATLAB implementation of the
p-Laplace problem in one and two space-dimensions, however it is easily
applicable to other energy formulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.02699</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.02699</id><submitter>Jeanne N. Clelland</submitter><version version="v1"><date>Wed, 3 Mar 2021 21:39:51 GMT</date><size>2334kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 01:39:59 GMT</date><size>2179kb</size><source_type>D</source_type></version><title>Compactness statistics for spanning tree recombination</title><authors>Jeanne N. Clelland, Nicholas Bossenbroek, Thomas Heckmaster, Adam
  Nelson, Peter Rock, Jade VanAusdall</authors><categories>physics.soc-ph cs.CY</categories><comments>20 pages, 17 figures, 3 tables</comments><msc-class>60J10, 05C70, 91F10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ensemble analysis has become an important tool for quantifying
gerrymandering; the main idea is to generate a large, random sample of
districting plans (an &quot;ensemble&quot;) to which any proposed plan may be compared.
If a proposed plan is an extreme outlier compared to the ensemble with regard
to various redistricting criteria, this may indicate that the plan was
deliberately engineered to produce a specific outcome.
  Many methods have been used to construct ensembles, and a fundamental
question that arises is: Given a method for constructing plans, can we identify
a probability distribution on the space of plans that describes the probability
of constructing any particular plan by that method?
  Recently, MCMC methods have become a predominant tool for constructing
ensembles. Here we focus on the MCMC method known as &quot;ReCom,&quot; which was
introduced in 2018 by the MGGG Redistricting Lab. ReCom tends to produce plans
with more compact districts than some other methods, and we sought to better
understand this phenomenon. We adopted a discrete analog of district perimeter
called &quot;cut edges&quot; as a quantitative measure for district compactness; this
measure was proposed by Duchin and Tenner, and it avoids some of the
difficulties associated with compactness measures based on geographic
perimeter, such as the Polsby-Popper score.
  To model the basic ReCom step, we constructed ensembles of 2-district plans
for two grid graphs and for the precinct graph of Boulder County, CO. We found
that the probability of sampling any particular plan -- which is roughly
proportional to the product of the numbers of spanning trees for each of the
two districts -- is also approximately proportional to an exponentially
decaying function of the number of cut edges in the plan. This is an important
step towards understanding compactness properties for districting plans
produced by the ReCom method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04590</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04590</id><submitter>Pranav Rajpurkar</submitter><version version="v1"><date>Mon, 8 Mar 2021 08:13:21 GMT</date><size>1988kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 05:15:55 GMT</date><size>2474kb</size><source_type>D</source_type></version><title>CheXseen: Unseen Disease Detection for Deep Learning Interpretation of
  Chest X-rays</title><authors>Siyu Shi, Ishaan Malhi, Kevin Tran, Andrew Y. Ng, Pranav Rajpurkar</authors><categories>cs.CV cs.AI cs.LG</categories><comments>Accepted at MIDL Conference 2021. Previous version accepted at ACM
  Conference on Health, Inference, and Learning (ACM-CHIL) Workshop 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We systematically evaluate the performance of deep learning models in the
presence of diseases not labeled for or present during training. First, we
evaluate whether deep learning models trained on a subset of diseases (seen
diseases) can detect the presence of any one of a larger set of diseases. We
find that models tend to falsely classify diseases outside of the subset
(unseen diseases) as &quot;no disease&quot;. Second, we evaluate whether models trained
on seen diseases can detect seen diseases when co-occurring with diseases
outside the subset (unseen diseases). We find that models are still able to
detect seen diseases even when co-occurring with unseen diseases. Third, we
evaluate whether feature representations learned by models may be used to
detect the presence of unseen diseases given a small labeled set of unseen
diseases. We find that the penultimate layer of the deep neural network
provides useful features for unseen disease detection. Our results can inform
the safe clinical deployment of deep learning models trained on a
non-exhaustive set of disease classes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04841</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04841</id><submitter>Alberto Termine Sig.</submitter><version version="v1"><date>Mon, 8 Mar 2021 15:47:40 GMT</date><size>47kb</size></version><version version="v2"><date>Tue, 18 May 2021 15:32:47 GMT</date><size>47kb</size></version><title>Robust Model Checking with Imprecise Markov Reward Models</title><authors>Alberto Termine, Alessandro Antonucci, Alessandro Facchini, Giuseppe
  Primiero</authors><categories>cs.LO math.LO math.PR</categories><comments>Forthcoming in the proceedings of ISIPTA 2021 (International
  Symposium of Imprecise Probability: Theory and Applications)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years probabilistic model checking has become an important area of
research because of the diffusion of computational systems of stochastic
nature. Despite its great success, standard probabilistic model checking
suffers the limitation of requiring a sharp specification of the probabilities
governing the model behaviour. The theory of imprecise probabilities offers a
natural approach to overcome such limitation by a sensitivity analysis with
respect to the values of these parameters. However, only extensions based on
discrete-time imprecise Markov chains have been considered so far for such a
robust approach to model checking. We present a further extension based on
imprecise Markov reward models. In particular, we derive efficient algorithms
to compute lower and upper bounds of the expected cumulative reward and
probabilistic bounded rewards based on existing results for imprecise Markov
chains. These ideas are tested on a real case study involving the spend-down
costs of geriatric medicine departments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.04850</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.04850</id><submitter>Andrew Jesson D</submitter><version version="v1"><date>Mon, 8 Mar 2021 15:58:06 GMT</date><size>1044kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 9 May 2021 15:37:31 GMT</date><size>2993kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 12:38:36 GMT</date><size>2992kb</size><source_type>D</source_type></version><title>Quantifying Ignorance in Individual-Level Causal-Effect Estimates under
  Hidden Confounding</title><authors>Andrew Jesson, S\&quot;oren Mindermann, Yarin Gal, Uri Shalit</authors><categories>cs.LG stat.ML</categories><comments>19 pages, 5 figures, ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning conditional average treatment effects (CATE)
from high-dimensional, observational data with unobserved confounders.
Unobserved confounders introduce ignorance -- a level of unidentifiability --
about an individual's response to treatment by inducing bias in CATE estimates.
We present a new parametric interval estimator suited for high-dimensional
data, that estimates a range of possible CATE values when given a predefined
bound on the level of hidden confounding. Further, previous interval estimators
do not account for ignorance about the CATE stemming from samples that may be
underrepresented in the original study, or samples that violate the overlap
assumption. Our novel interval estimator also incorporates model uncertainty so
that practitioners can be made aware of out-of-distribution data. We prove that
our estimator converges to tight bounds on CATE when there may be unobserved
confounding, and assess it using semi-synthetic, high-dimensional datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.06681</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.06681</id><submitter>Stephan Druskat</submitter><version version="v1"><date>Thu, 11 Mar 2021 14:13:51 GMT</date><size>7kb</size></version><title>Research Software Sustainability and Citation</title><authors>Stephan Druskat, Daniel S. Katz, Ilian T. Todorov</authors><categories>cs.SE</categories><comments>2 pages; accepted by ICSE 2021 BokSS Workshop
  (https://bokss.github.io/bokss2021/)</comments><doi>10.1109/BoKSS52540.2021.00008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software citation contributes to achieving software sustainability in two
ways: It provides an impact metric to incentivize stakeholders to make software
sustainable. It also provides references to software used in research, which
can be reused and adapted to become sustainable. While software citation faces
a host of technical and social challenges, community initiatives have defined
the principles of software citation and are working on implementing solutions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.06708</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.06708</id><submitter>Razvan Bunescu</submitter><version version="v1"><date>Sat, 6 Mar 2021 19:06:14 GMT</date><size>1289kb</size><source_type>D</source_type></version><title>LSTMs and Deep Residual Networks for Carbohydrate and Bolus
  Recommendations in Type 1 Diabetes Management</title><authors>Jeremy Beauchamp, Razvan Bunescu, Cindy Marling, Zhongen Li, and Chang
  Liu</authors><categories>cs.LG</categories><acm-class>I.2.1; J.3</acm-class><doi>10.3390/s21093303</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To avoid serious diabetic complications, people with type 1 diabetes must
keep their blood glucose levels (BGLs) as close to normal as possible. Insulin
dosages and carbohydrate consumption are important considerations in managing
BGLs. Since the 1960s, models have been developed to forecast blood glucose
levels based on the history of BGLs, insulin dosages, carbohydrate intake, and
other physiological and lifestyle factors. Such predictions can be used to
alert people of impending unsafe BGLs or to control insulin flow in an
artificial pancreas. In past work, we have introduced an LSTM-based approach to
blood glucose level prediction aimed at &quot;what if&quot; scenarios, in which people
could enter foods they might eat or insulin amounts they might take and then
see the effect on future BGLs. In this work, we invert the &quot;what-if&quot; scenario
and introduce a similar architecture based on chaining two LSTMs that can be
trained to make either insulin or carbohydrate recommendations aimed at
reaching a desired BG level in the future. Leveraging a recent state-of-the-art
model for time series forecasting, we then derive a novel architecture for the
same recommendation task, in which the two LSTM chain is used as a repeating
block inside a deep residual architecture. Experimental evaluations using real
patient data from the OhioT1DM dataset show that the new integrated
architecture compares favorably with the previous LSTM-based approach,
substantially outperforming the baselines. The promising results suggest that
this novel approach could potentially be of practical use to people with type 1
diabetes for self-management of BGLs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.07393</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.07393</id><submitter>Zolt\'an L\'or\'ant Nagy</submitter><version version="v1"><date>Fri, 12 Mar 2021 16:39:24 GMT</date><size>27kb</size></version><version version="v2"><date>Mon, 17 May 2021 07:49:02 GMT</date><size>28kb</size></version><title>Short minimal codes and covering codes via strong blocking sets in
  projective spaces</title><authors>Tam\'as H\'eger, Zolt\'an L\'or\'ant Nagy</authors><categories>math.CO cs.IT math.IT</categories><comments>Minor improvement for higgledy-piggledy line sets in the even order
  case. The main proof is slightly simplified. Some smaller mistakes are
  corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimal linear codes are in one-to-one correspondence with special types of
blocking sets of projective spaces over a finite field, which are called strong
or cutting blocking sets. In this paper we prove an upper bound on the minimal
length of minimal codes of dimension $k$ over the $q$-element Galois field
which is linear in both $q$ and $k$, hence improve the previous superlinear
bounds. This result determines the minimal length up to a small constant
factor. We also improve the lower and upper bounds on the size of so called
higgledy-piggledy line sets in projective spaces and apply these results to
present improved bounds on the size of covering codes and saturating sets in
projective spaces as well. The contributions rely on geometric and
probabilistic arguments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.07758</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.07758</id><submitter>Ali Ayub</submitter><version version="v1"><date>Sat, 13 Mar 2021 17:42:09 GMT</date><size>507kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 21:53:49 GMT</date><size>815kb</size><source_type>D</source_type></version><title>Learning Novel Objects Continually Through Curiosity</title><authors>Ali Ayub, Alan R. Wagner</authors><categories>cs.RO cs.CV cs.LG</categories><comments>Accepted at IEEE ICRA 2021 (Workshop titled, Towards Curious Robots:
  Modern Approaches for Intrinsically-Motivated Intelligent Behavior)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Children learn continually by asking questions about the concepts they are
most curious about. With robots becoming an integral part of our society, they
must also learn unknown concepts continually by asking humans questions. The
paper analyzes a recent state-of-the-art approach for continual learning. The
paper further develops a self-supervised technique to find most of the
uncertain objects in an environment by utilizing the cluster representation of
the previously learned classes. We test our approach on a benchmark dataset for
continual learning on robots. Our results show that our curiosity-driven
continual learning approach beats random sampling and softmax-based uncertainty
sampling in terms of classification accuracy and the total number of classes
learned.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.07863</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.07863</id><submitter>Kees Middelburg</submitter><version version="v1"><date>Sun, 14 Mar 2021 07:52:48 GMT</date><size>44kb</size></version><version version="v2"><date>Mon, 22 Mar 2021 16:11:30 GMT</date><size>44kb</size></version><version version="v3"><date>Tue, 18 May 2021 11:23:48 GMT</date><size>44kb</size></version><title>Imperative process algebra with abstraction</title><authors>C.A. Middelburg</authors><categories>cs.LO</categories><comments>29 pages, revision of v2 with section added about relatively uncommon
  choices made</comments><acm-class>D.2.4; D.4.6; F.1.1; F.1.2; F.3.1</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper introduces an imperative process algebra based on ACP (Algebra of
Communicating Processes). Like other imperative process algebras, this process
algebra deals with processes of the kind that arises from the execution of
imperative programs. It distinguishes itself from already existing imperative
process algebras among other things by supporting abstraction from actions that
are considered not to be visible. The support of abstraction opens interesting
application possibilities of the process algebra. This paper goes briefly into
the possibility of information-flow security analysis of the kind that is
concerned with the leakage of confidential data. For the presented
axiomatization, soundness and semi-completeness results with respect to a
notion of branching bisimulation equivalence are established.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.08236</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.08236</id><submitter>Lars V\&quot;ogtlin</submitter><version version="v1"><date>Mon, 15 Mar 2021 09:39:17 GMT</date><size>12938kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 18:52:10 GMT</date><size>46206kb</size><source_type>D</source_type></version><title>Generating Synthetic Handwritten Historical Documents With OCR
  Constrained GANs</title><authors>Lars V\&quot;ogtlin, Manuel Drazyk, Vinaychandran Pondenkandath, Michele
  Alberti, Rolf Ingold</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework to generate synthetic historical documents with
precise ground truth using nothing more than a collection of unlabeled
historical images. Obtaining large labeled datasets is often the limiting
factor to effectively use supervised deep learning methods for Document Image
Analysis (DIA). Prior approaches towards synthetic data generation either
require expertise or result in poor accuracy in the synthetic documents. To
achieve high precision transformations without requiring expertise, we tackle
the problem in two steps. First, we create template documents with
user-specified content and structure. Second, we transfer the style of a
collection of unlabeled historical images to these template documents while
preserving their text and layout. We evaluate the use of our synthetic
historical documents in a pre-training setting and find that we outperform the
baselines (randomly initialized and pre-trained). Additionally, with visual
examples, we demonstrate a high-quality synthesis that makes it possible to
generate large labeled historical document datasets with precise ground truth.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.08491</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.08491</id><submitter>Sherif Abdulatif</submitter><version version="v1"><date>Mon, 15 Mar 2021 16:08:23 GMT</date><size>423kb</size><source_type>D</source_type></version><title>Uncertainty-Based Biological Age Estimation of Brain MRI Scans</title><authors>Karim Armanious, Sherif Abdulatif, Wenbin Shi, Tobias Hepp, Sergios
  Gatidis, Bin Yang</authors><categories>cs.CV eess.IV</categories><comments>Accepted in ICASSP 2021. 5 pages, 4 figures. arXiv admin note:
  substantial text overlap with arXiv:2009.10765</comments><doi>10.1109/ICASSP39728.2021.9414112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Age is an essential factor in modern diagnostic procedures. However,
assessment of the true biological age (BA) remains a daunting task due to the
lack of reference ground-truth labels. Current BA estimation approaches are
either restricted to skeletal images or rely on non-imaging modalities that
yield a whole-body BA assessment. However, various organ systems may exhibit
different aging characteristics due to lifestyle and genetic factors. In this
initial study, we propose a new framework for organ-specific BA estimation
utilizing 3D magnetic resonance image (MRI) scans. As a first step, this
framework predicts the chronological age (CA) together with the corresponding
patient-dependent aleatoric uncertainty. An iterative training algorithm is
then utilized to segregate atypical aging patients from the given population
based on the predicted uncertainty scores. In this manner, we hypothesize that
training a new model on the remaining population should approximate the true BA
behavior. We apply the proposed methodology on a brain MRI dataset containing
healthy individuals as well as Alzheimer's patients. We demonstrate the
correlation between the predicted BAs and the expected cognitive deterioration
in Alzheimer's patients.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.09286</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.09286</id><submitter>Xavier Goaoc</submitter><version version="v1"><date>Tue, 16 Mar 2021 19:05:54 GMT</date><size>40kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 08:41:56 GMT</date><size>40kb</size><source_type>D</source_type></version><title>A Stepping-Up Lemma for Topological Set Systems</title><authors>Xavier Goaoc, Andreas F. Holmsen and Zuzana Pat\'akov\'a</authors><categories>cs.CG math.CO</categories><comments>37th International Symposium on Computational Geometry (SoCG'21)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Intersection patterns of convex sets in $\mathbb{R}^d$ have the remarkable
property that for $d+1 \le k \le \ell$, in any sufficiently large family of
convex sets in $\mathbb{R}^d$, if a constant fraction of the $k$-element
subfamilies have nonempty intersection, then a constant fraction of the
$\ell$-element subfamilies must also have nonempty intersection. Here, we prove
that a similar phenomenon holds for any topological set system $\mathcal{F}$ in
$\mathbb{R}^d$. Quantitatively, our bounds depend on how complicated the
intersection of $\ell$ elements of $\mathcal{F}$ can be, as measured by the sum
of the $\lceil\frac{d}2\rceil$ first Betti numbers. As an application, we
improve the fractional Helly number of set systems with bounded topological
complexity due to the third author, from a Ramsey number down to $d+1$. We also
shed some light on a conjecture of Kalai and Meshulam on intersection patterns
of sets with bounded homological VC dimension. A key ingredient in our proof is
the use of the stair convexity of Bukh, Matou\v{s}ek and Nivash to recast a
simplicial complex as a homological minor of a cubical complex.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.09635</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.09635</id><submitter>Javier Huertas-Tato</submitter><version version="v1"><date>Wed, 17 Mar 2021 13:23:53 GMT</date><size>94kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 18 Mar 2021 19:28:55 GMT</date><size>95kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 08:22:07 GMT</date><size>113kb</size><source_type>D</source_type></version><title>SILT: Efficient transformer training for inter-lingual inference</title><authors>Javier Huertas-Tato and Alejandro Mart\'in and David Camacho</authors><categories>cs.CL cs.LG</categories><comments>This research is funded by the project CIVIC: Intelligent
  characterisation of the veracity of the information related to COVID-19,
  granted by BBVA FOUNDATION GRANTS FOR SCIENTIFIC RESEARCH TEAMS SARS-CoV-2
  and COVID-19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability of transformers to perform precision tasks such as question
answering, Natural Language Inference (NLI) or summarising, have enabled them
to be ranked as one of the best paradigm to address Natural Language Processing
(NLP) tasks. NLI is one of the best scenarios to test these architectures, due
to the knowledge required to understand complex sentences and established
relationships between a hypothesis and a premise. Nevertheless, these models
suffer from incapacity to generalise to other domains or difficulties to face
multilingual and interlingual scenarios. The leading pathway in the literature
to address these issues involve designing and training extremely large
architectures, which leads to unpredictable behaviours and to establish
barriers which impede broad access and fine tuning. In this paper, we propose a
new architecture called Siamese Inter-Lingual Transformer (SILT), to
efficiently align multilingual embeddings for Natural Language Inference,
allowing for unmatched language pairs to be processed. SILT leverages siamese
pre-trained multi-lingual transformers with frozen weights where the two input
sentences attend each other to later be combined through a matrix alignment
method. The experimental results carried out in this paper evidence that SILT
allows to reduce drastically the number of trainable parameters while allowing
for inter-lingual NLI and achieving state-of-the-art performance on common
benchmarks.
  We make our code and dataset available at
https://github.com/jahuerta92/siamese-inter-lingual-transformer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.10489</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.10489</id><submitter>Ivan Srba</submitter><version version="v1"><date>Thu, 18 Mar 2021 19:19:44 GMT</date><size>247kb</size></version><title>Addressing Hate Speech with Data Science: An Overview from Computer
  Science Perspective</title><authors>Ivan Srba, Gabriele Lenzini, Matus Pikuliak, Samuel Pecar</authors><categories>cs.CY cs.CL</categories><journal-ref>Wachs S., Koch-Priewe B., Zick A. (eds) Hate Speech -
  Multidisziplinare Analysen und Handlungsoptionen. Springer VS, Wiesbaden.
  2021</journal-ref><doi>10.1007/978-3-658-31793-5_14</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  From a computer science perspective, addressing on-line hate speech is a
challenging task that is attracting the attention of both industry (mainly
social media platform owners) and academia. In this chapter, we provide an
overview of state-of-the-art data-science approaches - how they define hate
speech, which tasks they solve to mitigate the phenomenon, and how they address
these tasks. We limit our investigation mostly to (semi-)automatic detection of
hate speech, which is the task that the majority of existing computer science
works focus on. Finally, we summarize the challenges and the open problems in
the current data-science research and the future directions in this field. Our
aim is to prepare an easily understandable report, capable to promote the
multidisciplinary character of hate speech research. Researchers from other
domains (e.g., psychology and sociology) can thus take advantage of the
knowledge achieved in the computer science domain but also contribute back and
help improve how computer science is addressing that urgent and socially
relevant issue which is the prevalence of hate speech in social media.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.11216</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.11216</id><submitter>Steven Damelin Dr</submitter><version version="v1"><date>Sat, 20 Mar 2021 17:38:13 GMT</date><size>1412kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 20:27:09 GMT</date><size>1414kb</size><source_type>D</source_type></version><title>Preprocessing power weighted shortest path data using a s-Well Separated
  Pair Decomposition</title><authors>Gurpreet S. Kalsi and Steven B. Damelin</authors><categories>cs.CV cs.CG cs.DS cs.LG</categories><msc-class>68T10, 62H30, 52B05, 60F15, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For $s$ $&gt;$ 0, we consider an algorithm that computes all $s$-well separated
pairs in certain point sets in $\mathbb{R}^{n}$, $n$ $&gt;1$. For an integer $K$
$&gt;1$, we also consider an algorithm that is a permutation of Dijkstra's
algorithm, that computes $K$-nearest neighbors using a certain power weighted
shortest path metric in $\mathbb{R}^{n}$, $n$ $&gt;$ $1$. We describe each
algorithm and their respective dependencies on the input data. We introduce a
way to combine both algorithms into a fused algorithm. Several open problems
are given for future research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12068</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12068</id><submitter>Aqsa Saeed Qureshi</submitter><version version="v1"><date>Mon, 22 Mar 2021 06:04:45 GMT</date><size>879kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 24 Mar 2021 15:05:48 GMT</date><size>876kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 30 Mar 2021 18:06:28 GMT</date><size>0kb</size><source_type>I</source_type></version><version version="v4"><date>Mon, 17 May 2021 09:33:03 GMT</date><size>6772kb</size><source_type>D</source_type></version><title>Transfer Learning with Ensembles of Deep Neural Networks for Skin Cancer
  Detection in Imbalanced Data Sets</title><authors>Aqsa Saeed Qureshi and Teemu Roos</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several machine learning techniques for accurate detection of skin cancer
from medical images have been reported. Many of these techniques are based on
pre-trained convolutional neural networks (CNNs), which enable training the
models based on limited amounts of training data. However, the classification
accuracy of these models still tends to be severely limited by the scarcity of
representative images from malignant tumours. We propose a novel ensemble-based
CNN architecture where multiple CNN models, some of which are pre-trained and
some are trained only on the data at hand, along with auxiliary data in the
form of metadata associated with the input images, are combined using a
meta-learner. The proposed approach improves the model's ability to handle
limited and imbalanced data. We demonstrate the benefits of the proposed
technique using a dataset with 33126 dermoscopic images from 2056 patients. We
evaluate the performance of the proposed technique in terms of the F1-measure,
area under the ROC curve (AUC-ROC), and area under the PR-curve (AUC-PR), and
compare it with that of seven different benchmark methods, including two recent
CNN-based techniques. The proposed technique compares favourably in terms of
all the evaluation metrics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12192</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12192</id><submitter>Shufan Yang</submitter><version version="v1"><date>Mon, 22 Mar 2021 21:50:09 GMT</date><size>11280kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 21:35:58 GMT</date><size>11397kb</size><source_type>D</source_type></version><title>Reward-Reinforced Reinforcement Learning for Multi-agent Systems</title><authors>Changgang Zheng, Shufan Yang, Juan Parra-Ullauri, Antonio
  Garcia-Dominguez, and Nelly Bencomo</authors><categories>cs.MA</categories><comments>9 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Reinforcement learning algorithms in multi-agent systems deliver highly
resilient and adaptable solutions for common problems in
telecommunications,aerospace, and industrial robotics. However, achieving an
optimal global goal remains a persistent obstacle for collaborative multi-agent
systems, where learning affects the behaviour of more than one agent. A number
of nonlinear function approximation methods have been proposed for solving the
Bellman equation, which describe a recursive format of an optimal policy.
However, how to leverage the value distribution based on reinforcement
learning, and how to improve the efficiency and efficacy of such systems remain
a challenge. In this work, we developed a reward-reinforced generative
adversarial network to represent the distribution of the value function,
replacing the approximation of Bellman updates. We demonstrated our method is
resilient and outperforms other conventional reinforcement learning methods.
This method is also applied to a practical case study: maximising the number of
user connections to autonomous airborne base stations in a mobile communication
network. Our method maximises the data likelihood using a cost function under
which agents have optimal learned behaviours. This reward-reinforced generative
adversarial network can be used as ageneric framework for multi-agent learning
at the system level
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.12505</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.12505</id><submitter>Subhabrata Majumdar</submitter><version version="v1"><date>Wed, 17 Mar 2021 16:02:28 GMT</date><size>7138kb</size><source_type>D</source_type></version><title>Towards an Open Global Air Quality Monitoring Platform to Assess
  Children's Exposure to Air Pollutants in the Light of COVID-19 Lockdowns</title><authors>Christina Last, Prithviraj Pramanik, Nikita Saini, Akash Smaran
  Majety, Do-Hyung Kim, Manuel Garc\'ia-Herranz, Subhabrata Majumdar</authors><categories>cs.HC cs.LG</categories><comments>Accepted as CHI-2021 Late-Breaking Work</comments><journal-ref>Extended Abstracts of the 2021 CHI Conference on Human Factors in
  Computing Systems, article 434</journal-ref><doi>10.1145/3411763.3451768</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This ongoing work attempts to understand and address the requirements of
UNICEF, a leading organization working in children's welfare, where they aim to
tackle the problem of air quality for children at a global level. We are
motivated by the lack of a proper model to account for heavily fluctuating air
quality levels across the world in the wake of the COVID-19 pandemic, leading
to uncertainty among public health professionals on the exact levels of
children's exposure to air pollutants. We create an initial model as per the
agency's requirement to generate insights through a combination of virtual
meetups and online presentations. Our research team comprised of UNICEF's
researchers and a group of volunteer data scientists. The presentations were
delivered to a number of scientists and domain experts from UNICEF and
community champions working with open data. We highlight their feedback and
possible avenues to develop this research further.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.13696</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.13696</id><submitter>Phi Vu Tran</submitter><version version="v1"><date>Thu, 25 Mar 2021 09:19:13 GMT</date><size>34621kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 29 Mar 2021 06:22:43 GMT</date><size>34621kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 02:13:08 GMT</date><size>34628kb</size><source_type>D</source_type></version><title>SSLayout360: Semi-Supervised Indoor Layout Estimation from 360-Degree
  Panorama</title><authors>Phi Vu Tran</authors><categories>cs.CV cs.LG</categories><comments>CVPR 2021. File size 37MB. Project page at
  https://github.com/FlyreelAI/sslayout360</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent years have seen flourishing research on both semi-supervised learning
and 3D room layout reconstruction. In this work, we explore the intersection of
these two fields to advance the research objective of enabling more accurate 3D
indoor scene modeling with less labeled data. We propose the first approach to
learn representations of room corners and boundaries by using a combination of
labeled and unlabeled data for improved layout estimation in a 360-degree
panoramic scene. Through extensive comparative experiments, we demonstrate that
our approach can advance layout estimation of complex indoor scenes using as
few as 20 labeled examples. When coupled with a layout predictor pre-trained on
synthetic data, our semi-supervised method matches the fully supervised
counterpart using only 12% of the labels. Our work takes an important first
step towards robust semi-supervised layout estimation that can enable many
applications in 3D perception with limited labeled data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.13939</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.13939</id><submitter>Sebastian Musslick</submitter><version version="v1"><date>Thu, 25 Mar 2021 16:00:23 GMT</date><size>482kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 14:01:50 GMT</date><size>535kb</size><source_type>D</source_type></version><title>Recovering Quantitative Models of Human Information Processing with
  Differentiable Architecture Search</title><authors>Sebastian Musslick</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of behavioral phenomena into mechanistic models of cognitive
function is a fundamental staple of cognitive science. Yet, researchers are
beginning to accumulate increasing amounts of data without having the temporal
or monetary resources to integrate these data into scientific theories. We seek
to overcome these limitations by incorporating existing machine learning
techniques into an open-source pipeline for the automated construction of
quantitative models. This pipeline leverages the use of neural architecture
search to automate the discovery of interpretable model architectures, and
automatic differentiation to automate the fitting of model parameters to data.
We evaluate the utility of these methods based on their ability to recover
quantitative models of human information processing from synthetic data. We
find that these methods are capable of recovering basic quantitative motifs
from models of psychophysics, learning and decision making. We also highlight
weaknesses of this framework and discuss future directions for their
mitigation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.14796</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.14796</id><submitter>Junlong Lyu</submitter><version version="v1"><date>Sat, 27 Mar 2021 03:19:53 GMT</date><size>2343kb</size></version><version version="v2"><date>Sat, 15 May 2021 22:42:00 GMT</date><size>2343kb</size></version><title>A convergent interacting particle method and computation of KPP front
  speeds in chaotic flows</title><authors>Junlong Lyu, Zhongjian Wang, Jack Xin, Zhiwen Zhang</authors><categories>math.NA cs.NA</categories><comments>37 pages, 12 figures, planning to submit for Siam Journal on
  Numerical Analysis</comments><msc-class>35K57, 47D08, 65C35, 65L20, 65N25</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the propagation speeds of
reaction-diffusion-advection (RDA) fronts in time-periodic cellular and chaotic
flows with Kolmogorov-Petrovsky-Piskunov (KPP) nonlinearity. We first apply the
variational principle to reduce the computation of KPP front speeds to a
principal eigenvalue problem of a linear advection-diffusion operator with
space-time periodic coefficients on a periodic domain. To this end, we develop
efficient Lagrangian particle methods to compute the principal eigenvalue
through the Feynman-Kac formula. By estimating the convergence rate of
Feynman-Kac semigroups and the operator splitting methods for approximating the
linear advection-diffusion solution operators, we obtain convergence analysis
for the proposed numerical methods. Finally, we present numerical results to
demonstrate the accuracy and efficiency of the proposed method in computing KPP
front speeds in time-periodic cellular and chaotic flows, especially the
time-dependent Arnold-Beltrami-Childress (ABC) flow and time-dependent
Kolmogorov flow in three-dimensional space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.15147</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.15147</id><submitter>Jaan Kasak</submitter><version version="v1"><date>Sun, 28 Mar 2021 15:00:59 GMT</date><size>94kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 16 Apr 2021 15:45:30 GMT</date><size>411kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 10:37:52 GMT</date><size>551kb</size><source_type>D</source_type></version><title>Symbolic regression outperforms other models for small data sets</title><authors>Casper Wilstrup and Jaan Kasak</authors><categories>cs.LG</categories><comments>10 pages, 2 figures, 2 tables. Pending review at BMC Bioinformatics</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine learning is often applied in health science to obtain predictions and
new understandings of complex phenomena and relationships, but an availability
of sufficient data for model training is a widespread problem. Traditional
machine learning techniques, such as random forests and gradient boosting, tend
to overfit when working with data sets of only a few hundred observations. This
study demonstrates that for small training sets of 250 observations, symbolic
regression generalises better to out-of-sample data than traditional machine
learning frameworks, as measured by the coefficient of determination R2 on the
validation set. In 132 out of 240 cases, symbolic regression achieves a higher
R2 than any of the other models on the out-of-sample data. Furthermore,
symbolic regression also preserves the interpretability of linear models and
decision trees, an added benefit to its superior generalisation. The second
best algorithm was found to be a random forest, which performs best in 37 of
the 240 cases. When restricting the comparison to interpretable models,
symbolic regression performs best in 184 out of 240 cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.15244</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.15244</id><submitter>Zhengbo Luo</submitter><version version="v1"><date>Sun, 28 Mar 2021 23:29:57 GMT</date><size>3792kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 12 Apr 2021 12:31:21 GMT</date><size>3790kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 01:38:13 GMT</date><size>3790kb</size><source_type>D</source_type></version><title>Rethinking ResNets: Improved Stacking Strategies With High Order Schemes</title><authors>Zhengbo Luo and Zitang Sun and Weilian Zhou and Sei-ichiro Kamata</authors><categories>cs.CV cs.LG</categories><comments>10 pages</comments><msc-class>68T07</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Various Deep Neural Network architectures are keeping massive vital records
in computer vision. While drawing attention worldwide, the design of the
overall structure somehow lacks general guidance. Based on the relationship
between DNN design with numerical differential equations, which several
researchers observed in recent years, we perform a fair comparison of residual
design with higher-order perspectives. We show that the widely used DNN design
strategy, constantly stacking a small design, could be easily improved,
supported by solid theoretical knowledge and no extra parameters needed. We
reorganize the residual design in higher-order ways, which is inspired by the
observation that many effective networks could be interpreted as different
numerical discretizations of differential equations. The design of ResNet
follows a relatively simple scheme which is Euler forward; however, the
situation is getting complicated rapidly while stacking. We suppose stacked
ResNet is somehow equalled to a higher order scheme, then the current way of
forwarding propagation might be relatively weak compared with a typical
high-order method like Runge-Kutta. We propose higher order ResNet to verify
the hypothesis on widely used CV benchmarks with sufficient experiments. Stable
and noticeable rises in performance are observed, convergence and robustness
are benefited.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.15419</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.15419</id><submitter>Tobias Alt</submitter><version version="v1"><date>Mon, 29 Mar 2021 08:31:51 GMT</date><size>169kb</size></version><version version="v2"><date>Mon, 17 May 2021 10:04:21 GMT</date><size>35kb</size></version><title>Translating Numerical Concepts for PDEs into Neural Architectures</title><authors>Tobias Alt, Pascal Peter, Joachim Weickert, Karl Schrader</authors><categories>math.NA cs.LG cs.NA</categories><comments>In A. Elmoataz, J. Fadili, Y. Qu\'eau, J. Rabin, L. Simon (Eds.):
  Scale Space and Variational Methods in Computer Vision. Lecture Notes in
  Computer Science, Vol. 12679, Springer, Cham, 294-306, 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate what can be learned from translating numerical algorithms into
neural networks. On the numerical side, we consider explicit, accelerated
explicit, and implicit schemes for a general higher order nonlinear diffusion
equation in 1D, as well as linear multigrid methods. On the neural network
side, we identify corresponding concepts in terms of residual networks
(ResNets), recurrent networks, and U-nets. These connections guarantee
Euclidean stability of specific ResNets with a transposed convolution layer
structure in each block. We present three numerical justifications for skip
connections: as time discretisations in explicit schemes, as extrapolation
mechanisms for accelerating those methods, and as recurrent connections in
fixed point solvers for implicit schemes. Last but not least, we also motivate
uncommon design choices such as nonmonotone activation functions. Our findings
give a numerical perspective on the success of modern neural network
architectures, and they provide design criteria for stable networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16349</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16349</id><submitter>Yue Cui</submitter><version version="v1"><date>Tue, 30 Mar 2021 13:47:30 GMT</date><size>13kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 31 Mar 2021 15:58:05 GMT</date><size>13kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 12:12:03 GMT</date><size>338kb</size><source_type>D</source_type></version><title>Historical Inertia: A Neglected but Powerful Baseline for Long Sequence
  Time-series Forecasting</title><authors>Yue Cui, Jiandong Xie and Kai Zheng</authors><categories>cs.LG</categories><comments>7 pages, 1 figure, 5 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Long sequence time-series forecasting (LSTF) has become increasingly popular
for its wide range of applications. Though superior models have been proposed
to enhance the prediction effectiveness and efficiency, it is reckless to
neglect or underestimate one of the most natural and basic temporal properties
of time-series. In this paper, we introduce a new baseline for LSTF, the
historical inertia (HI), which refers to the most recent historical data-points
in the input time series. We experimentally evaluate the power of historical
inertia on four public real-word datasets. The results demonstrate that up to
82\% relative improvement over state-of-the-art works can be achieved even by
adopting HI directly as output.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16654</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16654</id><submitter>Mohammadreza Fani Sani</submitter><version version="v1"><date>Tue, 30 Mar 2021 19:56:29 GMT</date><size>2291kb</size></version><version version="v2"><date>Mon, 17 May 2021 08:21:48 GMT</date><size>3533kb</size><source_type>D</source_type></version><title>A Systematic Literature Review on Process-Aware Recommender Systems</title><authors>Mansoureh Yari Eili, Jalal Rezaeenour, Mohammadreza Fani Sani</authors><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Considering processes of a business in a recommender system is highly
advantageous. Although most studies in the business process analysis domain are
of descriptive and predictive nature, the feasibility of constructing a
process-aware recommender system is assessed in a few works. One reason can be
the lack of knowledge on process mining potential for recommendation problems.
Therefore, this paper aims to identify and analyze the published studies on
process-aware recommender system techniques in business process management and
process mining domain. A systematic review was conducted on 33 academic
articles published between 2008 and 2020 according to several aspects. In this
regard, we provide a state-of-the-art review with critical details and
researchers with a better perception of which path to pursue in this field.
Moreover, based on a knowledge base and holistic perspective, we discuss some
research gaps and open challenges in this field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.16813</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.16813</id><submitter>Xiaojie Chen</submitter><version version="v1"><date>Wed, 31 Mar 2021 05:11:20 GMT</date><size>935kb</size><source_type>D</source_type></version><title>Cooperator driven oscillation in a time-delayed feedback-evolving game</title><authors>Fang Yan, Xiaojie Chen, Zhipeng Qiu, and Attila Szolnoki</authors><categories>physics.soc-ph cs.GT cs.MA</categories><journal-ref>New Journal of Physics 23 (2021) 053017</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering feedback of collective actions of cooperation on common resources
has vital importance to reach sustainability. But such efforts may have not
immediate consequence on the state of environment and it is unclear how they
influence the strategic and environmental dynamics with feedbacks. To address
this issue, we construct a feedback-evolving game model in which we consider
the growth capacity of resources and the punishment efficiency on defectors who
do not provide returns to the environment. Importantly, we further assume a
delay in adopting the contribution of cooperative individuals to environmental
change in our model. We find that when this contribution amount from
cooperators' endowment is fixed, the time delay has no particular consequence
on the coevolutionary dynamics. However, when the return is proportional to
their endowment, then the time delay can induce periodic oscillatory dynamics
of cooperation level and environment. Our work reveals the potential effects of
time delay of cooperative actions on the coevolutionary dynamics in strategic
interactions with environmental feedback.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2103.17054</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2103.17054</id><submitter>Johannes Wachs</submitter><version version="v1"><date>Wed, 31 Mar 2021 13:21:36 GMT</date><size>213kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 12:29:28 GMT</date><size>213kb</size><source_type>D</source_type></version><title>Mining DEV for social and technical insights about software development</title><authors>Maria Papoutsoglou, Johannes Wachs, Georgia M. Kapitsaki</authors><categories>cs.SE cs.SI</categories><comments>To appear in the Proceedings of the 18th International Conference on
  Mining Software Repositories (MSR 2021)</comments><doi>10.1109/MSR52588.2021.00053</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Software developers are social creatures: they communicate, collaborate, and
promote their work in a variety of channels. Twitter, GitHub, Stack Overflow,
and other platforms offer developers opportunities to network and exchange
ideas. Researchers analyze content on these sites to learn about trends and
topics in software engineering. However, insight mined from the text of Stack
Overflow questions or GitHub issues is highly focused on detailed and technical
aspects of software development. In this paper, we present a relatively new
online community for software developers called DEV. On DEV users write
long-form posts about their experiences, preferences, and working life in
software, zooming out from specific issues and files to reflect on broader
topics. About 50,000 users have posted over 140,000 articles related to
software development. In this work, we describe the content of posts on DEV
using a topic model, showing that developers discuss a rich variety and mixture
of social and technical aspects of software development. We show that
developers use DEV to promote themselves and their work: 83% link their
profiles to their GitHub profiles and 56% to their Twitter profiles. 14% of
users pin specific GitHub repos in their profiles. We argue that DEV is
emerging as an important hub for software developers, and a valuable source of
insight for researchers to complement data from platforms like GitHub and Stack
Overflow.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.00547</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.00547</id><submitter>Michael Kuperberg</submitter><version version="v1"><date>Thu, 11 Mar 2021 15:34:20 GMT</date><size>223kb</size></version><version version="v2"><date>Mon, 17 May 2021 06:12:10 GMT</date><size>223kb</size></version><title>Blockchain and BIM (Building Information Modeling): Progress in Academia
  and Industry</title><authors>Michael Kuperberg, Matthias Geipel</authors><categories>cs.CR cs.DC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In construction, BIM (Building Information Modeling) promises to increase
quality of data and to provide a shared, uniform view to all parties. While BIM
tools and exchange formats exist, the distribution and safeguarding of data is
an ongoing challenge. Distributed Ledger Technology and Blockchains offer a
possible solution to this task, and they promise quality attributes such as
tamper resistance, traceability/auditability and safe digitalization of assets
and intellectual property. However, the practical application and adoption of
Distributed Ledger Technology in the built environment requires a good
understanding of tool maturity, performance and standardization. Also,
user-oriented integration of BIM tools with the blockchain backend needs
attention. The contribution of this paper is an overview over both industrial
and academic progress at the intersection of BIM and blockchains/DLT.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01104</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01104</id><submitter>Yanyuan Qin</submitter><version version="v1"><date>Fri, 2 Apr 2021 15:56:00 GMT</date><size>6678kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 20:48:51 GMT</date><size>6688kb</size><source_type>D</source_type></version><title>Adaptive Bitrate Streaming Over Cellular Networks: Rate Adaptation and
  Data Savings Strategies</title><authors>Yanyuan Qin</authors><categories>cs.NI cs.MM</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Adaptive bitrate streaming (ABR) has become thede factotechnique for
videostreaming over the Internet. Despite a flurry of techniques, achieving
high quality ABRstreaming over cellular networks remains a tremendous
challenge. First, the design ofan ABR scheme needs to balance conflicting
Quality of Experience (QoE) metrics suchas video quality, quality changes,
stalls and startup performance, which is even harderunder highly dynamic
bandwidth in cellular network. Second, streaming providers havebeen moving
towards using Variable Bitrate (VBR) encodings for the video content,which
introduces new challenges for ABR streaming, whose nature and implicationsare
little understood. Third, mobile video streaming consumes a lot of data.
Althoughmany video and network providers currently offer data saving options,
the existingpractices are suboptimal in QoE and resource usage. Last, when the
audio and videotracks are stored separately, video and audio rate adaptation
needs to be dynamicallycoordinated to achieve good overall streaming
experience, which presents interestingchallenges while, somewhat surprisingly,
has received little attention by the researchcommunity. In this dissertation,
we tackle each of the above four challenges.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01253</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01253</id><submitter>Stephen Thomas</submitter><version version="v1"><date>Fri, 2 Apr 2021 21:44:28 GMT</date><size>5261kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 6 Apr 2021 02:56:31 GMT</date><size>0kb</size><source_type>I</source_type></version><version version="v3"><date>Tue, 20 Apr 2021 14:53:15 GMT</date><size>4658kb</size><source_type>D</source_type></version><version version="v4"><date>Sat, 24 Apr 2021 14:39:50 GMT</date><size>5474kb</size><source_type>D</source_type></version><version version="v5"><date>Sat, 15 May 2021 23:27:42 GMT</date><size>5460kb</size><source_type>D</source_type></version><title>Low-Synch Gram-Schmidt with Delayed Reorthogonalization for Krylov
  Solvers</title><authors>Daniel Bielich, Julien Langou, Stephen Thomas, Kasia Swirydowicz,
  Ichitaro Yamazaki, Erik G. Boman</authors><categories>math.NA cs.NA</categories><comments>work is not ready yet, ongoing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallel strong-scaling of Krylov iterative methods is largely determined
by the number of global reductions required at each iteration. The GMRES and
Krylov-Schur algorithms employ the Arnoldi algorithm for nonsymmetric matrices.
The underlying orthogonalization scheme is left-looking and processes one
column at a time. Thus, at least one global reduction is required per
iteration. The traditional algorithm for generating the orthogonal Krylov basis
vectors for the Krylov-Schur algorithm is classical Gram Schmidt applied twice
with reorthogonalization (CGS2), requiring three global reductions per step. A
new variant of CGS2 that requires only one reduction per iteration is applied
to the Arnoldi-QR iteration. Strong-scaling results are presented for finding
eigenvalue-pairs of nonsymmetric matrices. A preliminary attempt to derive a
similar algorithm (one reduction per Arnoldi iteration with a robust
orthogonalization scheme) was presented by Hernandez et al.(2007). Unlike our
approach, their method is not forward stable for eigenvalues.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01646</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01646</id><submitter>Ayal Taitler</submitter><version version="v1"><date>Sun, 4 Apr 2021 17:12:24 GMT</date><size>14341kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 8 Apr 2021 12:48:09 GMT</date><size>10797kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 13:06:22 GMT</date><size>11103kb</size><source_type>D</source_type></version><title>SOLO: Search Online, Learn Offline for Combinatorial Optimization
  Problems</title><authors>Joel Oren, Chana Ross, Maksym Lefarov, Felix Richter, Ayal Taitler,
  Zohar Feldman, Christian Daniel, Dotan Di Castro</authors><categories>cs.LG math.OC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study combinatorial problems with real world applications such as machine
scheduling, routing, and assignment. We propose a method that combines
Reinforcement Learning (RL) and planning. This method can equally be applied to
both the offline, as well as online, variants of the combinatorial problem, in
which the problem components (e.g., jobs in scheduling problems) are not known
in advance, but rather arrive during the decision-making process. Our solution
is quite generic, scalable, and leverages distributional knowledge of the
problem parameters. We frame the solution process as an MDP, and take a Deep
Q-Learning approach wherein states are represented as graphs, thereby allowing
our trained policies to deal with arbitrary changes in a principled manner.
Though learned policies work well in expectation, small deviations can have
substantial negative effects in combinatorial settings. We mitigate these
drawbacks by employing our graph-convolutional policies as non-optimal
heuristics in a compatible search algorithm, Monte Carlo Tree Search, to
significantly improve overall performance. We demonstrate our method on two
problems: Machine Scheduling and Capacitated Vehicle Routing. We show that our
method outperforms custom-tailored mathematical solvers, state of the art
learning-based algorithms, and common heuristics, both in computation time and
performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01834</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01834</id><submitter>Simon Martinez-Rozas</submitter><version version="v1"><date>Mon, 5 Apr 2021 10:10:13 GMT</date><size>17013kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 10:13:53 GMT</date><size>29193kb</size><source_type>D</source_type></version><title>Skyeye Team at MBZIRC 2020: A team of aerial and ground robots for
  GPS-denied autonomous fire extinguishing in an urban building scenario</title><authors>Simon Martinez-Rozas, Rafael Rey, David Alejo, Domingo Acedo, Jose
  Antonio Cobano, Alejandro Rodriguez-Ramos, Pascual Campoy, Luis Merino,
  Fernando Caballero</authors><categories>cs.RO</categories><comments>The presents paper was accepted in the journal &quot;Field Robotics&quot; as
  part of MBZIRC'20 special Issues</comments><journal-ref>Field Robotics, 2020</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper presents a framework for fire extinguishing in an urban scenario by
a team of aerial and ground robots. The system was developed to address
Challenge 3 of the 2020Mohamed Bin Zayed International Robotics Challenge
(MBZIRC). The challenge required to autonomously detect, locate and extinguish
fires on different floors of a building, as well as in its surroundings. The
multi-robot system developed consists of a heterogeneous robot team of up to
three Unmanned Aerial Vehicles (UAV) and one Unmanned Ground Vehicle (UGV). We
describe the main hardware and software components for UAV and UGVplatforms and
also present the main algorithmic components of the system: a 3D LIDAR-based
mapping and localization module able to work in GPS-denied scenarios; a global
planner and a fast local re-planning system for robot navigation;
infrared-based perception and robot actuation control for fire extinguishing;
and a mission executive and coordination module based on Behavior Trees. The
paper finally describes the results obtained during the competition, where the
system worked fully autonomously and scored in all the trials performed. The
presented system ended in 7th position out of 20 teams in the Challenge3
competition and in 5th position (out of 17 teams) in the Challenge 3 entry to
the Grand Finale (Grand Challenge) of MBZIRC 2020 competition.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.01893</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.01893</id><submitter>Gen Li</submitter><version version="v1"><date>Mon, 5 Apr 2021 13:10:50 GMT</date><size>5942kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 12:49:14 GMT</date><size>5942kb</size><source_type>D</source_type></version><title>Adaptive Prototype Learning and Allocation for Few-Shot Segmentation</title><authors>Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun, Jonghyun Kim,
  Joongkyu Kim</authors><categories>cs.CV</categories><comments>Accepted to CVPR2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prototype learning is extensively used for few-shot segmentation. Typically,
a single prototype is obtained from the support feature by averaging the global
object information. However, using one prototype to represent all the
information may lead to ambiguities. In this paper, we propose two novel
modules, named superpixel-guided clustering (SGC) and guided prototype
allocation (GPA), for multiple prototype extraction and allocation.
Specifically, SGC is a parameter-free and training-free approach, which
extracts more representative prototypes by aggregating similar feature vectors,
while GPA is able to select matched prototypes to provide more accurate
guidance. By integrating the SGC and GPA together, we propose the Adaptive
Superpixel-guided Network (ASGNet), which is a lightweight model and adapts to
object scale and shape variation. In addition, our network can easily
generalize to k-shot segmentation with substantial improvement and no
additional computational cost. In particular, our evaluations on COCO
demonstrate that ASGNet surpasses the state-of-the-art method by 5% in 5-shot
segmentation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.02832</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.02832</id><submitter>Syed Talha Bukhari</submitter><version version="v1"><date>Wed, 7 Apr 2021 00:07:53 GMT</date><size>3237kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 04:20:00 GMT</date><size>3237kb</size><source_type>D</source_type></version><title>ARC: A Vision-based Automatic Retail Checkout System</title><authors>Syed Talha Bukhari, Abdul Wahab Amin, Muhammad Abdullah Naveed,
  Muhammad Rzi Abbas</authors><categories>cs.CV</categories><comments>Work was done during the academic year 2017-2018 as a Senior Year
  (undergraduate) Project (thesis)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Retail checkout systems employed at supermarkets primarily rely on barcode
scanners, with some utilizing QR codes, to identify the items being purchased.
These methods are time-consuming in practice, require a certain level of human
supervision, and involve waiting in long queues. In this regard, we propose a
system, that we call ARC, which aims at making the process of check-out at
retail store counters faster, autonomous, and more convenient, while reducing
dependency on a human operator. The approach makes use of a computer
vision-based system, with a Convolutional Neural Network at its core, which
scans objects placed beneath a webcam for identification. To evaluate the
proposed system, we curated an image dataset of one-hundred local retail items
of various categories. Within the given assumptions and considerations, the
system achieves a reasonable test-time accuracy, pointing towards an ambitious
future for the proposed setup. The project code and the dataset are made
publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.03408</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.03408</id><submitter>Tae Min Hong</submitter><version version="v1"><date>Wed, 7 Apr 2021 21:46:42 GMT</date><size>3152kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 21:52:58 GMT</date><size>3387kb</size><source_type>D</source_type></version><title>Nanosecond machine learning event classification with boosted decision
  trees in FPGA for high energy physics</title><authors>Tae Min Hong, Benjamin Carlson, Brandon Eubanks, Stephen Racz, Stephen
  Roche, Joerg Stelzer, Daniel Stumpp</authors><categories>hep-ex cs.LG</categories><comments>65 pages, 27 figures, 13 tables</comments><report-no>PITT-PACC-2103</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel implementation of classification using the machine
learning / artificial intelligence method called boosted decision trees (BDT)
on field programmable gate arrays (FPGA). The firmware implementation of binary
classification requiring 100 training trees with a maximum depth of 4 using
four input variables gives a latency value of about 10 ns, independent of the
clock speed from 100 to 320 MHz in our setup. The low timing values are
achieved by restructuring the BDT layout and reconfiguring its parameters. The
FPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our
setup. A software package called fwXmachina achieves this implementation. Our
intended user is an expert of custom electronics-based trigger systems in high
energy physics experiments or anyone that needs decisions at the lowest latency
values for real-time event classification. Two problems from high energy
physics are considered, in the separation of electrons vs. photons and in the
selection of vector boson fusion-produced Higgs bosons vs. the rejection of the
multijet processes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04049</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04049</id><submitter>David Von Dollen</submitter><version version="v1"><date>Thu, 8 Apr 2021 20:48:44 GMT</date><size>287kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 18:43:13 GMT</date><size>287kb</size><source_type>D</source_type></version><title>Quantum-Assisted Feature Selection for Vehicle Price Prediction Modeling</title><authors>David Von Dollen, Florian Neukart, Daniel Weimer, Thomas B\&quot;ack</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Within machine learning model evaluation regimes, feature selection is a
technique to reduce model complexity and improve model performance in regards
to generalization, model fit, and accuracy of prediction. However, the search
over the space of features to find the subset of $k$ optimal features is a
known NP-Hard problem. In this work, we study metrics for encoding the
combinatorial search as a binary quadratic model, such as Generalized Mean
Information Coefficient and Pearson Correlation Coefficient in application to
the underlying regression problem of price prediction. We investigate
trade-offs in the form of run-times and model performance, of leveraging
quantum-assisted vs. classical subroutines for the combinatorial search, using
minimum redundancy maximal relevancy as the heuristic for our approach. We
achieve accuracy scores of 0.9 (in the range of [0,1]) for finding optimal
subsets on synthetic data using a new metric that we define. We test and
cross-validate predictive models on a real-world problem of price prediction,
and show a performance improvement of mean absolute error scores for our
quantum-assisted method $(1471.02 \pm{135.6})$, vs. similar methodologies such
as recursive feature elimination $(1678.3 \pm{143.7})$. Our findings show that
by leveraging quantum-assisted routines we find solutions that increase the
quality of predictive model output while reducing the input dimensionality to
the learning algorithm on synthetic and real-world data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04571</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04571</id><submitter>Daniel Candeloro Cunha</submitter><version version="v1"><date>Wed, 7 Apr 2021 18:22:50 GMT</date><size>976kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 17:44:51 GMT</date><size>993kb</size><source_type>D</source_type></version><title>Finite Variation Sensitivity Analysis for Discrete Topology Optimization
  of Continuum Structures</title><authors>Daniel Candeloro Cunha, Breno Vincenzo de Almeida, Heitor Nigro Lopes,
  Renato Pavanello</authors><categories>cs.CE cs.NA math.NA math.OC</categories><comments>31 pages, 25 figures, submitted to Structural and Multidisciplinary
  Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes two novel approaches to perform more suitable sensitivity
analyses for discrete topology optimization methods. To properly support them,
we introduce a more formal description of the Bi-directional Evolutionary
Structural Optimization (BESO) method, in which the sensitivity analysis is
based on finite variations of the objective function. The proposed approaches
are compared to a naive strategy; to the conventional strategy, referred to as
First-Order Continuous Interpolation (FOCI) approach; and to a strategy
previously developed by other researchers, referred to as High-Order Continuous
Interpolation (HOCI) approach. The novel Woodbury approach provides exact
sensitivity values and is a better alternative to HOCI. Although HOCI and
Woodbury approaches may be computationally prohibitive, they provide useful
expressions for a better understanding of the problem. The novel Conjugate
Gradient Method (CGM) approach provides sensitivity values with arbitrary
precision and is computationally viable for a small number of steps. The CGM
approach is a better alternative to FOCI since, for appropriate initial
conditions, it is always more accurate than the conventional strategy. The
standard compliance minimization problem with volume constraint is considered
to illustrate the methodology. Numerical examples are presented together with a
broad discussion about BESO-type methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.04828</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.04828</id><submitter>Radu Tudor Ionescu</submitter><version version="v1"><date>Sat, 10 Apr 2021 18:21:53 GMT</date><size>638kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 07:28:47 GMT</date><size>638kb</size><source_type>D</source_type></version><title>FreSaDa: A French Satire Data Set for Cross-Domain Satire Detection</title><authors>Radu Tudor Ionescu, Adrian Gabriel Chifu</authors><categories>cs.CL cs.LG</categories><comments>Accepted at IJCNN 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce FreSaDa, a French Satire Data Set, which is
composed of 11,570 articles from the news domain. In order to avoid reporting
unreasonably high accuracy rates due to the learning of characteristics
specific to publication sources, we divided our samples into training,
validation and test, such that the training publication sources are distinct
from the validation and test publication sources. This gives rise to a
cross-domain (cross-source) satire detection task. We employ two classification
methods as baselines for our new data set, one based on low-level features
(character n-grams) and one based on high-level features (average of CamemBERT
word embeddings). As an additional contribution, we present an unsupervised
domain adaptation method based on regarding the pairwise similarities (given by
the dot product) between the training samples and the validation samples as
features. By including these domain-specific features, we attain significant
improvements for both character n-grams and CamemBERT embeddings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.05055</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.05055</id><submitter>Yang Zhang</submitter><version version="v1"><date>Sun, 11 Apr 2021 17:09:49 GMT</date><size>410kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 16:55:21 GMT</date><size>411kb</size><source_type>D</source_type></version><title>NeMo Inverse Text Normalization: From Development To Production</title><authors>Yang Zhang, Evelina Bakhturina, Kyle Gorman, Boris Ginsburg</authors><categories>cs.CL cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inverse text normalization (ITN) converts spoken-domain automatic speech
recognition (ASR) output into written-domain text to improve the readability of
the ASR output. Many state-of-the-art ITN systems use hand-written weighted
finite-state transducer(WFST) grammars since this task has extremely low
tolerance to unrecoverable errors. We introduce an open-source Python
WFST-based library for ITN which enables a seamless path from development to
production. We describe the specification of ITN grammar rules for English, but
the library can be adapted for other languages. It can also be used for
written-to-spoken text normalization. We evaluate the NeMo ITN library using a
modified version of the Google Text normalization dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.05634</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.05634</id><submitter>Cheuk Ting Li</submitter><version version="v1"><date>Mon, 12 Apr 2021 16:56:18 GMT</date><size>145kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 18:59:21 GMT</date><size>148kb</size><source_type>D</source_type></version><title>The Undecidability of Conditional Affine Information Inequalities and
  Conditional Independence Implication with a Binary Constraint</title><authors>Cheuk Ting Li</authors><categories>cs.IT math.IT math.PR</categories><comments>16 pages, 3 figures, submitted to the 2021 IEEE Information Theory
  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the undecidability of conditional affine information
inequalities, the undecidability of the conditional independence implication
problem with a constraint that one random variable is binary, and the
undecidability of the problem of deciding whether the intersection of the
entropic region and a given affine subspace is empty. This is a step towards
the conjecture on the undecidability of conditional independence implication.
The undecidability is proved via a reduction from the periodic tiling problem
(a variant of the domino problem).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.06133</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.06133</id><submitter>David Saulpic</submitter><version version="v1"><date>Tue, 13 Apr 2021 12:15:36 GMT</date><size>171kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 17:43:56 GMT</date><size>211kb</size><source_type>D</source_type></version><title>A New Coreset Framework for Clustering</title><authors>Vincent Cohen-Addad, David Saulpic, Chris Schwiegelshohn</authors><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given a metric space, the $(k,z)$-clustering problem consists of finding $k$
centers such that the sum of the of distances raised to the power $z$ of every
point to its closest center is minimized. This encapsulates the famous
$k$-median ($z=1$) and $k$-means ($z=2$) clustering problems. Designing
small-space sketches of the data that approximately preserves the cost of the
solutions, also known as \emph{coresets}, has been an important research
direction over the last 15 years.
  In this paper, we present a new, simple coreset framework that simultaneously
improves upon the best known bounds for a large variety of settings, ranging
from Euclidean space, doubling metric, minor-free metric, and the general
metric cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.07099</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.07099</id><submitter>Francesco Ferrante</submitter><version version="v1"><date>Wed, 14 Apr 2021 20:01:56 GMT</date><size>166kb</size></version><version version="v2"><date>Sat, 15 May 2021 10:48:41 GMT</date><size>147kb</size></version><title>Synchronization of Identical Boundary-Actuated Semilinear
  Infinite-Dimensional Systems</title><authors>Francesco Ferrante, Giacomo Casadei, and Christophe Prieur</authors><categories>eess.SY cs.SY</categories><comments>V1 is the extended version of the original submission before peer
  review. V2 is the extended version revised based on reviewers' feedback. V2
  fixes a number of mistakes and typos in V1. In particular, an oversight in
  the statement of Proposition 2 has been fixed and title has been updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with synchronization of a class of infinite-dimensional
systems. The considered network is described by a collection of semilinear
Lipschitz boundary-actuated infinite-dimensional dynamics. For undirected
connected graphs, sufficient conditions for asymptotic synchronization are
established. We show that the proposed conditions when applied to systems of
hyperbolic semilinear conservation laws can be recast into a set of matrix
inequalities. For this class of systems, sufficient conditions in the form of
linear matrix inequalities for the design of synchronizing policies are
provided.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.07129</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.07129</id><submitter>Jing Lu</submitter><version version="v1"><date>Wed, 14 Apr 2021 21:18:02 GMT</date><size>1096kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 29 Apr 2021 05:12:27 GMT</date><size>1180kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 02:21:08 GMT</date><size>1180kb</size><source_type>D</source_type></version><title>On the approximation of queue-length distributions in transportation
  networks</title><authors>Jing Lu</authors><categories>cs.GT cs.SY eess.SY math.OC math.PR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper focuses on the analytical probabilistic modeling of vehicular
traffic. It formulates a stochastic node model. It then formulates a network
model by coupling the node model with the link model of Lu and Osorio (2018),
which is a stochastic formulation of the traffic-theoretic link transmission
model. The proposed network model is scalable and computationally efficient,
making it suitable for urban network optimization. For a network with $r$
links, each of space capacity $\ell$, the model has a complexity of
$\mathcal{O}(r\ell)$. The network model yields the marginal distribution of
link states. The model is validated versus a simulation-based network
implementation of the stochastic link transmission model. The validation
experiments consider a set of small network with intricate traffic dynamics.
For all scenarios, the proposed model accurately captures the traffic dynamics.
The network model is used to address a signal control problem. Compared to the
probabilistic link model of Lu and Osorio (2018) with an exogenous node model
and a benchmark deterministic network loading model, the proposed network model
derives signal plans with better performance. The case study highlights the
added value of using between-link (i.e., across-node) interaction information
for traffic management and accounting for stochasticity in the network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08313</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08313</id><submitter>Benjamin Devillers</submitter><version version="v1"><date>Fri, 16 Apr 2021 18:54:14 GMT</date><size>836kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 17:23:52 GMT</date><size>143kb</size><source_type>D</source_type></version><title>Does language help generalization in vision models?</title><authors>Benjamin Devillers, Bhavin Choksi, Romain Bielawski and Rufin
  VanRullen</authors><categories>cs.AI cs.CL cs.CV</categories><comments>Paper accepted for presentation at the ViGIL 2021 workshop @NAACL.
  This version: added models to the comparison (ICMLM, TSM); added tests of
  adversarial robustness; mistake identified and corrected in the normalization
  of image features; results and conclusions updated accordingly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision models trained on multimodal datasets can benefit from the wide
availability of large image-caption datasets. A recent model (CLIP) was found
to generalize well in zero-shot and transfer learning settings. This could
imply that linguistic or &quot;semantic grounding&quot; confers additional generalization
abilities to the visual feature space. Here, we systematically evaluate various
multimodal architectures and vision-only models in terms of unsupervised
clustering, few-shot learning, transfer learning and adversarial robustness. In
each setting, multimodal training produced no additional generalization
capability compared to standard supervised visual training. We conclude that
work is still required for semantic grounding to help improve vision models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08462</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08462</id><submitter>Sitanshu Gakkhar</submitter><version version="v1"><date>Sat, 17 Apr 2021 05:58:16 GMT</date><size>133kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 04:06:55 GMT</date><size>134kb</size><source_type>D</source_type></version><title>Syntactic structures and the general Markov models</title><authors>Sitanshu Gakkhar, Matilde Marcolli</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We further the theme of studying syntactic structures data from Longobardi
(2017b), Collins (2010), Ceolin et al. (2020) and Koopman (2011) using general
Markov models initiated in Shu et al. (2017), exploring the question of how
consistent the data is with the idea that general Markov models. The ideas
explored in the present paper are more generally applicable than to the setting
of syntactic structures, and can be used when analyzing consistency of data
with general Markov models. Additionally, we give an interpretation of the
methods of Ceolin et al. (2020) as an infinite sites evolutionary model and
compare it to the Markov model and explore each in the context of evolutionary
processes acting on human language syntax.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08501</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08501</id><submitter>Jakub Tetek</submitter><version version="v1"><date>Sat, 17 Apr 2021 09:51:00 GMT</date><size>36kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 18:47:47 GMT</date><size>38kb</size><source_type>D</source_type></version><title>Approximate Triangle Counting via Sampling and Fast Matrix
  Multiplication</title><authors>Jakub T\v{e}tek</authors><categories>cs.DS</categories><comments>Improved presentation, many minor edits, improved comparison to
  related work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a trivial $O(\frac{n^3}{T})$ time algorithm for approximate triangle
counting where $T$ is the number of triangles in the graph and $n$ the number
of vertices. At the same time, one may count triangles exactly using fast
matrix multiplication in time $\tilde{O}(n^\omega)$. Is it possible to get a
negative dependency on the number of triangles $T$ while retaining the
$n^\omega$ dependency on $n$? We answer this question positively by providing
an algorithm which runs in time $O\big(\frac{n^\omega}{T^{\omega - 2}}\big)
\cdot \text{poly}(n^{o(1)}/\epsilon)$. This is optimal in the sense that as
long as the exponent of $T$ is independent of $n, T$, it cannot be improved
while retaining the dependency on $n$; this as follows from the lower bound of
Eden and Rosenbaum [APPROX/RANDOM 2018]. Our algorithm improves upon the state
of the art when $T = \omega(1)$ and $T = o(n)$.
  We also consider the problem of approximate triangle counting in sparse
graphs, parameterizing by the number of edges $m$. The best known algorithm
runs in time $\tilde{O}\big(\frac{m^{3/2}}{T}\big)$ [Eden et al., SIAM Journal
on Computing, 2017]. There is also a well known algorithm for exact triangle
counting that runs in time $\tilde{O}(m^{2\omega/(\omega + 1)})$. We again get
an algorithm that retains the exponent of $m$ while running faster on graphs
with larger number of triangles. Specifically, our algorithm runs in time
$O\Big(\frac{m^{2\omega/(\omega+1)}}{ T^{2(\omega-1)/(\omega+1)}}\Big) \cdot
\text{poly}(n^{o(1)}/\epsilon)$. This is again optimal in the sense that if the
exponent of $T$ is to be constant, it cannot be improved without worsening the
dependency on $m$. This algorithm improves upon the state of the art when $T =
\omega(1)$ and $T = o(\sqrt{m})$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.08671</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.08671</id><submitter>Lucia Zheng</submitter><version version="v1"><date>Sun, 18 Apr 2021 00:57:16 GMT</date><size>933kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 22:45:11 GMT</date><size>1040kb</size><source_type>D</source_type></version><title>When Does Pretraining Help? Assessing Self-Supervised Learning for Law
  and the CaseHOLD Dataset</title><authors>Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, Daniel
  E. Ho</authors><categories>cs.CL</categories><comments>ICAIL 2021. Code &amp; data available at
  https://github.com/reglab/casehold</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While self-supervised learning has made rapid advances in natural language
processing, it remains unclear when researchers should engage in
resource-intensive domain-specific pretraining (domain pretraining). The law,
puzzlingly, has yielded few documented instances of substantial gains to domain
pretraining in spite of the fact that legal language is widely seen to be
unique. We hypothesize that these existing results stem from the fact that
existing legal NLP tasks are too easy and fail to meet conditions for when
domain pretraining can help. To address this, we first present CaseHOLD (Case
Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple
choice questions to identify the relevant holding of a cited case. This dataset
presents a fundamental task to lawyers and is both legally meaningful and
difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second,
we assess performance gains on CaseHOLD and existing legal NLP datasets. While
a Transformer architecture (BERT) pretrained on a general corpus (Google Books
and Wikipedia) improves performance, domain pretraining (using corpus of
approximately 3.5M decisions across all courts in the U.S. that is larger than
BERT's) with a custom legal vocabulary exhibits the most substantial
performance gains with CaseHOLD (gain of 7.2% on F1, representing a 12%
improvement on BERT) and consistent performance gains across two other legal
tasks. Third, we show that domain pretraining may be warranted when the task
exhibits sufficient similarity to the pretraining corpus: the level of
performance increase in three legal tasks was directly tied to the domain
specificity of the task. Our findings inform when researchers should engage
resource-intensive pretraining and show that Transformer-based architectures,
too, learn embeddings suggestive of distinct legal language.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.09301</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.09301</id><submitter>William Beksi</submitter><version version="v1"><date>Mon, 19 Apr 2021 13:45:56 GMT</date><size>8112kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 00:34:47 GMT</date><size>8115kb</size><source_type>D</source_type></version><title>Vision-Based Guidance for Tracking Dynamic Objects</title><authors>Pritam Karmokar, Kashish Dhal, William J. Beksi, Animesh Chakravarthy</authors><categories>cs.CV cs.RO</categories><comments>To be published in the 2021 International Conference on Unmanned
  Aircraft Systems (ICUAS)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present a novel vision-based framework for tracking dynamic
objects using guidance laws based on a rendezvous cone approach. These guidance
laws enable an unmanned aircraft system equipped with a monocular camera to
continuously follow a moving object within the sensor's field of view. We
identify and classify feature point estimators for managing the occurrence of
occlusions during the tracking process in an exclusive manner. Furthermore, we
develop an open-source simulation environment and perform a series of
simulations to show the efficacy of our methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.09734</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.09734</id><submitter>Pasin Manurangsi</submitter><version version="v1"><date>Tue, 20 Apr 2021 03:07:31 GMT</date><size>172kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 05:27:59 GMT</date><size>172kb</size><source_type>D</source_type></version><title>Locally Private k-Means in One Round</title><authors>Alisa Chang, Badih Ghazi, Ravi Kumar, Pasin Manurangsi</authors><categories>cs.DS cs.CR cs.LG</categories><comments>35 pages. To appear in ICML'21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an approximation algorithm for k-means clustering in the one-round
(aka non-interactive) local model of differential privacy (DP). This algorithm
achieves an approximation ratio arbitrarily close to the best non private
approximation algorithm, improving upon previously known algorithms that only
guarantee large (constant) approximation ratios. Furthermore, this is the first
constant-factor approximation algorithm for k-means that requires only one
round of communication in the local DP model, positively resolving an open
question of Stemmer (SODA 2020). Our algorithmic framework is quite flexible;
we demonstrate this by showing that it also yields a similar near-optimal
approximation algorithm in the (one-round) shuffle DP model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.09791</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.09791</id><submitter>Xinyu Ma</submitter><version version="v1"><date>Tue, 20 Apr 2021 07:07:56 GMT</date><size>1148kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 16:00:17 GMT</date><size>1151kb</size><source_type>D</source_type></version><title>B-PROP: Bootstrapped Pre-training with Representative Words Prediction
  for Ad-hoc Retrieval</title><authors>Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li and Xueqi
  Cheng</authors><categories>cs.IR</categories><comments>Accepted by SIGIR 2021. arXiv admin note: text overlap with
  arXiv:2010.10137</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pre-training and fine-tuning have achieved remarkable success in many
downstream natural language processing (NLP) tasks. Recently, pre-training
methods tailored for information retrieval (IR) have also been explored, and
the latest success is the PROP method which has reached new SOTA on a variety
of ad-hoc retrieval benchmarks. The basic idea of PROP is to construct the
\textit{representative words prediction} (ROP) task for pre-training inspired
by the query likelihood model. Despite its exciting performance, the
effectiveness of PROP might be bounded by the classical unigram language model
adopted in the ROP task construction process. To tackle this problem, we
propose a bootstrapped pre-training method (namely B-PROP) based on BERT for
ad-hoc retrieval. The key idea is to use the powerful contextual language model
BERT to replace the classical unigram language model for the ROP task
construction, and re-train BERT itself towards the tailored objective for IR.
Specifically, we introduce a novel contrastive method, inspired by the
divergence-from-randomness idea, to leverage BERT's self-attention mechanism to
sample representative words from the document. By further fine-tuning on
downstream ad-hoc retrieval tasks, our method achieves significant improvements
over baselines without pre-training or with other pre-training methods, and
further pushes forward the SOTA on a variety of ad-hoc retrieval tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10100</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10100</id><submitter>Son T. Luu</submitter><version version="v1"><date>Tue, 20 Apr 2021 16:32:56 GMT</date><size>5301kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 10:47:01 GMT</date><size>5524kb</size><source_type>D</source_type></version><title>UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with
  BiLSTM-CRF and Toxic Bert Comment Classification</title><authors>Son T. Luu, Ngan Luu-Thuy Nguyen</authors><categories>cs.CL</categories><comments>Accepted by The 15th International Workshop on Semantic Evaluation
  (SemEval 2021)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This
task aims to build a model for identifying toxic words in whole posts. We use
the BiLSTM-CRF model combining with Toxic Bert Classification to train the
detection model for identifying toxic words in posts. Our model achieves 62.23%
by F1-score on the Toxic Spans Detection task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.10640</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.10640</id><submitter>Sushant Singh</submitter><version version="v1"><date>Tue, 23 Mar 2021 22:38:20 GMT</date><size>3130kb</size></version><version version="v2"><date>Thu, 22 Apr 2021 05:38:37 GMT</date><size>2473kb</size></version><version version="v3"><date>Sat, 24 Apr 2021 17:31:46 GMT</date><size>2432kb</size></version><title>The NLP Cookbook: Modern Recipes for Transformer based Deep Learning
  Architectures</title><authors>Sushant Singh and Ausif Mahmood</authors><categories>cs.CL cs.LG</categories><comments>27 pages and 29 figures</comments><journal-ref>IEEE Access (Volume: 9), 2021</journal-ref><doi>10.1109/ACCESS.2021.3077350</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In recent years, Natural Language Processing (NLP) models have achieved
phenomenal success in linguistic and semantic tasks like text classification,
machine translation, cognitive dialogue systems, information retrieval via
Natural Language Understanding (NLU), and Natural Language Generation (NLG).
This feat is primarily attributed due to the seminal Transformer architecture,
leading to designs such as BERT, GPT (I, II, III), etc. Although these
large-size models have achieved unprecedented performances, they come at high
computational costs. Consequently, some of the recent NLP architectures have
utilized concepts of transfer learning, pruning, quantization, and knowledge
distillation to achieve moderate model sizes while keeping nearly similar
performances as achieved by their predecessors. Additionally, to mitigate the
data size challenge raised by language models from a knowledge extraction
perspective, Knowledge Retrievers have been built to extricate explicit data
documents from a large corpus of databases with greater efficiency and
accuracy. Recent research has also focused on superior inference by providing
efficient attention to longer input sequences. In this paper, we summarize and
examine the current state-of-the-art (SOTA) NLP models that have been employed
for numerous NLP tasks for optimal performance and efficiency. We provide a
detailed understanding and functioning of the different architectures, a
taxonomy of NLP designs, comparative evaluations, and future directions in NLP.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.11734</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.11734</id><submitter>Jacob Zavatone-Veth</submitter><version version="v1"><date>Fri, 23 Apr 2021 17:31:42 GMT</date><size>1664kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 17:42:44 GMT</date><size>2093kb</size><source_type>D</source_type></version><title>Exact priors of finite neural networks</title><authors>Jacob A. Zavatone-Veth and Cengiz Pehlevan</authors><categories>cs.LG cond-mat.dis-nn stat.ML</categories><comments>12+11 pages, 4 figures; v2: references and figures added</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Bayesian neural networks are theoretically well-understood only in the
infinite-width limit, where Gaussian priors over network weights yield Gaussian
priors over network outputs. Recent work has suggested that finite Bayesian
networks may outperform their infinite counterparts, but their non-Gaussian
output priors have been characterized only though perturbative approaches.
Here, we derive exact solutions for the output priors for individual input
examples of a class of finite fully-connected feedforward Bayesian neural
networks. For deep linear networks, the prior has a simple expression in terms
of the Meijer $G$-function. The prior of a finite ReLU network is a mixture of
the priors of linear networks of smaller widths, corresponding to different
numbers of active units in each layer. Our results unify previous descriptions
of finite network priors in terms of their tail decay and large-width behavior.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.11888</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.11888</id><submitter>Thien-Minh Nguyen</submitter><version version="v1"><date>Sat, 24 Apr 2021 06:03:25 GMT</date><size>7750kb</size><source_type>D</source_type></version><title>MILIOM: Tightly Coupled Multi-Input Lidar-Inertia Odometry and Mapping</title><authors>Thien-Minh Nguyen, Shenghai Yuan, Muqing Cao, Yang Lyu, Thien Hoang
  Nguyen, Lihua Xie</authors><categories>cs.RO cs.SY eess.SY</categories><comments>Submitted to RAL-IROS 2021</comments><doi>10.1109/LRA.2021.3080633</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we investigate a tightly coupled Lidar-Inertia Odometry and
Mapping (LIOM) scheme, with the capability to incorporate multiple lidars with
complementary field of view (FOV). In essence, we devise a time-synchronized
scheme to combine extracted features from separate lidars into a single
pointcloud, which is then used to construct a local map and compute the
feature-map matching (FMM) coefficients. These coefficients, along with the IMU
preinteration observations, are then used to construct a factor graph that will
be optimized to produce an estimate of the sliding window trajectory. We also
propose a key frame-based map management strategy to marginalize certain poses
and pointclouds in the sliding window to grow a global map, which is used to
assemble the local map in the later stage. The use of multiple lidars with
complementary FOV and the global map ensures that our estimate has low drift
and can sustain good localization in situations where single lidar use gives
poor result, or even fails to work. Multi-thread computation implementations
are also adopted to fractionally cut down the computation time and ensure
real-time performance. We demonstrate the efficacy of our system via a series
of experiments on public datasets collected from an aerial vehicle.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.12292</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.12292</id><submitter>Erica Cooper</submitter><version version="v1"><date>Sun, 25 Apr 2021 23:59:00 GMT</date><size>900kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 28 Apr 2021 01:25:38 GMT</date><size>900kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 23:32:41 GMT</date><size>900kb</size><source_type>D</source_type></version><title>Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</title><authors>Erica Cooper, Xin Wang, Junichi Yamagishi</authors><categories>cs.SD eess.AS</categories><comments>Submitted to ISCA Speech Synthesis Workshop 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech synthesis and music audio generation from symbolic input differ in
many aspects but share some similarities. In this study, we investigate how
text-to-speech synthesis techniques can be used for piano MIDI-to-audio
synthesis tasks. Our investigation includes Tacotron and neural source-filter
waveform models as the basic components, with which we build MIDI-to-audio
synthesis systems in similar ways to TTS frameworks. We also include reference
systems using conventional sound modeling techniques such as sample-based and
physical-modeling-based methods. The subjective experimental results
demonstrate that the investigated TTS components can be applied to piano
MIDI-to-audio synthesis with minor modifications. The results also reveal the
performance bottleneck -- while the waveform model can synthesize high quality
piano sound given natural acoustic features, the conversion from MIDI to
acoustic features is challenging. The full MIDI-to-audio synthesis system is
still inferior to the sample-based or physical-modeling-based approaches, but
we encourage TTS researchers to test their TTS models for this new task and
improve the performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.12678</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.12678</id><submitter>Yuchang Sun</submitter><version version="v1"><date>Mon, 26 Apr 2021 16:11:47 GMT</date><size>556kb</size></version><version version="v2"><date>Tue, 27 Apr 2021 04:27:54 GMT</date><size>563kb</size></version><version version="v3"><date>Tue, 18 May 2021 06:38:39 GMT</date><size>563kb</size></version><title>Semi-Decentralized Federated Edge Learning for Fast Convergence on
  Non-IID Data</title><authors>Yuchang Sun and Jiawei Shao and Yuyi Mao and Jun Zhang</authors><categories>cs.NI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated edge learning (FEEL) has emerged as an effective alternative to
reduce the large communication latency in Cloud-based machine learning
solutions, while preserving data privacy. Unfortunately, the learning
performance of FEEL may be compromised due to limited training data in a single
edge cluster. In this paper, we investigate a novel framework of FEEL, namely
semi-decentralized federated edge learning (SD-FEEL). By allowing model
aggregation between different edge clusters, SD-FEEL enjoys the benefit of FEEL
in reducing training latency and improves the learning performance by accessing
richer training data from multiple edge clusters. A training algorithm for
SD-FEEL with three main procedures in each round is presented, including local
model updates, intra-cluster and inter-cluster model aggregations, and it is
proved to converge on non-independent and identically distributed (non-IID)
data. We also characterize the interplay between the network topology of the
edge servers and the communication overhead of inter-cluster model aggregation
on training performance. Experiment results corroborate our analysis and
demonstrate the effectiveness of SD-FFEL in achieving fast convergence.
Besides, guidelines on choosing critical hyper-parameters of the training
algorithm are also provided.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13398</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13398</id><submitter>Dominik Dold</submitter><version version="v1"><date>Tue, 27 Apr 2021 18:00:12 GMT</date><size>676kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 09:14:28 GMT</date><size>677kb</size><source_type>D</source_type></version><title>SpikE: spike-based embeddings for multi-relational graph data</title><authors>Dominik Dold, Josep Soler Garrido</authors><categories>cs.NE cs.LG q-bio.NC</categories><comments>Accepted for publication at IJCNN 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the recent success of reconciling spike-based coding with the error
backpropagation algorithm, spiking neural networks are still mostly applied to
tasks stemming from sensory processing, operating on traditional data
structures like visual or auditory data. A rich data representation that finds
wide application in industry and research is the so-called knowledge graph - a
graph-based structure where entities are depicted as nodes and relations
between them as edges. Complex systems like molecules, social networks and
industrial factory systems can be described using the common language of
knowledge graphs, allowing the usage of graph embedding algorithms to make
context-aware predictions in these information-packed environments. We propose
a spike-based algorithm where nodes in a graph are represented by single spike
times of neuron populations and relations as spike time differences between
populations. Learning such spike-based embeddings only requires knowledge about
spike times and spike time differences, compatible with recently proposed
frameworks for training spiking neural networks. The presented model is easily
mapped to current neuromorphic hardware systems and thereby moves inference on
knowledge graphs into a domain where these architectures thrive, unlocking a
promising industrial application area for this technology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13559</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13559</id><submitter>Tariq Alhindi</submitter><version version="v1"><date>Wed, 28 Apr 2021 03:38:24 GMT</date><size>4840kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 05:41:05 GMT</date><size>4838kb</size><source_type>D</source_type></version><title>AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance
  Detection for Fact Checking</title><authors>Tariq Alhindi, Amal Alabdulkarim, Ali Alshehri, Muhammad Abdul-Mageed
  and Preslav Nakov</authors><categories>cs.CL</categories><comments>Accepted to the 2021 Workshop on NLP4IF: Censorship, Disinformation,
  and Propaganda</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the continuing spread of misinformation and disinformation online, it is
of increasing importance to develop combating mechanisms at scale in the form
of automated systems that support multiple languages. One task of interest is
claim veracity prediction, which can be addressed using stance detection with
respect to relevant documents retrieved online. To this end, we present our new
Arabic Stance Detection dataset (AraStance) of 4,063 claim--article pairs from
a diverse set of sources comprising three fact-checking websites and one news
website. AraStance covers false and true claims from multiple domains (e.g.,
politics, sports, health) and several Arab countries, and it is well-balanced
between related and unrelated documents with respect to the claims. We
benchmark AraStance, along with two other stance detection datasets, using a
number of BERT-based models. Our best model achieves an accuracy of 85\% and a
macro F1 score of 78\%, which leaves room for improvement and reflects the
challenging nature of AraStance and the task of stance detection in general.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.13916</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.13916</id><submitter>Yi Zhang</submitter><version version="v1"><date>Wed, 28 Apr 2021 17:56:04 GMT</date><size>17576kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 30 Apr 2021 14:39:06 GMT</date><size>17576kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 21:32:35 GMT</date><size>3722kb</size><source_type>D</source_type></version><title>Learning Synergistic Attention for Light Field Salient Object Detection</title><authors>Yi Zhang, Geng Chen, Qian Chen, Yujia Sun, Olivier Deforges, Wassim
  Hamidouche and Lu Zhang</authors><categories>cs.CV</categories><comments>14 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel Synergistic Attention Network (SA-Net) to address the
light field salient object detection by establishing a synergistic effect
between multi-modal features with advanced attention mechanisms. Our SA-Net
exploits the rich information of focal stacks via 3D convolutional neural
networks, decodes the high-level features of multi-modal light field data with
two cascaded synergistic attention modules, and predicts the saliency map using
an effective feature fusion module in a progressive manner. Extensive
experiments on three widely-used benchmark datasets show that our SA-Net
outperforms 28 state-of-the-art models, sufficiently demonstrating its
effectiveness and superiority. Our code will be made publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14118</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14118</id><submitter>Hanbo Zhang</submitter><version version="v1"><date>Thu, 29 Apr 2021 05:31:21 GMT</date><size>7047kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 06:46:10 GMT</date><size>5035kb</size><source_type>D</source_type></version><title>REGRAD: A Large-Scale Relational Grasp Dataset for Safe and
  Object-Specific Robotic Grasping in Clutter</title><authors>Hanbo Zhang, Deyu Yang, Han Wang, Binglei Zhao, Xuguang Lan, Nanning
  Zheng</authors><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the impressive progress achieved in robust grasp detection, robots
are not skilled in sophisticated grasping tasks (e.g. search and grasp a
specific object in clutter). Such tasks involve not only grasping, but
comprehensive perception of the visual world (e.g. the relationship between
objects). Recently, the advanced deep learning techniques provide a promising
way for understanding the high-level visual concepts. It encourages robotic
researchers to explore solutions for such hard and complicated fields. However,
deep learning usually means data-hungry. The lack of data severely limits the
performance of deep-learning-based algorithms. In this paper, we present a new
dataset named \regrad to sustain the modeling of relationships among objects
and grasps. We collect the annotations of object poses, segmentations, grasps,
and relationships in each image for comprehensive perception of grasping. Our
dataset is collected in both forms of 2D images and 3D point clouds. Moreover,
since all the data are generated automatically, users are free to import their
own object models for the generation of as many data as they want. We have
released our dataset and codes. A video that demonstrates the process of data
generation is also available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14237</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14237</id><submitter>Umar Khan</submitter><version version="v1"><date>Thu, 29 Apr 2021 09:59:46 GMT</date><size>2160kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 14:31:19 GMT</date><size>2658kb</size><source_type>D</source_type></version><title>TabAug: Data Driven Augmentation for Enhanced Table Structure
  Recognition</title><authors>Umar Khan, Sohaib Zahid, Muhammad Asad Ali, Adnan ul Hassan, Faisal
  Shafait</authors><categories>cs.CV cs.AI cs.LG</categories><comments>to be published in ICDAR2021 , 15 pages , &quot; packages and articles for
  this work and its extensions at http://umarky.com &quot; , &quot; official repository
  https://github.com/sohaib023/splerge-tab-aug?fbclid=IwAR37V79vDLMqLGcC5YCyqY_CsFYQRDZ1-wUMW7GJUYTzkf9oM1bZ25HPmgo
  &quot;</comments><acm-class>I.4.6; I.4.8; I.4.9; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Table Structure Recognition is an essential part of end-to-end tabular data
extraction in document images. The recent success of deep learning model
architectures in computer vision remains to be non-reflective in table
structure recognition, largely because extensive datasets for this domain are
still unavailable while labeling new data is expensive and time-consuming.
Traditionally, in computer vision, these challenges are addressed by standard
augmentation techniques that are based on image transformations like color
jittering and random cropping. As demonstrated by our experiments, these
techniques are not effective for the task of table structure recognition. In
this paper, we propose TabAug, a re-imagined Data Augmentation technique that
produces structural changes in table images through replication and deletion of
rows and columns. It also consists of a data-driven probabilistic model that
allows control over the augmentation process. To demonstrate the efficacy of
our approach, we perform experimentation on ICDAR 2013 dataset where our
approach shows consistent improvements in all aspects of the evaluation
metrics, with cell-level correct detections improving from 92.16% to 96.11%
over the baseline.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14641</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14641</id><submitter>Yao Wang</submitter><version version="v1"><date>Thu, 29 Apr 2021 20:22:02 GMT</date><size>458kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 11 May 2021 19:04:53 GMT</date><size>458kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 02:44:50 GMT</date><size>460kb</size><source_type>D</source_type></version><title>Tuna: A Static Analysis Approach to Optimizing Deep Neural Networks</title><authors>Yao Wang, Xingyu Zhou, Yanming Wang, Rui Li, Yong Wu, Vin Sharma</authors><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce Tuna, a static analysis approach to optimizing deep neural
network programs. The optimization of tensor operations such as convolutions
and matrix multiplications is the key to improving the performance of deep
neural networks. Many deep learning model optimization mechanisms today use
dynamic analysis, which relies on experimental execution on a target device to
build a data-driven cost model of the program. The reliance on dynamic
profiling not only requires access to target hardware at compilation time but
also incurs significant cost in machine resources. We introduce an approach
that profiles the program by constructing features based on the target hardware
characteristics in order. We use static analysis of the relative performance of
tensor operations to optimize the deep learning program. Experiments show that
our approach can achieve up to 11x performance compared to dynamic profiling
based methods with the same compilation time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14670</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14670</id><submitter>Doseok Jang</submitter><version version="v1"><date>Thu, 29 Apr 2021 21:55:04 GMT</date><size>2681kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 06:09:31 GMT</date><size>2870kb</size><source_type>D</source_type></version><title>Using Meta Reinforcement Learning to Bridge the Gap between Simulation
  and Experiment in Energy Demand Response</title><authors>Doseok Jang, Lucas Spangher, Manan Khattar, Utkarsha Agwan, Costas
  Spanos</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Our team is proposing to run a full-scale energy demand response experiment
in an office building. Although this is an exciting endeavor which will provide
value to the community, collecting training data for the reinforcement learning
agent is costly and will be limited. In this work, we apply a meta-learning
architecture to warm start the experiment with simulated tasks, to increase
sample efficiency. We present results that demonstrate a similar a step up in
complexity still corresponds with better learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14674</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14674</id><submitter>Jiawei Zhou</submitter><version version="v1"><date>Thu, 29 Apr 2021 22:01:41 GMT</date><size>7592kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 10 May 2021 20:48:31 GMT</date><size>7592kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 05:52:41 GMT</date><size>7592kb</size><source_type>D</source_type></version><title>AMR Parsing with Action-Pointer Transformer</title><authors>Jiawei Zhou, Tahira Naseem, Ram\'on Fernandez Astudillo, Radu Florian</authors><categories>cs.CL</categories><comments>Accepted at NAACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract Meaning Representation parsing is a sentence-to-graph prediction
task where target nodes are not explicitly aligned to sentence tokens. However,
since graph nodes are semantically based on one or more sentence tokens,
implicit alignments can be derived. Transition-based parsers operate over the
sentence from left to right, capturing this inductive bias via alignments at
the cost of limited expressiveness. In this work, we propose a transition-based
system that combines hard-attention over sentences with a target-side action
pointer mechanism to decouple source tokens from node representations and
address alignments. We model the transitions as well as the pointer mechanism
through straightforward modifications within a single Transformer architecture.
Parser state and graph structure information are efficiently encoded using
attention heads. We show that our action-pointer approach leads to increased
expressiveness and attains large gains (+1.6 points) against the best
transition-based AMR parser in very similar conditions. While using no graph
re-categorization, our single model yields the second best Smatch score on AMR
2.0 (81.8), which is further improved to 83.4 with silver data and ensemble
decoding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14937</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14937</id><submitter>Zheng Wang</submitter><version version="v1"><date>Fri, 30 Apr 2021 12:02:03 GMT</date><size>288kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 11:23:17 GMT</date><size>289kb</size><source_type>D</source_type></version><title>Federated Learning with Fair Averaging</title><authors>Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang,
  Rongshan Yu</authors><categories>cs.LG</categories><comments>to be published in IJCAI2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fairness has emerged as a critical problem in federated learning (FL). In
this work, we identify a cause of unfairness in FL -- \emph{conflicting}
gradients with large differences in the magnitudes. To address this issue, we
propose the federated fair averaging (FedFV) algorithm to mitigate potential
conflicts among clients before averaging their gradients. We first use the
cosine similarity to detect gradient conflicts, and then iteratively eliminate
such conflicts by modifying both the direction and the magnitude of the
gradients. We further show the theoretical foundation of FedFV to mitigate the
issue conflicting gradients and converge to Pareto stationary solutions.
Extensive experiments on a suite of federated datasets confirm that FedFV
compares favorably against state-of-the-art methods in terms of fairness,
accuracy and efficiency.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.14951</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.14951</id><submitter>Haoying Li</submitter><version version="v1"><date>Fri, 30 Apr 2021 12:31:25 GMT</date><size>878kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 14:41:12 GMT</date><size>878kb</size><source_type>D</source_type></version><title>SRDiff: Single Image Super-Resolution with Diffusion Probabilistic
  Models</title><authors>Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li,
  Yueting Chen</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single image super-resolution (SISR) aims to reconstruct high-resolution (HR)
images from the given low-resolution (LR) ones, which is an ill-posed problem
because one LR image corresponds to multiple HR images. Recently,
learning-based SISR methods have greatly outperformed traditional ones, while
suffering from over-smoothing, mode collapse or large model footprint issues
for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve
these problems, we propose a novel single image super-resolution diffusion
probabilistic model (SRDiff), which is the first diffusion-based model for
SISR. SRDiff is optimized with a variant of the variational bound on the data
likelihood and can provide diverse and realistic SR predictions by gradually
transforming the Gaussian noise into a super-resolution (SR) image conditioned
on an LR input through a Markov chain. In addition, we introduce residual
prediction to the whole framework to speed up convergence. Our extensive
experiments on facial and general benchmarks (CelebA and DIV2K datasets) show
that 1) SRDiff can generate diverse SR results in rich details with
state-of-the-art performance, given only one LR input; 2) SRDiff is easy to
train with a small footprint; and 3) SRDiff can perform flexible image
manipulation including latent space interpolation and content fusion.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2104.15090</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2104.15090</id><submitter>Sebastian H\&quot;onel</submitter><version version="v1"><date>Fri, 30 Apr 2021 16:16:32 GMT</date><size>941kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 3 May 2021 14:52:43 GMT</date><size>949kb</size><source_type>D</source_type></version><title>Technical Reports Compilation: Detecting the Fire Drill anti-pattern
  using Source Code</title><authors>Sebastian H\&quot;onel</authors><categories>cs.SE stat.CO stat.ML</categories><comments>132 pages</comments><acm-class>D.2; G.3; I.5; I.6</acm-class><doi>10.13140/RG.2.2.35805.33766/1</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Detecting the presence of project management anti-patterns (AP) currently
requires experts on the matter and is an expensive endeavor. Worse, experts may
introduce their individual subjectivity or bias. Using the Fire Drill AP, we
first introduce a novel way to translate descriptions into detectable AP that
are comprised of arbitrary metrics and events such as maintenance activities,
which are mined from the underlying source code, thus making the description
objective as it becomes data-based. Secondly, we demonstrate a novel method to
quantify and score the deviations of real-world projects to data-based AP
descriptions. Using nine real-world projects that exhibit a Fire Drill to some
degree, we show how to further enhance the translated AP. The ground truth in
these projects was extracted from two individual experts and consensus was
found between them. Our evaluation spans three kinds of pattern, where the
first is purely derived from description, the second type is enhanced by data,
and the third kind is derived from data only. The Fire Drill AP as translated
from description only shows weak potential of confidently detecting the
presence of the anti-pattern in a project. Enriching the AP with data from
real-world projects significantly improves the detection. Using patterns
derived from data only leads to almost perfect correlations of the scores with
the ground truth. Some APs share symptoms with the Fire Drill AP, and we
conclude that the presence of similar patterns is most certainly detectable.
Furthermore, any pattern that can be characteristically modelled using the
proposed approach is potentially well detectable.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01542</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01542</id><submitter>Son T. Luu</submitter><version version="v1"><date>Tue, 4 May 2021 14:50:39 GMT</date><size>398kb</size><source_type>D</source_type></version><version version="v2"><date>Wed, 5 May 2021 01:48:26 GMT</date><size>400kb</size><source_type>D</source_type></version><version version="v3"><date>Sat, 15 May 2021 10:43:18 GMT</date><size>398kb</size><source_type>D</source_type></version><title>Conversational Machine Reading Comprehension for Vietnamese Healthcare
  Texts</title><authors>Son T. Luu, Mao Nguyen Bui, Loi Duc Nguyen, Khiem Vinh Tran, Kiet Van
  Nguyen, Ngan Luu-Thuy Nguyen</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Machine reading comprehension (MRC) is a sub-field in natural language
processing which aims to help computers understand unstructured texts and then
answer questions related to them. In practice, conversation is an essential way
to communicate and transfer information. To help machines understand
conversation texts, we present UIT-ViCoQA - a new corpus for conversational
machine reading comprehension in the Vietnamese language. This corpus consists
of 10,000 questions with answers to over 2,000 conversations about health news
articles. Then, we evaluate several baseline approaches for conversational
machine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1
score of 45.27%, which is 30.91 points behind human performance (76.18%),
indicating that there is ample room for improvement.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01601</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01601</id><submitter>Ilya Tolstikhin</submitter><version version="v1"><date>Tue, 4 May 2021 16:17:21 GMT</date><size>3805kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 12:48:26 GMT</date><size>8998kb</size><source_type>D</source_type></version><title>MLP-Mixer: An all-MLP Architecture for Vision</title><authors>Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas
  Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas
  Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey
  Dosovitskiy</authors><categories>cs.CV cs.AI cs.LG</categories><comments>Fixed parameter counts in Table 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) are the go-to model for computer vision.
Recently, attention-based networks, such as the Vision Transformer, have also
become popular. In this paper we show that while convolutions and attention are
both sufficient for good performance, neither of them are necessary. We present
MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).
MLP-Mixer contains two types of layers: one with MLPs applied independently to
image patches (i.e. &quot;mixing&quot; the per-location features), and one with MLPs
applied across patches (i.e. &quot;mixing&quot; spatial information). When trained on
large datasets, or with modern regularization schemes, MLP-Mixer attains
competitive scores on image classification benchmarks, with pre-training and
inference cost comparable to state-of-the-art models. We hope that these
results spark further research beyond the realms of well established CNNs and
Transformers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01637</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01637</id><submitter>Quentin Bertrand</submitter><version version="v1"><date>Tue, 4 May 2021 17:31:28 GMT</date><size>4600kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 13:07:28 GMT</date><size>4070kb</size><source_type>D</source_type></version><title>Implicit differentiation for fast hyperparameter selection in non-smooth
  convex learning</title><authors>Quentin Bertrand, Quentin Klopfenstein, Mathurin Massias, Mathieu
  Blondel, Samuel Vaiter, Alexandre Gramfort, Joseph Salmon</authors><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the optimal hyperparameters of a model can be cast as a bilevel
optimization problem, typically solved using zero-order techniques. In this
work we study first-order methods when the inner optimization problem is convex
but non-smooth. We show that the forward-mode differentiation of proximal
gradient descent and proximal coordinate descent yield sequences of Jacobians
converging toward the exact Jacobian. Using implicit differentiation, we show
it is possible to leverage the non-smoothness of the inner problem to speed up
the computation. Finally, we provide a bound on the error made on the
hypergradient when the inner optimization problem is solved approximately.
Results on regression and classification problems reveal computational benefits
for hyperparameter optimization, especially when multiple hyperparameters are
required.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01928</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01928</id><submitter>Yuxin Fang</submitter><version version="v1"><date>Wed, 5 May 2021 08:38:25 GMT</date><size>4278kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 16:46:21 GMT</date><size>4277kb</size><source_type>D</source_type></version><title>QueryInst: Parallelly Supervised Mask Query for Instance Segmentation</title><authors>Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan,
  Bin Feng, Wenyu Liu</authors><categories>cs.CV</categories><comments>Add results with Swin-L backbone, mask AP = 49.1 &amp; box AP = 56.1 @
  COCO test-dev</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, query based object detection frameworks achieve comparable
performance with previous state-of-the-art object detectors. However, how to
fully leverage such frameworks to perform instance segmentation remains an open
problem. In this paper, we present QueryInst, a query based instance
segmentation method driven by parallel supervision on dynamic mask heads. The
key insight of QueryInst is to leverage the intrinsic one-to-one correspondence
in object queries across different stages, as well as one-to-one correspondence
between mask RoI features and object queries in the same stage. This approach
eliminates the explicit multi-stage mask head connection and the proposal
distribution inconsistency issues inherent in non-query based multi-stage
instance segmentation methods. We conduct extensive experiments on three
challenging benchmarks, i.e., COCO, CityScapes, and YouTube-VIS to evaluate the
effectiveness of QueryInst in instance segmentation and video instance
segmentation (VIS) task. Specifically, using ResNet-101-FPN backbone, QueryInst
obtains 48.1 box AP and 42.8 mask AP on COCO test-dev, which is 2 points higher
than HTC in terms of both box AP and mask AP, while runs 2.4 times faster. For
video instance segmentation, QueryInst achieves the best performance among all
online VIS approaches and strikes a decent speed-accuracy trade-off. Code is
available at \url{https://github.com/hustvl/QueryInst}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01949</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01949</id><submitter>Peter Wallis</submitter><version version="v1"><date>Wed, 5 May 2021 09:37:21 GMT</date><size>46kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 11:35:05 GMT</date><size>46kb</size><source_type>D</source_type></version><title>Mind Reading at Work: Cooperation without common ground</title><authors>Peter Wallis</authors><categories>cs.CL</categories><comments>Working Paper. Contact the author for missing references or
  suggestions (6 pages)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As Stefan Kopp and Nicole Kramer say in their recent paper[Frontiers in
Psychology 12 (2021) 597], despite some very impressive demonstrations over the
last decade or so, we still don't know how how to make a computer have a half
decent conversation with a human. They argue that the capabilities required to
do this include incremental joint co-construction and mentalizing. Although
agreeing whole heartedly with their statement of the problem, this paper argues
for a different approach to the solution based on the &quot;new&quot; AI of situated
action.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.01974</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.01974</id><submitter>Marco Valentino</submitter><version version="v1"><date>Wed, 5 May 2021 10:59:26 GMT</date><size>187kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 10:11:43 GMT</date><size>187kb</size><source_type>D</source_type></version><title>Do Natural Language Explanations Represent Valid Logical Arguments?
  Verifying Entailment in Explainable NLI Gold Standards</title><authors>Marco Valentino, Ian Pratt-Hartmann, Andr\'e Freitas</authors><categories>cs.CL cs.AI</categories><comments>To appear in IWCS 2021 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emerging line of research in Explainable NLP is the creation of datasets
enriched with human-annotated explanations and rationales, used to build and
evaluate models with step-wise inference and explanation generation
capabilities. While human-annotated explanations are used as ground-truth for
the inference, there is a lack of systematic assessment of their consistency
and rigour. In an attempt to provide a critical quality assessment of
Explanation Gold Standards (XGSs) for NLI, we propose a systematic annotation
methodology, named Explanation Entailment Verification (EEV), to quantify the
logical validity of human-annotated explanations. The application of EEV on
three mainstream datasets reveals the surprising conclusion that a majority of
the explanations, while appearing coherent on the surface, represent logically
invalid arguments, ranging from being incomplete to containing clearly
identifiable logical errors. This conclusion confirms that the inferential
properties of explanations are still poorly formalised and understood, and that
additional work on this line of research is necessary to improve the way
Explanation Gold Standards are constructed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02368</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02368</id><submitter>Hao Sun</submitter><version version="v1"><date>Wed, 5 May 2021 23:32:43 GMT</date><size>3741kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 00:41:11 GMT</date><size>2044kb</size><source_type>D</source_type></version><title>Physics-informed Spline Learning for Nonlinear Dynamics Discovery</title><authors>Fangzheng Sun, Yang Liu, Hao Sun</authors><categories>cs.LG cs.AI nlin.CD</categories><journal-ref>The 30th International Joint Conference on Artificial Intelligence
  (IJCAI-2021)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamical systems are typically governed by a set of linear/nonlinear
differential equations. Distilling the analytical form of these equations from
very limited data remains intractable in many disciplines such as physics,
biology, climate science, engineering and social science. To address this
fundamental challenge, we propose a novel Physics-informed Spline Learning
(PiSL) framework to discover parsimonious governing equations for nonlinear
dynamics, based on sparsely sampled noisy data. The key concept is to (1)
leverage splines to interpolate locally the dynamics, perform analytical
differentiation and build the library of candidate terms, (2) employ sparse
representation of the governing equations, and (3) use the physics residual in
turn to inform the spline learning. The synergy between splines and discovered
underlying physics leads to the robust capacity of dealing with high-level data
scarcity and noise. A hybrid sparsity-promoting alternating direction
optimization strategy is developed for systematically pruning the sparse
coefficients that form the structure and explicit expression of the governing
equations. The efficacy and superiority of the proposed method have been
demonstrated by multiple well-known nonlinear dynamical systems, in comparison
with two state-of-the-art methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02412</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02412</id><submitter>Wenqi Zhao</submitter><version version="v1"><date>Thu, 6 May 2021 03:11:54 GMT</date><size>121kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 9 May 2021 17:00:55 GMT</date><size>121kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 08:47:18 GMT</date><size>721kb</size><source_type>D</source_type></version><title>Handwritten Mathematical Expression Recognition with Bidirectionally
  Trained Transformer</title><authors>Wenqi Zhao, Liangcai Gao, Zuoyu Yan, Shuai Peng, Lin Du, Ziyin Zhang</authors><categories>cs.CV</categories><comments>Accept by ICDAR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encoder-decoder models have made great progress on handwritten mathematical
expression recognition recently. However, it is still a challenge for existing
methods to assign attention to image features accurately. Moreover, those
encoder-decoder models usually adopt RNN-based models in their decoder part,
which makes them inefficient in processing long $\LaTeX{}$ sequences. In this
paper, a transformer-based decoder is employed to replace RNN-based ones, which
makes the whole model architecture very concise. Furthermore, a novel training
strategy is introduced to fully exploit the potential of the transformer in
bidirectional language modeling. Compared to several methods that do not use
data augmentation, experiments demonstrate that our model improves the ExpRate
of current state-of-the-art methods on CROHME 2014 by 2.23%. Similarly, on
CROHME 2016 and CROHME 2019, we improve the ExpRate by 1.92% and 2.28%
respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02432</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02432</id><submitter>Taotao Jing</submitter><version version="v1"><date>Thu, 6 May 2021 04:22:29 GMT</date><size>10970kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 22:32:43 GMT</date><size>11153kb</size><source_type>D</source_type></version><title>Towards Novel Target Discovery Through Open-Set Domain Adaptation</title><authors>Taotao Jing, Hongfu Liu, Zhengming Ding</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open-set domain adaptation (OSDA) considers that the target domain contains
samples from novel categories unobserved in external source domain.
Unfortunately, existing OSDA methods always ignore the demand for the
information of unseen categories and simply recognize them as &quot;unknown&quot; set
without further explanation. This motivates us to understand the unknown
categories more specifically by exploring the underlying structures and
recovering their interpretable semantic attributes. In this paper, we propose a
novel framework to accurately identify the seen categories in target domain,
and effectively recover the semantic attributes for unseen categories.
Specifically, structure preserving partial alignment is developed to recognize
the seen categories through domain-invariant feature learning. Attribute
propagation over visual graph is designed to smoothly transit attributes from
seen to unseen categories via visual-semantic mapping. Moreover, two new
cross-main benchmarks are constructed to evaluate the proposed framework in the
novel and practical challenge. Experimental results on open-set recognition and
semantic recovery demonstrate the superiority of the proposed method over other
compared baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.02629</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.02629</id><submitter>Yifan Hou</submitter><version version="v1"><date>Thu, 6 May 2021 13:01:57 GMT</date><size>127kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 13:25:16 GMT</date><size>155kb</size><source_type>D</source_type></version><title>Bird's Eye: Probing for Linguistic Graph Structures with a Simple
  Information-Theoretic Approach</title><authors>Yifan Hou and Mrinmaya Sachan</authors><categories>cs.CL</categories><comments>this paper will appear at ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NLP has a rich history of representing our prior understanding of language in
the form of graphs. Recent work on analyzing contextualized text
representations has focused on hand-designed probe models to understand how and
to what extent do these representations encode a particular linguistic
phenomenon. However, due to the inter-dependence of various phenomena and
randomness of training probe models, detecting how these representations encode
the rich information in these linguistic graphs remains a challenging problem.
In this paper, we propose a new information-theoretic probe, Bird's Eye, which
is a fairly simple probe method for detecting if and how these representations
encode the information in these linguistic graphs. Instead of using classifier
performance, our probe takes an information-theoretic view of probing and
estimates the mutual information between the linguistic graph embedded in a
continuous space and the contextualized word representations. Furthermore, we
also propose an approach to use our probe to investigate localized linguistic
information in the linguistic graphs using perturbation analysis. We call this
probing setup Worm's Eye. Using these probes, we analyze BERT models on their
ability to encode a syntactic and a semantic graph structure, and find that
these models encode to some degree both syntactic as well as semantic
information; albeit syntactic information to a greater extent.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03178</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03178</id><submitter>Gongxu Luo</submitter><version version="v1"><date>Fri, 7 May 2021 11:40:29 GMT</date><size>424kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 11 May 2021 12:29:48 GMT</date><size>690kb</size><source_type>D</source_type></version><version version="v3"><date>Mon, 17 May 2021 06:50:03 GMT</date><size>704kb</size><source_type>D</source_type></version><title>Graph Entropy Guided Node Embedding Dimension Selection for Graph Neural
  Networks</title><authors>Gongxu Luo, Jianxin Li, Hao Peng, Carl Yang, Lichao Sun, Philip S. Yu,
  Lifang He</authors><categories>cs.LG cs.AI</categories><comments>Accept by IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph representation learning has achieved great success in many areas,
including e-commerce, chemistry, biology, etc. However, the fundamental problem
of choosing the appropriate dimension of node embedding for a given graph still
remains unsolved. The commonly used strategies for Node Embedding Dimension
Selection (NEDS) based on grid search or empirical knowledge suffer from heavy
computation and poor model performance. In this paper, we revisit NEDS from the
perspective of minimum entropy principle. Subsequently, we propose a novel
Minimum Graph Entropy (MinGE) algorithm for NEDS with graph data. To be
specific, MinGE considers both feature entropy and structure entropy on graphs,
which are carefully designed according to the characteristics of the rich
information in them. The feature entropy, which assumes the embeddings of
adjacent nodes to be more similar, connects node features and link topology on
graphs. The structure entropy takes the normalized degree as basic unit to
further measure the higher-order structure of graphs. Based on them, we design
MinGE to directly calculate the ideal node embedding dimension for any graph.
Finally, comprehensive experiments with popular Graph Neural Networks (GNNs) on
benchmark datasets demonstrate the effectiveness and generalizability of our
proposed MinGE.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03296</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03296</id><submitter>Thien-Minh Nguyen</submitter><version version="v1"><date>Fri, 7 May 2021 14:35:33 GMT</date><size>6944kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 14:17:15 GMT</date><size>14689kb</size><source_type>D</source_type></version><title>VIRAL SLAM: Tightly Coupled Camera-IMU-UWB-Lidar SLAM</title><authors>Thien-Minh Nguyen, Shenghai Yuan, Muqing Cao, Thien Hoang Nguyen,
  Lihua Xie</authors><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a tightly-coupled, multi-modal simultaneous
localization and mapping (SLAM) framework, integrating an extensive set of
sensors: IMU, cameras, multiple lidars, and Ultra-wideband (UWB) range
measurements, hence referred to as VIRAL (visual-inertial-ranging-lidar) SLAM.
To achieve such a comprehensive sensor fusion system, one has to tackle several
challenges such as data synchronization, multi-threading programming, bundle
adjustment (BA), and conflicting coordinate frames between UWB and the onboard
sensors, so as to ensure real-time localization and smooth updates in the state
estimates.
  To this end, we propose a two stage approach. In the first stage, lidar,
camera, and IMU data on a local sliding window are processed in a core odometry
thread. From this local graph, new key frames are evaluated for admission to a
global map. Visual feature-based loop closure is also performed to supplement
the global factor graph with loop constraints. When the global factor graph
satisfies a condition on spatial diversity, the BA process will be triggered,
which updates the coordinate transform between UWB and onboard SLAM systems.
The system then seamlessly transitions to the second stage where all sensors
are tightly integrated in the odometry thread. The capability of our system is
demonstrated via several experiments on high-fidelity graphical-physical
simulation and public datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03306</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03306</id><submitter>Juncheng Wang</submitter><version version="v1"><date>Fri, 7 May 2021 14:56:43 GMT</date><size>355kb</size></version><version version="v2"><date>Mon, 17 May 2021 05:01:47 GMT</date><size>367kb</size></version><title>Online Multi-Cell Coordinated MIMO Wireless Network Virtualization with
  Imperfect CSI</title><authors>Juncheng Wang, Ben Liang, Min Dong, and Gary Boudreau</authors><categories>cs.IT math.IT</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online coordinated precoding design for downlink wireless network
virtualization (WNV) in a multi-cell multiple-input multiple-output (MIMO)
network with imperfect channel state information (CSI). In our WNV framework,
an infrastructure provider (InP) owns each base station that is shared by
several service providers (SPs) oblivious of each other. The SPs design their
precoders as virtualization demands for user services, while the InP designs
the actual precoding solution to meet the service demands from the SPs. Our aim
is to minimize the long-term time-averaged expected precoding deviation over
MIMO fading channels, subject to both per-cell long-term and short-term
transmit power limits. We propose an online coordinated precoding algorithm for
virtualization, which provides a fully distributed semi-closed-form precoding
solution at each cell, based only on the current imperfect CSI without any CSI
exchange across cells. Taking into account the two-fold impact of imperfect CSI
on both the InP and the SPs, we show that our proposed algorithm is within an
$O(\delta)$ gap from the optimum over any time horizon, where $\delta$ is a CSI
inaccuracy indicator. Simulation results validate the performance of our
proposed algorithm under two commonly used precoding techniques in a typical
urban micro-cell network environment.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03736</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03736</id><submitter>Sourjya Roy</submitter><version version="v1"><date>Sat, 8 May 2021 16:39:24 GMT</date><size>1724kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 19:47:44 GMT</date><size>1869kb</size><source_type>D</source_type></version><title>PIM-DRAM: Accelerating Machine Learning Workloads using Processing in
  Commodity DRAM</title><authors>Sourjya Roy, Mustafa Ali and Anand Raghunathan</authors><categories>cs.LG cs.AI cs.AR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep Neural Networks (DNNs) have transformed the field of machine learning
and are widely deployed in many applications involving image, video, speech and
natural language processing. The increasing compute demands of DNNs have been
widely addressed through Graphics Processing Units (GPUs) and specialized
accelerators. However, as model sizes grow, these von Neumann architectures
require very high memory bandwidth to keep the processing elements utilized as
a majority of the data resides in the main memory. Processing in memory has
been proposed as a promising solution for the memory wall bottleneck for ML
workloads. In this work, we propose a new DRAM-based processing-in-memory (PIM)
multiplication primitive coupled with intra-bank accumulation to accelerate
matrix vector operations in ML workloads. The proposed multiplication primitive
adds &lt; 1% area overhead and does not require any change in the DRAM
peripherals. Therefore, the proposed multiplication can be easily adopted in
commodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture,
data mapping scheme and dataflow for executing DNNs within DRAM. System
evaluations performed on networks like AlexNet, VGG16 and ResNet18 show that
the proposed architecture, mapping, and data flow can provide up to 23x speedup
over an NVIDIA Titan Xp GPU. Furthermore, it achieves upto 6.5x speedup over an
ideal von Neumann architecture with infinite computational throughput,
highlighting the need to overcome the memory bottleneck in future generations
of DNN hardware.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03760</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03760</id><submitter>Mahmoud Z. Khairallah</submitter><version version="v1"><date>Sat, 8 May 2021 18:30:44 GMT</date><size>3775kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 01:27:16 GMT</date><size>3775kb</size><source_type>D</source_type></version><title>PCA Event-Based Optical Flow for Visual Odometry</title><authors>Mahmoud Z. Khairallah, Fabien Bonardi, David Roussel and Samia
  Bouchafa</authors><categories>cs.CV</categories><comments>9 pages, 8 figures, not published yet</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the advent of neuromorphic vision sensors such as event-based cameras, a
paradigm shift is required for most computer vision algorithms. Among these
algorithms, optical flow estimation is a prime candidate for this process
considering that it is linked to a neuromorphic vision approach. Usage of
optical flow is widespread in robotics applications due to its richness and
accuracy. We present a Principal Component Analysis (PCA) approach to the
problem of event-based optical flow estimation. In this approach, we examine
different regularization methods which efficiently enhance the estimation of
the optical flow. We show that the best variant of our proposed method,
dedicated to the real-time context of visual odometry, is about two times
faster compared to state-of-the-art implementations while significantly
improves optical flow accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03834</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03834</id><submitter>Hyung-Jin Yoon</submitter><version version="v1"><date>Sun, 9 May 2021 04:34:10 GMT</date><size>3310kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 19:01:33 GMT</date><size>5335kb</size><source_type>D</source_type></version><title>Learning Image Attacks toward Vision Guided Autonomous Vehicles</title><authors>Hyung-Jin Yoon, Hamidreza Jafarnejadsani, Petros Voulgaris</authors><categories>cs.RO cs.CR cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While adversarial neural networks have been shown successful for static image
attacks, very few approaches have been developed for attacking online image
streams while taking into account the underlying physical dynamics of
autonomous vehicles, their mission, and environment. This paper presents an
online adversarial machine learning framework that can effectively misguide
autonomous vehicles' missions. In the existing image attack methods devised
toward autonomous vehicles, optimization steps are repeated for every image
frame. This framework removes the need for fully converged optimization at
every frame to realize image attacks in real-time. Using reinforcement
learning, a generative neural network is trained over a set of image frames to
obtain an attack policy that is more robust to dynamic and uncertain
environments. A state estimator is introduced for processing image streams to
reduce the attack policy's sensitivity to physical variables such as unknown
position and velocity. A simulation study is provided to validate the results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03857</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03857</id><submitter>Yimin Dou</submitter><version version="v1"><date>Sun, 9 May 2021 07:13:40 GMT</date><size>4706kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 08:36:17 GMT</date><size>4707kb</size><source_type>D</source_type></version><title>Seismic Fault Segmentation via 3D-CNN Training by a Few 2D Slices Labels</title><authors>YiMin Dou, Kewen Li, Jianbing Zhu, Xiao Li, Yingjie Xi</authors><categories>cs.CV physics.geo-ph</categories><comments>22 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection faults in seismic data is a crucial step for seismic structural
interpretation, reservoir characterization and well placement. Some recent
works regard it as an image segmentation task. The task of image segmentation
requires huge labels, especially 3D seismic data, which has a complex structure
and lots of noise. Therefore, its annotation requires expert experience and a
huge workload. In this study, we present {\lambda}-BCE and {\lambda}-smooth
L1loss to effectively train 3D-CNN by some slices from 3D seismic data, so that
the model can learn the segmentation of 3D seismic data from a few 2D slices.
In order to fully extract information from limited data and suppress seismic
noise, we propose an attention module that can be used for active supervision
training and embedded in the network. The attention heatmap target is generated
by the original label, and letting it supervise the attention module using the
{\lambda}-smooth L1loss. The experiment proves the effectiveness of our loss
function and attention module, it also shows that our method can extract 3D
seismic features from a few 2D slices labels, and the segmentation effect
achieves state-of-the-art. We only use 3.3% of the all labels, and we can
achieve similar performance as using all labels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.03943</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.03943</id><submitter>Rishi Hazra</submitter><version version="v1"><date>Sun, 9 May 2021 13:44:55 GMT</date><size>499kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 01:20:15 GMT</date><size>499kb</size><source_type>D</source_type></version><title>gComm: An environment for investigating generalization in Grounded
  Language Acquisition</title><authors>Rishi Hazra and Sonu Dixit</authors><categories>cs.CL cs.AI</categories><comments>Accepted in NAACL 2021 workshop: Visually Grounded Interaction and
  Language (ViGIL)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  gComm is a step towards developing a robust platform to foster research in
grounded language acquisition in a more challenging and realistic setting. It
comprises a 2-d grid environment with a set of agents (a stationary speaker and
a mobile listener connected via a communication channel) exposed to a
continuous array of tasks in a partially observable setting. The key to solving
these tasks lies in agents developing linguistic abilities and utilizing them
for efficiently exploring the environment. The speaker and listener have access
to information provided in different modalities, i.e. the speaker's input is a
natural language instruction that contains the target and task specifications
and the listener's input is its grid-view. Each must rely on the other to
complete the assigned task, however, the only way they can achieve the same, is
to develop and use some form of communication. gComm provides several tools for
studying different forms of communication and assessing their generalization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04201</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04201</id><submitter>Fangkai Jiao</submitter><version version="v1"><date>Mon, 10 May 2021 08:54:46 GMT</date><size>998kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 02:56:09 GMT</date><size>1433kb</size><source_type>D</source_type></version><title>REPT: Bridging Language Models and Machine Reading Comprehension via
  Retrieval-Based Pre-training</title><authors>Fangkai Jiao, Yangyang Guo, Yilin Niu, Feng Ji, Feng-Lin Li, Liqiang
  Nie</authors><categories>cs.CL</categories><comments>14 pages, 3 figures, Findings of ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pre-trained Language Models (PLMs) have achieved great success on Machine
Reading Comprehension (MRC) over the past few years. Although the general
language representation learned from large-scale corpora does benefit MRC, the
poor support in evidence extraction which requires reasoning across multiple
sentences hinders PLMs from further advancing MRC. To bridge the gap between
general PLMs and MRC, we present REPT, a REtrieval-based Pre-Training approach.
In particular, we introduce two self-supervised tasks to strengthen evidence
extraction during pre-training, which is further inherited by downstream MRC
tasks through the consistent retrieval operation and model architecture. To
evaluate our proposed method, we conduct extensive experiments on five MRC
datasets that require collecting evidence from and reasoning across multiple
sentences. Experimental results demonstrate the effectiveness of our
pre-training approach. Moreover, further analysis shows that our approach is
able to enhance the capacity of evidence extraction without explicit
supervision.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04284</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04284</id><submitter>Sartaj Ul Hasan</submitter><version version="v1"><date>Mon, 10 May 2021 11:50:43 GMT</date><size>7kb</size></version><version version="v2"><date>Tue, 18 May 2021 08:24:17 GMT</date><size>7kb</size></version><title>Boomerang uniformity of a class of power maps</title><authors>Sartaj Ul Hasan, Mohit Pal, Pantelimon Stanica</authors><categories>cs.IT math.IT</categories><comments>9 pages</comments><msc-class>12E20, 11T06, 94A60</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the boomerang uniformity of an infinite class of power maps and
show that its boomerang uniformity over the finite field F2n is 2 and 4,when
n=0 (mod 4)and n=2 (mod 4),respectively. As a consequence, we show that for
this class of power maps, the differential uniformity is strictly greater than
its boomerang uniformity, contrary to popular belief.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04308</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04308</id><submitter>Luca Manzoni</submitter><version version="v1"><date>Mon, 10 May 2021 12:40:18 GMT</date><size>63kb</size></version><version version="v2"><date>Tue, 11 May 2021 09:11:18 GMT</date><size>97kb</size></version><version version="v3"><date>Sun, 16 May 2021 19:53:41 GMT</date><size>93kb</size></version><title>Parallel Sandpiles or Spurious Bidirectional Icepiles?</title><authors>Gianpiero Cattaneo and Luca Manzoni</authors><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper E. Formenti and K. Perrot (FP) introduce a global rule
assumed to describe the discrete time dynamics associated with a sandpile model
under the parallel application of a suitable local rule acting on d dimensional
lattices of cells equipped with uniform neighborhood. In this paper we submit
this approach to a critical analysis, in the simplest elementary particular
case of a one-dimensional lattice, which can be divided in two parts. In the
first part we prove that the FP global rule does not describe the dynamics of
standard sandpiles, but rather furnishes a description of the quite different
situation of height difference between consecutive piles. This is a semantic
uncorrect difference of interpretation. In the second part we investigate the
consequences of the uncorrect FP assumption proving that their global rule
describes a bidirectional spurious dynamics of icepiles (rather than
sandpiles), in the sense that this latter is the consequence of application of
three local rules: bidirectional vertical rule, bidirectional horizontal rule
(typical of icepiles), and a granule jump from the bottom to the top (spurious
rule of the dynamics).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04319</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04319</id><submitter>Leon Bungert</submitter><version version="v1"><date>Mon, 10 May 2021 12:56:01 GMT</date><size>1248kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 10:57:49 GMT</date><size>240kb</size><source_type>D</source_type></version><title>A Bregman Learning Framework for Sparse Neural Networks</title><authors>Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger</authors><categories>cs.LG cs.NA math.NA math.OC</categories><comments>Corrected stochastic convergence proof</comments><msc-class>65K10, 68T05, 90C26</msc-class><acm-class>I.2.6; F.2.1; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a learning framework based on stochastic Bregman iterations to
train sparse neural networks with an inverse scale space approach. We derive a
baseline algorithm called LinBreg, an accelerated version using momentum, and
AdaBreg, which is a Bregmanized generalization of the Adam algorithm. In
contrast to established methods for sparse training the proposed family of
algorithms constitutes a regrowth strategy for neural networks that is solely
optimization-based without additional heuristics. Our Bregman learning
framework starts the training with very few initial parameters, successively
adding only significant ones to obtain a sparse and expressive network. The
proposed approach is extremely easy and efficient, yet supported by the rich
mathematical theory of inverse scale space methods. We derive a statistically
profound sparse parameter initialization strategy and provide a rigorous
stochastic convergence analysis of the loss decay and additional convergence
proofs in the convex regime. Using only 3.4% of the parameters of ResNet-18 we
achieve 90.2% test accuracy on CIFAR-10, compared to 93.6% using the dense
network. Our algorithm also unveils an autoencoder architecture for a denoising
task. The proposed framework also has a huge potential for integrating sparse
backpropagation and resource-friendly training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04387</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04387</id><submitter>Jinjie Ni</submitter><version version="v1"><date>Mon, 10 May 2021 14:07:49 GMT</date><size>2469kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 13:45:12 GMT</date><size>2473kb</size><source_type>D</source_type></version><version version="v3"><date>Tue, 18 May 2021 04:23:43 GMT</date><size>2473kb</size><source_type>D</source_type></version><title>Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
  Survey</title><authors>Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, Vinay Adiga, Erik
  Cambria</authors><categories>cs.CL cs.AI cs.IR</categories><comments>75 pages, 19 figures</comments><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dialogue systems are a popular Natural Language Processing (NLP) task as it
is promising in real-life applications. It is also a complicated task since
many NLP tasks deserving study are involved. As a result, a multitude of novel
works on this task are carried out, and most of them are deep learning-based
due to the outstanding performance. In this survey, we mainly focus on the deep
learning-based dialogue systems. We comprehensively review state-of-the-art
research outcomes in dialogue systems and analyze them from two angles: model
type and system type. Specifically, from the angle of model type, we discuss
the principles, characteristics, and applications of different models that are
widely used in dialogue systems. This will help researchers acquaint these
models and see how they are applied in state-of-the-art frameworks, which is
rather helpful when designing a new dialogue system. From the angle of system
type, we discuss task-oriented and open-domain dialogue systems as two streams
of research, providing insight into the hot topics related. Furthermore, we
comprehensively review the evaluation methods and datasets for dialogue systems
to pave the way for future research. Finally, some possible research trends are
identified based on the recent research outcomes. To the best of our knowledge,
this survey is the most comprehensive and up-to-date one at present in the area
of dialogue systems and dialogue-related tasks, extensively covering the
popular frameworks, topics, and datasets.
  Keywords: Dialogue Systems, Chatbots, Conversational AI, Task-oriented, Open
Domain, Chit-chat, Question Answering, Artificial Intelligence, Natural
Language Processing, Information Retrieval, Deep Learning, Neural Networks,
CNN, RNN, Hierarchical Recurrent Encoder-Decoder, Memory Networks, Attention,
Transformer, Pointer Net, CopyNet, Reinforcement Learning, GANs, Knowledge
Graph, Survey, Review
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04475</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04475</id><submitter>Liang Ding</submitter><version version="v1"><date>Mon, 10 May 2021 16:12:14 GMT</date><size>5360kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 10:20:12 GMT</date><size>5381kb</size><source_type>D</source_type></version><title>Self-Guided Curriculum Learning for Neural Machine Translation</title><authors>Lei Zhou, Liang Ding, Kevin Duh, Shinji Watanabe, Ryohei Sasano,
  Koichi Takeda</authors><categories>cs.CL cs.AI</categories><comments>Work in progress</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In the field of machine learning, the well-trained model is assumed to be
able to recover the training labels, i.e. the synthetic labels predicted by the
model should be as close to the ground-truth labels as possible. Inspired by
this, we propose a self-guided curriculum strategy to encourage the learning of
neural machine translation (NMT) models to follow the above recovery criterion,
where we cast the recovery degree of each training example as its learning
difficulty. Specifically, we adopt the sentence level BLEU score as the proxy
of recovery degree. Different from existing curricula relying on linguistic
prior knowledge or third-party language models, our chosen learning difficulty
is more suitable to measure the degree of knowledge mastery of the NMT models.
Experiments on translation benchmarks, including WMT14
English$\Rightarrow$German and WMT17 Chinese$\Rightarrow$English, demonstrate
that our approach can consistently improve translation performance against
strong baseline Transformer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04547</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04547</id><submitter>Chengdong Yao</submitter><version version="v1"><date>Sat, 24 Apr 2021 11:38:05 GMT</date><size>412kb</size></version><version version="v2"><date>Sun, 16 May 2021 05:38:51 GMT</date><size>413kb</size></version><title>Highly Efficient Memory Failure Prediction using Mcelog-based Data
  Mining and Machine Learning</title><authors>Chengdong Yao</authors><categories>cs.DB cs.LG cs.PF cs.SE</categories><comments>11 pages, 2 figures, 1 table. Codes has been open source to
  https://www.github.com/ycd2016/acaioc2</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In the data center, unexpected downtime caused by memory failures can lead to
a decline in the stability of the server and even the entire information
technology infrastructure, which harms the business. Therefore, whether the
memory failure can be accurately predicted in advance has become one of the
most important issues to be studied in the data center. However, for the memory
failure prediction in the production system, it is necessary to solve technical
problems such as huge data noise and extreme imbalance between positive and
negative samples, and at the same time ensure the long-term stability of the
algorithm. This paper compares and summarizes some commonly used skills and the
improvement they can bring. The single model we proposed won the top 14th in
the 2nd Alibaba Cloud AIOps Competition belonging to the 25th PAKDD conference.
It takes only 30 minutes to pass the online test, while most of the other
contestants' solution need more than 3 hours. Codes has been open source to
https://www.github.com/ycd2016/acaioc2.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04652</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04652</id><submitter>Rahul Arya</submitter><version version="v1"><date>Mon, 10 May 2021 20:19:42 GMT</date><size>121kb</size></version><version version="v2"><date>Mon, 17 May 2021 07:42:49 GMT</date><size>121kb</size></version><title>Stabilizability of Vector Systems with Uniform Actuation
  Unpredictability</title><authors>Rahul Arya, Chih-Yuan Chiu and Gireeja Ranade</authors><categories>math.OC cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the fundamental limits of a simple system, inspired by
the intermittent Kalman filtering model, where the actuation direction is drawn
uniformly from the unit hypersphere. The model allows us to focus on a
fundamental tension in the control of underactuated vector systems -- the need
to balance the growth of the system in different dimensions.
  We characterize the stabilizability of $d$-dimensional systems with symmetric
gain matrices by providing tight necessary and sufficient conditions that
depend on the eigenvalues of the system. The proof technique is slightly
different from the standard dynamic programming approach and relies on the fact
that the second moment stability of the system can also be understood by
examining any arbitrary weighted two-norm of the state.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04661</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04661</id><submitter>Michael Rathjen</submitter><version version="v1"><date>Mon, 10 May 2021 20:39:09 GMT</date><size>51kb</size></version><version version="v2"><date>Tue, 18 May 2021 09:37:48 GMT</date><size>24kb</size></version><title>No speedup for geometric theories</title><authors>Michael Rathjen</authors><categories>math.LO cs.CC</categories><msc-class>(Primary) 03F20, 03F50, 03F55 (Secondary) 03B35, 03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric theories based on classical logic are conservative over their
intuitionistic counterparts for geometric implications. The latter result
(sometimes referred to as Barr's theorem) is squarely a consequence of
Gentzen's Hauptsatz. Prima facie though, cut elimination can result in
superexponentially longer proofs. In this paper it is shown that the
transformation of a classical proof of a geometric implication in a geometric
theory into an intuitionistic proof can be achieved in feasibly many steps.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.04950</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.04950</id><submitter>Krishna Narasimhan</submitter><version version="v1"><date>Tue, 11 May 2021 11:40:24 GMT</date><size>403kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 07:38:38 GMT</date><size>425kb</size><source_type>D</source_type></version><title>Dealing with Variability in API Misuse Specification</title><authors>Rodrigo Bonifacio, Stefan Kr\&quot;uger, Krishna Narasimhan, Eric Bodden,
  Mira Mezini</authors><categories>cs.CR cs.SE</categories><comments>28 pages, 16 figures</comments><msc-class>68N19</msc-class><acm-class>D.2.1; D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  APIs are the primary mechanism for developers to gain access to externally
defined services and tools. However, previous research has revealed API misuses
that violate the contract of APIs to be prevalent. Such misuses can have
harmful consequences, especially in the context of cryptographic libraries.
Various API misuse detectors have been proposed to address this issue including
CogniCrypt, one of the most versatile of such detectors and that uses a
language CrySL to specify cryptographic API usage contracts. Nonetheless,
existing approaches to detect API misuse had not been designed for systematic
reuse, ignoring the fact that different versions of a library, different
versions of a platform, and different recommendations or guidelines might
introduce variability in the correct usage of an API. Yet, little is known
about how such variability impacts the specification of the correct API usage.
This paper investigates this question by analyzing the impact of various
sources of variability on widely used Java cryptographic libraries including
JCA, Bouncy Castle, and Google Tink. The results of our investigation show that
sources of variability like new versions of the API and security standards
significantly impact the specifications. We then use the insights gained from
our investigation to motivate an extension to the CrySL language named
MetaCrySL, which builds on meta programming concepts. We evaluate MetaCrySL by
specifying usage rules for a family of Android versions and illustrate that
MetaCrySL can model all forms of variability we identified and drastically
reduce the size of a family of specifications for the correct usage of
cryptographic APIs
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05004</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05004</id><submitter>Jindian Liu</submitter><version version="v1"><date>Tue, 11 May 2021 13:13:50 GMT</date><size>3063kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 11:31:14 GMT</date><size>0kb</size><source_type>I</source_type></version><title>Smart Name Lookup for NDN Forwarding Plane via Neural Networks</title><authors>Zhuo Li, Jindian Liu, Liu Yan, Beichuan Zhang, Peng Luo, Kaihua Liu</authors><categories>cs.NI</categories><comments>We need to refine the paper further including the title and the
  structure of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Name lookup is a key technology for the forwarding plane of content router in
Named Data Networking (NDN). To realize the efficient name lookup, what counts
is deploying a highperformance index in content routers. So far, the proposed
indexes have shown good performance, most of which are optimized for or
evaluated with URLs collected from the current Internet, as the large-scale NDN
names are not available yet. Unfortunately, the performance of these indexes is
always impacted in terms of lookup speed, memory consumption and false positive
probability, as the distributions of URLs retrieved in memory may differ from
those of real NDN names independently generated by content-centric applications
online. Focusing on this gap, a smart mapping model named Pyramid-NN via neural
networks is proposed to build an index called LNI for NDN forwarding plane.
Through learning the distributions of the names retrieved in the static memory,
LNI can not only reduce the memory consumption and the probability of false
positive, but also ensure the performance of real NDN name lookup. Experimental
results show that LNI-based FIB can reduce the memory consumption to 58.258 MB
for 2 million names. Moreover, as it can be deployed on SRAMs, the throughput
is about 177 MSPS, which well meets the current network requirement for fast
packet processing.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05134</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05134</id><submitter>Goran Muric</submitter><version version="v1"><date>Tue, 11 May 2021 15:43:41 GMT</date><size>8206kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 21:05:27 GMT</date><size>8211kb</size><source_type>D</source_type></version><title>COVID-19 Vaccine Hesitancy on Social Media: Building a Public Twitter
  Dataset of Anti-vaccine Content, Vaccine Misinformation and Conspiracies</title><authors>Goran Muric, Yusong Wu, Emilio Ferrara</authors><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  False claims about COVID-19 vaccines can undermine public trust in ongoing
vaccination campaigns, thus posing a threat to global public health.
Misinformation originating from various sources has been spreading online since
the beginning of the COVID-19 pandemic. In this paper, we present a dataset of
Twitter posts that exhibit a strong anti-vaccine stance. The dataset consists
of two parts: a) a streaming keyword-centered data collection with more than
1.8 million tweets, and b) a historical account-level collection with more than
135 million tweets. The former leverages the Twitter streaming API to follow a
set of specific vaccine-related keywords starting from mid-October 2020. The
latter consists of all historical tweets of 70K accounts that were engaged in
the active spreading of anti-vaccine narratives. We present descriptive
analyses showing the volume of activity over time, geographical distributions,
topics, news sources, and inferred account political leaning. This dataset can
be used in studying anti-vaccine misinformation on social media and enable a
better understanding of vaccine hesitancy. In compliance with Twitter's Terms
of Service, our anonymized dataset is publicly available at:
https://github.com/gmuric/avax-tweets-dataset
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05181</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05181</id><submitter>Anthony LaTorre</submitter><version version="v1"><date>Tue, 11 May 2021 16:34:12 GMT</date><size>9kb</size></version><version version="v2"><date>Tue, 18 May 2021 16:29:10 GMT</date><size>9kb</size></version><title>Factoring Multidimensional Data to Create a Sophisticated Bayes
  Classifier</title><authors>Anthony LaTorre</authors><categories>cs.LG physics.data-an stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we derive an explicit formula for calculating the marginal
likelihood of a given factorization of a categorical dataset. Since the
marginal likelihood is proportional to the posterior probability of the
factorization, these likelihoods can be used to order all possible
factorizations and select the &quot;best&quot; way to factor the overall distribution
from which the dataset is drawn. The best factorization can then be used to
construct a Bayes classifier which benefits from factoring out mutually
independent sets of variables.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05564</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05564</id><submitter>Nikolaos Nomikos Dr.</submitter><version version="v1"><date>Wed, 12 May 2021 10:30:56 GMT</date><size>433kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 09:04:15 GMT</date><size>433kb</size><source_type>D</source_type></version><version version="v3"><date>Fri, 14 May 2021 18:57:45 GMT</date><size>433kb</size><source_type>D</source_type></version><title>A Survey on Reinforcement Learning-Aided Caching in Mobile Edge Networks</title><authors>Nikolaos Nomikos, Spyros Zoupanos, Themistoklis Charalambous, Ioannis
  Krikidis, Athina Petropulu</authors><categories>cs.NI cs.IT cs.LG math.IT</categories><comments>26 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Mobile networks are experiencing tremendous increase in data volume and user
density. An efficient technique to alleviate this issue is to bring the data
closer to the users by exploiting the caches of edge network nodes, such as
fixed or mobile access points and even user devices. Meanwhile, the fusion of
machine learning and wireless networks offers a viable way for network
optimization as opposed to traditional optimization approaches which incur high
complexity, or fail to provide optimal solutions. Among the various machine
learning categories, reinforcement learning operates in an online and
autonomous manner without relying on large sets of historical data for
training. In this survey, reinforcement learning-aided mobile edge caching is
presented, aiming at highlighting the achieved network gains over conventional
caching approaches. Taking into account the heterogeneity of sixth generation
(6G) networks in various wireless settings, such as fixed, vehicular and flying
networks, learning-aided edge caching is presented, departing from traditional
architectures. Furthermore, a categorization according to the desirable
performance metric, such as spectral, energy and caching efficiency, average
delay, and backhaul and fronthaul offloading is provided. Finally, several open
issues are discussed, targeting to stimulate further interest in this important
research field.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05574</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05574</id><submitter>Yannic Maus</submitter><version version="v1"><date>Wed, 12 May 2021 10:50:43 GMT</date><size>1082kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 09:47:30 GMT</date><size>1082kb</size><source_type>D</source_type></version><title>Locally Checkable Labelings with Small Messages</title><authors>Alkida Balliu, Keren Censor-Hillel, Yannic Maus, Dennis Olivetti,
  Jukka Suomela</authors><categories>cs.DS cs.DC</categories><comments>fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rich line of work has been addressing the computational complexity of
locally checkable labelings (LCLs), illustrating the landscape of possible
complexities. In this paper, we study the landscape of LCL complexities under
bandwidth restrictions. Our main results are twofold. First, we show that on
trees, the CONGEST complexity of an LCL problem is asymptotically equal to its
complexity in the LOCAL model. An analog statement for general (non-LCL)
problems is known to be false. Second, we show that for general graphs this
equivalence does not hold, by providing an LCL problem for which we show that
it can be solved in $O(\log n)$ rounds in the LOCAL model, but requires
$\tilde{\Omega}(n^{1/2})$ rounds in the CONGEST model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05590</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05590</id><submitter>Jan Staschulat</submitter><version version="v1"><date>Wed, 12 May 2021 11:09:47 GMT</date><size>465kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 10:40:07 GMT</date><size>465kb</size><source_type>D</source_type></version><title>Budget-based real-time Executor for Micro-ROS</title><authors>Jan Staschulat (1), Ralph Lange (1), Dakshina Narahari Dasari (1) ((1)
  Robert Bosch GmbH, Stuttgart, Germany)</authors><categories>cs.RO cs.OS</categories><comments>4 pages, 5 figures, submitted to RTAS conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Robot Operating System (ROS) is a popular robotics middleware framework.
In the last years, it underwent a redesign and reimplementation under the name
ROS~2. It now features QoS-configurable communication and a flexible layered
architecture. Micro-ROS is a variant developed specifically for
resource-constrained microcontrollers (MCU). Such MCUs are commonly used in
robotics for sensors and actuators, for time-critical control functions, and
for safety. While the execution management of ROS 2 has been addressed by an
Executor concept, its lack of real-time capabilities make it unsuitable for
industrial use. Neither defining an execution order nor the assignment of
scheduling parameters to tasks is possible, despite the fact that advanced
real-time scheduling algorithms are well-known and available in modern RTOS's.
For example, the NuttX RTOS supports a variant of the reservation-based
scheduling which is very attractive for industrial applications: It allows to
assign execution time budgets to software components so that a system
integrator can thereby guarantee the real-time requirements of the entire
system. This paper presents for the first time a ROS~2 Executor design which
enables the real-time scheduling capabilities of the operating system. In
particular, we successfully demonstrate the budget-based scheduling of the
NuttX RTOS with a micro-ROS application on an STM32 microcontroller.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05612</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05612</id><submitter>Damien Teney</submitter><version version="v1"><date>Wed, 12 May 2021 12:12:24 GMT</date><size>4059kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 08:49:35 GMT</date><size>4058kb</size><source_type>D</source_type></version><title>Evading the Simplicity Bias: Training a Diverse Set of Models Discovers
  Solutions with Superior OOD Generalization</title><authors>Damien Teney, Ehsan Abbasnejad, Simon Lucey, Anton van den Hengel</authors><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks trained with SGD were recently shown to rely preferentially
on linearly-predictive features and can ignore complex, equally-predictive
ones. This simplicity bias can explain their lack of robustness out of
distribution (OOD). The more complex the task to learn, the more likely it is
that statistical artifacts (i.e. selection biases, spurious correlations) are
simpler than the mechanisms to learn. We demonstrate that the simplicity bias
can be mitigated and OOD generalization improved. We train a set of similar
models to fit the data in different ways using a penalty on the alignment of
their input gradients. We show theoretically and empirically that this induces
the learning of more complex predictive patterns. OOD generalization
fundamentally requires information beyond i.i.d. examples, such as multiple
training environments, counterfactual examples, or other side information. Our
approach shows that we can defer this requirement to an independent model
selection stage. We obtain SOTA results in visual recognition on biased data
and generalization across visual domains. The method - the first to evade the
simplicity bias - highlights the need for a better understanding and control of
inductive biases in deep learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05674</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05674</id><submitter>Ismo Horppu</submitter><version version="v1"><date>Wed, 12 May 2021 14:13:21 GMT</date><size>445kb</size></version><version version="v2"><date>Mon, 17 May 2021 09:40:29 GMT</date><size>445kb</size></version><title>Automatic Classification of Games using Support Vector Machine</title><authors>Ismo Horppu, Antti Nikander, Elif Buyukcan, Jere M\&quot;akiniemi, Amin
  Sorkhei, Frederick Ayala-G\'omez</authors><categories>cs.LG</categories><comments>7 pages, 7 figures, updated contact information of one author</comments><acm-class>I.5.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Game developers benefit from availability of custom game genres when doing
game market analysis. This information can help them to spot opportunities in
market and make them more successful in planning a new game. In this paper we
find good classifier for predicting category of a game. Prediction is based on
description and title of a game. We use 2443 iOS App Store games as data set to
generate a document-term matrix. To reduce the curse of dimensionality we use
Latent Semantic Indexing, which, reduces the term dimension to approximately
1/9. Support Vector Machine supervised learning model is fit to pre-processed
data. Model parameters are optimized using grid search and 20-fold cross
validation. Best model yields to 77% mean accuracy or roughly 70% accuracy with
95% confidence. Developed classifier has been used in-house to assist games
market research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05725</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05725</id><submitter>Manuel Sorge</submitter><version version="v1"><date>Wed, 12 May 2021 15:17:35 GMT</date><size>38kb</size></version><version version="v2"><date>Mon, 17 May 2021 17:12:37 GMT</date><size>46kb</size></version><title>On (Coalitional) Exchange-Stable Matching</title><authors>Jiehua Chen, Adrian Chmurovic, Fabian Jogl, and Manuel Sorge</authors><categories>cs.GT cs.DS cs.MA</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We study (coalitional) exchange stability, which Alcalde [Economic Design,
1995] introduced as an alternative solution concept for matching markets
involving property rights, such as assigning persons to two-bed rooms. Here, a
matching of a given Stable Marriage or Stable Roommates instance is called
coalitional exchange-stable if it does not admit any exchange-blocking
coalition, that is, a subset S of agents in which everyone prefers the partner
of some other agent in S. The matching is exchange-stable if it does not admit
any exchange-blocking pair, that is, an exchange-blocking coalition of size
two.
  We investigate the computational and parameterized complexity of the
Coalitional Exchange-Stable Marriage (resp. Coalitional Exchange Roommates)
problem, which is to decide whether a Stable Marriage (resp. Stable Roommates)
instance admits a coalitional exchange-stable matching. Our findings resolve an
open question and confirm the conjecture of Cechl\'arov\'a and Manlove
[Discrete Applied Mathematics, 2005] that Coalitional Exchange-Stable Marriage
is NP-hard even for complete preferences without ties. We also study
bounded-length preference lists and a local-search variant of deciding whether
a given matching can reach an exchange-stable one after at most k swaps, where
a swap is defined as exchanging the partners of the two agents in an
exchange-blocking pair.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05727</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05727</id><submitter>Jiwei Li</submitter><version version="v1"><date>Wed, 12 May 2021 15:20:01 GMT</date><size>313kb</size><source_type>D</source_type></version><version version="v2"><date>Thu, 13 May 2021 11:32:18 GMT</date><size>313kb</size><source_type>D</source_type></version><version version="v3"><date>Sun, 16 May 2021 07:57:32 GMT</date><size>313kb</size><source_type>D</source_type></version><title>BertGCN: Transductive Text Classification by Combining GCN and BERT</title><authors>Yuxiao Lin, Yuxian Meng, Xiaofei Sun, Qinghong Han, Kun Kuang, Jiwei
  Li and Fei Wu</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this work, we propose BertGCN, a model that combines large scale
pretraining and transductive learning for text classification. BertGCN
constructs a heterogeneous graph over the dataset and represents documents as
nodes using BERT representations. By jointly training the BERT and GCN modules
within BertGCN, the proposed model is able to leverage the advantages of both
worlds: large-scale pretraining which takes the advantage of the massive amount
of raw data and transductive learning which jointly learns representations for
both training data and unlabeled test data by propagating label influence
through graph convolution. Experiments show that BertGCN achieves SOTA
performances on a wide range of text classification datasets. Code is available
at https://github.com/ZeroRin/BertGCN.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05788</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05788</id><submitter>Jagadeesh Harshan</submitter><version version="v1"><date>Wed, 12 May 2021 17:01:31 GMT</date><size>100kb</size></version><version version="v2"><date>Mon, 17 May 2021 10:12:30 GMT</date><size>102kb</size></version><title>XOR-Based Codes for Private Information Retrieval with Private Side
  Information</title><authors>Murali Krishnan K. H. and J. Harshan</authors><categories>cs.IT math.IT</categories><comments>45 pages in single column format. Added two tables in Section IV to
  summarize the code construction under joint privacy proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Private Information Retrieval with Private Side
Information (PIR-PSI), wherein a user wants to retrieve a file from replication
based non-colluding databases by using the prior knowledge of a subset of the
files stored on the databases. The PIR-PSI framework ensures that the privacy
of the demand and the side information are jointly preserved, thereby finding
potential applications when multiple files have to be downloaded spread across
different time-instants. Although the capacity of the PIR-PSI setting is known,
we observe that the underlying capacity achieving code construction uses
Maximum Distance Separable (MDS) codes thereby contributing to high
computational complexity when retrieving the demand. Pointing at this drawback
of MDS-based PIR-PSI codes, we propose XOR-based PIR-PSI codes for a simple yet
non-trivial setting of two non-colluding databases and two side information
files at the user. While our codes offer substantial reduction in complexity
when compared to MDS based codes, the code-rate marginally falls short of the
capacity of the PIR-PSI setting. Nevertheless, we show that our code-rate is
strictly higher than that of XOR-based codes for PIR with no side information,
thereby implying that our codes can be useful when downloading multiple files
in a sequential manner, instead of applying XOR-based PIR codes on each file.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05842</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05842</id><submitter>Raaz Dwivedi</submitter><version version="v1"><date>Wed, 12 May 2021 17:56:42 GMT</date><size>479kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 17:59:23 GMT</date><size>480kb</size><source_type>D</source_type></version><title>Kernel Thinning</title><authors>Raaz Dwivedi, Lester Mackey</authors><categories>stat.ML cs.LG math.ST stat.CO stat.ME stat.TH</categories><comments>55 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce kernel thinning, a new procedure for compressing a distribution
$\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given
a suitable reproducing kernel $\mathbf{k}$ and $\mathcal{O}(n^2)$ time, kernel
thinning compresses an $n$-point approximation to $\mathbb{P}$ into a
$\sqrt{n}$-point approximation with comparable worst-case integration error in
the associated reproducing kernel Hilbert space. With high probability, the
maximum discrepancy in integration error is
$\mathcal{O}_d(n^{-\frac{1}{2}}\sqrt{\log n})$ for compactly supported
$\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} \sqrt{(\log n)^{d+1}\log\log
n})$ for sub-exponential $\mathbb{P}$ on $\mathbb{R}^d$. In contrast, an
equal-sized i.i.d. sample from $\mathbb{P}$ suffers $\Omega(n^{-\frac14})$
integration error. Our sub-exponential guarantees resemble the classical
quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply
to general distributions on $\mathbb{R}^d$ and a wide range of common kernels.
We use our results to derive explicit non-asymptotic maximum mean discrepancy
bounds for Gaussian, Mat\'ern, and B-spline kernels and present two vignettes
illustrating the practical benefits of kernel thinning over i.i.d. sampling and
standard Markov chain Monte Carlo thinning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05849</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05849</id><submitter>Sean Kross</submitter><version version="v1"><date>Thu, 13 May 2021 06:00:28 GMT</date><size>6115kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 16:06:46 GMT</date><size>6114kb</size><source_type>D</source_type></version><title>Orienting, Framing, Bridging, Magic, and Counseling: How Data Scientists
  Navigate the Outer Loop of Client Collaborations in Industry and Academia</title><authors>Sean Kross, Philip J. Guo</authors><categories>cs.HC</categories><comments>28 pages, To Appear in the Proceedings of the ACM (PACM)
  Human-Computer Interaction, CSCW 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data scientists often collaborate with clients to analyze data to meet a
client's needs. What does the end-to-end workflow of a data scientist's
collaboration with clients look like throughout the lifetime of a project? To
investigate this question, we interviewed ten data scientists (5 female, 4
male, 1 non-binary) in diverse roles across industry and academia. We
discovered that they work with clients in a six-stage outer-loop workflow,
which involves 1) laying groundwork by building trust before a project begins,
2) orienting to the constraints of the client's environment, 3) collaboratively
framing the problem, 4) bridging the gap between data science and domain
expertise, 5) the inner loop of technical data analysis work, 6) counseling to
help clients emotionally cope with analysis results. This novel outer-loop
workflow contributes to CSCW by expanding the notion of what collaboration
means in data science beyond the widely-known inner-loop technical workflow
stages of acquiring, cleaning, analyzing, modeling, and visualizing data. We
conclude by discussing the implications of our findings for data science
education, parallels to design work, and unmet needs for tool development.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05874</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05874</id><submitter>Sarthak Pati</submitter><version version="v1"><date>Wed, 12 May 2021 18:00:20 GMT</date><size>674kb</size><source_type>D</source_type></version><version version="v2"><date>Fri, 14 May 2021 00:54:23 GMT</date><size>674kb</size><source_type>D</source_type></version><title>The Federated Tumor Segmentation (FeTS) Challenge</title><authors>Sarthak Pati, Ujjwal Baid, Maximilian Zenk, Brandon Edwards, Micah
  Sheller, G. Anthony Reina, Patrick Foley, Alexey Gruzdev, Jason Martin, Shadi
  Albarqouni, Yong Chen, Russell Taki Shinohara, Annika Reinke, David Zimmerer,
  John B. Freymann, Justin S. Kirby, Christos Davatzikos, Rivka R. Colen,
  Aikaterini Kotrotsou, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Hassan
  Fathallah-Shaykh, Roland Wiest, Andras Jakab, Marc-Andre Weber, Abhishek
  Mahajan, Lena Maier-Hein, Jens Kleesiek, Bjoern Menze, Klaus Maier-Hein,
  Spyridon Bakas</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript describes the first challenge on Federated Learning, namely
the Federated Tumor Segmentation (FeTS) challenge 2021. International
challenges have become the standard for validation of biomedical image analysis
methods. However, the actual performance of participating (even the winning)
algorithms on &quot;real-world&quot; clinical data often remains unclear, as the data
included in challenges are usually acquired in very controlled settings at few
institutions. The seemingly obvious solution of just collecting increasingly
more data from more institutions in such challenges does not scale well due to
privacy and ownership hurdles. Towards alleviating these concerns, we are
proposing the FeTS challenge 2021 to cater towards both the development and the
evaluation of models for the segmentation of intrinsically heterogeneous (in
appearance, shape, and histology) brain tumors, namely gliomas. Specifically,
the FeTS 2021 challenge uses clinically acquired, multi-institutional magnetic
resonance imaging (MRI) scans from the BraTS 2020 challenge, as well as from
various remote independent institutions included in the collaborative network
of a real-world federation (https://www.fets.ai/). The goals of the FeTS
challenge are directly represented by the two included tasks: 1) the
identification of the optimal weight aggregation approach towards the training
of a consensus model that has gained knowledge via federated learning from
multiple geographically distinct institutions, while their data are always
retained within each institution, and 2) the federated evaluation of the
generalizability of brain tumor segmentation models &quot;in the wild&quot;, i.e. on data
from institutional distributions that were not part of the training datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05920</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05920</id><submitter>Yang Gao</submitter><version version="v1"><date>Wed, 12 May 2021 19:32:24 GMT</date><size>2041kb</size></version><version version="v2"><date>Fri, 14 May 2021 21:23:51 GMT</date><size>780kb</size></version><title>Attention-based Neural Beamforming Layers for Multi-channel Speech
  Recognition</title><authors>Bhargav Pulugundla, Yang Gao, Brian King, Gokce Keskin, Harish
  Mallidi, Minhua Wu, Jasha Droppo, Roland Maas</authors><categories>eess.AS cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention-based beamformers have recently been shown to be effective for
multi-channel speech recognition. However, they are less capable at capturing
local information. In this work, we propose a 2D Conv-Attention module which
combines convolution neural networks with attention for beamforming. We apply
self- and cross-attention to explicitly model the correlations within and
between the input channels. The end-to-end 2D Conv-Attention model is compared
with a multi-head self-attention and superdirective-based neural beamformers.
We train and evaluate on an in-house multi-channel dataset. The results show a
relative improvement of 3.8% in WER by the proposed model over the baseline
neural beamformer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.05996</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.05996</id><submitter>Tharindu Ranasinghe Mr</submitter><version version="v1"><date>Wed, 12 May 2021 22:50:16 GMT</date><size>1018kb</size><source_type>D</source_type></version><version version="v2"><date>Sat, 15 May 2021 15:39:26 GMT</date><size>1018kb</size><source_type>D</source_type></version><title>Multilingual Offensive Language Identification for Low-resource
  Languages</title><authors>Tharindu Ranasinghe, Marcos Zampieri</authors><categories>cs.CL cs.AI cs.LG cs.SI</categories><comments>Accepted to ACM Transactions on Asian and Low-Resource Language
  Information Processing (TALLIP). This is an extended version of a paper
  accepted to EMNLP (arXiv:2010.05324). arXiv admin note: substantial text
  overlap with arXiv:2010.05324</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Offensive content is pervasive in social media and a reason for concern to
companies and government organizations. Several studies have been recently
published investigating methods to detect the various forms of such content
(e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of
these studies deal with English partially because most annotated datasets
available contain English data. In this paper, we take advantage of available
English datasets by applying cross-lingual contextual word embeddings and
transfer learning to make predictions in low-resource languages. We project
predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi,
Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in
TRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in
OffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513
F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our
approach compares favourably to the best systems submitted to recent shared
tasks on these three languages. Additionally, we report competitive performance
on Arabic, and Turkish using the training and development sets of OffensEval
2020 shared task. The results for all languages confirm the robustness of
cross-lingual contextual embeddings and transfer learning for this task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06022</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06022</id><submitter>Chenjia Bai</submitter><version version="v1"><date>Thu, 13 May 2021 01:15:44 GMT</date><size>532kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 00:22:00 GMT</date><size>8391kb</size><source_type>D</source_type></version><title>Principled Exploration via Optimistic Bootstrapping and Backward
  Induction</title><authors>Chenjia Bai, Lingxiao Wang, Lei Han, Jianye Hao, Animesh Garg, Peng
  Liu, Zhaoran Wang</authors><categories>cs.LG</categories><comments>ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One principled approach for provably efficient exploration is incorporating
the upper confidence bound (UCB) into the value function as a bonus. However,
UCB is specified to deal with linear and tabular settings and is incompatible
with Deep Reinforcement Learning (DRL). In this paper, we propose a principled
exploration method for DRL through Optimistic Bootstrapping and Backward
Induction (OB2I). OB2I constructs a general-purpose UCB-bonus through
non-parametric bootstrap in DRL. The UCB-bonus estimates the epistemic
uncertainty of state-action pairs for optimistic exploration. We build
theoretical connections between the proposed UCB-bonus and the LSVI-UCB in a
linear setting. We propagate future uncertainty in a time-consistent manner
through episodic backward update, which exploits the theoretical advantage and
empirically improves the sample-efficiency. Our experiments in the MNIST maze
and Atari suite suggest that OB2I outperforms several state-of-the-art
exploration approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06056</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06056</id><submitter>Weifeng Sun</submitter><version version="v1"><date>Thu, 13 May 2021 03:14:01 GMT</date><size>1300kb</size><source_type>D</source_type></version><version version="v2"><date>Sun, 16 May 2021 04:41:49 GMT</date><size>0kb</size><source_type>I</source_type></version><title>VPPS-ART: An Efficient Implementation of Fixed-Size-Candidate-Set
  Adaptive Random Testing using Vantage Point Partitioning Strategy</title><authors>Rubing Huang, Chenhui Cui, Dave Towey, Weifeng Sun, Junlong Lian</authors><categories>cs.SE</categories><comments>we have found a serious technique problem of the proposed method</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As an enhanced version of Random Testing (RT), Adaptive Random Testing (ART)
aims to improve the failure detection effectiveness of RT by distributing the
test cases more evenly in the input domain. Many ART algorithms have been
proposed based on different criteria. Among them, the Fixed-Size-Candidate-Set
ART (FSCS-ART) is one of the most effective and classical algorithms. FSCS-ART
ensures high failure detection effectiveness using selecting the candidate test
case which farthest from the previously executed test cases as the next test
case. Although FSCS-ART has good failure detection effectiveness, it also has
some drawbacks, such as computational overhead problem. In this paper, we
propose an enhanced version of FSCS-ART, namely Vantage Point Partitioning
Strategy based ART (VPPS-ART). VPPS-ART attempts to solve the computational
overhead problem of FSCS-ART using vantage point partitioning strategy and
ensures the failure detection effectiveness of FSCS-ART. VPPS-ART achieves the
partitioning of the input domain space by using a Vantage Point tree (VP-tree)
and finds the nearest executed test cases of a candidate test case in the
partitioned sub-domains, which reduces the time overhead significantly compared
to the entire input domain search computation. Besides, to match the dynamic
insertion process of FSCS-ART, we modify the structure of the traditional
VP-tree to support dynamic data. The simulation results present that VPPS-ART
has a great lower time overhead compared to FSCS-ART, and also guarantees
similar or better failure detection effectiveness than FSCS-ART. The VPPS-ART
also shows strength in comparison with the KDFC-ART algorithms, a series of
enhanced ART algorithms based on KD-tree. Our empirical studies also reveal
that VPPS-ART is more cost-effective compared to FSCS-ART and KDFC-ART.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06201</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06201</id><submitter>Rony Bou Rouphael</submitter><version version="v1"><date>Thu, 13 May 2021 11:41:36 GMT</date><size>34kb</size></version><version version="v2"><date>Fri, 14 May 2021 08:43:58 GMT</date><size>35kb</size></version><version version="v3"><date>Mon, 17 May 2021 11:08:48 GMT</date><size>36kb</size></version><title>Strategic Successive Refinement Coding for Bayesian Persuasion with Two
  Decoders</title><authors>Rony Bou Rouphael and Mael Le Treust</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the multi-user Bayesian persuasion game between one encoder and two
decoders, where the first decoder is better informed than the second decoder.
We consider two perfect links, one to the first decoder only, and the other to
both decoders. We consider that the encoder and both decoders are endowed with
distinct and arbitrary distortion functions. We investigate the strategic
source coding problem in which the encoder commits to an encoding while the
decoders select the sequences of symbols that minimize their respective
distortion functions. We characterize the optimal encoder distortion value by
considering successive refinement coding with respect to a specific probability
distribution which involves two auxiliary random variables, and captures the
incentives constraints of both decoders.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06451</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06451</id><submitter>Rami Ezzine</submitter><version version="v1"><date>Thu, 13 May 2021 17:48:08 GMT</date><size>666kb</size></version><version version="v2"><date>Mon, 17 May 2021 19:57:34 GMT</date><size>660kb</size></version><title>Outage Common Randomness Capacity Characterization of Multiple-Antenna
  Slow Fading Channels</title><authors>Rami Ezzine and Moritz Wiese and Christian Deppe and Holger Boche</authors><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:2102.01197</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of common randomness (CR) generation from discrete
correlated sources aided by one-way communication over single-user
multiple-input multiple-output (MIMO) slow fading channels with additive white
Gaussian noise (AWGN) and arbitrary state distribution. MIMO slow fading
channels are indispensable in many scenarios in modern wireless communication.
We completely solve the MIMO slow fading case by providing first an outage
formulation of its channel capacity that holds for arbitrary state
distribution. For this purpose, we provide an achievable rate for a specific
MIMO compound Gaussian channel. Second, we establish the outage CR capacity
over the MIMO slow fading channel using our result on its outage transmission
capacity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06494</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06494</id><submitter>Carlos Javier Fern\'andez Candel</submitter><version version="v1"><date>Thu, 13 May 2021 18:13:42 GMT</date><size>1368kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 08:42:51 GMT</date><size>1368kb</size><source_type>D</source_type></version><title>A Unified Metamodel for NoSQL and Relational Databases</title><authors>Carlos J. Fern\'andez Candel, Diego Sevilla Ruiz and Jes\'us J.
  Garc\'ia-Molina</authors><categories>cs.DB</categories><comments>31 pages, 18 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Database field is undergoing significant changes. Although relational
systems are still predominant, the interest in NoSQL systems is continuously
increasing. In this scenario, polyglot persistence is envisioned as the
database architecture to be prevalent in the future.
  Multi-model database tools normally use a generic or unified metamodel to
represent schemas of the data model that they support. Such metamodels
facilitate developing utilities, as they can be built on a common
representation. Also, the number of mappings required to migrate databases from
a data model to another is reduced, and integrability is favored.
  In this paper, we present the U-Schema unified metamodel able to represent
logical schemas for the four most popular NoSQL paradigms (columnar, document,
key-value, and graph) as well as relational schemas. We will formally define
the mappings between U-Schema and the data model defined for each paradigm. How
these mappings have been implemented and validated will be discussed, and some
applications of U-Schema will be shown.
  To achieve flexibility to respond to data changes, most of NoSQL systems are
&quot;schema-on-write,&quot; and the declaration of schemas is not required. Such an
absence of schema declaration makes structural variability possible, i.e.,
stored data of the same entity type can have different structure. Moreover,
data relationships supported by each data model are different. We will show how
all these issues have been tackled in our approach.
  Our metamodel goes beyond the existing proposals by distinguishing entity
types and relationship types, representing aggregation and reference
relationships, and including the notion of structural variability. Our
contributions also include developing schema extraction strategies for
schemaless systems of each NoSQL data model, and tackling performance and
scalability in the implementation for each store.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06551</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06551</id><submitter>Nathan Lambert</submitter><version version="v1"><date>Mon, 26 Apr 2021 16:49:04 GMT</date><size>271kb</size><source_type>D</source_type></version><title>Axes for Sociotechnical Inquiry in AI Research</title><authors>Sarah Dean, Thomas Krendl Gilbert, Nathan Lambert and Tom Zick</authors><categories>cs.CY cs.AI</categories><comments>9 pages, 1 figure</comments><doi>10.1109/TTS.2021.3074097</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of artificial intelligence (AI) technologies has far exceeded
the investigation of their relationship with society. Sociotechnical inquiry is
needed to mitigate the harms of new technologies whose potential impacts remain
poorly understood. To date, subfields of AI research develop primarily
individual views on their relationship with sociotechnics, while tools for
external investigation, comparison, and cross-pollination are lacking. In this
paper, we propose four directions for inquiry into new and evolving areas of
technological development: value--what progress and direction does a field
promote, optimization--how the defined system within a problem formulation
relates to broader dynamics, consensus--how agreement is achieved and who is
included in building it, and failure--what methods are pursued when the problem
specification is found wanting. The paper provides a lexicon for sociotechnical
inquiry and illustrates it through the example of consumer drone technology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06564</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06564</id><submitter>Yingbo Li</submitter><version version="v1"><date>Thu, 13 May 2021 21:46:46 GMT</date><size>375kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 00:38:03 GMT</date><size>375kb</size><source_type>D</source_type></version><title>Physical Artificial Intelligence: The Concept Expansion of
  Next-Generation Artificial Intelligence</title><authors>Yingbo Li, Yucong Duan, Anamaria-Beatrice Spulber, Haoyang Che,
  Zakaria Maamar, Zhao Li, Chen Yang, Yu lei</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Artificial Intelligence has been a growth catalyst to our society and is
cosidered across all idustries as a fundamental technology. However, its
development has been limited to the signal processing domain that relies on the
generated and collected data from other sensors. In recent research, concepts
of Digital Artificial Intelligence and Physicial Artifical Intelligence have
emerged and this can be considered a big step in the theoretical development of
Artifical Intelligence. In this paper we explore the concept of Physicial
Artifical Intelligence and propose two subdomains: Integrated Physicial
Artifical Intelligence and Distributed Physicial Artifical Intelligence. The
paper will also examine the trend and governance of Physicial Artifical
Intelligence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06582</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06582</id><submitter>Derek Prijatelj</submitter><version version="v1"><date>Thu, 13 May 2021 23:01:07 GMT</date><size>1304kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 20:14:41 GMT</date><size>1307kb</size><source_type>D</source_type></version><title>Handwriting Recognition with Novelty</title><authors>Derek S. Prijatelj (1), Samuel Grieggs (1), Futoshi Yumoto (2), Eric
  Robertson (2), Walter J. Scheirer (1) ((1) University of Notre Dame, (2) PAR
  Government)</authors><categories>cs.CV</categories><comments>16 pages, 3 Figures, 2 Tables, To be published in ICDAR 2021.
  Camera-ready version 1. Supplementary Material 22 pages, 4 Figures, 18
  Tables. Moved novelty type examples from supp mat to main. Added brief
  explanation of usefulness of formalization. Added comment on joint
  information between transcription and style tasks in CRNN's encoding</comments><acm-class>I.7.5; I.5.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces an agent-centric approach to handle novelty in the
visual recognition domain of handwriting recognition (HWR). An ideal
transcription agent would rival or surpass human perception, being able to
recognize known and new characters in an image, and detect any stylistic
changes that may occur within or across documents. A key confound is the
presence of novelty, which has continued to stymie even the best machine
learning-based algorithms for these tasks. In handwritten documents, novelty
can be a change in writer, character attributes, writing attributes, or overall
document appearance, among other things. Instead of looking at each aspect
independently, we suggest that an integrated agent that can process known
characters and novelties simultaneously is a better strategy. This paper
formalizes the domain of handwriting recognition with novelty, describes a
baseline agent, introduces an evaluation protocol with benchmark data, and
provides experimentation to set the state-of-the-art. Results show feasibility
for the agent-centric approach, but more work is needed to approach
human-levels of reading ability, giving the HWR community a formal basis to
build upon as they solve this challenging problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06631</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06631</id><submitter>Xiaoqiang Wang</submitter><version version="v1"><date>Fri, 14 May 2021 03:49:59 GMT</date><size>473kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 02:33:16 GMT</date><size>473kb</size><source_type>D</source_type></version><title>Ordering-Based Causal Discovery with Reinforcement Learning</title><authors>Xiaoqiang Wang, Yali Du, Shengyu Zhu, Liangjun Ke, Zhitang Chen,
  Jianye Hao and Jun Wang</authors><categories>cs.LG</categories><comments>Accepted to IJCAI'2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06649</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06649</id><submitter>Cangning Fan</submitter><version version="v1"><date>Fri, 14 May 2021 05:21:17 GMT</date><size>824kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 07:23:54 GMT</date><size>825kb</size><source_type>D</source_type></version><title>Importance Weighted Adversarial Discriminative Transfer for Anomaly
  Detection</title><authors>Cangning Fan, Fangyi Zhang, Peng Liu, Xiuyu Sun, Hao Li, Ting Xiao,
  Wei Zhao, Xianglong Tang</authors><categories>cs.LG cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1904.02639,
  arXiv:1803.09210 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous transfer methods for anomaly detection generally assume the
availability of labeled data in source or target domains. However, such an
assumption is not valid in most real applications where large-scale labeled
data are too expensive. Therefore, this paper proposes an importance weighted
adversarial autoencoder-based method to transfer anomaly detection knowledge in
an unsupervised manner, particularly for a rarely studied scenario where a
target domain has no labeled normal/abnormal data while only normal data from a
related source domain exist. Specifically, the method learns to align the
distributions of normal data in both source and target domains, but leave the
distribution of abnormal data in the target domain unchanged. In this way, an
obvious gap can be produced between the distributions of normal and abnormal
data in the target domain, therefore enabling the anomaly detection in the
domain. Extensive experiments on multiple synthetic datasets and the UCSD
benchmark demonstrate the effectiveness of our approach. The code is available
at https://github.com/fancangning/anomaly_detection_transfer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06903</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06903</id><submitter>Weipeng Huang</submitter><version version="v1"><date>Fri, 14 May 2021 15:41:15 GMT</date><size>9705kb</size><source_type>D</source_type></version><version version="v2"><date>Mon, 17 May 2021 17:03:57 GMT</date><size>9699kb</size><source_type>D</source_type></version><title>Posterior Regularisation on Bayesian Hierarchical Mixture Clustering</title><authors>Weipeng Huang, Tin Lok James Ng, Nishma Laitonjam, Neil J. Hurley</authors><categories>stat.ML cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study a recent inferential framework, named posterior regularisation, on
the Bayesian hierarchical mixture clustering (BHMC) model. This framework
facilitates a simple way to impose extra constraints on a Bayesian model to
overcome some weakness of the original model. It narrows the search space of
the parameters of the Bayesian model through a formalism that imposes certain
constraints on the features of the found solutions. In this paper, in order to
enhance the separation of clusters, we apply posterior regularisation to impose
max-margin constraints on the nodes at every level of the hierarchy. This paper
shows how the framework integrates with BHMC and achieves the expected
improvements over the original Bayesian model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06932</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06932</id><submitter>Rudy Raymond</submitter><version version="v1"><date>Fri, 14 May 2021 16:23:08 GMT</date><size>47kb</size></version><version version="v2"><date>Mon, 17 May 2021 01:17:09 GMT</date><size>47kb</size><source_type>D</source_type></version><title>Decision Diagrams for Quantum Measurements with Shallow Circuits</title><authors>Stefan Hillmich and Charles Hadfield and Rudy Raymond and Antonio
  Mezzacapo and Robert Wille</authors><categories>quant-ph cs.DS</categories><comments>Omitting labels of vertices in the figures due to the differences of
  outputs by pdflatex from LuaLaTeX. No changes in contents. 19 pages and 7
  figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider the problem of estimating quantum observables on a collection of
qubits, given as a linear combination of Pauli operators, with shallow quantum
circuits consisting of single-qubit rotations. We introduce estimators based on
randomised measurements, which use decision diagrams to sample from probability
distributions on measurement bases. This approach generalises previously known
uniform and locally-biased randomised estimators. The decision diagrams are
constructed given target quantum operators and can be optimised considering
different strategies. We show numerically that the estimators introduced here
can produce more precise estimates on some quantum chemistry Hamiltonians,
compared to previously known randomised protocols and Pauli grouping methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.06998</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.06998</id><submitter>Elisa Ferrari</submitter><version version="v1"><date>Fri, 14 May 2021 15:58:18 GMT</date><size>1972kb</size><source_type>D</source_type></version><title>A causal learning framework for the analysis and interpretation of
  COVID-19 clinical data</title><authors>Elisa Ferrari, Luna Gargani, Greta Barbieri, Lorenzo Ghiadoni,
  Francesco Faita, Davide Bacciu</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We present a workflow for clinical data analysis that relies on Bayesian
Structure Learning (BSL), an unsupervised learning approach, robust to noise
and biases, that allows to incorporate prior medical knowledge into the
learning process and that provides explainable results in the form of a graph
showing the causal connections among the analyzed features. The workflow
consists in a multi-step approach that goes from identifying the main causes of
patient's outcome through BSL, to the realization of a tool suitable for
clinical practice, based on a Binary Decision Tree (BDT), to recognize patients
at high-risk with information available already at hospital admission time. We
evaluate our approach on a feature-rich COVID-19 dataset, showing that the
proposed framework provides a schematic overview of the multi-factorial
processes that jointly contribute to the outcome. We discuss how these
computational findings are confirmed by current understanding of the COVID-19
pathogenesis. Further, our approach yields to a highly interpretable tool
correctly predicting the outcome of 85% of subjects based exclusively on 3
features: age, a previous history of chronic obstructive pulmonary disease and
the PaO2/FiO2 ratio at the time of arrival to the hospital. The inclusion of
additional information from 4 routine blood tests (Creatinine, Glucose, pO2 and
Sodium) increases predictive accuracy to 94.5%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07006</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07006</id><submitter>Leon Kellerhals</submitter><version version="v1"><date>Fri, 14 May 2021 18:00:02 GMT</date><size>115kb</size><source_type>D</source_type></version><title>Optimal Virtual Network Embeddings for Tree Topologies</title><authors>Aleksander Figiel, Leon Kellerhals, Rolf Niedermeier, Matthias Rost,
  Stefan Schmid and Philipp Zschoche</authors><categories>cs.DS cs.NI</categories><comments>An extended abstract of this work appears in the Proceedings of the
  33rd ACM Symposium on Parallelism in Algorithms and Architectures (SPAA '21)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of distributed and data-centric applications often critically
depends on the interconnecting network. Applications are hence modeled as
virtual networks, also accounting for resource demands on links. At the heart
of provisioning such virtual networks lies the NP-hard Virtual Network
Embedding Problem (VNEP): how to jointly map the virtual nodes and links onto a
physical substrate network at minimum cost while obeying capacities.
  This paper studies the VNEP in the light of parameterized complexity. We
focus on tree topology substrates, a case often encountered in practice and for
which the VNEP remains NP-hard. We provide the first fixed-parameter algorithm
for the VNEP with running time $O(3^r (s+r^2))$ for requests and substrates of
$r$ and $s$ nodes, respectively. In a computational study our algorithm yields
running time improvements in excess of 200x compared to state-of-the-art
integer programming approaches. This makes it comparable in speed to the
well-established ViNE heuristic while providing optimal solutions. We
complement our algorithmic study with hardness results for the VNEP and related
problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07014</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07014</id><submitter>Rico Jonschkowski</submitter><version version="v1"><date>Fri, 14 May 2021 18:02:50 GMT</date><size>5636kb</size><source_type>D</source_type></version><title>SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image
  Warping</title><authors>Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, Rico
  Jonschkowski</authors><categories>cs.CV</categories><comments>Accepted at CVPR 2021, all code available at
  https://github.com/google-research/google-research/tree/master/smurf</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present SMURF, a method for unsupervised learning of optical flow that
improves state of the art on all benchmarks by $36\%$ to $40\%$ (over the prior
best method UFlow) and even outperforms several supervised approaches such as
PWC-Net and FlowNet2. Our method integrates architecture improvements from
supervised optical flow, i.e. the RAFT model, with new ideas for unsupervised
learning that include a sequence-aware self-supervision loss, a technique for
handling out-of-frame motion, and an approach for learning effectively from
multi-frame video data while still only requiring two frames for inference.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07017</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07017</id><submitter>Thomas Bendokat</submitter><version version="v1"><date>Fri, 14 May 2021 18:07:09 GMT</date><size>49kb</size></version><title>Efficient Quasi-Geodesics on the Stiefel Manifold</title><authors>Thomas Bendokat and Ralf Zimmermann</authors><categories>math.NA cs.NA math.DG</categories><comments>8 pages, 1 figure, conference GSI2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Solving the so-called geodesic endpoint problem, i.e., finding a geodesic
that connects two given points on a manifold, is at the basis of virtually all
data processing operations, including averaging, clustering, interpolation and
optimization. On the Stiefel manifold of orthonormal frames, this problem is
computationally involved. A remedy is to use quasi-geodesics as a replacement
for the Riemannian geodesics. Quasi-geodesics feature constant speed and
covariant acceleration with constant (but possibly non-zero) norm. For a
well-known type of quasi-geodesics, we derive a new representation that is
suited for large-scale computations. Moreover, we introduce a new kind of
quasi-geodesics that turns out to be much closer to the Riemannian geodesics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07019</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07019</id><submitter>Riya Shah</submitter><version version="v1"><date>Fri, 14 May 2021 18:14:53 GMT</date><size>288kb</size></version><title>Chord Recognition- Music and Audio Information Retrieval</title><authors>Shah Riya Chiragkumar</authors><categories>cs.SD cs.IR eess.AS</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Music Information Retrieval (MIR) is a collaborative scientific study that
help to build innovative information research themes, novel frameworks, and
developing connected delivery mechanisms in addition to making the world's
massive collection of music open for everyone. Modern rock music proved to be
difficult to estimate tempo and chord recognition did not work. All of the
findings indicate that modern rock and metal music can be analysed, despite its
complexity, but that further research is needed in this area to make it useful.
Using a neural network has been one of the simplest ways of dealing with it.
The pitch class profile vector is used in the neural network method. Because
the vector only contains 12 elements of semi-tone values, it is enough for
chord recognition. Of course, there are other ways of achieving this work, most
of them depend on pitch class profiling to transform the chord into a type that
can be recognised, but the recognition process is time-consuming centred on
extremely complicated and memory-intensive methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07020</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07020</id><submitter>Geoff Boeing</submitter><version version="v1"><date>Fri, 14 May 2021 18:20:50 GMT</date><size>131kb</size></version><title>Urban Analytics: History, Trajectory, and Critique</title><authors>Geoff Boeing, Michael Batty, Shan Jiang, Lisa Schweitzer</authors><categories>cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban analytics combines spatial analysis, statistics, computer science, and
urban planning to understand and shape city futures. While it promises better
policymaking insights, concerns exist around its epistemological scope and
impacts on privacy, ethics, and social control. This chapter reflects on the
history and trajectory of urban analytics as a scholarly and professional
discipline. In particular, it considers the direction in which this field is
going and whether it improves our collective and individual welfare. It first
introduces early theories, models, and deductive methods from which the field
originated before shifting toward induction. It then explores urban network
analytics that enrich traditional representations of spatial interaction and
structure. Next it discusses urban applications of spatiotemporal big data and
machine learning. Finally, it argues that privacy and ethical concerns are too
often ignored as ubiquitous monitoring and analytics can empower social
repression. It concludes with a call for a more critical urban analytics that
recognizes its epistemological limits, emphasizes human dignity, and learns
from and supports marginalized communities.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07024</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07024</id><submitter>Kelsey Maass</submitter><version version="v1"><date>Fri, 14 May 2021 18:37:00 GMT</date><size>2934kb</size><source_type>D</source_type></version><title>A feasibility study of a hyperparameter tuning approach to automated
  inverse planning in radiotherapy</title><authors>Kelsey Maass and Aleksandr Aravkin and Minsun Kim</authors><categories>physics.med-ph cs.LG</categories><comments>27 pages, 14 figures</comments><msc-class>65K10, 90C26, 97M60</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Radiotherapy inverse planning requires treatment planners to modify multiple
parameters in the objective function to produce clinically acceptable plans.
Due to manual steps in this process, plan quality can vary widely depending on
planning time available and planner's skills. The purpose of this study is to
automate the inverse planning process to reduce active planning time while
maintaining plan quality. We propose a hyperparameter tuning approach for
automated inverse planning, where a treatment plan utility is maximized with
respect to the limit dose parameters and weights of each organ-at-risk (OAR)
objective. Using 6 patient cases, we investigated the impact of the choice of
dose parameters, random and Bayesian search methods, and utility function form
on planning time and plan quality. For given parameters, the plan was optimized
in RayStation, using the scripting interface to obtain the dose distributions
deliverable. We normalized all plans to have the same target coverage and
compared the OAR dose metrics in the automatically generated plans with those
in the manually generated clinical plans. Using 100 samples was found to
produce satisfactory plan quality, and the average planning time was 2.3 hours.
The OAR doses in the automatically generated plans were lower than the clinical
plans by up to 76.8%. When the OAR doses were larger than the clinical plans,
they were still between 0.57% above and 98.9% below the limit doses, indicating
they are clinically acceptable. For a challenging case, a dimensionality
reduction strategy produced a 92.9% higher utility using only 38.5% of the time
needed to optimize over the original problem. This study demonstrates our
hyperparameter tuning framework for automated inverse planning can
significantly reduce the treatment planner's planning time with plan quality
that is similar to or better than manually generated plans.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07025</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07025</id><submitter>Lu Li</submitter><version version="v1"><date>Fri, 14 May 2021 18:38:48 GMT</date><size>4469kb</size><source_type>D</source_type></version><title>Minimal Cycle Representatives in Persistent Homology using Linear
  Programming: an Empirical Study with User's Guide</title><authors>Lu Li, Connor Thompson, Gregory Henselman-Petrusek, Chad Giusti, Lori
  Ziegelmeier</authors><categories>math.AT cs.CG stat.ML</categories><doi>10.3389/frai.2021.681117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cycle representatives of persistent homology classes can be used to provide
descriptions of topological features in data. However, the non-uniqueness of
these representatives creates ambiguity and can lead to many different
interpretations of the same set of classes. One approach to solving this
problem is to optimize the choice of representative against some measure that
is meaningful in the context of the data. In this work, we provide a study of
the effectiveness and computational cost of several $\ell_1$-minimization
optimization procedures for constructing homological cycle bases for persistent
homology with rational coefficients in dimension one, including
uniform-weighted and length-weighted edge-loss algorithms as well as
uniform-weighted and area-weighted triangle-loss algorithms. We conduct these
optimizations via standard linear programming methods, applying general-purpose
solvers to optimize over column bases of simplicial boundary matrices.
  Our key findings are: (i) optimization is effective in reducing the size of
cycle representatives, (ii) the computational cost of optimizing a basis of
cycle representatives exceeds the cost of computing such a basis in most data
sets we consider, (iii) the choice of linear solvers matters a lot to the
computation time of optimizing cycles, (iv) the computation time of solving an
integer program is not significantly longer than the computation time of
solving a linear program for most of the cycle representatives, using the
Gurobi linear solver, (v) strikingly, whether requiring integer solutions or
not, we almost always obtain a solution with the same cost and almost all
solutions found have entries in {-1, 0, 1} and therefore, are also solutions to
a restricted $\ell_0$ optimization problem, and (vi) we obtain qualitatively
different results for generators in Erd\H{o}s-R\'enyi random clique complexes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07026</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07026</id><submitter>Amin Asadi</submitter><version version="v1"><date>Fri, 14 May 2021 18:39:32 GMT</date><size>3536kb</size><source_type>D</source_type></version><title>A Monotone Approximate Dynamic Programming Approach for the Stochastic
  Scheduling, Allocation, and Inventory Replenishment Problem: Applications to
  Drone and Electric Vehicle Battery Swap Stations</title><authors>Amin Asadi, Sarah Nurre Pinkley</authors><categories>math.OC cs.AI cs.LG math.PR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  There is a growing interest in using electric vehicles (EVs) and drones for
many applications. However, battery-oriented issues, including range anxiety
and battery degradation, impede adoption. Battery swap stations are one
alternative to reduce these concerns that allow the swap of depleted for full
batteries in minutes. We consider the problem of deriving actions at a battery
swap station when explicitly considering the uncertain arrival of swap demand,
battery degradation, and replacement. We model the operations at a battery swap
station using a finite horizon Markov Decision Process model for the stochastic
scheduling, allocation, and inventory replenishment problem (SAIRP), which
determines when and how many batteries are charged, discharged, and replaced
over time. We present theoretical proofs for the monotonicity of the value
function and monotone structure of an optimal policy for special SAIRP cases.
Due to the curses of dimensionality, we develop a new monotone approximate
dynamic programming (ADP) method, which intelligently initializes a value
function approximation using regression. In computational tests, we demonstrate
the superior performance of the new regression-based monotone ADP method as
compared to exact methods and other monotone ADP methods. Further, with the
tests, we deduce policy insights for drone swap stations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07028</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07028</id><submitter>Michael Crusoe</submitter><version version="v1"><date>Fri, 14 May 2021 18:44:48 GMT</date><size>763kb</size><source_type>D</source_type></version><title>Methods Included: Standardizing Computational Reuse and Portability with
  the Common Workflow Language</title><authors>Michael R. Crusoe, Sanne Abeln, Alexandru Iosup, Peter Amstutz, John
  Chilton, Neboj\v{s}a Tijani\'c, Herv\'e M\'enager, Stian Soiland-Reyes,
  Carole Goble (for the CWL Community)</authors><categories>cs.DC</categories><comments>8 pages, 3 figures. For the LaTex source code of this paper, see
  https://github.com/mr-c/cwl_methods_included</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A widely used standard for portable multilingual data analysis pipelines
would enable considerable benefits to scholarly publication reuse,
research/industry collaboration, regulatory cost control, and to the
environment. Published research that used multiple computer languages for their
analysis pipelines would include a complete and reusable description of that
analysis that is runnable on a diverse set of computing environments.
Researchers would be able to easier collaborate and reuse these pipelines,
adding or exchanging components regardless of programming language used;
collaborations with and within the industry would be easier; approval of new
medical interventions that rely on such pipelines would be faster. Time will be
saved and environmental impact would also be reduced, as these descriptions
contain enough information for advanced optimization without user intervention.
Workflows are widely used in data analysis pipelines, enabling innovation and
decision-making for the modern society. In many domains the analysis components
are numerous and written in multiple different computer languages by third
parties. However, lacking a standard for reusable and portable multilingual
workflows, then reusing published multilingual workflows, collaborating on open
problems, and optimizing their execution would be severely hampered. Moreover,
only a standard for multilingual data analysis pipelines that was widely used
would enable considerable benefits to research-industry collaboration,
regulatory cost control, and to preserving the environment. Prior to the start
of the CWL project, there was no standard for describing multilingual analysis
pipelines in a portable and reusable manner. Even today / currently, although
there exist hundreds of single-vendor and other single-source systems that run
workflows, none is a general, community-driven, and consensus-built standard.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07029</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07029</id><submitter>Eleni Triantafillou</submitter><version version="v1"><date>Fri, 14 May 2021 18:46:06 GMT</date><size>1756kb</size><source_type>D</source_type></version><title>Learning a Universal Template for Few-shot Dataset Generalization</title><authors>Eleni Triantafillou, Hugo Larochelle, Richard Zemel and Vincent
  Dumoulin</authors><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Few-shot dataset generalization is a challenging variant of the well-studied
few-shot classification problem where a diverse training set of several
datasets is given, for the purpose of training an adaptable model that can then
learn classes from new datasets using only a few examples. To this end, we
propose to utilize the diverse training set to construct a universal template:
a partial model that can define a wide array of dataset-specialized models, by
plugging in appropriate components. For each new few-shot classification
problem, our approach therefore only requires inferring a small number of
parameters to insert into the universal template. We design a separate network
that produces an initialization of those parameters for each given task, and we
then fine-tune its proposed initialization via a few steps of gradient descent.
Our approach is more parameter-efficient, scalable and adaptable compared to
previous methods, and achieves the state-of-the-art on the challenging
Meta-Dataset benchmark.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07031</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07031</id><submitter>Shawn Hershey</submitter><version version="v1"><date>Fri, 14 May 2021 18:48:20 GMT</date><size>2637kb</size><source_type>D</source_type></version><title>The Benefit Of Temporally-Strong Labels In Audio Event Classification</title><authors>Shawn Hershey, Daniel P W Ellis, Eduardo Fonseca, Aren Jansen,
  Caroline Liu, R Channing Moore, Manoj Plakal</authors><categories>cs.SD eess.AS</categories><comments>Accepted for publication at ICASSP 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To reveal the importance of temporal precision in ground truth audio event
labels, we collected precise (~0.1 sec resolution) &quot;strong&quot; labels for a
portion of the AudioSet dataset. We devised a temporally strong evaluation set
(including explicit negatives of varying difficulty) and a small strong-labeled
training subset of 67k clips (compared to the original dataset's 1.8M clips
labeled at 10 sec resolution). We show that fine-tuning with a mix of weak and
strongly labeled data can substantially improve classifier performance, even
when evaluated using only the original weak labels. For a ResNet50
architecture, d' on the strong evaluation data including explicit negatives
improves from 1.13 to 1.41. The new labels are available as an update to
AudioSet.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07032</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07032</id><submitter>Amit Verma Dr.</submitter><version version="v1"><date>Fri, 14 May 2021 18:49:07 GMT</date><size>154kb</size><source_type>D</source_type></version><title>Variable Reduction For Quadratic Unconstrained Binary Optimization</title><authors>Amit Verma and Mark Lewis</authors><categories>math.OC cs.DM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Quadratic Unconstrained Binary Optimization models are useful for solving a
diverse range of optimization problems. Constraints can be added by
incorporating quadratic penalty terms into the objective, often with the
introduction of slack variables needed for conversion of inequalities. This
transformation can lead to a significant increase in the size and density of
the problem. Herein, we propose an efficient approach for recasting inequality
constraints that reduces the number of linear and quadratic variables.
Experimental results illustrate the efficacy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07033</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07033</id><submitter>Mohammad Nokhbeh Zaeem</submitter><version version="v1"><date>Fri, 14 May 2021 18:54:17 GMT</date><size>10290kb</size><source_type>D</source_type></version><title>Cause and Effect: Concept-based Explanation of Neural Networks</title><authors>Mohammad Nokhbeh Zaeem and Majid Komeili</authors><categories>cs.LG</categories><comments>14 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many scenarios, human decisions are explained based on some high-level
concepts. In this work, we take a step in the interpretability of neural
networks by examining their internal representation or neuron's activations
against concepts. A concept is characterized by a set of samples that have
specific features in common. We propose a framework to check the existence of a
causal relationship between a concept (or its negation) and task classes. While
the previous methods focus on the importance of a concept to a task class, we
go further and introduce four measures to quantitatively determine the order of
causality. Through experiments, we demonstrate the effectiveness of the
proposed method in explaining the relationship between a concept and the
predictive behaviour of a neural network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07037</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07037</id><submitter>Giulia Cisotto</submitter><version version="v1"><date>Fri, 14 May 2021 18:58:44 GMT</date><size>410kb</size><source_type>D</source_type></version><title>Information Theoretic Key Agreement Protocol based on ECG signals</title><authors>Anna V. Guglielmi, Alberto Muraro, Giulia Cisotto, Nicola Laurenti</authors><categories>cs.CR cs.CY eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless body area networks (WBANs) are becoming increasingly popular as they
allow individuals to continuously monitor their vitals and physiological
parameters remotely from the hospital. With the spread of the SARS-CoV-2
pandemic, the availability of portable pulse-oximeters and wearable heart rate
detectors has boomed in the market. At the same time, in 2020 we assisted to an
unprecedented increase of healthcare breaches, revealing the extreme
vulnerability of the current generation of WBANs. Therefore, the development of
new security protocols to ensure data protection, authentication, integrity and
privacy within WBANs are highly needed. Here, we targeted a WBAN collecting ECG
signals from different sensor nodes on the individual's body, we extracted the
inter-pulse interval (i.e., R-R interval) sequence from each of them, and we
developed a new information theoretic key agreement protocol that exploits the
inherent randomness of ECG to ensure authentication between sensor pairs within
the WBAN. After proper pre-processing, we provide an analytical solution that
ensures robust authentication; we provide a unique information reconciliation
matrix, which gives good performance for all ECG sensor pairs; and we can show
that a relationship between information reconciliation and privacy
amplification matrices can be found. Finally, we show the trade-off between the
level of security, in terms of key generation rate, and the complexity of the
error correction scheme implemented in the system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07043</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07043</id><submitter>Bob de Ruiter</submitter><version version="v1"><date>Fri, 14 May 2021 19:30:48 GMT</date><size>1386kb</size></version><title>Post-processing Multi-Model Medium-Term Precipitation Forecasts Using
  Convolutional Neural Networks</title><authors>Bob de Ruiter</authors><categories>cs.LG physics.ao-ph</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The goal of this study was to improve the post-processing of precipitation
forecasts using convolutional neural networks (CNNs). Instead of
post-processing forecasts on a per-pixel basis, as is usually done when
employing machine learning in meteorological post-processing, input forecast
images were combined and transformed into probabilistic output forecast images
using fully convolutional neural networks. CNNs did not outperform regularized
logistic regression. Additionally, an ablation analysis was performed.
Combining input forecasts from a global low-resolution weather model and a
regional high-resolution weather model improved performance over either one.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07044</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07044</id><submitter>Hajar Emami Gohari</submitter><version version="v1"><date>Fri, 14 May 2021 19:34:23 GMT</date><size>1955kb</size><source_type>D</source_type></version><title>SA-GAN: Structure-Aware Generative Adversarial Network for
  Shape-Preserving Synthetic CT Generation</title><authors>Hajar Emami, Ming Dong, Siamak Nejad-Davarani, and Carri Glide-Hurst</authors><categories>eess.IV cs.CV</categories><comments>Accepted to MICCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In medical image synthesis, model training could be challenging due to the
inconsistencies between images of different modalities even with the same
patient, typically caused by internal status/tissue changes as different
modalities are usually obtained at a different time. This paper proposes a
novel deep learning method, Structure-aware Generative Adversarial Network
(SA-GAN), that preserves the shapes and locations of in-consistent structures
when generating medical images. SA-GAN is employed to generate synthetic
computed tomography (synCT) images from magnetic resonance imaging (MRI) with
two parallel streams: the global stream translates the input from the MRI to
the CT domain while the local stream automatically segments the inconsistent
organs, maintains their locations and shapes in MRI, and translates the organ
intensities to CT. Through extensive experiments on a pelvic dataset, we
demonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs
and organ segmentation and supports MR-only treatment planning in disease sites
with internal organ status changes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07045</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07045</id><submitter>Lukas Burgholzer</submitter><version version="v1"><date>Fri, 14 May 2021 19:38:56 GMT</date><size>55kb</size><source_type>D</source_type></version><title>Hybrid Schr\&quot;odinger-Feynman Simulation of Quantum Circuits With
  Decision Diagrams</title><authors>Lukas Burgholzer, Hartwig Bauer and Robert Wille</authors><categories>quant-ph cs.ET</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical simulations of quantum computations are vital for the future
development of this emerging technology. To this end, decision diagrams have
been proposed as a complementary technique which frequently allows to tackle
the inherent exponential complexity of these simulations. In the worst case,
however, they still cannot escape this complexity. Additionally, while other
techniques make use of all the available processing power, decision
diagram-based simulation to date cannot exploit the many processing units of
today's systems. In this work, we show that both problems can be tackled
together by employing a hybrid Schr\&quot;odinger-Feynman scheme for the simulation.
More precisely, we show that realizing such a scheme with decision diagrams is
indeed possible, we discuss the resulting problems in its realization, and
propose solutions how they can be handled. Experimental evaluations confirm
that this significantly advances the state of the art in decision diagram-based
simulation -- allowing to simulate certain hard circuits within minutes that
could not be simulated in a whole day thus far.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07052</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07052</id><submitter>Mushu Li</submitter><version version="v1"><date>Fri, 14 May 2021 19:52:35 GMT</date><size>12231kb</size></version><title>Slicing-Based AI Service Provisioning on Network Edge</title><authors>Mushu Li, Jie Gao, Conghao Zhou, Xuemin (Sherman) Shen, Weihua Zhuang</authors><categories>cs.NI cs.SY eess.SY</categories><comments>8 pages, 6 figures, Submitted to IEEE Vehicular Technology Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge intelligence leverages computing resources on network edge to provide
artificial intelligence (AI) services close to network users. As it enables
fast inference and distributed learning, edge intelligence is envisioned to be
an important component of 6G networks. In this article, we investigate AI
service provisioning for supporting edge intelligence. First, we present the
features and requirements of AI services. Then, we introduce AI service data
management, and customize network slicing for AI services. Specifically, we
propose a novel resource pooling method to jointly manage service data and
network resources for AI services. A trace-driven case study demonstrates the
effectiveness of the proposed resource pooling method. Through this study, we
illustrate the necessity, challenge, and potential of AI service provisioning
on network edge.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07054</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07054</id><submitter>Matheus Diniz</submitter><version version="v1"><date>Fri, 14 May 2021 19:54:24 GMT</date><size>5939kb</size><source_type>D</source_type></version><title>Face Attributes as Cues for Deep Face Recognition Understanding</title><authors>Matheus Alves Diniz and William Robson Schwartz</authors><categories>cs.CV</categories><comments>7 pages, 5 figures, published at automatic face and gesture
  recognition 2020</comments><doi>10.1109/FG47880.2020.00088</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deeply learned representations are the state-of-the-art descriptors for face
recognition methods. These representations encode latent features that are
difficult to explain, compromising the confidence and interpretability of their
predictions. Most attempts to explain deep features are visualization
techniques that are often open to interpretation. Instead of relying only on
visualizations, we use the outputs of hidden layers to predict face attributes.
The obtained performance is an indicator of how well the attribute is
implicitly learned in that layer of the network. Using a variable selection
technique, we also analyze how these semantic concepts are distributed inside
each layer, establishing the precise location of relevant neurons for each
attribute. According to our experiments, gender, eyeglasses and hat usage can
be predicted with over 96% accuracy even when only a single neural output is
used to predict each attribute. These performances are less than 3 percentage
points lower than the ones achieved by deep supervised face attribute networks.
In summary, our experiments show that, inside DCNNs optimized for face
identification, there exists latent neurons encoding face attributes almost as
accurately as DCNNs optimized for these attributes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07055</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07055</id><submitter>Morteza Banagar</submitter><version version="v1"><date>Fri, 14 May 2021 20:05:47 GMT</date><size>713kb</size><source_type>D</source_type></version><title>3D Two-Hop Cellular Networks with Wireless Backhauled UAVs: Modeling and
  Fundamentals</title><authors>Morteza Banagar and Harpreet S. Dhillon</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the performance of a three-dimensional (3D)
two-hop cellular network in which terrestrial base stations (BSs) coexist with
unmanned aerial vehicles (UAVs) to serve a set of ground user equipment (UE).
In particular, UEs connect either directly to BSs by access links or indirectly
through UAVs to BSs by joint access and backhaul links, where the BSs provide
wireless backhaul to the UAVs. We consider realistic antenna radiation patterns
for both BSs and UAVs using practical models developed by the third generation
partnership project (3GPP). We assume a probabilistic channel model for the
air-to-ground transmission, which incorporates both line-of-sight (LoS) and
non-line-of-sight (NLoS) links. Assuming the max-power association policy, we
study the performance of the network in both amplify-and-forward (AF) and
decode-and-forward (DF) relaying protocols. Using tools from stochastic
geometry, we analyze the joint distribution of distance and zenith angle of the
closest (and serving) UAV to the origin in a 3D setting. Further, we identify
and extensively study key mathematical constructs as the building blocks of
characterizing the received signal-to-interference-plus-noise ratio (SINR)
distribution. Using these results, we obtain exact mathematical expressions for
the coverage probability in both AF and DF relaying protocols. Furthermore,
considering the fact that backhaul links could be quite weak because of the
downtilted antennas at the BSs, we propose and analyze the addition of a
directional uptilted antenna at the BS that is solely used for backhaul
purposes. The superiority of having directional antennas with wirelessly
backhauled UAVs is further demonstrated via simulation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07056</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07056</id><submitter>Michael Siegel</submitter><version version="v1"><date>Fri, 14 May 2021 20:11:46 GMT</date><size>1143kb</size><source_type>D</source_type></version><title>Convergence of the boundary integral method for interfacial Stokes flow</title><authors>David M. Ambrose, Michael Siegel, and Keyang Zhang</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boundary integral numerical methods are among the most accurate methods for
interfacial Stokes flow, and are widely applied. They have the advantage that
only the boundary of the domain must be discretized, which reduces the number
of discretization points and allows the treatment of complicated interfaces.
Despite their popularity, there is no analysis of the convergence of these
methods for interfacial Stokes flow. In practice, the stability of
discretizations of the boundary integral formulation can depend sensitively on
details of the discretization and on the application of numerical filters. We
present a convergence analysis of the boundary integral method for Stokes flow,
focusing on a rather general method for computing the evolution of an elastic
capsule, viscous drop, or inviscid bubble in 2D strain and shear flows. The
analysis clarifies the role of numerical filters in practical computations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07059</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07059</id><submitter>Chenyu You</submitter><version version="v1"><date>Fri, 14 May 2021 20:27:23 GMT</date><size>1392kb</size><source_type>D</source_type></version><title>Momentum Contrastive Voxel-wise Representation Learning for
  Semi-supervised Volumetric Medical Image Segmentation</title><authors>Chenyu You, Ruihan Zhao, Lawrence Staib, James S. Duncan</authors><categories>cs.CV cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, manually annotating
medical data is often laborious, and most existing learning-based approaches
fail to accurately delineate object boundaries without effective geometric
constraints. Contrastive learning, a sub-area of self-supervised learning, has
recently been noted as a promising direction in multiple application fields. In
this work, we present a novel Contrastive Voxel-wise Representation Learning
(CVRL) method with geometric constraints to learn global-local visual
representations for volumetric medical image segmentation with limited
annotations. Our framework can effectively learn global and local features by
capturing 3D spatial context and rich anatomical information. Specifically, we
introduce a voxel-to-volume contrastive algorithm to learn global information
from 3D images, and propose to perform local voxel-to-voxel contrast to
explicitly make use of local cues in the embedding space. Moreover, we
integrate an elastic interaction-based active contour model as a geometric
regularization term to enable fast and reliable object delineations in an
end-to-end learning manner. Results on the Atrial Segmentation Challenge
dataset demonstrate superiority of our proposed scheme, especially in a setting
with a very limited number of annotated data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07062</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07062</id><submitter>Nicol\`o Felicioni</submitter><version version="v1"><date>Fri, 14 May 2021 20:33:51 GMT</date><size>4556kb</size><source_type>D</source_type></version><title>Measuring the User Satisfaction in a Recommendation Interface with
  Multiple Carousels</title><authors>Nicol\`o Felicioni, Maurizio Ferrari Dacrema, Paolo Cremonesi</authors><categories>cs.IR cs.HC cs.LG cs.MM</categories><journal-ref>ACM International Conference on Interactive Media Experiences (IMX
  '21), June 21--23, 2021, Virtual Event, NY, USA</journal-ref><doi>10.1145/3452918.3465493</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common for video-on-demand and music streaming services to adopt a user
interface composed of several recommendation lists, i.e. widgets or swipeable
carousels, each generated according to a specific criterion or algorithm (e.g.
most recent, top popular, recommended for you, editors' choice, etc.).
Selecting the appropriate combination of carousel has significant impact on
user satisfaction. A crucial aspect of this user interface is that to measure
the relevance a new carousel for the user it is not sufficient to account
solely for its individual quality. Instead, it should be considered that other
carousels will already be present in the interface. This is not considered by
traditional evaluation protocols for recommenders systems, in which each
carousel is evaluated in isolation, regardless of (i) which other carousels are
displayed to the user and (ii) the relative position of the carousel with
respect to other carousels. Hence, we propose a two-dimensional evaluation
protocol for a carousel setting that will measure the quality of a
recommendation carousel based on how much it improves upon the quality of an
already available set of carousels. Our evaluation protocol takes into account
also the position bias, i.e. users do not explore the carousels sequentially,
but rather concentrate on the top-left corner of the screen.
  We report experiments on the movie domain and notice that under a carousel
setting the definition of which criteria has to be preferred to generate a list
of recommended items changes with respect to what is commonly understood.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07065</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07065</id><submitter>Nicholas Ichien</submitter><version version="v1"><date>Fri, 14 May 2021 20:56:02 GMT</date><size>2766kb</size></version><title>Visual analogy: Deep learning versus compositional models</title><authors>Nicholas Ichien, Qing Liu, Shuhao Fu, Keith J. Holyoak, Alan Yuille,
  Hongjing Lu</authors><categories>cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Is analogical reasoning a task that must be learned to solve from scratch by
applying deep learning models to massive numbers of reasoning problems? Or are
analogies solved by computing similarities between structured representations
of analogs? We address this question by comparing human performance on visual
analogies created using images of familiar three-dimensional objects (cars and
their subregions) with the performance of alternative computational models.
Human reasoners achieved above-chance accuracy for all problem types, but made
more errors in several conditions (e.g., when relevant subregions were
occluded). We compared human performance to that of two recent deep learning
models (Siamese Network and Relation Network) directly trained to solve these
analogy problems, as well as to that of a compositional model that assesses
relational similarity between part-based representations. The compositional
model based on part representations, but not the deep learning models,
generated qualitative performance similar to that of human reasoners.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07066</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07066</id><submitter>Hongda Wu</submitter><version version="v1"><date>Fri, 14 May 2021 20:56:09 GMT</date><size>264kb</size></version><title>Node Selection Toward Faster Convergence for Federated Learning on
  Non-IID Data</title><authors>Hongda Wu, Ping Wang</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Federated Learning (FL) is a distributed learning paradigm that enables a
large number of resource-limited nodes to collaboratively train a model without
data sharing. The non-independent-and-identically-distributed (non-i.i.d.) data
samples invoke discrepancy between global and local objectives, making the FL
model slow to converge. In this paper, we proposed Optimal Aggregation
algorithm for better aggregation, which finds out the optimal subset of local
updates of participating nodes in each global round, by identifying and
excluding the adverse local updates via checking the relationship between the
local gradient and the global gradient. Then, we proposed a Probabilistic Node
Selection framework (FedPNS) to dynamically change the probability for each
node to be selected based on the output of Optimal Aggregation. FedPNS can
preferentially select nodes that propel faster model convergence. The
unbiasedness of the proposed FedPNS design is illustrated and the convergence
rate improvement of FedPNS over the commonly adopted Federated Averaging
(FedAvg) algorithm is analyzed theoretically. Experimental results demonstrate
the effectiveness of FedPNS in accelerating the FL convergence rate, as
compared to FedAvg with random node selection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07071</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07071</id><submitter>Minhua Wu</submitter><version version="v1"><date>Fri, 14 May 2021 21:19:30 GMT</date><size>632kb</size><source_type>D</source_type></version><title>Listen with Intent: Improving Speech Recognition with Audio-to-Intent
  Front-End</title><authors>Swayambhu Nath Ray, Minhua Wu, Anirudh Raju, Pegah Ghahremani,
  Raghavendra Bilgi, Milind Rao, Harish Arsikere, Ariya Rastrow, Andreas
  Stolcke, Jasha Droppo</authors><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comprehending the overall intent of an utterance helps a listener recognize
the individual words spoken. Inspired by this fact, we perform a novel study of
the impact of explicitly incorporating intent representations as additional
information to improve a recurrent neural network-transducer (RNN-T) based
automatic speech recognition (ASR) system. An audio-to-intent (A2I) model
encodes the intent of the utterance in the form of embeddings or posteriors,
and these are used as auxiliary inputs for RNN-T training and inference.
Experimenting with a 50k-hour far-field English speech corpus, this study shows
that when running the system in non-streaming mode, where intent representation
is extracted from the entire utterance and then used to bias streaming RNN-T
search from the start, it provides a 5.56% relative word error rate reduction
(WERR). On the other hand, a streaming system using per-frame intent posteriors
as extra inputs for the RNN-T ASR system yields a 3.33% relative WERR. A
further detailed analysis of the streaming system indicates that our proposed
method brings especially good gain on media-playing related intents (e.g. 9.12%
relative WERR on PlayMusicIntent).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07073</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07073</id><submitter>Ioannis Xezonakis</submitter><version version="v1"><date>Fri, 14 May 2021 21:24:34 GMT</date><size>490kb</size></version><title>N-ary Huffman Encoding Using High-Degree Trees -- A Performance
  Comparison</title><authors>Ioannis S. Xezonakis, Svoronos Leivadaros</authors><categories>cs.IT cs.DS math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we implement an n-ary Huffman Encoding and Decoding application
using different degrees of tree structures. Our goal is to compare the
performance of the algorithm in terms of compression ratio, decompression speed
and weighted path length when using higher degree trees, compared to the 2-ary
Huffman Code. The Huffman tree degrees that we compare are 2-ary, 3-ary, 4-ary,
5-ary, 6-ary, 7-ary, 8-ary and 16-mal. We also present the impact that branch
prediction has on the performance of the n-ary Huffman Decoding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07074</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07074</id><submitter>Mohamed A. Abd-Elmagid</submitter><version version="v1"><date>Fri, 14 May 2021 21:32:58 GMT</date><size>811kb</size><source_type>D</source_type></version><title>Closed-form Characterization of the MGF of AoI in Energy Harvesting
  Status Update Systems</title><authors>Mohamed A. Abd-Elmagid and Harpreet S. Dhillon</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a real-time status update system in which an energy
harvesting (EH)-powered transmitter node observes some physical process, and
sends its sensed measurements in the form of status updates to a destination
node. The status update and harvested energy packets are assumed to arrive at
the transmitter according to independent Poisson processes, and the service
time of each status update is assumed to be exponentially distributed. We
quantify the freshness of status updates when they reach the destination using
the concept of Age of Information (AoI). Unlike most of the existing analyses
of AoI focusing on the evaluation of its average value when the transmitter is
not subject to energy constraints, our analysis is focused on understanding the
distributional properties of AoI through the characterization of its moment
generating function (MGF). In particular, we use the stochastic hybrid systems
(SHS) framework to derive closed-form expressions of the MGF of AoI under
several queueing disciplines at the transmitter, including non-preemptive and
preemptive in service/waiting strategies. Using these MGF results, we further
obtain closed-form expressions for the first and second moments of AoI in each
queueing discipline. We demonstrate the generality of this analysis by
recovering several existing results for the corresponding system with no energy
constraints as special cases of the new results. Our numerical results verify
the analytical findings, and demonstrate the necessity of incorporating the
higher moments of AoI in the implementation/optimization of real-time status
update systems rather than just relying on its average value.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07076</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07076</id><submitter>Rishi Advani</submitter><version version="v1"><date>Fri, 14 May 2021 21:40:11 GMT</date><size>1101kb</size><source_type>D</source_type></version><title>Efficient Algorithms for Constructing an Interpolative Decomposition</title><authors>Rishi Advani and Sean O'Hagan</authors><categories>math.NA cs.NA</categories><comments>Submitted to SIURO</comments><msc-class>65F55</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank approximations are essential in modern data science. The
interpolative decomposition provides one such approximation. Its distinguishing
feature is that it reuses columns from the original matrix. This enables it to
preserve matrix properties such as sparsity and non-negativity. It also helps
save space in memory. In this work, we introduce two optimized algorithms to
construct an interpolative decomposition along with numerical evidence that
they outperform the current state of the art.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07078</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07078</id><submitter>Siyue Wang</submitter><version version="v1"><date>Fri, 14 May 2021 21:48:23 GMT</date><size>2242kb</size><source_type>D</source_type></version><title>High-Robustness, Low-Transferability Fingerprinting of Neural Networks</title><authors>Siyue Wang, Xiao Wang, Pin-Yu Chen, Pu Zhao and Xue Lin</authors><categories>cs.LG cs.AI cs.CR cs.CV</categories><comments>ICLR 2021 Workshop on Security and Safety in Machine Learning Systems</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes Characteristic Examples for effectively fingerprinting
deep neural networks, featuring high-robustness to the base model against model
pruning as well as low-transferability to unassociated models. This is the
first work taking both robustness and transferability into consideration for
generating realistic fingerprints, whereas current methods lack practical
assumptions and may incur large false positive rates. To achieve better
trade-off between robustness and transferability, we propose three kinds of
characteristic examples: vanilla C-examples, RC-examples, and LTRC-example, to
derive fingerprints from the original base model. To fairly characterize the
trade-off between robustness and transferability, we propose Uniqueness Score,
a comprehensive metric that measures the difference between robustness and
transferability, which also serves as an indicator to the false alarm problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07080</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07080</id><submitter>Shenyu Liu</submitter><version version="v1"><date>Fri, 14 May 2021 22:01:48 GMT</date><size>2779kb</size><source_type>D</source_type></version><title>Iterative Algorithms for Assessing Network Resilience Against Structured
  Perturbations</title><authors>Shenyu Liu, Sonia Martinez, Jorge Cortes</authors><categories>eess.SY cs.SY</categories><comments>13 pages, 5 figures, 2 tables, intended for TCNS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies network resilience against structured additive
perturbations to its topology. We consider dynamic networks modeled as linear
time-invariant systems subject to perturbations of bounded energy satisfying
specific sparsity and entry-wise constraints. Given an energy level, the
structured pseudospectral abscissa captures the worst-possible perturbation an
adversary could employ to de-stabilize the network, and the structured
stability radius is the maximum energy in the structured perturbation that the
network can withstand without becoming unstable. Building on a novel
characterization of the worst-case structured perturbation, we propose
iterative algorithms that efficiently compute the structured pseudospectral
abscissa and structured stability radius. We provide theoretical guarantees of
the local convergence of the algorithms and illustrate their efficacy and
accuracy on several network examples.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07082</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07082</id><submitter>Zehao Dong</submitter><version version="v1"><date>Fri, 14 May 2021 22:20:29 GMT</date><size>4841kb</size></version><title>Interpretable Drug Synergy Prediction with Graph Neural Networks for
  Human-AI Collaboration in Healthcare</title><authors>Zehao Dong, Heming Zhang, Yixin Chen, Fuhai Li</authors><categories>cs.LG cs.AI q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate molecular mechanisms of resistant or sensitive response of
cancer drug combination therapies in an inductive and interpretable manner.
Though deep learning algorithms are widely used in the drug synergy prediction
problem, it is still an open problem to formulate the prediction model with
biological meaning to investigate the mysterious mechanisms of synergy (MoS)
for the human-AI collaboration in healthcare systems. To address the
challenges, we propose a deep graph neural network, IDSP (Interpretable Deep
Signaling Pathways), to incorporate the gene-gene as well as gene-drug
regulatory relationships in synergic drug combination predictions. IDSP
automatically learns weights of edges based on the gene and drug node
relations, i.e., signaling interactions, by a multi-layer perceptron (MLP) and
aggregates information in an inductive manner. The proposed architecture
generates interpretable drug synergy prediction by detecting important
signaling interactions, and can be implemented when the underlying molecular
mechanism encounters unseen genes or signaling pathways. We test IDWSP on
signaling networks formulated by genes from 46 core cancer signaling pathways
and drug combinations from NCI ALMANAC drug combination screening data. The
experimental results demonstrated that 1) IDSP can learn from the underlying
molecular mechanism to make prediction without additional drug chemical
information while achieving highly comparable performance with current
state-of-art methods; 2) IDSP show superior generality and flexibility to
implement the synergy prediction task on both transductive tasks and inductive
tasks. 3) IDSP can generate interpretable results by detecting different
salient signaling patterns (i.e. MoS) for different cell lines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07085</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07085</id><submitter>Taojiannan Yang</submitter><version version="v1"><date>Fri, 14 May 2021 22:30:13 GMT</date><size>4065kb</size><source_type>D</source_type></version><title>MutualNet: Adaptive ConvNet via Mutual Learning from Different Model
  Configurations</title><authors>Taojiannan Yang, Sijie Zhu, Matias Mendieta, Pu Wang, Ravikumar
  Balakrishnan, Minwoo Lee, Tao Han, Mubarak Shah, Chen Chen</authors><categories>cs.CV</categories><comments>Extended version of arXiv:1909.12978. More experiments on 3D networks
  (SlowFast, X3D) and analyses on training cost</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing deep neural networks are static, which means they can only do
inference at a fixed complexity. But the resource budget can vary substantially
across different devices. Even on a single device, the affordable budget can
change with different scenarios, and repeatedly training networks for each
required budget would be incredibly expensive. Therefore, in this work, we
propose a general method called MutualNet to train a single network that can
run at a diverse set of resource constraints. Our method trains a cohort of
model configurations with various network widths and input resolutions. This
mutual learning scheme not only allows the model to run at different
width-resolution configurations but also transfers the unique knowledge among
these configurations, helping the model to learn stronger representations
overall. MutualNet is a general training methodology that can be applied to
various network structures (e.g., 2D networks: MobileNets, ResNet, 3D networks:
SlowFast, X3D) and various tasks (e.g., image classification, object detection,
segmentation, and action recognition), and is demonstrated to achieve
consistent improvements on a variety of datasets. Since we only train the model
once, it also greatly reduces the training cost compared to independently
training several models. Surprisingly, MutualNet can also be used to
significantly boost the performance of a single network, if dynamic resource
constraint is not a concern. In summary, MutualNet is a unified method for both
static and adaptive, 2D and 3D networks. Codes and pre-trained models are
available at \url{https://github.com/taoyang1122/MutualNet}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07086</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07086</id><submitter>Nikolajs Skuratovs</submitter><version version="v1"><date>Fri, 14 May 2021 22:45:58 GMT</date><size>1132kb</size><source_type>D</source_type></version><title>Divergence Estimation in Message Passing algorithms</title><authors>Nikolajs Skuratovs, Michael Davies</authors><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern imaging applications can be modeled as compressed sensing linear
inverse problems. When the measurement operator involved in the inverse problem
is sufficiently random, denoising Scalable Message Passing (SMP) algorithms
have a potential to demonstrate high efficiency in recovering compressed data.
One of the key components enabling SMP to achieve fast convergence, stability
and predictable dynamics is the Onsager correction that must be updated at each
iteration of the algorithm. This correction involves the denoiser's divergence
that is traditionally estimated via the Black-Box Monte Carlo (BB-MC) method
\cite{MC-divergence}. While the BB-MC method demonstrates satisfying accuracy
of estimation, it requires executing the denoiser additional times at each
iteration and might lead to a substantial increase in computational cost of the
SMP algorithms. In this work we develop two Large System Limit models of the
Onsager correction for denoisers operating within SMP algorithms and use these
models to propose two practical classes of divergence estimators that require
no additional executions of the denoiser and demonstrate similar or superior
correction compared to the BB-MC method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07088</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07088</id><submitter>Hai Dao</submitter><version version="v1"><date>Fri, 14 May 2021 22:51:55 GMT</date><size>218kb</size></version><title>The Achilles Heel of Some Optical Network Designs and Performance
  Comparisons</title><authors>Dao Thanh Hai</authors><categories>cs.NI</categories><comments>6 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This non-conventional paper represents the first attempt to uncover a
possible vulnerability in some proposals for optical network designs and
performance comparisons. While optical network designs and planning lie at the
heart of achieving fiber capacity efficiency and/or operational efficiency, its
combinatorial nature makes it computationally hard to reach optimal solutions
for realistic scenarios. Therefore, the well-established way that have been
taken for granted by not-so-small number of research papers is that an
optimization model based on mixed integer linear programming (MILP) is first
proposed and then due to the intractability of such combinatorial model, an
heuristic algorithm is offered as an approximation. The solution-quality
comparison between the MILP and heuristic is then carried out on small-scale
instances including topologies and traffic tests to verify the efficacy of the
proposed heuristic and the next step is to use such allegedly verified
heuristic for optical network designs of realistic scenarios. This approach may
nevertheless leave a critical vulnerability as there is no guarantee that one
performs well in small tests will generalize adequately for large-scale cases,
a common pitfall widely referred as the peril of extrapolation and/or
overfitting. Besides, it is not uncommon that in some research works, for
benchmarking purpose, the comparison between a new design proposal whose
performance is obtained from on one heuristic and a reference design based on
another heuristic is carried out. As the result of missing solution quality
check, such performance comparison relied merely on heuristic solutions may be
equally vulnerable as its results can be distorted and thus, be far from the
possibly achieved zones. In this work, we pinpoint those issues and provide a
realistic case study to highlight and demonstrate the impact of such
vulnerabilities.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07089</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07089</id><submitter>Sajjad Nassirpour</submitter><version version="v1"><date>Fri, 14 May 2021 22:59:17 GMT</date><size>7006kb</size><source_type>D</source_type></version><title>On the Stability Region of Intermittent Interference Networks</title><authors>Sajjad Nassirpour and Alireza Vahid</authors><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent information-theoretic studies have resulted in several interference
management (IM) techniques that promise significant capacity improvements over
interference avoidance techniques. However, in practice, the stable throughput
region is a more relevant metric compared to the capacity region. In this work,
we focus on the stable throughput region of a two-pair intermittent
interference network with distributed transmitters and propose a queue-based
transmission protocol in different regimes to handle the data between queues.
In this context, we translate physical-layer IM protocols to accommodate
stochastic message arrivals. To evaluate our proposed techniques, we compare
the stable throughput region to the capacity region and show, through
simulations, that the stable throughput region matches the capacity region when
the latter is known. We show that in order to achieve the optimal stable
throughput region, new ingredients are needed when compared to prior results.
We quantify the trade-off between the encoding/decoding complexity of the
proposed scheme (in terms of number of required algebraic operations), and the
achievable rates. Finally, we study the lifetime of messages (i.e. the duration
from arrival to successful delivery) vis-a-vis the total communication time,
and we observe that the average lifetime scales as the square root of the total
communication time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07091</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07091</id><submitter>Sydney Katz</submitter><version version="v1"><date>Fri, 14 May 2021 23:18:05 GMT</date><size>484kb</size><source_type>D</source_type></version><title>Verification of Image-based Neural Network Controllers Using Generative
  Models</title><authors>Sydney M. Katz, Anthony L. Corso, Christopher A. Strong, Mykel J.
  Kochenderfer</authors><categories>cs.LG cs.AI cs.RO</categories><comments>10 pages, 12 figures, presented at the 2021 AIAA Digital Avionics
  Systems Conference (DASC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are often used to process information from image-based
sensors to produce control actions. While they are effective for this task, the
complex nature of neural networks makes their output difficult to verify and
predict, limiting their use in safety-critical systems. For this reason, recent
work has focused on combining techniques in formal methods and reachability
analysis to obtain guarantees on the closed-loop performance of neural network
controllers. However, these techniques do not scale to the high-dimensional and
complicated input space of image-based neural network controllers. In this
work, we propose a method to address these challenges by training a generative
adversarial network (GAN) to map states to plausible input images. By
concatenating the generator network with the control network, we obtain a
network with a low-dimensional input space. This insight allows us to use
existing closed-loop verification tools to obtain formal guarantees on the
performance of image-based controllers. We apply our approach to provide safety
guarantees for an image-based neural network controller for an autonomous
aircraft taxi problem. We guarantee that the controller will keep the aircraft
on the runway and guide the aircraft towards the center of the runway. The
guarantees we provide are with respect to the set of input images modeled by
our generator network, so we provide a recall metric to evaluate how well the
generator captures the space of plausible images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07099</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07099</id><submitter>Seyed Omid Davoudi</submitter><version version="v1"><date>Fri, 14 May 2021 23:43:11 GMT</date><size>502kb</size><source_type>D</source_type></version><title>Feature-Based Interpretable Reinforcement Learning based on
  State-Transition Models</title><authors>Omid Davoodi, Majid Komeili</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Growing concerns regarding the operational usage of AI models in the
real-world has caused a surge of interest in explaining AI models' decisions to
humans. Reinforcement Learning is not an exception in this regard. In this
work, we propose a method for offering local explanations on risk in
reinforcement learning. Our method only requires a log of previous interactions
between the agent and the environment to create a state-transition model. It is
designed to work on RL environments with either continuous or discrete state
and action spaces. After creating the model, actions of any agent can be
explained in terms of the features most influential in increasing or decreasing
risk or any other desirable objective function in the locality of the agent.
Through experiments, we demonstrate the effectiveness of the proposed method in
providing such explanations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07102</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07102</id><submitter>Robert Cohen</submitter><version version="v1"><date>Sat, 15 May 2021 00:10:12 GMT</date><size>2117kb</size><source_type>D</source_type></version><title>Lightweight Compression of Intermediate Neural Network Features for
  Collaborative Intelligence</title><authors>Robert A. Cohen, Hyomin Choi, Ivan V. Baji\'c</authors><categories>cs.LG eess.IV</categories><comments>Accepted for publication in IEEE Open Journal of Circuits and Systems</comments><journal-ref>IEEE Open Journal of Circuits and Systems, vol. 2, 13 May 2021,
  pp. 350-362</journal-ref><doi>10.1109/OJCAS.2021.3072884</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In collaborative intelligence applications, part of a deep neural network
(DNN) is deployed on a lightweight device such as a mobile phone or edge
device, and the remaining portion of the DNN is processed where more computing
resources are available, such as in the cloud. This paper presents a novel
lightweight compression technique designed specifically to quantize and
compress the features output by the intermediate layer of a split DNN, without
requiring any retraining of the network weights. Mathematical models for
estimating the clipping and quantization error of ReLU and leaky-ReLU
activations at this intermediate layer are developed and used to compute
optimal clipping ranges for coarse quantization. We also present a modified
entropy-constrained design algorithm for quantizing clipped activations. When
applied to popular object-detection and classification DNNs, we were able to
compress the 32-bit floating point intermediate activations down to 0.6 to 0.8
bits, while keeping the loss in accuracy to less than 1%. When compared to
HEVC, we found that the lightweight codec consistently provided better
inference accuracy, by up to 1.3%. The performance and simplicity of this
lightweight compression technique makes it an attractive option for coding an
intermediate layer of a split neural network for edge/cloud applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07106</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07106</id><submitter>Lane Smith</submitter><version version="v1"><date>Sat, 15 May 2021 00:41:58 GMT</date><size>533kb</size><source_type>D</source_type></version><title>Impacts of Time-of-Use Rate Changes on the Electricity Bills of
  Commercial Consumers</title><authors>Lane D. Smith, Daniel S. Kirschen</authors><categories>eess.SY cs.SY</categories><comments>Accepted to the 2021 IEEE Power and Energy Society (PES) General
  Meeting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Changes in the profile of prices in wholesale electricity markets prompt
utilities to redesign their tariffs and adjust their time-of-use periods to
ensure a more adequate cost recovery. However, changing the rate structures
could adversely affect commercial consumers by increasing their electricity
bills and hindering their ability to reduce costs using techniques like net
energy metering. As time-of-use periods are adjusted, consumers will need to
rely on the flexibility of distributed energy resources to achieve cost
reductions. This paper explores the effect that Pacific Gas and Electric
Company's redesigned rates have on the electricity bills of consumers with
different demand profiles. Sensitivity analyses are conducted to examine the
effect of asset sizing on reducing costs under each tariff.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07107</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07107</id><submitter>Sushil Thapa</submitter><version version="v1"><date>Sat, 15 May 2021 00:46:11 GMT</date><size>1082kb</size><source_type>D</source_type></version><title>An Effective Baseline for Robustness to Distributional Shift</title><authors>Sunil Thulasidasan, Sushil Thapa, Sayera Dhaubhadel, Gopinath
  Chennupati, Tanmoy Bhattacharya, Jeff Bilmes</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Refraining from confidently predicting when faced with categories of inputs
different from those seen during training is an important requirement for the
safe deployment of deep learning systems. While simple to state, this has been
a particularly challenging problem in deep learning, where models often end up
making overconfident predictions in such situations. In this work we present a
simple, but highly effective approach to deal with out-of-distribution
detection that uses the principle of abstention: when encountering a sample
from an unseen class, the desired behavior is to abstain from predicting. Our
approach uses a network with an extra abstention class and is trained on a
dataset that is augmented with an uncurated set that consists of a large number
of out-of-distribution (OoD) samples that are assigned the label of the
abstention class; the model is then trained to learn an effective discriminator
between in and out-of-distribution samples. We compare this relatively simple
approach against a wide variety of more complex methods that have been proposed
both for out-of-distribution detection as well as uncertainty modeling in deep
learning, and empirically demonstrate its effectiveness on a wide variety of of
benchmarks and deep architectures for image recognition and text
classification, often outperforming existing approaches by significant margins.
Given the simplicity and effectiveness of this method, we propose that this
approach be used as a new additional baseline for future work in this domain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07109</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07109</id><submitter>Evan Hernandez</submitter><version version="v1"><date>Sat, 15 May 2021 00:58:08 GMT</date><size>572kb</size><source_type>D</source_type></version><title>The Low-Dimensional Linear Geometry of Contextualized Word
  Representations</title><authors>Evan Hernandez and Jacob Andreas</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Black-box probing models can reliably extract linguistic features like tense,
number, and syntactic role from pretrained word representations. However, the
manner in which these features are encoded in representations remains poorly
understood. We present a systematic study of the linear geometry of
contextualized word representations in ELMO and BERT. We show that a variety of
linguistic features (including structured dependency relationships) are encoded
in low-dimensional subspaces. We then refine this geometric picture, showing
that there are hierarchical relations between the subspaces encoding general
linguistic categories and more specific ones, and that low-dimensional feature
encodings are distributed rather than aligned to individual neurons. Finally,
we demonstrate that these linear subspaces are causally related to model
behavior, and can be used to perform fine-grained manipulation of BERT's output
distribution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07111</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07111</id><submitter>Zahra Dasht Bozorgi</submitter><version version="v1"><date>Sat, 15 May 2021 01:19:04 GMT</date><size>1515kb</size><source_type>D</source_type></version><title>Prescriptive Process Monitoring for Cost-Aware Cycle Time Reduction</title><authors>Zahra Dasht Bozorgi, Irene Teinemaa, Marlon Dumas, Marcello La Rosa</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reducing cycle time is a recurrent concern in the field of business process
management. Depending on the process, various interventions may be triggered to
reduce the cycle time of a case, for example, using a faster shipping service
in an order-to-delivery process or giving a phone call to a customer to obtain
missing information rather than waiting passively. Each of these interventions
comes with a cost. This paper tackles the problem of determining if and when to
trigger a time-reducing intervention in a way that maximizes the total net
gain. The paper proposes a prescriptive process monitoring method that uses
orthogonal random forest models to estimate the causal effect of triggering a
time-reducing intervention for each ongoing case of a process. Based on this
causal effect estimate, the method triggers interventions according to a
user-defined policy. The method is evaluated on two real-life logs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07112</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07112</id><submitter>Celong Liu</submitter><version version="v1"><date>Sat, 15 May 2021 01:20:30 GMT</date><size>33835kb</size><source_type>D</source_type></version><title>NeLF: Practical Novel View Synthesis with Neural Light Field</title><authors>Celong Liu, Zhong Li, Junsong Yuan, Yi Xu</authors><categories>cs.CV cs.GR</categories><comments>13 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we present a practical and robust deep learning solution for
the novel view synthesis of complex scenes. In our approach, a continuous scene
is represented as a light field, i.e., a set of rays, each of which has a
corresponding color. We adopt a 4D parameterization of the light field. We then
formulate the light field as a 4D function that maps 4D coordinates to
corresponding color values. We train a deep fully connected network to optimize
this function. Then, the scene-specific model is used to synthesize novel
views. Previous light field approaches usually require dense view sampling to
reliably render high-quality novel views. Our method can render novel views by
sampling rays and querying the color for each ray from the network directly;
thus enabling fast light field rendering with a very sparse set of input
images. Our method achieves state-of-the-art novel view synthesis results while
maintaining an interactive frame rate.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07113</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07113</id><submitter>Christian Mejia-Escobar</submitter><version version="v1"><date>Sat, 15 May 2021 01:31:25 GMT</date><size>3614kb</size><source_type>D</source_type></version><title>A Large Visual, Qualitative and Quantitative Dataset of Web Pages</title><authors>Christian Mejia-Escobar, Miguel Cazorla, Ester Martinez-Martin</authors><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The World Wide Web is not only one of the most important platforms of
communication and information at present, but also an area of growing interest
for scientific research. This motivates a lot of work and projects that require
large amounts of data. However, there is no dataset that integrates the
parameters and visual appearance of Web pages, because its collection is a
costly task in terms of time and effort. With the support of various computer
tools and programming scripts, we have created a large dataset of 49,438 Web
pages. It consists of visual, textual and numerical data types, includes all
countries worldwide, and considers a broad range of topics such as art,
entertainment, economy, business, education, government, news, media, science,
and environment, covering different cultural characteristics and varied design
preferences. In this paper, we describe the process of collecting, debugging
and publishing the final product, which is freely available. To demonstrate the
usefulness of our dataset, we expose a binary classification model for
detecting error Web pages, and a multi-class Web subject-based categorization,
both problems using convolutional neural networks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07115</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07115</id><submitter>Syed Mohsin Abbas Dr.</submitter><version version="v1"><date>Sat, 15 May 2021 01:38:52 GMT</date><size>2132kb</size><source_type>D</source_type></version><title>High-Throughput VLSI architecture for Soft-Decision decoding with
  ORBGRAND</title><authors>Syed Mohsin Abbas, Thibaud Tonnellier, Furkan Ercan, Marwan
  Jalaleddine and Warren J. Gross</authors><categories>cs.IT cs.AR math.IT</categories><comments>Please note that a mislabeling in Fig. 1 has occurred in the IEEE
  Xplore version of this paper. This error has been corrected in this version
  of the manuscript. (Accepted in ICASSP 2021)</comments><doi>10.1109/ICASSP39728.2021.9414908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guessing Random Additive Noise Decoding (GRAND) is a recently proposed
approximate Maximum Likelihood (ML) decoding technique that can decode any
linear error-correcting block code. Ordered Reliability Bits GRAND (ORBGRAND)
is a powerful variant of GRAND, which outperforms the original GRAND technique
by generating error patterns in a specific order. Moreover, their simplicity at
the algorithm level renders GRAND family a desirable candidate for applications
that demand very high throughput. This work reports the first-ever hardware
architecture for ORBGRAND, which achieves an average throughput of up to $42.5$
Gbps for a code length of $128$ at an SNR of $10$ dB. Moreover, the proposed
hardware can be used to decode any code provided the length and rate
constraints. Compared to the state-of-the-art fast dynamic successive
cancellation flip decoder (Fast-DSCF) using a 5G polar $(128,105)$ code, the
proposed VLSI implementation has $49\times$ more average throughput while
maintaining similar decoding performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07116</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07116</id><submitter>Mohammadreza Mohseni</submitter><version version="v1"><date>Sat, 15 May 2021 02:01:16 GMT</date><size>6538kb</size><source_type>D</source_type></version><title>Can self-training identify suspicious ugly duckling lesions?</title><authors>Mohammadreza Mohseni, Jordan Yap, William Yolland, Arash Koochek and M
  Stella Atkins</authors><categories>cs.CV</categories><comments>Accepted at Sixth ISIC Skin Image Analysis Workshop @ CVPR 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One commonly used clinical approach towards detecting melanomas recognises
the existence of Ugly Duckling nevi, or skin lesions which look different from
the other lesions on the same patient. An automatic method of detecting and
analysing these lesions would help to standardize studies, compared with manual
screening methods. However, it is difficult to obtain expertly-labelled images
for ugly duckling lesions. We therefore propose to use self-supervised machine
learning to automatically detect outlier lesions. We first automatically detect
and extract all the lesions from a wide-field skin image, and calculate an
embedding for each detected lesion in a patient image, based on automatically
identified features. These embeddings are then used to calculate the L2
distances as a way to measure dissimilarity. Using this deep learning method,
Ugly Ducklings are identified as outliers which should deserve more attention
from the examining physician. We evaluate through comparison with
dermatologists, and achieve a sensitivity rate of 72.1% and diagnostic accuracy
of 94.2% on the held-out test set.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07120</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07120</id><submitter>Akinori Kawachi</submitter><version version="v1"><date>Sat, 15 May 2021 03:08:01 GMT</date><size>35kb</size><source_type>D</source_type></version><title>Communication Complexity of Private Simultaneous Quantum Messages
  Protocols</title><authors>Akinori Kawachi and Harumichi Nishimura</authors><categories>quant-ph cs.CC cs.CR</categories><comments>19 pages, to be published in Proc. ITC 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The private simultaneous messages model is a non-interactive version of the
multiparty secure computation, which has been intensively studied to examine
the communication cost of the secure computation. We consider its quantum
counterpart, the private simultaneous quantum messages (PSQM) model, and
examine the advantages of quantum communication and prior entanglement of this
model. In the PSQM model, $k$ parties $P_1,\ldots,P_k$ initially share a common
random string (or entangled states in a stronger setting), and they have
private classical inputs $x_1,\ldots, x_k$. Every $P_i$ generates a quantum
message from the private input $x_i$ and the shared random string (entangled
states), and then sends it to the referee $R$. Receiving the messages, $R$
computes $F(x_1,\ldots,x_k)$. Then, $R$ learns nothing except for
$F(x_1,\ldots,x_k)$ as the privacy condition. We obtain the following results
for this PSQM model. (1) We demonstrate that the privacy condition inevitably
increases the communication cost in the two-party PSQM model as well as in the
classical case presented by Applebaum, Holenstein, Mishra, and Shayevitz. In
particular, we prove a lower bound $(3-o(1))n$ of the communication complexity
in PSQM protocols with a shared random string for random Boolean functions of
$2n$-bit input, which is larger than the trivial upper bound $2n$ of the
communication complexity without the privacy condition. (2) We demonstrate a
factor two gap between the communication complexity of PSQM protocols with
shared entangled states and with shared random strings by designing a
multiparty PSQM protocol with shared entangled states for a total function that
extends the two-party equality function. (3) We demonstrate an exponential gap
between the communication complexity of PSQM protocols with shared entangled
states and with shared random strings for a two-party partial function.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07122</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07122</id><submitter>Qingxiu Dong</submitter><version version="v1"><date>Sat, 15 May 2021 03:25:42 GMT</date><size>17233kb</size><source_type>D</source_type></version><title>Premise-based Multimodal Reasoning: A Human-like Cognitive Process</title><authors>Qingxiu Dong, Ziwei Qin, Heming Xia, Tian Feng, Shoujie Tong, Haoran
  Meng, Lin Xu, Tianyu Liu, Zuifang Sui, Weidong Zhan, Sujian Li and Zhongyu
  Wei</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoning is one of the major challenges of Human-like AI and has recently
attracted intensive attention from natural language processing (NLP)
researchers. However, cross-modal reasoning needs further research. For
cross-modal reasoning, we observe that most methods fall into shallow feature
matching without in-depth human-like reasoning.The reason lies in that existing
cross-modal tasks directly ask questions for a image. However, human reasoning
in real scenes is often made under specific background information, a process
that is studied by the ABC theory in social psychology. We propose a shared
task named &quot;Premise-based Multimodal Reasoning&quot; (PMR), which requires
participating models to reason after establishing a profound understanding of
background information. We believe that the proposed PMR would contribute to
and help shed a light on human-like in-depth reasoning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07123</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07123</id><submitter>Costas Busch</submitter><version version="v1"><date>Sat, 15 May 2021 03:37:46 GMT</date><size>46kb</size></version><title>Byzantine-Resilient Population Protocols</title><authors>Costas Busch and Dariusz R. Kowalski</authors><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Population protocols model information spreading in networks where pairwise
node exchanges are determined by an external random scheduler. Most of the
population protocols in the literature assume that the participating $n$ nodes
are honest. Such assumption may not be, however, accurate for large-scale
systems of small devices. Hence, in this work, we study a variant of population
protocols, where up to $f$ nodes can be Byzantine. We examine the majority
(binary) consensus problem against different levels of adversary strengths,
ranging from the Full adversary that has complete knowledge of all the node
states to the Weak adversary that has only knowledge about which exchanges take
place. We also take into account Dynamic vs Static node corruption by the
adversary. We give lower bounds that require any algorithm solving the majority
consensus to have initial difference $d = \Omega(f + 1)$ for the tally between
the two proposed values, which holds for both the Full Static and Weak Dynamic
adversaries. We then present an algorithm that solves the majority consensus
problem and tolerates $f \leq n / c$ Byzantine nodes, for some constant $c&gt;0$,
with $d = \Omega(f + \sqrt{n \log n})$ and $O(\log^3 n)$ parallel time steps,
using $O(\log n)$ states per node. We also give an alternative algorithm with
$d = \Omega(\min\{f \log^2 n + 1,n\})$. Moreover, we combine both algorithms
into one using random coins. The only other known previous work on
Byzantine-resilient population protocols tolerates up to $f = o(\sqrt n)$
faulty nodes and works against a static adversary; hence, our protocols
significantly improve the tolerance by an $\omega(\sqrt n)$ factor and all of
them work correctly against a stronger dynamic adversary.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07127</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07127</id><submitter>Gushu Li</submitter><version version="v1"><date>Sat, 15 May 2021 03:45:26 GMT</date><size>5129kb</size><source_type>D</source_type></version><title>Software-Hardware Co-Optimization for Computational Chemistry on
  Superconducting Quantum Processors</title><authors>Gushu Li, Yunong Shi, and Ali Javadi-Abhari</authors><categories>quant-ph cs.ET</categories><comments>12 pages, 11 figures, to appear in ISCA 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Computational chemistry is the leading application to demonstrate the
advantage of quantum computing in the near term. However, large-scale
simulation of chemical systems on quantum computers is currently hindered due
to a mismatch between the computational resource needs of the program and those
available in today's technology. In this paper we argue that significant new
optimizations can be discovered by co-designing the application, compiler, and
hardware. We show that multiple optimization objectives can be coordinated
through the key abstraction layer of Pauli strings, which are the basic
building blocks of computational chemistry programs. In particular, we leverage
Pauli strings to identify critical program components that can be used to
compress program size with minimal loss of accuracy. We also leverage the
structure of Pauli string simulation circuits to tailor a novel hardware
architecture and compiler, leading to significant execution overhead reduction
by up to 99%. While exploiting the high-level domain knowledge reveals
significant optimization opportunities, our hardware/software framework is not
tied to a particular program instance and can accommodate the full family of
computational chemistry problems with such structure. We believe the co-design
lessons of this study can be extended to other domains and hardware
technologies to hasten the onset of quantum advantage.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07128</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07128</id><submitter>Xin Liu Dr.</submitter><version version="v1"><date>Sat, 15 May 2021 03:53:48 GMT</date><size>1619kb</size><source_type>D</source_type></version><title>FDDH: Fast Discriminative Discrete Hashing for Large-Scale Cross-Modal
  Retrieval</title><authors>Xin Liu, Xingzhi Wang and Yiu-ming Cheung</authors><categories>cs.CV</categories><comments>16 pages, 7 figures</comments><journal-ref>IEEE Transactions on Neural Networks and Learning Systems, 2021</journal-ref><doi>10.1109/TNNLS.2021.3076684</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Cross-modal hashing, favored for its effectiveness and efficiency, has
received wide attention to facilitating efficient retrieval across different
modalities. Nevertheless, most existing methods do not sufficiently exploit the
discriminative power of semantic information when learning the hash codes,
while often involving time-consuming training procedure for handling the
large-scale dataset. To tackle these issues, we formulate the learning of
similarity-preserving hash codes in terms of orthogonally rotating the semantic
data so as to minimize the quantization loss of mapping such data to hamming
space, and propose an efficient Fast Discriminative Discrete Hashing (FDDH)
approach for large-scale cross-modal retrieval. More specifically, FDDH
introduces an orthogonal basis to regress the targeted hash codes of training
examples to their corresponding semantic labels, and utilizes &quot;-dragging
technique to provide provable large semantic margins. Accordingly, the
discriminative power of semantic information can be explicitly captured and
maximized. Moreover, an orthogonal transformation scheme is further proposed to
map the nonlinear embedding data into the semantic subspace, which can well
guarantee the semantic consistency between the data feature and its semantic
representation. Consequently, an efficient closed form solution is derived for
discriminative hash code learning, which is very computationally efficient. In
addition, an effective and stable online learning strategy is presented for
optimizing modality-specific projection functions, featuring adaptivity to
different training sizes and streaming data. The proposed FDDH approach
theoretically approximates the bi-Lipschitz continuity, runs sufficiently fast,
and also significantly improves the retrieval performance over the
state-of-the-art methods. The source code is released at:
https://github.com/starxliu/FDDH.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07129</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07129</id><submitter>Wen Lu</submitter><version version="v1"><date>Sat, 15 May 2021 03:54:32 GMT</date><size>5628kb</size><source_type>D</source_type></version><title>Regularized Deep Linear Discriminant Analysis</title><authors>Hongwei Chen and Wen Lu</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As a non-linear extension of the classic Linear Discriminant Analysis(LDA),
Deep Linear Discriminant Analysis(DLDA) replaces the original Categorical Cross
Entropy(CCE) loss function with eigenvalue-based loss function to make a deep
neural network(DNN) able to learn linearly separable hidden representations. In
this paper, we first point out DLDA focuses on training the cooperative
discriminative ability of all the dimensions in the latent subspace, while put
less emphasis on training the separable capacity of single dimension. To
improve DLDA, a regularization method on within-class scatter matrix is
proposed to strengthen the discriminative ability of each dimension, and also
keep them complement each other. Experiment results on STL-10, CIFAR-10 and
Pediatric Pneumonic Chest X-ray Dataset showed that our proposed regularization
method Regularized Deep Linear Discriminant Analysis(RDLDA) outperformed DLDA
and conventional neural network with CCE as objective. To further improve the
discriminative ability of RDLDA in the local space, an algorithm named Subclass
RDLDA is also proposed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07131</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07131</id><submitter>Reza Sameni</submitter><version version="v1"><date>Sat, 15 May 2021 04:00:28 GMT</date><size>916kb</size><source_type>D</source_type></version><title>Hardware Synthesis of State-Space Equations; Application to FPGA
  Implementation of Shallow and Deep Neural Networks</title><authors>Amir-Hossein Kiamarzi, Pezhman Torabi, Reza Sameni</authors><categories>cs.AR cs.AI eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, shallow and deep Neural Networks (NNs) have vast applications
including biomedical engineering, image processing, computer vision, and speech
recognition. Many researchers have developed hardware accelerators including
field-programmable gate arrays (FPGAs) for implementing high-performance and
energy efficient NNs. Apparently, the hardware architecture design process is
specific and time-consuming for each NN. Therefore, a systematic way to design,
implement and optimize NNs is highly demanded. The paper presents a systematic
approach to implement state-space models in register transfer level (RTL), with
special interest for NN implementation. The proposed design flow is based on
the iterative nature of state-space models and the analogy between state-space
formulations and finite-state machines. The method can be used in
linear/nonlinear and time-varying/time-invariant systems. It can also be used
to implement either intrinsically iterative systems (widely used in various
domains such as signal processing, numerical analysis, computer arithmetic, and
control engineering), or systems that could be rewritten in equivalent
iterative forms. The implementation of recurrent NNs such as long short-term
memory (LSTM) NNs, which have intrinsic state-space forms, are another major
applications for this framework. As a case study, it is shown that state-space
systems can be used for the systematic implementation and optimization of NNs
(as nonlinear and time-varying dynamic systems). An RTL code generating
software is also provided online, which simplifies the automatic generation of
NNs of arbitrary size.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07132</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07132</id><submitter>Keisuke Okumura</submitter><version version="v1"><date>Sat, 15 May 2021 04:05:01 GMT</date><size>24148kb</size><source_type>D</source_type></version><title>Offline Time-Independent Multi-Agent Path Planning</title><authors>Keisuke Okumura, Fran\c{c}ois Bonnet, Yasumasa Tamura, Xavier D\'efago</authors><categories>cs.MA cs.RO</categories><comments>32 pages, preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a novel planning problem for multiple agents moving on
graphs that we call offline time-independent multi-agent path planning
(OTIMAPP). The motivation is to overcome time uncertainties in multi-agent
scenarios where we cannot expect agents to act perfectly following timed plans,
e.g., executions with mobile robots. For this purpose, OTIMAPP abandons all
timing assumptions; it is offline planning that assumes event-driven executions
without or less run-time effort. The problem is finding plans to be terminated
correctly in any action orders of agents, i.e., guaranteeing that all agents
eventually reach their destinations. We address a bunch of questions for this
problem: required conditions for feasible solutions, computational complexity,
comparison with well-known other multi-agent problems, construction of solvers,
effective relaxation of a solution concept, and how to implement the plans by
actual robots. Throughout the paper, we establish the foundation of OTIMAPP and
demonstrate its utility. A video is available at
https://kei18.github.io/otimapp.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07135</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07135</id><submitter>Anant Baijal</submitter><version version="v1"><date>Sat, 15 May 2021 04:14:47 GMT</date><size>1086kb</size></version><title>Analyzing Images for Music Recommendation</title><authors>Anant Baijal, Vivek Agarwal and Danny Hyun</authors><categories>cs.MM cs.AI cs.SD eess.AS eess.IV</categories><comments>IEEE International Conference on Consumer Electronics (IEEE ICCE
  2021)</comments><doi>10.1109/ICCE50685.2021.9427619</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experiencing images with suitable music can greatly enrich the overall user
experience. The proposed image analysis method treats an artwork image
differently from a photograph image. Automatic image classification is
performed using deep-learning based models. An illustrative analysis showcasing
the ability of our deep-models to inherently learn and utilize perceptually
relevant features when classifying artworks is also presented. The Mean Opinion
Score (MOS) obtained from subjective assessments of the respective image and
recommended music pairs supports the effectiveness of our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07139</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07139</id><submitter>Wei Zhou</submitter><version version="v1"><date>Sat, 15 May 2021 04:31:48 GMT</date><size>2805kb</size><source_type>D</source_type></version><title>Image Super-Resolution Quality Assessment: Structural Fidelity Versus
  Statistical Naturalness</title><authors>Wei Zhou, Zhou Wang, Zhibo Chen</authors><categories>eess.IV cs.CV cs.MM</categories><comments>Accepted by QoMEX 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single image super-resolution (SISR) algorithms reconstruct high-resolution
(HR) images with their low-resolution (LR) counterparts. It is desirable to
develop image quality assessment (IQA) methods that can not only evaluate and
compare SISR algorithms, but also guide their future development. In this
paper, we assess the quality of SISR generated images in a two-dimensional (2D)
space of structural fidelity versus statistical naturalness. This allows us to
observe the behaviors of different SISR algorithms as a tradeoff in the 2D
space. Specifically, SISR methods are traditionally designed to achieve high
structural fidelity but often sacrifice statistical naturalness, while recent
generative adversarial network (GAN) based algorithms tend to create more
natural-looking results but lose significantly on structural fidelity.
Furthermore, such a 2D evaluation can be easily fused to a scalar quality
prediction. Interestingly, we find that a simple linear combination of a
straightforward local structural fidelity and a global statistical naturalness
measures produce surprisingly accurate predictions of SISR image quality when
tested using public subject-rated SISR image datasets. Code of the proposed
SFSN model is publicly available at \url{https://github.com/weizhou-geek/SFSN}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07140</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07140</id><submitter>Zijin Gu</submitter><version version="v1"><date>Sat, 15 May 2021 04:36:39 GMT</date><size>14688kb</size><source_type>D</source_type></version><title>NeuroGen: activation optimized image synthesis for discovery
  neuroscience</title><authors>Zijin Gu, Keith W. Jamison, Meenakshi Khosla, Emily J. Allen, Yihan
  Wu, Thomas Naselaris, Kendrick Kay, Mert R. Sabuncu, Amy Kuceyeski</authors><categories>q-bio.NC cs.CV q-bio.QM</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Functional MRI (fMRI) is a powerful technique that has allowed us to
characterize visual cortex responses to stimuli, yet such experiments are by
nature constructed based on a priori hypotheses, limited to the set of images
presented to the individual while they are in the scanner, are subject to noise
in the observed brain responses, and may vary widely across individuals. In
this work, we propose a novel computational strategy, which we call NeuroGen,
to overcome these limitations and develop a powerful tool for human vision
neuroscience discovery. NeuroGen combines an fMRI-trained neural encoding model
of human vision with a deep generative network to synthesize images predicted
to achieve a target pattern of macro-scale brain activation. We demonstrate
that the reduction of noise that the encoding model provides, coupled with the
generative network's ability to produce images of high fidelity, results in a
robust discovery architecture for visual neuroscience. By using only a small
number of synthetic images created by NeuroGen, we demonstrate that we can
detect and amplify differences in regional and individual human brain response
patterns to visual stimuli. We then verify that these discoveries are reflected
in the several thousand observed image responses measured with fMRI. We further
demonstrate that NeuroGen can create synthetic images predicted to achieve
regional response patterns not achievable by the best-matching natural images.
The NeuroGen framework extends the utility of brain encoding models and opens
up a new avenue for exploring, and possibly precisely controlling, the human
visual system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07141</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07141</id><submitter>Nihar Shrikant Bendre</submitter><version version="v1"><date>Sat, 15 May 2021 04:51:51 GMT</date><size>7366kb</size><source_type>D</source_type></version><title>Show Why the Answer is Correct! Towards Explainable AI using
  Compositional Temporal Attention</title><authors>Nihar Bendre, Kevin Desai and Peyman Najafirad</authors><categories>cs.CV</categories><comments>7 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual Question Answering (VQA) models have achieved significant success in
recent times. Despite the success of VQA models, they are mostly black-box
models providing no reasoning about the predicted answer, thus raising
questions for their applicability in safety-critical such as autonomous systems
and cyber-security. Current state of the art fail to better complex questions
and thus are unable to exploit compositionality. To minimize the black-box
effect of these models and also to make them better exploit compositionality,
we propose a Dynamic Neural Network (DMN), which can understand a particular
question and then dynamically assemble various relatively shallow deep learning
modules from a pool of modules to form a network. We incorporate compositional
temporal attention to these deep learning based modules to increase
compositionality exploitation. This results in achieving better understanding
of complex questions and also provides reasoning as to why the module predicts
a particular answer. Experimental analysis on the two benchmark datasets,
VQA2.0 and CLEVR, depicts that our model outperforms the previous approaches
for Visual Question Answering task as well as provides better reasoning, thus
making it reliable for mission critical applications like safety and security.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07142</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07142</id><submitter>Sagnik Majumder</submitter><version version="v1"><date>Sat, 15 May 2021 04:58:08 GMT</date><size>683kb</size><source_type>D</source_type></version><title>Move2Hear: Active Audio-Visual Source Separation</title><authors>Sagnik Majumder, Ziad Al-Halah, Kristen Grauman</authors><categories>cs.CV cs.LG cs.RO cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the active audio-visual source separation problem, where an
agent must move intelligently in order to better isolate the sounds coming from
an object of interest in its environment. The agent hears multiple audio
sources simultaneously (e.g., a person speaking down the hall in a noisy
household) and must use its eyes and ears to automatically separate out the
sounds originating from the target object within a limited time budget. Towards
this goal, we introduce a reinforcement learning approach that trains movement
policies controlling the agent's camera and microphone placement over time,
guided by the improvement in predicted audio separation quality. We demonstrate
our approach in scenarios motivated by both augmented reality (system is
already co-located with the target object) and mobile robotics (agent begins
arbitrarily far from the target object). Using state-of-the-art realistic
audio-visual simulations in 3D environments, we demonstrate our model's ability
to find minimal movement sequences with maximal payoff for audio source
separation. Project: http://vision.cs.utexas.edu/projects/move2hear.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07143</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07143</id><submitter>Monu Verma</submitter><version version="v1"><date>Sat, 15 May 2021 05:10:47 GMT</date><size>29920kb</size><source_type>D</source_type></version><title>One for All: An End-to-End Compact Solution for Hand Gesture Recognition</title><authors>Monu Verma, Ayushi Gupta, santosh kumar Vipparthi</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The HGR is a quite challenging task as its performance is influenced by
various aspects such as illumination variations, cluttered backgrounds,
spontaneous capture, etc. The conventional CNN networks for HGR are following
two stage pipeline to deal with the various challenges: complex signs,
illumination variations, complex and cluttered backgrounds. The existing
approaches needs expert expertise as well as auxiliary computation at stage 1
to remove the complexities from the input images. Therefore, in this paper, we
proposes an novel end-to-end compact CNN framework: fine grained feature
attentive network for hand gesture recognition (Fit-Hand) to solve the
challenges as discussed above. The pipeline of the proposed architecture
consists of two main units: FineFeat module and dilated convolutional (Conv)
layer. The FineFeat module extracts fine grained feature maps by employing
attention mechanism over multiscale receptive fields. The attention mechanism
is introduced to capture effective features by enlarging the average behaviour
of multi-scale responses. Moreover, dilated convolution provides global
features of hand gestures through a larger receptive field. In addition,
integrated layer is also utilized to combine the features of FineFeat module
and dilated layer which enhances the discriminability of the network by
capturing complementary context information of hand postures. The effectiveness
of Fit- Hand is evaluated by using subject dependent (SD) and subject
independent (SI) validation setup over seven benchmark datasets: MUGD-I,
MUGD-II, MUGD-III, MUGD-IV, MUGD-V, Finger Spelling and OUHANDS, respectively.
Furthermore, to investigate the deep insights of the proposed Fit-Hand
framework, we performed ten ablation study.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07144</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07144</id><submitter>Jason Wei</submitter><version version="v1"><date>Sat, 15 May 2021 05:37:42 GMT</date><size>5389kb</size><source_type>D</source_type></version><title>A Cognitive Regularizer for Language Modeling</title><authors>Jason Wei, Clara Meister, and Ryan Cotterell</authors><categories>cs.CL</categories><comments>To appear in ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The uniform information density (UID) hypothesis, which posits that speakers
prefer utterances that distribute information uniformly across the signal, has
gained substantial traction in psycholinguistics as an explanation for certain
syntactic, morphological, and prosodic choices. Could we operationalize uniform
information density as an inductive bias for statistical language modeling? In
this paper, we augment the canonical MLE objective for training language models
by encoding UID as regularization. In experiments on ten languages spanning
five language families, we find that using UID regularization consistently
improves perplexity in language models, having a larger effect when training
data is limited. Moreover, via analysis of generated sequences, we find that
UID-regularized language models are higher-entropy and produce text that is
longer and more lexically diverse. Our results not only suggest that UID is a
reasonable inductive bias for language modeling, but also provide an
alternative validation of the UID hypothesis using modern-day NLP tools.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07145</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07145</id><submitter>Wu-Te Yang</submitter><version version="v1"><date>Sat, 15 May 2021 05:53:01 GMT</date><size>2935kb</size><source_type>D</source_type></version><title>Development of Soft Tactile Sensor for Force Measurement and Position
  Detection</title><authors>Wu-Te Yang, Zhian Kuang, Changhao Wang and Masayoshi Tomizuka</authors><categories>cs.RO</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As more robots are implemented for contact-rich tasks, tactile sensors are in
increasing demand. For many circumstances, the contact is required to be
compliant, and soft sensors are in need. This paper introduces a novelly
designed soft sensor that can simultaneously estimate the contact force and
contact location. Inspired by humans' skin, which contains multi-layers of
receptors, the designed tactile sensor has a dual-layer structure. The first
layer is made of a conductive fabric that is responsible for sensing the
contact force. The second layer is composed of four small conductive rubbers
that can detect the contact location. Signals from the two layers are firstly
processed by Wheatstone bridges and amplifier circuits so that the measurement
noises are eliminated, and the sensitivity is improved. An Arduino chip is used
for processing the signal and analyzing the data. The contact force can be
obtained by a pre-trained model that maps from the voltage to force, and the
contact location is estimated by the voltage signal from the conductive rubbers
in the second layer. In addition, filtering methods are applied to eliminate
the estimation noise. Finally, experiments are provided to show the accuracy
and robustness of the sensor.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07146</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07146</id><submitter>Kecheng Chen</submitter><version version="v1"><date>Sat, 15 May 2021 05:59:01 GMT</date><size>11924kb</size><source_type>D</source_type></version><title>RIDnet: Radiologist-Inspired Deep Neural Network for Low-dose CT
  Denoising</title><authors>Kecheng Chen, Jiayu Sun, Jiang Shen, Jixiang Luo, Xinyu Zhang, Xuelin
  Pan, Dongsheng Wu, Yue Zhao, Miguel Bento, Yazhou Ren and Xiaorong Pu</authors><categories>eess.IV cs.CV</categories><comments>under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Being low-level radiation exposure and less harmful to health, low-dose
computed tomography (LDCT) has been widely adopted in the early screening of
lung cancer and COVID-19. LDCT images inevitably suffer from the degradation
problem caused by complex noises. It was reported that, compared with
commercial iterative reconstruction methods, deep learning (DL)-based LDCT
denoising methods using convolutional neural network (CNN) achieved competitive
performance. Most existing DL-based methods focus on the local information
extracted by CNN, while ignoring both explicit non-local and context
information (which are leveraged by radiologists). To address this issue, we
propose a novel deep learning model named radiologist-inspired deep denoising
network (RIDnet) to imitate the workflow of a radiologist reading LDCT images.
Concretely, the proposed model explicitly integrates all the local, non-local
and context information rather than local information only. Our
radiologist-inspired model is potentially favoured by radiologists as a
familiar workflow. A double-blind reader study on a public clinical dataset
shows that, compared with state-of-the-art methods, our proposed model achieves
the most impressive performance in terms of the structural fidelity, the noise
suppression and the overall score. As a physicians-inspired model, RIDnet gives
a new research roadmap that takes into account the behavior of physicians when
designing decision support tools for assisting clinical diagnosis. Models and
code are available at https://github.com/tonyckc/RIDnet_demo.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07147</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07147</id><submitter>Zhiwen Fan</submitter><version version="v1"><date>Sat, 15 May 2021 06:01:11 GMT</date><size>7505kb</size><source_type>D</source_type></version><title>FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol
  Spotting</title><authors>Zhiwen Fan, Lingjie Zhu, Honghua Li, Xiaohao Chen, Siyu Zhu, Ping Tan</authors><categories>cs.CV</categories><comments>17 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access to large and diverse computer-aided design (CAD) drawings is critical
for developing symbol spotting algorithms. In this paper, we present
FloorPlanCAD, a large-scale real-world CAD drawing dataset containing over
10,000 floor plans, ranging from residential to commercial buildings. CAD
drawings in the dataset are all represented as vector graphics, which enable us
to provide line-grained annotations of 30 object categories. Equipped by such
annotations, we introduce the task of panoptic symbol spotting, which requires
to spot not only instances of countable things, but also the semantic of
uncountable stuff. Aiming to solve this task, we propose a novel method by
combining Graph Convolutional Networks (GCNs) with Convolutional Neural
Networks (CNNs), which captures both non-Euclidean and Euclidean features and
can be trained end-to-end. The proposed CNN-GCN method achieved
state-of-the-art (SOTA) performance on the task of semantic symbol spotting,
and help us build a baseline network for the panoptic symbol spotting task. Our
contributions are three-fold: 1) to the best of our knowledge, the presented
CAD drawing dataset is the first of its kind; 2) the panoptic symbol spotting
task considers the spotting of both thing instances and stuff semantic as one
recognition problem; and 3) we presented a baseline solution to the panoptic
symbol spotting task based on a novel CNN-GCN method, which achieved SOTA
performance on semantic symbol spotting. We believe that these contributions
will boost research in related areas.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07148</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07148</id><submitter>Wei Liu</submitter><version version="v1"><date>Sat, 15 May 2021 06:13:39 GMT</date><size>450kb</size></version><title>Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter</title><authors>Wei Liu, Xiyan Fu, Yue Zhang and Wenming Xiao</authors><categories>cs.CL</categories><comments>accepted by ACL2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexicon information and pre-trained models, such as BERT, have been combined
to explore Chinese sequence labelling tasks due to their respective strengths.
However, existing methods solely fuse lexicon features via a shallow and random
initialized sequence layer and do not integrate them into the bottom layers of
BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese
sequence labelling, which integrates external lexicon knowledge into BERT
layers directly by a Lexicon Adapter layer. Compared with the existing methods,
our model facilitates deep lexicon knowledge fusion at the lower layers of
BERT. Experiments on ten Chinese datasets of three tasks including Named Entity
Recognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT
achieves the state-of-the-art results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07149</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07149</id><submitter>Qu Cui</submitter><version version="v1"><date>Sat, 15 May 2021 06:18:49 GMT</date><size>1530kb</size><source_type>D</source_type></version><title>DirectQE: Direct Pretraining for Machine Translation Quality Estimation</title><authors>Qu Cui, Shujian Huang, Jiahuan Li, Xiang Geng, Zaixiang Zheng, Guoping
  Huang, Jiajun Chen</authors><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Translation Quality Estimation (QE) is a task of predicting the
quality of machine translations without relying on any reference. Recently, the
predictor-estimator framework trains the predictor as a feature extractor,
which leverages the extra parallel corpora without QE labels, achieving
promising QE performance. However, we argue that there are gaps between the
predictor and the estimator in both data quality and training objectives, which
preclude QE models from benefiting from a large number of parallel corpora more
directly. We propose a novel framework called DirectQE that provides a direct
pretraining for QE tasks. In DirectQE, a generator is trained to produce pseudo
data that is closer to the real QE data, and a detector is pretrained on these
data with novel objectives that are akin to the QE task. Experiments on widely
used benchmarks show that DirectQE outperforms existing methods, without using
any pretraining models such as BERT. We also give extensive analyses showing
how fixing the two gaps contributes to our improvements.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07153</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07153</id><submitter>Abdullah-Al-Zubaer Imran</submitter><version version="v1"><date>Sat, 15 May 2021 07:01:07 GMT</date><size>1533kb</size><source_type>D</source_type></version><title>Window-Level is a Strong Denoising Surrogate</title><authors>Ayaan Haque, Adam Wang, Abdullah-Al-Zubaer Imran</authors><categories>eess.IV cs.CV</categories><comments>11 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  CT image quality is heavily reliant on radiation dose, which causes a
trade-off between radiation dose and image quality that affects the subsequent
image-based diagnostic performance. However, high radiation can be harmful to
both patients and operators. Several (deep learning-based) approaches have been
attempted to denoise low dose images. However, those approaches require access
to large training sets, specifically the full dose CT images for reference,
which can often be difficult to obtain. Self-supervised learning is an emerging
alternative for lowering the reference data requirement facilitating
unsupervised learning. Currently available self-supervised CT denoising works
are either dependent on foreign domain or pretexts are not very task-relevant.
To tackle the aforementioned challenges, we propose a novel self-supervised
learning approach, namely Self-Supervised Window-Leveling for Image DeNoising
(SSWL-IDN), leveraging an innovative, task-relevant, simple, yet effective
surrogate -- prediction of the window-leveled equivalent. SSWL-IDN leverages
residual learning and a hybrid loss combining perceptual loss and MSE, all
incorporated in a VAE framework. Our extensive (in- and cross-domain)
experimentation demonstrates the effectiveness of SSWL-IDN in aggressive
denoising of CT (abdomen and chest) images acquired at 5\% dose level only.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07157</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07157</id><submitter>Yuyang Wei</submitter><version version="v1"><date>Sat, 15 May 2021 07:08:30 GMT</date><size>29kb</size></version><title>A Feature Table approach to decomposing monolithic applications into
  microservices</title><authors>Yuyang Wei, Yijun Yu, Minxue Pan, Tian Zhang</authors><categories>cs.SE</categories><doi>10.1145/3457913.3457939</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microservice architecture refers to the use of numerous small-scale and
independently deployed services, instead of encapsulating all functions into
one monolith. It has been a challenge in software engineering to decompose a
monolithic system into smaller parts. In this paper, we propose the Feature
Table approach, a structured approach to service decomposition based on the
correlation between functional features and microservices: (1) we defined the
concept of {\em Feature Cards} and 12 instances of such cards; (2) we
formulated {\em Decomposition Rules} to decompose monolithic applications; (3)
we designed the {\em Feature Table Analysis Tool} to provide semi-automatic
analysis for identification of microservices; and (4) we formulated {\em
Mapping Rules} to help developers implement microservice candidates. We
performed a case study on Cargo Tracking System to validate our
microservice-oriented decomposition approach. Cargo Tracking System is a
typical case that has been decomposed by other related methods (dataflow-driven
approach, Service Cutter, and API Analysis). Through comparison with the
related methods in terms of specific coupling and cohesion metrics, the results
show that the proposed Feature Table approach can deliver more reasonable
microservice candidates, which are feasible in implementation with
semi-automatic support.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07161</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07161</id><submitter>Felix Zhou</submitter><version version="v1"><date>Sat, 15 May 2021 07:25:21 GMT</date><size>156kb</size></version><title>On the Complexity of Nucleolus Computation for Bipartite b-Matching
  Games</title><authors>Jochen Koenemann, Justin Toth, Felix Zhou</authors><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the complexity of nucleolus computation in b-matching games on
bipartite graphs. We show that computing the nucleolus of a simple b-matching
game is NP-hard even on bipartite graphs of maximum degree 7. We complement
this with partial positive results in the special case where b values are
bounded by 2. In particular, we describe an efficient algorithm when a constant
number of vertices satisfy b(v) = 2 as well as an efficient algorithm for
computing the non-simple b-matching nucleolus when b = 2.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07165</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07165</id><submitter>Deepak Kumar</submitter><version version="v1"><date>Sat, 15 May 2021 07:41:30 GMT</date><size>1703kb</size></version><title>Electromagnetic Crimping on Threaded Surface: FEM Modelling, Validation
  and Effects of Pitch and Discharge Energy on Deformation in an Empirical
  Relation</title><authors>Deepak Kumar, Shafeeque E. S., Sachin D. Kore, Arup Nandy</authors><categories>hep-ex cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electromagnetic crimping is a high-velocity joining method to join highly
conductive workpieces where a pulsed magnetic field is applied without any
working medium or mechanical contact to deform the workpiece. This work
explores tube-to-tube joining of Copper outer tube and Stainless steel threaded
inner tube using electromagnetic crimping. A non-coupled simulation model is
developed for the finite element analysis. ANSYS Maxwell package is used to
obtain the magnetic field intensity, which is later converted to pressure using
an analytical equation, and this pressure is applied to the two-tube working
domain in ANSYS Explicit Dynamics. Numerical simulations are done for different
combinations of discharge energies and pitches of the thread to analyse
deformation, stress and strain. The converged finite element results are
validated using experimental data. The amount of deformation is found to be
proportional to discharge energy and the pitch of the thread used. An empirical
relation is developed for the deformation as a function of discharge energy and
pitch. The relation is able to predict the deformation for other discharge
energies, which is later verified with ANSYS simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07167</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07167</id><submitter>Olivier Rioul</submitter><version version="v1"><date>Sat, 15 May 2021 07:57:46 GMT</date><size>36kb</size><source_type>D</source_type></version><title>On Conditional $\alpha$-Information and its Application to Side-Channel
  Analysis</title><authors>Yi Liu, Wei Cheng, Sylvain Guilley, and Olivier Rioul</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conditional version of Sibson's $\alpha$-information is defined using a
simple closed-form &quot;log-expectation&quot; expression, which satisfies important
properties such as consistency, uniform expansion, and data processing
inequalities. This definition is compared to previous ones, which in contrast
do not satisfy all of these properties. Based on our proposal and on a
generalized Fano inequality, we extend the case $\alpha = 1$ of previous works
to obtain sharp universal upper bounds for the probability of success of any
type side-channel attack, particularly when $\alpha = 2$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07168</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07168</id><submitter>Masayoshi Mase</submitter><version version="v1"><date>Sat, 15 May 2021 08:02:18 GMT</date><size>809kb</size><source_type>D</source_type></version><title>Cohort Shapley value for algorithmic fairness</title><authors>Masayoshi Mase, Art B. Owen, Benjamin B. Seiler</authors><categories>cs.LG cs.AI econ.EM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cohort Shapley value is a model-free method of variable importance grounded
in game theory that does not use any unobserved and potentially impossible
feature combinations. We use it to evaluate algorithmic fairness, using the
well known COMPAS recidivism data as our example. This approach allows one to
identify for each individual in a data set the extent to which they were
adversely or beneficially affected by their value of a protected attribute such
as their race. The method can do this even if race was not one of the original
predictors and even if it does not have access to a proprietary algorithm that
has made the predictions. The grounding in game theory lets us define aggregate
variable importance for a data set consistently with its per subject
definitions. We can investigate variable importance for multiple quantities of
interest in the fairness literature including false positive predictions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07172</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07172</id><submitter>Masoud Hayeri Khyavi</submitter><version version="v1"><date>Sat, 15 May 2021 08:19:41 GMT</date><size>467kb</size></version><title>Rescue Network: Using UAVs (drones) in Earthquake Crisis Management</title><authors>Masoud Hayeri Khyavi</authors><categories>cs.NI cs.CY cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Earthquake is one of the natural disasters which cannot be either controlled
or predicted absolutely. Since preventing earthquake is impossible, preventing
its damages is also difficult. Unfortunately, after each earthquake and its
financial and life losses, the initial panic of the people results in the
second wave of accidents and damages. Inrush of confused people to escape the
cities, streets and houses is a great problem. Apart from training in seismic
areas which is very important, considering security arrangements and observing
security principles in construction, instructing the people is also important.
Other than searching for and rescuing the people who are trapped under
detrimental or are in danger, those who thieve the damaged area is another
important issue after each earthquake. Thus, a solution is proposed to use
modern technology to reduce threats of natural disasters including earthquake.
Today, UAVs are being used in natural disasters and accidents. To this end and
considering the ever-increasing developments of network technologies and
communication including IoT and cloud, an efficient design is presented which
increases rescue factor of live creatures in natural disasters that can be used
to rescue human lives and prevent subsequent outcomes after a few seconds. In
this study, focus is on time of occurrence of earthquake and after earthquake
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07174</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07174</id><submitter>Saikat Dutta</submitter><version version="v1"><date>Sat, 15 May 2021 08:45:20 GMT</date><size>14235kb</size><source_type>D</source_type></version><title>Stacked Deep Multi-Scale Hierarchical Network for Fast Bokeh Effect
  Rendering from a Single Image</title><authors>Saikat Dutta, Sourya Dipta Das, Nisarg A. Shah, Anil Kumar Tiwari</authors><categories>cs.CV cs.AI</categories><comments>Accepted to MAI workshop, CVPR 2021. Code and models:
  https://github.com/saikatdutta/Stacked_DMSHN_bokeh</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Bokeh Effect is one of the most desirable effects in photography for
rendering artistic and aesthetic photos. Usually, it requires a DSLR camera
with different aperture and shutter settings and certain photography skills to
generate this effect. In smartphones, computational methods and additional
sensors are used to overcome the physical lens and sensor limitations to
achieve such effect. Most of the existing methods utilized additional sensor's
data or pretrained network for fine depth estimation of the scene and sometimes
use portrait segmentation pretrained network module to segment salient objects
in the image. Because of these reasons, networks have many parameters, become
runtime intensive and unable to run in mid-range devices. In this paper, we
used an end-to-end Deep Multi-Scale Hierarchical Network (DMSHN) model for
direct Bokeh effect rendering of images captured from the monocular camera. To
further improve the perceptual quality of such effect, a stacked model
consisting of two DMSHN modules is also proposed. Our model does not rely on
any pretrained network module for Monocular Depth Estimation or Saliency
Detection, thus significantly reducing the size of model and run time. Stacked
DMSHN achieves state-of-the-art results on a large scale EBB! dataset with
around 6x less runtime compared to the current state-of-the-art model in
processing HD quality images.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07175</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07175</id><submitter>Tianrui Hui</submitter><version version="v1"><date>Sat, 15 May 2021 08:55:51 GMT</date><size>2459kb</size><source_type>D</source_type></version><title>Cross-Modal Progressive Comprehension for Referring Segmentation</title><authors>Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, Guanbin Li</authors><categories>cs.CV cs.MM</categories><comments>Accepted by TPAMI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a natural language expression and an image/video, the goal of referring
segmentation is to produce the pixel-level masks of the entities described by
the subject of the expression. Previous approaches tackle this problem by
implicit feature interaction and fusion between visual and linguistic
modalities in a one-stage manner. However, human tends to solve the referring
problem in a progressive manner based on informative words in the expression,
i.e., first roughly locating candidate entities and then distinguishing the
target one. In this paper, we propose a Cross-Modal Progressive Comprehension
(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I
(Image) module and a CMPC-V (Video) module to improve referring image and video
segmentation models. For image data, our CMPC-I module first employs entity and
attribute words to perceive all the related entities that might be considered
by the expression. Then, the relational words are adopted to highlight the
target entity as well as suppress other irrelevant ones by spatial graph
reasoning. For video data, our CMPC-V module further exploits action words
based on CMPC-I to highlight the correct entity matched with the action cues by
temporal graph reasoning. In addition to the CMPC, we also introduce a simple
yet effective Text-Guided Feature Exchange (TGFE) module to integrate the
reasoned multimodal features corresponding to different levels in the visual
backbone under the guidance of textual information. In this way, multi-level
features can communicate with each other and be mutually refined based on the
textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or
video version referring segmentation frameworks and our frameworks achieve new
state-of-the-art performances on four referring image segmentation benchmarks
and three referring video segmentation benchmarks respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07176</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07176</id><submitter>Natasha Fernandes</submitter><version version="v1"><date>Sat, 15 May 2021 09:11:50 GMT</date><size>276kb</size><source_type>D</source_type></version><title>The Laplace Mechanism has optimal utility for differential privacy over
  continuous queries</title><authors>Natasha Fernandes and Annabelle McIver and Carroll Morgan</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential Privacy protects individuals' data when statistical queries are
published from aggregated databases: applying &quot;obfuscating&quot; mechanisms to the
query results makes the released information less specific but, unavoidably,
also decreases its utility. Yet it has been shown that for discrete data (e.g.
counting queries), a mandated degree of privacy and a reasonable interpretation
of loss of utility, the Geometric obfuscating mechanism is optimal: it loses as
little utility as possible. For continuous query results however (e.g. real
numbers) the optimality result does not hold. Our contribution here is to show
that optimality is regained by using the Laplace mechanism for the obfuscation.
The technical apparatus involved includes the earlier discrete result by Ghosh
et al., recent work on abstract channels and their geometric representation as
hyper-distributions, and the dual interpretations of distance between
distributions provided by the Kantorovich-Rubinstein Theorem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07179</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07179</id><submitter>Hanfeng Zhai</submitter><version version="v1"><date>Sat, 15 May 2021 09:17:56 GMT</date><size>18678kb</size><source_type>D</source_type></version><title>Inferring micro-bubble dynamics with physics-informed deep learning</title><authors>Hanfeng Zhai, Guohui Hu</authors><categories>physics.flu-dyn cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Micro-bubbles and bubbly flows are widely observed and applied to medicine,
involves deformation, rupture, and collision of bubbles, phase mixture, etc. We
study bubble dynamics by setting up two numerical simulation cases: bubbly flow
with a single bubble and multiple bubbles, both confined in the microtube, with
parameters corresponding to their medical backgrounds. Both the cases have
their medical background applications. Multiphase flow simulation requires high
computation accuracy due to possible component losses that may be caused by
sparse meshing during the computation. Hence, data-driven methods can be
adopted as a useful tool. Based on physics-informed neural networks (PINNs), we
propose a novel deep learning framework BubbleNet, which entails three main
parts: deep neural networks (DNN) with sub nets for predicting different
physics fields; the physics-informed part, with the fluid continuum condition
encoded within; the time discretized normalizer (TDN), an algorithm to
normalize field data per time step before training. We apply the traditional
DNN and our BubbleNet to train the simulation data and predict the physics
fields of both the two bubbly flow cases. Results indicate our framework can
predict the physics fields more accurately, estimating the prediction absolute
errors. The proposed network can potentially be applied to many other
engineering fields.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07181</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07181</id><submitter>Xinyu Peng</submitter><version version="v1"><date>Sat, 15 May 2021 09:20:36 GMT</date><size>1152kb</size><source_type>D</source_type></version><title>Drill the Cork of Information Bottleneck by Inputting the Most Important
  Data</title><authors>Xinyu Peng, Jiawei Zhang, Fei-Yue Wang and Li Li</authors><categories>cs.LG</categories><comments>11 pages, to be published in IEEE Transactions on Neural Networks and
  Learning Systems</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning has become the most powerful machine learning tool in the last
decade. However, how to efficiently train deep neural networks remains to be
thoroughly solved. The widely used minibatch stochastic gradient descent (SGD)
still needs to be accelerated. As a promising tool to better understand the
learning dynamic of minibatch SGD, the information bottleneck (IB) theory
claims that the optimization process consists of an initial fitting phase and
the following compression phase. Based on this principle, we further study
typicality sampling, an efficient data selection method, and propose a new
explanation of how it helps accelerate the training process of the deep
networks. We show that the fitting phase depicted in the IB theory will be
boosted with a high signal-to-noise ratio of gradient approximation if the
typicality sampling is appropriately adopted. Furthermore, this finding also
implies that the prior information of the training set is critical to the
optimization process and the better use of the most important data can help the
information flow through the bottleneck faster. Both theoretical analysis and
experimental results on synthetic and real-world datasets demonstrate our
conclusions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07182</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07182</id><submitter>Tom Luan</submitter><version version="v1"><date>Sat, 15 May 2021 09:25:11 GMT</date><size>3229kb</size><source_type>D</source_type></version><title>The Paradigm of Digital Twin Communications</title><authors>Tom H. Luan, Ruhan Liu, Longxiang Gao, Rui Li, Haibo Zhou</authors><categories>cs.NI</categories><comments>7 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the fast evolving of cloud computing and artificial intelligence (AI),
the concept of digital twin (DT) has recently been proposed and finds broad
applications in industrial Internet, IoT, smart city, etc. The DT builds a
mirror integrated multi-physics of the physical system in the digital space. By
doing so, the DT can utilize the rich computing power and AI at the cloud to
operate on the mirror physical system, and accordingly provides feedbacks to
help the real-world physical system in their practical task completion. The
existing literature mainly considers DT as a simulation/emulation approach,
whereas the communication framework for DT has not been clearly defined and
discussed. In this article, we describe the basic DT communication models and
present the open research issues. By combining wireless communications,
artificial intelligence (AI) and cloud computing, we show that the DT
communication provides a novel framework for futuristic mobile agent systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07183</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07183</id><submitter>Anton V. Proskurnikov</submitter><version version="v1"><date>Sat, 15 May 2021 09:34:49 GMT</date><size>549kb</size></version><title>Delay Robustness of Consensus Algorithms: Beyond The Uniform
  Connectivity (Extended Version)</title><authors>Anton V. Proskurnikov and Giuseppe Carlo Calafiore</authors><categories>math.OC cs.MA cs.SY eess.SY math.DS nlin.AO</categories><comments>a shortened version is submitted to IEEE TAC</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Consensus of autonomous agents is a benchmark problem in multi-agent control.
In this paper, we consider continuous-time averaging consensus policies (or
Laplacian flows) and their discrete-time counterparts over time-varying graphs
in presence of unknown but bounded communication delays. It is known that
consensus is established (no matter how large the delays are) if the graph is
periodically, or uniformly quasi-strongly connected (UQSC). The UQSC condition
is often believed to be the weakest sufficient condition under which consensus
can be proved. We show that the UQSC condition can actually be substantially
relaxed and replaced by a condition that we call aperiodic quasi-strong
connectivity (AQSC), which, in some sense, proves to be very close to the
necessary condition of integral connectivity. Furthermore, in some special
situations such as undirected or type-symmetric graph, we find a necessary and
sufficient condition for consensus in presence of bounded delay; the relevant
results have been previously proved only in the undelayed case. The consensus
criteria established in this paper generalize a number of results known in the
literature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07187</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07187</id><submitter>Carlos Pedro Gon\c{c}alves</submitter><version version="v1"><date>Sat, 15 May 2021 09:42:36 GMT</date><size>161kb</size><source_type>D</source_type></version><title>Cyberattacks on Quantum Networked Computation and Communications --
  Hacking the Superdense Coding Protocol on IBM's Quantum Computers</title><authors>Carlos Pedro Gon\c{c}alves</authors><categories>quant-ph cs.CR cs.CY cs.ET</categories><msc-class>81P68, 94A40, 94-08, 94-05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of automated gate specification for quantum communications
and quantum networked computation opens up the way for malware designed at
corrupting the automation software, changing the automated quantum
communications protocols and algorithms. We study two types of attacks on
automated quantum communications protocols and simulate these attacks on the
superdense coding protocol, using remote access to IBM's Quantum Computers
available through IBM Q Experience to simulate these attacks on what would be a
low noise quantum communications network. The first type of attack leads to a
hacker-controlled bijective transformation of the final measured strings, the
second type of attack is a unitary scrambling attack that modifies the
automated gate specification to effectively scramble the final measurement,
disrupting quantum communications and taking advantage of quantum randomness
upon measurement in a way that makes it difficult to distinguish from hardware
malfunction or from a sudden rise in environmental noise. We show that, due to
quantum entanglement and symmetries, the second type of attack works as a way
to strategically disrupt quantum communications networks and quantum networked
computation in a way that makes it difficult to ascertain which node was
attacked. The main findings are discussed in the wider setting of quantum
cybersecurity and quantum networked computation, where ways of hacking
including the role of insider threats are discussed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07189</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07189</id><submitter>Peter Kokol PhD</submitter><version version="v1"><date>Sat, 15 May 2021 09:48:46 GMT</date><size>393kb</size></version><title>Content Analysis Application in Nursing: A Synthetic Knowledge Synthesis
  Meta-Study</title><authors>Helena Bla\v{z}un Vo\v{s}ner, Peter Kokol, Jernej Zavr\v{s}nik, Danica
  \v{Z}eleznik</authors><categories>cs.DL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Theoretical issues: With the explosive growth in the research literature
production, the need for new approaches to structure knowledge emerged. Method:
Synthetic content analysis was used in our meta-study. Results and discussion:
Our meta-study showed that content analysis is frequently used in nursing
research in a very wide spectrum of applications. The trend of its use is
positive and it is used globally in a variety of research settings. The
synthetic content analysis used in our study showed to be a very helpful tool
in performing knowledge synthesis, replacing many of the routine activities of
conventional synthesis with automated activities this making such studies more
economically viable and easier to perform.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07190</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07190</id><submitter>Gesina Schwalbe</submitter><version version="v1"><date>Sat, 15 May 2021 09:52:00 GMT</date><size>229kb</size><source_type>D</source_type></version><title>XAI Method Properties: A (Meta-)study</title><authors>Gesina Schwalbe, Bettina Finzel</authors><categories>cs.LG cs.AI</categories><comments>37 pages, 2 figures, submitted to Data Mining and Knowledge Discovery</comments><acm-class>I.2.0; I.2.6; I.2.m</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the meantime, a wide variety of terminologies, motivations, approaches and
evaluation criteria have been developed within the scope of research on
explainable artificial intelligence (XAI). Many taxonomies can be found in the
literature, each with a different focus, but also showing many points of
overlap. In this paper, we summarize the most cited and current taxonomies in a
meta-analysis in order to highlight the essential aspects of the
state-of-the-art in XAI. We also present and add terminologies as well as
concepts from a large number of survey articles on the topic. Last but not
least, we illustrate concepts from the higher-level taxonomy with more than 50
example methods, which we categorize accordingly, thus providing a wide-ranging
overview of aspects of XAI and paving the way for use case-appropriate as well
as context-specific subsequent research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07193</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07193</id><submitter>Vishal Kumar</submitter><version version="v1"><date>Sat, 15 May 2021 10:06:13 GMT</date><size>2468kb</size><source_type>D</source_type></version><title>Make Bipedal Robots Learn How to Imitate</title><authors>Vishal Kumar and Sinnu Susan Thomas</authors><categories>cs.RO cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bipedal robots do not perform well as humans since they do not learn to walk
like we do. In this paper we propose a method to train a bipedal robot to
perform some basic movements with the help of imitation learning (IL) in which
an instructor will perform the movement and the robot will try to mimic the
instructor movement. To the best of our knowledge, this is the first time we
train the robot to perform movements with a single video of the instructor and
as the training is done based on joint angles the robot will keep its joint
angles always in physical limits which in return help in faster training. The
joints of the robot are identified by OpenPose architecture and then joint
angle data is extracted with the help of angle between three points resulting
in a noisy solution. We smooth the data using Savitzky-Golay filter and
preserve the Simulatore data anatomy. An ingeniously written Deep Q Network
(DQN) is trained with experience replay to make the robot learn to perform the
movements as similar as the instructor. The implementation of the paper is made
publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07194</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07194</id><submitter>Cheng Qiu</submitter><version version="v1"><date>Sat, 15 May 2021 10:10:47 GMT</date><size>5463kb</size><source_type>D</source_type></version><title>An even-load-distribution design for composite bolted joints using a
  novel circuit model and artificial neural networks</title><authors>Cheng Qiu, Yuzi Han, Logesh Shanmugam, Fengyang Jiang, Zhidong Guan,
  Shanyi Du, Jinglei Yang</authors><categories>cs.LG physics.app-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Due to the brittle feature of carbon fiber reinforced plastic laminates,
mechanical multi-joint within these composite components show uneven load
distribution for each bolt, which weaken the strength advantage of composite
laminates. In order to reduce this defect and achieve the goal of even load
distribution in mechanical joints, we propose a machine learning-based
framework as an optimization method. Since that the friction effect has been
proven to be a significant factor in determining bolt load distribution, our
framework aims at providing optimal parameters including bolt-hole clearances
and tightening torques for a minimum unevenness of bolt load. A novel circuit
model is established to generate data samples for the training of artificial
networks at a relatively low computational cost. A database for all the
possible inputs in the design space is built through the machine learning
model. The optimal dataset of clearances and torques provided by the database
is validated by both the finite element method, circuit model, and an
experimental measurement based on the linear superposition principle, which
shows the effectiveness of this general framework for the optimization problem.
Then, our machine learning model is further compared and worked in
collaboration with commonly used optimization algorithms, which shows the
potential of greatly increasing computational efficiency for the inverse design
problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07196</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07196</id><submitter>Jitkomut Songsiri</submitter><version version="v1"><date>Sat, 15 May 2021 10:29:02 GMT</date><size>4137kb</size><source_type>D</source_type></version><title>Joint estimation of multiple Granger causal networks: Inference of
  group-level brain connectivity</title><authors>Parinthorn Manomaisaowapak and Jitkomut Songsiri</authors><categories>cs.LG eess.SP q-bio.NC</categories><comments>22 pages, 9 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper considers joint learning of multiple sparse Granger graphical
models to discover underlying common and differential Granger causality (GC)
structures across multiple time series. This can be applied to drawing
group-level brain connectivity inferences from a homogeneous group of subjects
or discovering network differences among groups of signals collected under
heterogeneous conditions. By recognizing that the GC of a single multivariate
time series can be characterized by common zeros of vector autoregressive (VAR)
lag coefficients, a group sparse prior is included in joint regularized
least-squares estimations of multiple VAR models. Group-norm regularizations
based on group- and fused-lasso penalties encourage a decomposition of multiple
networks into a common GC structure, with other remaining parts defined in
individual-specific networks. Prior information about sparseness and sparsity
patterns of desired GC networks are incorporated as relative weights, while a
non-convex group norm in the penalty is proposed to enhance the accuracy of
network estimation in low-sample settings. Extensive numerical results on
simulations illustrated our method's improvements over existing sparse
estimation approaches on GC network sparsity recovery. Our methods were also
applied to available resting-state fMRI time series from the ADHD-200 data sets
to learn the differences of causality mechanisms, called effective brain
connectivity, between adolescents with ADHD and typically developing children.
Our analysis revealed that parts of the causality differences between the two
groups often resided in the orbitofrontal region and areas associated with the
limbic system, which agreed with clinical findings and data-driven results in
previous studies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07197</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07197</id><submitter>Shikhar Tuli</submitter><version version="v1"><date>Sat, 15 May 2021 10:33:35 GMT</date><size>7322kb</size><source_type>D</source_type></version><title>Are Convolutional Neural Networks or Transformers more like human
  vision?</title><authors>Shikhar Tuli, Ishita Dasgupta, Erin Grant, Thomas L. Griffiths</authors><categories>cs.CV</categories><comments>Accepted at CogSci 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modern machine learning models for computer vision exceed humans in accuracy
on specific visual recognition tasks, notably on datasets like ImageNet.
However, high accuracy can be achieved in many ways. The particular decision
function found by a machine learning system is determined not only by the data
to which the system is exposed, but also the inductive biases of the model,
which are typically harder to characterize. In this work, we follow a recent
trend of in-depth behavioral analyses of neural network models that go beyond
accuracy as an evaluation metric by looking at patterns of errors. Our focus is
on comparing a suite of standard Convolutional Neural Networks (CNNs) and a
recently-proposed attention-based network, the Vision Transformer (ViT), which
relaxes the translation-invariance constraint of CNNs and therefore represents
a model with a weaker set of inductive biases. Attention-based networks have
previously been shown to achieve higher accuracy than CNNs on vision tasks, and
we demonstrate, using new metrics for examining error consistency with more
granularity, that their errors are also more consistent with those of humans.
These results have implications both for building more human-like vision
models, as well as for understanding visual object recognition in humans.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07199</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07199</id><submitter>Linbin Huang</submitter><version version="v1"><date>Sat, 15 May 2021 11:08:51 GMT</date><size>2676kb</size><source_type>D</source_type></version><title>Robust Data-Enabled Predictive Control: Tractable Formulations and
  Performance Guarantees</title><authors>Linbin Huang, Jianzhe Zhen, John Lygeros, Florian D\&quot;orfler</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a general framework for robust data-enabled predictive control
(DeePC) for linear time-invariant (LTI) systems. The proposed framework enables
us to obtain model-free optimal control for LTI systems based on noisy
input/output data. More specifically, robust DeePC solves a min-max
optimization problem to compute the optimal control sequence that is resilient
to all possible realizations of the uncertainties in the input/output data
within a prescribed uncertainty set. We present computationally tractable
reformulations of the min-max problem with various uncertainty sets.
Furthermore, we show that even though an accurate prediction of the future
behavior is unattainable in practice due to inaccessibility of the perfect
input/output data, the obtained robust optimal control sequence provides
performance guarantees for the actually realized input/output cost. We further
show that the robust DeePC generalizes and robustifies the regularized DeePC
(with quadratic regularization or 1-norm regularization) proposed in the
literature. Finally, we demonstrate the performance of the proposed robust
DeePC algorithm on high-fidelity, nonlinear, and noisy simulations of a
grid-connected power converter system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07200</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07200</id><submitter>Gang Yu</submitter><version version="v1"><date>Sat, 15 May 2021 11:09:05 GMT</date><size>1922kb</size></version><title>Multi-scale super-resolution generation of low-resolution scanned
  pathological images</title><authors>Yanhua Gao (1), Ting Xie (2), Xun Wang (2), Qingqing Yang (2), Le Chen
  (2), Kai Sun (2), Youmin Guo (1), Gang Yu (2), Kuansong Wang (3) ((1)
  Department of Medical Imaging, The First Affiliated Hospital of Xi'an
  Jiaotong University, 277 Yanta West Road, Xi'an, 710061, China. (2)
  Department of Biomedical Engineering, School of Basic Medical Sciences,
  Central South University, 172 Tongzipo Road, Changsha, 410013, China. (3)
  Department of Pathology, School of Basic Medical Sciences, Central South
  University, 172 Tongzipo Road, Changsha, 410013, China.)</authors><categories>eess.IV cs.CV</categories><comments>27 pages,12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital pathology slide is easy to store and manage, convenient to browse and
transmit. However, because of the high-resolution scan for example 40 times
magnification(40X) during the digitization, the file size of each whole slide
image exceeds 1Gigabyte, which eventually leads to huge storage capacity and
very slow network transmission. We design a strategy to scan slides with low
resolution (5X) and a super-resolution method is proposed to restore the image
details when in diagnosis. The method is based on a multi-scale generative
adversarial network, which sequentially generate three high-resolution images
such as 10X, 20X and 40X. The perceived loss, generator loss of the generated
images and real images are compared on three image resolutions, and a
discriminator is used to evaluate the difference of highest-resolution
generated image and real image. A dataset consisting of 100,000 pathological
images from 10 types of human tissues is performed for training and testing the
network. The generated images have high peak-signal-to-noise-ratio (PSNR) and
structural-similarity-index (SSIM). The PSNR of 10X to 40X image are 24.16,
22.27 and 20.44, and the SSIM are 0.845, 0.680 and 0.512, which are better than
other super-resolution networks such as DBPN, ESPCN, RDN, EDSR and MDSR.
Moreover, visual inspections show that the generated high-resolution images by
our network have enough details for diagnosis, good color reproduction and
close to real images, while other five networks are severely blurred, local
deformation or miss important details. Moreover, no significant differences can
be found on pathological diagnosis based on the generated and real images. The
proposed multi-scale network can generate good high-resolution pathological
images, and will provide a low-cost storage (about 15MB/image on 5X), faster
image sharing method for digital pathology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07202</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07202</id><submitter>Berkan H\&quot;oke</submitter><version version="v1"><date>Sat, 15 May 2021 11:29:12 GMT</date><size>180kb</size><source_type>D</source_type></version><title>FOGA: Flag Optimization with Genetic Algorithm</title><authors>Burak Ta\u{g}tekin, Berkan H\&quot;oke, Mert Kutay Sezer, Mahiye
  Uluya\u{g}mur \&quot;Ozt\&quot;urk</authors><categories>cs.NE</categories><comments>6 pages, 7 figures, to be published in IEEE INISTA 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, program autotuning has become very popular especially in embedded
systems, when we have limited resources such as computing power and memory
where these systems run generally time-critical applications. Compiler
optimization space gradually expands with the renewed compiler options and
inclusion of new architectures. These advancements bring autotuning even more
important position. In this paper, we introduced Flag Optimization with Genetic
Algorithm (FOGA) as an autotuning solution for GCC flag optimization. FOGA has
two main advantages over the other autotuning approaches: the first one is the
hyperparameter tuning of the genetic algorithm (GA), the second one is the
maximum iteration parameter to stop when no further improvement occurs. We
demonstrated remarkable speedup in the execution time of C++ source codes with
the help of optimization flags provided by FOGA when compared to the state of
the art framework OpenTuner.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07203</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07203</id><submitter>Grzegorz Kwasniewski</submitter><version version="v1"><date>Sat, 15 May 2021 11:35:00 GMT</date><size>911kb</size><source_type>D</source_type></version><title>Pebbles, Graphs, and a Pinch of Combinatorics: Towards Tight I/O Lower
  Bounds for Statically Analyzable Programs</title><authors>Grzegorz Kwasniewski, Tal Ben-Nun, Lukas Gianinazzi, Alexandru
  Calotoiu, Timo Schneider, Alexandros Nikolaos Ziogas, Maciej Besta, Torsten
  Hoefler</authors><categories>cs.CC</categories><comments>13 pages, 4 figures, published at Proceedings of the 33rd ACM
  Symposium on Parallelism in Algorithms and Architectures (SPAA'21)</comments><doi>10.1145/3409964.3461796</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining I/O lower bounds is a crucial step in obtaining
communication-efficient parallel algorithms, both across the memory hierarchy
and between processors. Current approaches either study specific algorithms
individually, disallow programmatic motifs such as recomputation, or produce
asymptotic bounds that exclude important constants. We propose a novel approach
for obtaining precise I/O lower bounds on a general class of programs, which we
call Simple Overlap Access Programs (SOAP). SOAP analysis covers a wide variety
of algorithms, from ubiquitous computational kernels to full scientific
computing applications. Using the red-blue pebble game and combinatorial
methods, we are able to bound the I/O of the SOAP-induced Computational
Directed Acyclic Graph (CDAG), taking into account multiple statements,
input/output reuse, and optimal tiling. To deal with programs that are outside
of our representation (e.g., non-injective access functions), we describe
methods to approximate them with SOAP. To demonstrate our method, we analyze 38
different applications, including kernels from the Polybench benchmark suite,
deep learning operators, and -- for the first time -- applications in
unstructured physics simulations, numerical weather prediction stencil
compositions, and full deep neural networks. We derive tight I/O bounds for
several linear algebra kernels, such as Cholesky decomposition, improving the
existing reported bounds by a factor of two. For stencil applications, we
improve the existing bounds by a factor of up to 14. We implement our method as
an open-source tool, which can derive lower bounds directly from provided C
code.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07205</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07205</id><submitter>Fenglin Liu</submitter><version version="v1"><date>Sat, 15 May 2021 11:44:49 GMT</date><size>503kb</size><source_type>D</source_type></version><title>Rethinking Skip Connection with Layer Normalization in Transformers and
  ResNets</title><authors>Fenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun, Yuexian Zou</authors><categories>cs.LG cs.CL cs.CV</categories><comments>Accepted by COLING2020 (The 28th International Conference on
  Computational Linguistics (COLING 2020))</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Skip connection, is a widely-used technique to improve the performance and
the convergence of deep neural networks, which is believed to relieve the
difficulty in optimization due to non-linearity by propagating a linear
component through the neural network layers. However, from another point of
view, it can also be seen as a modulating mechanism between the input and the
output, with the input scaled by a pre-defined value one. In this work, we
investigate how the scale factors in the effectiveness of the skip connection
and reveal that a trivial adjustment of the scale will lead to spurious
gradient exploding or vanishing in line with the deepness of the models, which
could be addressed by normalization, in particular, layer normalization, which
induces consistent improvements over the plain skip connection. Inspired by the
findings, we further propose to adaptively adjust the scale of the input by
recursively applying skip connection with layer normalization, which promotes
the performance substantially and generalizes well across diverse tasks
including both machine translation and image classification datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07206</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07206</id><submitter>Alexander Zlotnik</submitter><version version="v1"><date>Sat, 15 May 2021 11:49:53 GMT</date><size>17kb</size></version><title>On properties of an explicit in time fourth-order vector compact scheme
  for the multidimensional wave equation</title><authors>Alexander Zlotnik</authors><categories>math.NA cs.NA</categories><comments>15 pages</comments><msc-class>65M06, 65M12, 65M15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An initial-boundary value problem for the $n$-dimensional wave equation is
considered. A three-level explicit in time and conditionally stable 4th-order
compact scheme constructed recently for $n=2$ and the square mesh is
generalized to the case of any $n\geq 1$ and the rectangular uniform mesh.
Another approach to approximate the solution at the first time level (not
exploiting high-order derivatives of the initial functions) is suggested. New
stability bounds in the mesh energy norms and the discrete energy conservation
laws are given, and the 4th order error bound is rigorously proved.
Generalizations to the cases of the non-uniform meshes in space and time as
well as of the wave equation with variable coefficients are suggested.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07207</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07207</id><submitter>Sourabh Pal</submitter><version version="v1"><date>Sat, 15 May 2021 11:51:17 GMT</date><size>660kb</size><source_type>D</source_type></version><title>Generative Adversarial Network-based Cross-Project Fault Prediction</title><authors>Sourabh Pal</authors><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The early stage of defect prediction in the software development
life cycle can reduce testing effort and ensure the quality of software. Due to
the lack of historical data within the same project, Cross-Project Defect
Prediction (CPDP) has become a popular research topic among researchers. CPDP
trained classifiers based on labeled data sets of one project to predict fault
in another project. Goals: Software Defect Prediction (SDP) data sets consist
of manually designed static features, which are software metrics. In CPDP,
source and target project data divergence is the major challenge in achieving
high performance. In this paper, we propose a Generative Adversarial Network
(GAN)-based data transformation to reduce data divergence between source and
target projects. Method: We apply the Generative Adversarial Method where label
data sets are choosing as real data, while target data sets are choosing as
fake data. The Discriminator tries to measure the perfection of domain
adaptation through loss function. Through the generator, target data sets try
to adapt the source project domain and, finally, apply machine learning
classifier (i.e., Naive Bayes) to classify faulty modules. Results: Our result
shows that it is possible to predict defects based on the Generative
Adversarial Method. Our model performs quite well in a cross-project
environment when we choose JDT as a target data sets. However, all chosen data
sets are facing a large class imbalance problem which affects the performance
of our model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07209</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07209</id><submitter>Kailun Yang</submitter><version version="v1"><date>Sat, 15 May 2021 12:01:16 GMT</date><size>1647kb</size><source_type>D</source_type></version><title>Aerial-PASS: Panoramic Annular Scene Segmentation in Drone Videos</title><authors>Lei Sun, Jia Wang, Kailun Yang, Kaikai Wu, Xiangdong Zhou, Kaiwei
  Wang, Jian Bai</authors><categories>cs.CV cs.RO eess.IV</categories><comments>Our dataset will be made publicly available at:
  http://wangkaiwei.org/downloadeg.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aerial pixel-wise scene perception of the surrounding environment is an
important task for UAVs (Unmanned Aerial Vehicles). Previous research works
mainly adopt conventional pinhole cameras or fisheye cameras as the imaging
device. However, these imaging systems cannot achieve large Field of View
(FoV), small size, and lightweight at the same time. To this end, we design a
UAV system with a Panoramic Annular Lens (PAL), which has the characteristics
of small size, low weight, and a 360-degree annular FoV. A lightweight
panoramic annular semantic segmentation neural network model is designed to
achieve high-accuracy and real-time scene parsing. In addition, we present the
first drone-perspective panoramic scene segmentation dataset Aerial-PASS, with
annotated labels of track, field, and others. A comprehensive variety of
experiments shows that the designed system performs satisfactorily in aerial
panoramic scene parsing. In particular, our proposed model strikes an excellent
trade-off between segmentation performance and inference speed suitable,
validated on both public street-scene and our established aerial-scene
datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07211</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07211</id><submitter>Yucheng Liu</submitter><version version="v1"><date>Sat, 15 May 2021 12:19:59 GMT</date><size>267kb</size><source_type>D</source_type></version><title>On Converse Results for Secure Index Coding</title><authors>Yucheng Liu, Lawrence Ong, Parastoo Sadeghi, Neda Aboutorab, Arman
  Sharififar</authors><categories>cs.IT math.IT</categories><comments>A shortened version submitted to IEEE Information Theory Workshop
  (ITW) 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the secure index coding problem where there are
security constraints on both legitimate receivers and eavesdroppers. We develop
two performance bounds (i.e., converse results) on the symmetric secure
capacity. The first one is an extended version of the basic acyclic chain bound
(Liu and Sadeghi, 2019) that takes security constraints into account. The
second converse result is a novel information-theoretic lower bound on the
symmetric secure capacity, which is interesting as all the existing converse
results in the literature for secure index coding give upper bounds on the
capacity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07212</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07212</id><submitter>Wenyi Zhang</submitter><version version="v1"><date>Sat, 15 May 2021 12:23:25 GMT</date><size>1561kb</size></version><title>Generalized Nearest Neighbor Decoding for MIMO Channels with Imperfect
  Channel State Information</title><authors>Shuqin Pang and Wenyi Zhang</authors><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information transmission over a multiple-input-multiple-output (MIMO) fading
channel with imperfect channel state information (CSI) is investigated, under a
new receiver architecture which combines the recently proposed generalized
nearest neighbor decoding rule (GNNDR) and a successive procedure in the spirit
of successive interference cancellation (SIC). Recognizing that the channel
input-output relationship is a nonlinear mapping under imperfect CSI, the GNNDR
is capable of extracting the information embedded in the joint observation of
channel output and imperfect CSI more efficiently than the conventional linear
scheme, as revealed by our achievable rate analysis via generalized mutual
information (GMI). Numerical results indicate that the proposed scheme achieves
performance close to the channel capacity with perfect CSI, and significantly
outperforms the conventional pilot-assisted scheme, which first estimates the
CSI and then uses the estimated CSI as the true one for coherent decoding.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07214</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07214</id><submitter>Taro Shibayama</submitter><version version="v1"><date>Sat, 15 May 2021 12:30:26 GMT</date><size>11kb</size></version><title>The equivalence between correctability of deletions and insertions of
  separable states in quantum codes</title><authors>Taro Shibayama, Yingkai Ouyang</authors><categories>quant-ph cs.IT math.IT</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove the equivalence of inserting separable quantum states
and deletions. Hence any quantum code that corrects deletions automatically
corrects separable insertions. First, we describe the quantum
insertion/deletion error using the Kraus operators. Next, we develop an algebra
for commuting Kraus operators corresponding to insertions and deletions. Using
this algebra, we prove the equivalence between quantum insertion codes and
quantum deletion codes using the Knill-Laflamme conditions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07219</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07219</id><submitter>Arindam Khan</submitter><version version="v1"><date>Sat, 15 May 2021 13:13:05 GMT</date><size>13459kb</size><source_type>D</source_type></version><title>Peak Demand Minimization via Sliced Strip Packing</title><authors>Max A. Deppert, Klaus Jansen, Arindam Khan, Malin Rau, Malte Tutas</authors><categories>cs.DS</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We study Nonpreemptive Peak Demand Minimization (NPDM) problem, where we are
given a set of jobs, specified by their processing times and energy
requirements. The goal is to schedule all jobs within a fixed time period such
that the peak load (the maximum total energy requirement at any time) is
minimized. This problem has recently received significant attention due to its
relevance in smart-grids. Theoretically, the problem is related to the
classical strip packing problem (SP). In SP, a given set of axis-aligned
rectangles must be packed into a fixed-width strip, such that the height of the
strip is minimized. NPDM can be modeled as strip packing with slicing and
stacking constraint: each rectangle may be cut vertically into multiple slices
and the slices may be packed into the strip as individual pieces. The stacking
constraint forbids solutions where two slices of the same rectangle are
intersected by the same vertical line. Nonpreemption enforces the slices to be
placed in contiguous horizontal locations (but may be placed at different
vertical locations).
  We obtain a $(5/3+\epsilon)$-approximation algorithm for the problem. We also
provide an asymptotic efficient polynomial-time approximation scheme (AEPTAS)
which generates a schedule for almost all jobs with energy consumption
$(1+\epsilon)OPT$. The remaining jobs fit into a thin container of height $1$.
The previous best for NPDM was 2.7 approximation based on FFDH [Ranjan et al.
2015]. One of our key ideas is providing several new lower bounds on the
optimal solution of a geometric packing, which could be useful in other related
problems. These lower bounds help us to obtain approximative solutions based on
Steinberg's algorithm in many cases. In addition, we show how to split
schedules generated by the AEPTAS into few segments and to rearrange the
corresponding jobs to insert the thin container mentioned above.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07220</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07220</id><submitter>Mitja Kulczynski</submitter><version version="v1"><date>Sat, 15 May 2021 13:13:50 GMT</date><size>74kb</size></version><title>String Theories involving Regular Membership Predicates: From Practice
  to Theory and Back</title><authors>Murphy Berzish, Joel D. Day, Vijay Ganesh, Mitja Kulczynski, Florin
  Manea, Federico Mora, Dirk Nowotka</authors><categories>cs.CL</categories><comments>arXiv admin note: substantial text overlap with arXiv:2010.07253</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Widespread use of string solvers in formal analysis of string-heavy programs
has led to a growing demand for more efficient and reliable techniques which
can be applied in this context, especially for real-world cases. Designing an
algorithm for the (generally undecidable) satisfiability problem for systems of
string constraints requires a thorough understanding of the structure of
constraints present in the targeted cases. In this paper, we investigate
benchmarks presented in the literature containing regular expression membership
predicates, extract different first order logic theories, and prove their
decidability, resp. undecidability. Notably, the most common theories in
real-world benchmarks are PSPACE-complete and directly lead to the
implementation of a more efficient algorithm to solving string constraints.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07221</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07221</id><submitter>Julianne Chung</submitter><version version="v1"><date>Sat, 15 May 2021 13:21:32 GMT</date><size>9275kb</size><source_type>D</source_type></version><title>Computational methods for large-scale inverse problems: a survey on
  hybrid projection methods</title><authors>Julianne Chung and Silvia Gazzola</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys an important class of methods that combine iterative
projection methods and variational regularization methods for large-scale
inverse problems. Iterative methods such as Krylov subspace methods are
invaluable in the numerical linear algebra community and have proved important
in solving inverse problems due to their inherent regularizing properties and
their ability to handle large-scale problems. Variational regularization
describes a broad and important class of methods that are used to obtain
reliable solutions to inverse problems, whereby one solves a modified problem
that incorporates prior knowledge. Hybrid projection methods combine iterative
projection methods with variational regularization techniques in a synergistic
way, providing researchers with a powerful computational framework for solving
very large inverse problems. Although the idea of a hybrid Krylov method for
linear inverse problems goes back to the 1980s, several recent advances on new
regularization frameworks and methodologies have made this field ripe for
extensions, further analyses, and new applications. In this paper, we provide a
practical and accessible introduction to hybrid projection methods in the
context of solving large (linear) inverse problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07222</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07222</id><submitter>Zhiyi Zhang</submitter><version version="v1"><date>Sat, 15 May 2021 13:45:16 GMT</date><size>2339kb</size><source_type>D</source_type></version><title>On the Distributional Properties of Adaptive Gradients</title><authors>Zhang Zhiyi, Liu Ziyin</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive gradient methods have achieved remarkable success in training deep
neural networks on a wide variety of tasks. However, not much is known about
the mathematical and statistical properties of this family of methods. This
work aims at providing a series of theoretical analyses of its statistical
properties justified by experiments. In particular, we show that when the
underlying gradient obeys a normal distribution, the variance of the magnitude
of the \textit{update} is an increasing and bounded function of time and does
not diverge. This work suggests that the divergence of variance is not the
cause of the need for warm up of the Adam optimizer, contrary to what is
believed in the current literature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07224</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07224</id><submitter>Mohammad Arif Ul Alam</submitter><version version="v1"><date>Sat, 15 May 2021 13:52:20 GMT</date><size>12150kb</size><source_type>D</source_type></version><title>Heterogeneous Causal Effect of Polysubstance Usage on Drug Overdose</title><authors>Vaishali Mahipal, Mohammad Arif Ul Alam</authors><categories>cs.AI</categories><comments>Submitted to EMBS BHI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a system to estimate heterogeneous concurrent drug
usage effects on overdose estimation, that consists of efficient co-variate
selection, sub-group selection, generation of and heterogeneous causal effect
estimation. Although, there has been several association studies have been
proposed in the state-of-art methods, heterogeneous causal effects have never
been studied in concurrent drug usage and drug overdose problem. We apply our
framework to answer a critical question, &quot;can concurrent usage of
benzodiazepines and opioids has heterogeneous causal effects on opioid overdose
epidemic?&quot; Using Truven MarketScan claim data collected from 2001 to 2013 have
shown significant promise of our proposed framework's efficacy. Our efficient
causal inference model estimated that the causal effect is higher (19%) than
the regression studies (15%) to estimate the risks associated with the
concurrent usage of opioid and benzodiazepines on opioid overdose.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07228</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07228</id><submitter>Tizian Wenzel</submitter><version version="v1"><date>Sat, 15 May 2021 14:10:35 GMT</date><size>455kb</size><source_type>D</source_type></version><title>Universality and Optimality of Structured Deep Kernel Networks</title><authors>Tizian Wenzel, Gabriele Santin, Bernard Haasdonk</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel based methods yield approximation models that are flexible, efficient
and powerful. In particular, they utilize fixed feature maps of the data, being
often associated to strong analytical results that prove their accuracy. On the
other hand, the recent success of machine learning methods has been driven by
deep neural networks (NNs). They achieve a significant accuracy on very
high-dimensional data, in that they are able to learn also efficient data
representations or data-based feature maps. In this paper, we leverage a recent
deep kernel representer theorem to connect the two approaches and understand
their interplay. In particular, we show that the use of special types of
kernels yield models reminiscent of neural networks that are founded in the
same theoretical framework of classical kernel methods, while enjoying many
computational properties of deep neural networks. Especially the introduced
Structured Deep Kernel Networks (SDKNs) can be viewed as neural networks with
optimizable activation functions obeying a representer theorem. Analytic
properties show their universal approximation properties in different
asymptotic regimes of unbounded number of centers, width and depth. Especially
in the case of unbounded depth, the constructions is asymptotically better than
corresponding constructions for ReLU neural networks, which is made possible by
the flexibility of kernel approximation
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07229</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07229</id><submitter>Amr Alanwar</submitter><version version="v1"><date>Sat, 15 May 2021 14:11:57 GMT</date><size>2806kb</size><source_type>D</source_type></version><title>Data-Driven Reachability Analysis from Noisy Data</title><authors>Amr Alanwar, Anne Koch, Frank Allg\&quot;ower, Karl Henrik Johansson</authors><categories>eess.SY cs.LG cs.SY</categories><comments>Submitted to Transactions on Automatic Control. arXiv admin note:
  text overlap with arXiv:2011.08472</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing reachable sets directly from noisy data
without a given system model. Several reachability algorithms are presented,
and their accuracy is shown to depend on the underlying system generating the
data. First, an algorithm for computing over-approximated reachable sets based
on matrix zonotopes is proposed for linear systems. Constrained matrix
zonotopes are introduced to provide less conservative reachable sets at the
cost of increased computational expenses and utilized to incorporate prior
knowledge about the unknown system model. Then we extend the approach to
polynomial systems and under the assumption of Lipschitz continuity to
nonlinear systems. Theoretical guarantees are given for these algorithms in
that they give a proper over-approximative reachable set containing the true
reachable set. Multiple numerical examples show the applicability of the
introduced algorithms, and accuracy comparisons are made between algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07231</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07231</id><submitter>Christopher Zach</submitter><version version="v1"><date>Sat, 15 May 2021 14:19:00 GMT</date><size>107kb</size><source_type>D</source_type></version><title>Bilevel Programming and Deep Learning: A Unifying View on Inference
  Learning Methods</title><authors>Christopher Zach</authors><categories>cs.LG math.OC</categories><comments>16 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this work we unify a number of inference learning methods, that are
proposed in the literature as alternative training algorithms to the ones based
on regular error back-propagation. These inference learning methods were
developed with very diverse motivations, mainly aiming to enhance the
biological plausibility of deep neural networks and to improve the intrinsic
parallelism of training methods. We show that these superficially very
different methods can all be obtained by successively applying a particular
reformulation of bilevel optimization programs. As a by-product it becomes also
evident that all considered inference learning methods include back-propagation
as a special case, and therefore at least approximate error back-propagation in
typical settings. Finally, we propose Fenchel back-propagation, that replaces
the propagation of infinitesimal corrections performed in standard
back-propagation with finite targets as the learning signal. Fenchel
back-propagation can therefore be seen as an instance of learning via explicit
target propagation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07233</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07233</id><submitter>Stephany Rajeh</submitter><version version="v1"><date>Sat, 15 May 2021 14:36:06 GMT</date><size>21598kb</size><source_type>D</source_type></version><title>Characterizing the Interactions Between Classical and Community-aware
  Centrality Measures in Complex Networks</title><authors>Stephany Rajeh, Marinette Savonnet, Eric Leclercq, and Hocine Cherifi</authors><categories>cs.SI cs.CY</categories><comments>Article already published at Scientific Reports</comments><journal-ref>Sci Rep 11, 10088 (2021)</journal-ref><doi>10.1038/s41598-021-89549-x</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Identifying vital nodes in networks exhibiting a community structure is a
fundamental issue. Indeed, community structure is one of the main properties of
real-world networks. Recent works have shown that community-aware centrality
measures compare favorably with classical measures agnostic about this
ubiquitous property. Nonetheless, there is no clear consensus about how they
relate and in which situation it is better to use a classical or a
community-aware centrality measure. To this end, in this paper, we perform an
extensive investigation to get a better understanding of the relationship
between classical and community-aware centrality measures reported in the
literature. Experiments use artificial networks with controlled community
structure properties and a large sample of real-world networks originating from
various domains. Results indicate that the stronger the community structure,
the more appropriate the community-aware centrality measures. Furthermore,
variations of the degree and community size distribution parameters do not
affect the results. Finally, network transitivity and community structure
strength are the most significant drivers controlling the interactions between
classical and community-aware centrality measures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07237</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07237</id><submitter>Angad Wadhwa</submitter><version version="v1"><date>Sat, 15 May 2021 14:42:17 GMT</date><size>6647kb</size></version><title>Brain Inspired Object Recognition System</title><authors>Pinaki Roy Chowdhury, Angad Wadhwa, Antariksha Kar and Nikhil Tyagi</authors><categories>cs.CV</categories><comments>24 Pages, 26 Tables, 12 Figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents a new proposal of an efficient computational model of
face and object recognition which uses cues from the distributed face and
object recognition mechanism of the brain, and by gathering engineering
equivalent of these cues from existing literature. Three distinct and widely
used features, Histogram of Oriented Gradients, Local Binary Patterns, and
Principal components extracted from target images are used in a manner which is
simple, and yet effective. Our model uses multi-layer perceptrons (MLP) to
classify these three features and fuse them at the decision level using sum
rule. A computational theory is first developed by using concepts from the
information processing mechanism of the brain. Extensive experiments are
carried out using fifteen publicly available datasets to validate the
performance of our proposed model in recognizing faces and objects with extreme
variation of illumination, pose angle, expression, and background. Results
obtained are extremely promising when compared with other face and object
recognition algorithms including CNN and deep learning based methods. This
highlights that simple computational processes, if clubbed properly, can
produce competing performance with best algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07238</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07238</id><submitter>Amrapali Maitra</submitter><version version="v1"><date>Sat, 15 May 2021 14:56:25 GMT</date><size>1680kb</size></version><title>Using Ethnographic Methods to Classify the Human Experience in Medicine:
  A Case Study of the Presence Ontology</title><authors>Amrapali Maitra, Maulik R. Kamdar, Donna M. Zulman, Marie C.
  Haverfield, Cati Brown-Johnson, Rachel Schwartz, Sonoo Thadaney Israni,
  Abraham Verghese, and Mark A. Musen</authors><categories>cs.IT math.IT</categories><comments>15 pages, 4 figures, 57 references</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Objective Although social and environmental factors are central to provider
patient interactions, the data that reflect these factors can be incomplete,
vague, and subjective. We sought to create a conceptual framework to describe
and classify data about presence, the domain of interpersonal connection in
medicine.
  Methods Our top down approach for ontology development based on the concept
of relationality included 1) broad survey of social sciences literature and
systematic literature review of more than 20,000 articles around interpersonal
connection in medicine, 3) relational ethnography of clinical encounters (5
pilot, 27 full) and 4) interviews about relational work with 40 medical and
nonmedical professionals. We formalized the model using the Web Ontology
Language in the Protege ontology editor. We iteratively evaluated and refined
the Presence Ontology through manual expert review and automated annotation of
literature.
  Results and Discussion The Presence Ontology facilitates the naming and
classification of concepts that would otherwise be vague. Our model categorizes
contributors to healthcare encounters and factors such as Communication,
Emotions, Tools, and Environment. Ontology evaluation indicated that Cognitive
Models (both patients explanatory models and providers caregiving approaches)
influenced encounters and were subsequently incorporated. We show how
ethnographic methods based in relationality can aid the representation of
experiential concepts (e.g., empathy, trust). Our ontology could support
informatics applications to improve healthcare such annotation of videotaped
encounters, clinical instruments to measure presence, or EHR based reminders
for providers.
  Conclusion The Presence Ontology provides a model for using ethnographic
approaches to classify interpersonal data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07239</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07239</id><submitter>Zhizhong Huang</submitter><version version="v1"><date>Sat, 15 May 2021 15:02:07 GMT</date><size>3572kb</size><source_type>D</source_type></version><title>AgeFlow: Conditional Age Progression and Regression with Normalizing
  Flows</title><authors>Zhizhong Huang, Shouzhen Chen, Junping Zhang, Hongming Shan</authors><categories>cs.CV cs.AI</categories><comments>IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Age progression and regression aim to synthesize photorealistic appearance of
a given face image with aging and rejuvenation effects, respectively. Existing
generative adversarial networks (GANs) based methods suffer from the following
three major issues: 1) unstable training introducing strong ghost artifacts in
the generated faces, 2) unpaired training leading to unexpected changes in
facial attributes such as genders and races, and 3) non-bijective age mappings
increasing the uncertainty in the face transformation. To overcome these
issues, this paper proposes a novel framework, termed AgeFlow, to integrate the
advantages of both flow-based models and GANs. The proposed AgeFlow contains
three parts: an encoder that maps a given face to a latent space through an
invertible neural network, a novel invertible conditional translation module
(ICTM) that translates the source latent vector to target one, and a decoder
that reconstructs the generated face from the target latent vector using the
same encoder network; all parts are invertible achieving bijective age
mappings. The novelties of ICTM are two-fold. First, we propose an
attribute-aware knowledge distillation to learn the manipulation direction of
age progression while keeping other unrelated attributes unchanged, alleviating
unexpected changes in facial attributes. Second, we propose to use GANs in the
latent space to ensure the learned latent vector indistinguishable from the
real ones, which is much easier than traditional use of GANs in the image
domain. Experimental results demonstrate superior performance over existing
GANs-based methods on two benchmarked datasets. The source code is available at
https://github.com/Hzzone/AgeFlow.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07244</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07244</id><submitter>Poushali Sengupta</submitter><version version="v1"><date>Sat, 15 May 2021 15:19:43 GMT</date><size>29kb</size></version><title>Fairly Private Through Group Tagging and Relation Impact</title><authors>Poushali Sengupta and Subhankar Mishra</authors><categories>cs.CR</categories><comments>Accepted at MDAI 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Privacy and Fairness both are very important nowadays. For most of the cases
in the online service providing system, users have to share their personal
information with the organizations. In return, the clients not only demand a
high privacy guarantee to their sensitive data but also expected to be treated
fairly irrespective of their age, gender, religion, race, skin color, or other
sensitive protected attributes. Our work introduces a novel architecture that
is balanced among the privacy-utility-fairness trade-off. The proposed
mechanism applies Group Tagging Method and Fairly Iterative Shuffling (FIS)
that amplifies privacy through random shuffling and prevents linkage attack.
The algorithm introduces a fair classification problem by Relation Impact based
on Equalized Minimal FPR-FNR among the protected tagged group. For the count
report generation, the aggregator uses TF-IDF to add noise for providing
longitudinal Differential Privacy guarantee. Lastly, the mechanism boosts the
utility through risk minimization function and obtain the optimal
privacy-utility budget of the system. In our work, we have done a case study on
gender equality in the admission system and helps to obtain a satisfying result
which implies that the proposed architecture achieves the group fairness and
optimal privacy-utility trade-off for both the numerical and decision making
Queries.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07245</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07245</id><submitter>Zifan Chen</submitter><version version="v1"><date>Sat, 15 May 2021 15:22:27 GMT</date><size>20660kb</size><source_type>D</source_type></version><title>Composite Localization for Human Pose Estimation</title><authors>ZiFan Chen, Xin Qin, Chao Yang, Li Zhang</authors><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing human pose estimation methods are confronted with inaccurate
long-distance regression or high computational cost due to the complex learning
objectives. This work proposes a novel deep learning framework for human pose
estimation called composite localization to divide the complex learning
objective into two simpler ones: a sparse heatmap to find the keypoint's
approximate location and two short-distance offsetmaps to obtain its final
precise coordinates. To realize the framework, we construct two types of
composite localization networks: CLNet-ResNet and CLNet-Hourglass. We evaluate
the networks on three benchmark datasets, including the Leeds Sports Pose
dataset, the MPII Human Pose dataset, and the COCO keypoints detection dataset.
The experimental results show that our CLNet-ResNet50 outperforms
SimpleBaseline by 1.14% with about 1/2 GFLOPs. Our CLNet-Hourglass outperforms
the original stacked-hourglass by 4.45% on COCO.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07246</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07246</id><submitter>Minkai Xu</submitter><version version="v1"><date>Sat, 15 May 2021 15:22:29 GMT</date><size>10862kb</size><source_type>D</source_type></version><title>An End-to-End Framework for Molecular Conformation Generation via
  Bilevel Programming</title><authors>Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael
  Gomez-Bombarelli, Jian Tang</authors><categories>cs.LG q-bio.BM</categories><comments>Accepted by ICML 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Predicting molecular conformations (or 3D structures) from molecular graphs
is a fundamental problem in many applications. Most existing approaches are
usually divided into two steps by first predicting the distances between atoms
and then generating a 3D structure through optimizing a distance geometry
problem. However, the distances predicted with such two-stage approaches may
not be able to consistently preserve the geometry of local atomic
neighborhoods, making the generated structures unsatisfying. In this paper, we
propose an end-to-end solution for molecular conformation prediction called
ConfVAE based on the conditional variational autoencoder framework.
Specifically, the molecular graph is first encoded in a latent space, and then
the 3D structures are generated by solving a principled bilevel optimization
program. Extensive experiments on several benchmark data sets prove the
effectiveness of our proposed approach over existing state-of-the-art
approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07253</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07253</id><submitter>Zhenghai Xue</submitter><version version="v1"><date>Sat, 15 May 2021 16:08:45 GMT</date><size>8509kb</size><source_type>D</source_type></version><title>Regret Minimization Experience Replay</title><authors>Zhenghai Xue, Xu-Hui Liu, Jing-Cheng Pang, Shengyi Jiang, Feng Xu,
  Yang Yu</authors><categories>cs.LG cs.AI</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experience replay is widely used in various deep off-policy reinforcement
learning (RL) algorithms. It stores previously collected samples for further
reuse. To better utilize these samples, prioritized sampling is a promising
technique to improve the performance of RL agents. Previous prioritization
methods based on temporal-difference (TD) error are highly heuristic and
divergent from the objective of RL. In this work, we analyze the optimal
prioritization strategy that can minimize the regret of RL policy
theoretically. Our theory suggests that the data with higher TD error, better
on-policiness and more corrective feedback should be assigned with higher
weights during sampling. Based on this theory, we propose two practical
algorithms, RM-DisCor and RM-TCE. RM-DisCor is a general algorithm and RM-TCE
is a more efficient variant relying on the temporal ordering of states. Both
algorithms improve the performance of off-policy RL algorithms in challenging
RL benchmarks, including MuJoCo, Atari and Meta-World.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07255</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07255</id><submitter>Yekaterina Epshteyn</submitter><version version="v1"><date>Sat, 15 May 2021 16:20:16 GMT</date><size>13377kb</size><source_type>D</source_type></version><title>Grain Growth and the Effect of Different Time Scales</title><authors>Katayun Barmak, Anastasia Dunca, Yekaterina Epshteyn, Chun Liu and
  Masashi Mizuno</authors><categories>cond-mat.mtrl-sci cs.NA math.AP math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many technologically useful materials are polycrystals composed of a myriad
of small monocrystalline grains separated by grain boundaries. Dynamics of
grain boundaries play a crucial role in determining the grain structure and
defining the materials properties across multiple scales. In this work, we
consider two models for the motion of grain boundaries with the dynamic lattice
misorientations and the triple junctions drag, and we conduct extensive
numerical study of the models, as well as present relevant experimental results
of grain growth in thin films.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07258</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07258</id><submitter>Habib Ben Abdallah</submitter><version version="v1"><date>Sat, 15 May 2021 16:25:17 GMT</date><size>174kb</size><source_type>D</source_type></version><title>Polynomial degree reduction in the $\mathcal{L}^2$-norm on a symmetric
  interval for the canonical basis</title><authors>Habib Ben Abdallah, Christopher J. Henry, Sheela Ramanna</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a direct formula for determining the coefficients
in the canonical basis of the best polynomial of degree $M$ that approximates a
polynomial of degree $N&gt;M$ on a symmetric interval for the
$\mathcal{L}^2$-norm. We also formally prove that using the formula is more
computationally efficient than using a classical matrix multiplication approach
and we provide an example to illustrate that it is more numerically stable than
the classical approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07260</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07260</id><submitter>Zeyu Ding</submitter><version version="v1"><date>Sat, 15 May 2021 16:33:32 GMT</date><size>151kb</size></version><title>The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with
  Exponential Noise</title><authors>Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke,
  Yuxin Wang, Yingtai Xiao, Danfeng Zhang</authors><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The permute-and-flip mechanism is a recently proposed differentially private
selection algorithm that was shown to outperform the exponential mechanism. In
this paper, we show that permute-and-flip is equivalent to the well-known
report noisy max algorithm with exponential noise. This equivalence is used to
simplify the utility and privacy analysis, and extend it to bounded range and
concentrated differential privacy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07263</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07263</id><submitter>Nicholas Andrews</submitter><version version="v1"><date>Sat, 15 May 2021 17:06:31 GMT</date><size>90kb</size><source_type>D</source_type></version><title>A Deep Metric Learning Approach to Account Linking</title><authors>Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop, Nicholas
  Andrews</authors><categories>cs.SI cs.AI cs.CL cs.LG</categories><comments>13 pages; to be published in NAACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of linking social media accounts that belong to the same
author in an automated fashion on the basis of the content and metadata of
their corresponding document streams. We focus on learning an embedding that
maps variable-sized samples of user activity -- ranging from single posts to
entire months of activity -- to a vector space, where samples by the same
author map to nearby points. The approach does not require human-annotated data
for training purposes, which allows us to leverage large amounts of social
media content. The proposed model outperforms several competitive baselines
under a novel evaluation framework modeled after established recognition
benchmarks in other domains. Our method achieves high linking accuracy, even
with small samples from accounts not seen at training time, a prerequisite for
practical applications of the proposed linking framework.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07264</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07264</id><submitter>Rajat Talak</submitter><version version="v1"><date>Sat, 15 May 2021 17:08:20 GMT</date><size>872kb</size><source_type>D</source_type></version><title>Neural Trees for Learning on Graphs</title><authors>Rajat Talak, Siyi Hu, Lisa Peng, and Luca Carlone</authors><categories>cs.LG cs.CV cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach
for learning over graphs. Despite this success, existing GNNs are constrained
by their local message-passing architecture and are provably limited in their
expressive power. In this work, we propose a new GNN architecture -- the Neural
Tree. The neural tree architecture does not perform message passing on the
input graph but on a tree-structured graph, called the H-tree, that is
constructed from the input graph. Nodes in the H-tree correspond to subgraphs
in the input graph, and they are reorganized in a hierarchical manner such that
a parent-node of a node in the H-tree always corresponds to a larger subgraph
in the input graph. We show that the neural tree architecture can approximate
any smooth probability distribution function over an undirected graph, as well
as emulate the junction tree algorithm. We also prove that the number of
parameters needed to achieve an $\epsilon$-approximation of the distribution
function is exponential in the treewidth of the input graph, but linear in its
size. We apply the neural tree to semi-supervised node classification in 3D
scene graphs, and show that these theoretical properties translate into
significant gains in prediction accuracy, over the more traditional GNN
architectures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07269</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07269</id><submitter>Ajinkya Tejankar</submitter><version version="v1"><date>Sat, 15 May 2021 17:42:19 GMT</date><size>37692kb</size><source_type>D</source_type></version><title>Mean Shift for Self-Supervised Learning</title><authors>Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Most recent self-supervised learning (SSL) algorithms learn features by
contrasting between instances of images or by clustering the images and then
contrasting between the image clusters. We introduce a simple mean-shift
algorithm that learns representations by grouping images together without
contrasting between them or adopting much of prior on the structure of the
clusters. We simply &quot;shift&quot; the embedding of each image to be close to the
&quot;mean&quot; of its neighbors. Since in our setting, the closest neighbor is always
another augmentation of the same image, our model will be identical to BYOL
when using only one nearest neighbor instead of 5 as used in our experiments.
Our model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200
epochs outperforming BYOL. Our code is available here:
https://github.com/UMBCvision/MSF
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07270</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07270</id><submitter>Marcel Wever</submitter><version version="v1"><date>Sat, 15 May 2021 17:45:29 GMT</date><size>763kb</size><source_type>D</source_type></version><title>Annotation Uncertainty in the Context of Grammatical Change</title><authors>Marie-Luis Merten, Marcel Wever, Michaela Geierhos, Doris Tophinke,
  Eyke H\&quot;ullermeier</authors><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper elaborates on the notion of uncertainty in the context of
annotation in large text corpora, specifically focusing on (but not limited to)
historical languages. Such uncertainty might be due to inherent properties of
the language, for example, linguistic ambiguity and overlapping categories of
linguistic description, but could also be caused by lacking annotation
expertise. By examining annotation uncertainty in more detail, we identify the
sources and deepen our understanding of the nature and different types of
uncertainty encountered in daily annotation practice. Moreover, some practical
implications of our theoretical findings are also discussed. Last but not
least, this article can be seen as an attempt to reconcile the perspectives of
the main scientific disciplines involved in corpus projects, linguistics and
computer science, to develop a unified view and to highlight the potential
synergies between these disciplines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07272</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07272</id><submitter>Dandan Zhang</submitter><version version="v1"><date>Sat, 15 May 2021 17:57:24 GMT</date><size>251kb</size></version><title>An Ergonomic Interaction Workspace Analysis Method for the Optimal
  Design of a Surgical Master Manipulator</title><authors>Dandan Zhang, Jindong Liu, Guangzhong Yang</authors><categories>cs.RO</categories><comments>Accepted by the Hamlyn Symposium on Medical Robotics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Master control console is a place where robots collaborate with humans in a
shared environment. To this end, ergonomics is an important aspect to be
considered. With ergonomic design, the surgeons can feel more comfortable to
conduct the surgical tasks with higher efficiency, and the quality of the
teleoperated robotic surgery can be improved. In this paper, an Ergonomic
Interaction Workspace Analysis method is proposed to optimize master
manipulators and fulfil ergonomics consideration for designing a master
manipulator for teleoperated robotic surgery.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07273</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07273</id><submitter>Mengyu Yang</submitter><version version="v1"><date>Sat, 15 May 2021 18:06:38 GMT</date><size>12674kb</size><source_type>D</source_type></version><title>Mask-Guided Discovery of Semantic Manifolds in Generative Models</title><authors>Mengyu Yang, David Rokeby, Xavier Snelgrove</authors><categories>cs.CV</categories><comments>In the 4th Workshop on Machine Learning for Creativity and Design at
  NeurIPS 2020, Vancouver, Canada</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Advances in the realm of Generative Adversarial Networks (GANs) have led to
architectures capable of producing amazingly realistic images such as
StyleGAN2, which, when trained on the FFHQ dataset, generates images of human
faces from random vectors in a lower-dimensional latent space. Unfortunately,
this space is entangled - translating a latent vector along its axes does not
correspond to a meaningful transformation in the output space (e.g., smiling
mouth, squinting eyes). The model behaves as a black box, providing neither
control over its output nor insight into the structures it has learned from the
data. We present a method to explore the manifolds of changes of spatially
localized regions of the face. Our method discovers smoothly varying sequences
of latent vectors along these manifolds suitable for creating animations.
Unlike existing disentanglement methods that either require labelled data or
explicitly alter internal model parameters, our method is an optimization-based
approach guided by a custom loss function and manually defined region of
change. Our code is open-sourced, which can be found, along with supplementary
results, on our project page: https://github.com/bmolab/masked-gan-manifold
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07277</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07277</id><submitter>Thomas Wahl</submitter><version version="v1"><date>Sat, 15 May 2021 19:04:06 GMT</date><size>61kb</size><source_type>D</source_type></version><title>Delay-Bounded Scheduling Without Delay! (Extended Technical Report)</title><authors>Andrew Johnson and Thomas Wahl</authors><categories>cs.PL</categories><comments>This is an extended technical report of a paper published in CAV 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider the broad problem of analyzing safety properties of asynchronous
concurrent programs under arbitrary thread interleavings. Delay-bounded
deterministic scheduling, introduced in prior work, is an efficient bug-finding
technique to curb the large cost associated with full scheduling
nondeterminism. In this paper we first present a technique to lift the delay
bound for the case of finite-domain variable programs, thus adding to the
efficiency of bug detection the ability to prove safety of programs under
arbitrary thread interleavings. Second, we demonstrate how, combined with
predicate abstraction, our technique can both refute and verify safety
properties of programs with unbounded variable domains, even for unbounded
thread counts. Previous work has established that, for non-trivial concurrency
routines, predicate abstraction induces a highly complex abstract program
semantics. Our technique, however, never statically constructs an abstract
parametric program; it only requires some abstract-states set to be closed
under certain actions, thus eliminating the dependence on the existence of
verification algorithms for abstract programs. We demonstrate the efficiency of
our technique on many examples used in prior work, and showcase its simplicity
compared to earlier approaches on the unbounded-thread Ticket Lock protocol.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07283</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07283</id><submitter>Dirk Tasche</submitter><version version="v1"><date>Sat, 15 May 2021 19:48:28 GMT</date><size>37kb</size><source_type>D</source_type></version><title>Calibrating sufficiently</title><authors>Dirk Tasche</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>22 pages, 1 figure, appendices</comments><msc-class>62B05, 62P99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When probabilistic classifiers are trained and calibrated, the so-called
grouping loss component of the calibration loss can easily be overlooked.
Grouping loss refers to the gap between observable information and information
actually exploited in the calibration exercise. We investigate the relation
between grouping loss and the concept of sufficiency, identifying
comonotonicity as a useful criterion for sufficiency. We revisit the probing
reduction approach of Langford &amp; Zadrozny (2005) and find that it produces an
estimator of probabilistic classifiers that reduces information loss. Finally,
we discuss Brier curves as tools to support training and `sufficient'
calibration of probabilistic classifiers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07284</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07284</id><submitter>Joseph Monaco</submitter><version version="v1"><date>Sat, 15 May 2021 19:49:32 GMT</date><size>2118kb</size><source_type>D</source_type></version><title>A brain basis of dynamical intelligence for AI and computational
  neuroscience</title><authors>Joseph D. Monaco, Kanaka Rajan, Grace M. Hwang</authors><categories>q-bio.NC cs.AI</categories><comments>Perspective article: 178 references, 24 pages, 3 figures, and 1
  glossary box</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The deep neural nets of modern artificial intelligence (AI) have not achieved
defining features of biological intelligence, including abstraction, causal
learning, and energy-efficiency. While scaling to larger models has delivered
performance improvements for current applications, more brain-like capacities
may demand new theories, models, and methods for designing artificial learning
systems. Here, we argue that this opportunity to reassess insights from the
brain should stimulate cooperation between AI research and theory-driven
computational neuroscience (CN). To motivate a brain basis of neural
computation, we present a dynamical view of intelligence from which we
elaborate concepts of sparsity in network structure, temporal dynamics, and
interactive learning. In particular, we suggest that temporal dynamics, as
expressed through neural synchrony, nested oscillations, and flexible
sequences, provide a rich computational layer for reading and updating
hierarchical models distributed in long-term memory networks. Moreover,
embracing agent-centered paradigms in AI and CN will accelerate our
understanding of the complex dynamics and behaviors that build useful world
models. A convergence of AI/CN theories and objectives will reveal dynamical
principles of intelligence for brains and engineered learning systems. This
article was inspired by our symposium on dynamical neuroscience and machine
learning at the 6th Annual US/NIH BRAIN Initiative Investigators Meeting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07289</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07289</id><submitter>Abdalaziz Hamdan</submitter><version version="v1"><date>Sat, 15 May 2021 20:13:08 GMT</date><size>150kb</size><source_type>D</source_type></version><title>A new mixed finite-element method for the biharmonic problem</title><authors>Patrick E. Farrell, Abdalaziz Hamdan, and Scott P. MacLachlan</authors><categories>math.NA cs.NA</categories><msc-class>65N30, 65N55, 65F08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fourth-order differential equations play an important role in many
applications in science and engineering. In this paper, we present a
three-field mixed finite-element formulation for fourth-order problems, with a
focus on the effective treatment of the different boundary conditions that
arise naturally in a variational formulation. Our formulation is based on
introducing the gradient of the solution as an explicit variable, constrained
using a Lagrange multiplier. The essential boundary conditions are enforced
weakly, using Nitsche's method where required. As a result, the problem is
rewritten as a saddle-point system, requiring analysis of the resulting
finite-element discretization and the construction of optimal linear solvers.
Here, we discuss the analysis of the well-posedness and accuracy of the
finite-element formulation. Moreover, we develop monolithic multigrid solvers
for the resulting linear systems. Two and three-dimensional numerical results
are presented to demonstrate the accuracy of the discretization and efficiency
of the multigrid solvers proposed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07290</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07290</id><submitter>Ali Alijani</submitter><version version="v1"><date>Sat, 15 May 2021 20:18:12 GMT</date><size>1345kb</size></version><title>Circumferential Crack Modeling of Thin Cylindrical Shells in Modal
  Deformation</title><authors>Ali Alijani, Olga Barrera, and Stephane P.A. Bordas</authors><categories>math.NA cs.CE cs.NA</categories><comments>23 pages, 12 figures, and 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An innovative technique, called conversion, is introduced to model
circumferential cracks in thin cylindrical shells. The semi-analytical finite
element method is applied to investigate the modal deformation of the cylinder.
An element including the crack is divided into three sub-elements with four
nodes in which the stiffness matrix is enriched. The crack characteristics are
included in the finite element method relations through conversion matrices and
a rotational spring corresponding to the crack. Conversion matrices obtained by
applying continuity conditions at the crack tip are used to transform
displacements of the middle nodes to those of the main nodes. Moreover, another
technique, called spring set, is represented based on a set of springs to model
the crack as a separated element. Components of the stiffness matrix related to
the separated element are incorporated while the geometric boundary conditions
at the crack tip are satisfied. The effects of the circumferential mode number,
the crack depth and the length of the cylinder on the critical buckling load
are investigated. Experimental tests, ABAQUS modeling and results from
literature are used to verify and validate the results and derived relations.
In addition, the crack effect on the natural frequency is examined using the
vibration analysis based on the conversion technique.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07291</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07291</id><submitter>Jonathan Lacotte</submitter><version version="v1"><date>Sat, 15 May 2021 20:24:26 GMT</date><size>5443kb</size><source_type>D</source_type></version><title>Adaptive Newton Sketch: Linear-time Optimization with Quadratic
  Convergence and Effective Hessian Dimensionality</title><authors>Jonathan Lacotte, Yifei Wang, Mert Pilanci</authors><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a randomized algorithm with quadratic convergence rate for convex
optimization problems with a self-concordant, composite, strongly convex
objective function. Our method is based on performing an approximate Newton
step using a random projection of the Hessian. Our first contribution is to
show that, at each iteration, the embedding dimension (or sketch size) can be
as small as the effective dimension of the Hessian matrix. Leveraging this
novel fundamental result, we design an algorithm with a sketch size
proportional to the effective dimension and which exhibits a quadratic rate of
convergence. This result dramatically improves on the classical
linear-quadratic convergence rates of state-of-the-art sub-sampled Newton
methods. However, in most practical cases, the effective dimension is not known
beforehand, and this raises the question of how to pick a sketch size as small
as the effective dimension while preserving a quadratic convergence rate. Our
second and main contribution is thus to propose an adaptive sketch size
algorithm with quadratic convergence rate and which does not require prior
knowledge or estimation of the effective dimension: at each iteration, it
starts with a small sketch size, and increases it until quadratic progress is
achieved. Importantly, we show that the embedding dimension remains
proportional to the effective dimension throughout the entire path and that our
method achieves state-of-the-art computational complexity for solving convex
optimization programs with a strongly convex component.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07292</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07292</id><submitter>Shweta Jain</submitter><version version="v1"><date>Sat, 15 May 2021 20:29:45 GMT</date><size>4653kb</size></version><title>Storage Aware Routing for Generalized Delay Tolerant Networks</title><authors>Shweta Jain, Snehapreethi Gopinath, Dipankar Raychaudhuri</authors><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a novel storage aware routing (STAR) protocol designed to
provide a general networking solution over a broad range of wired and wireless
usage scenarios. STAR enables routing policies which adapt seamlessly from a
well-connected wired network to a disconnected wireless network. STAR uses a
2-Dimensional routing metric composed of a short and a long term route cost and
storage availability on downstream routers to make store or forward routing
decisions. Temporary in-network storage is preferred over forwarding along a
path that is slower than average and opportunistic transmission is encouraged
when faster than average routes become available. Results from ns2 based
simulations show that STAR achieves $40-50\%$ higher throughput compared to
OLSR in mobile vehicular and DTN scenarios and does $12-20\%$ better than OLSR
in the static mesh case. Experimental evaluation of STAR on the ORBIT testbed
validates the protocol implementation, and demonstrates significant performance
improvements with 25\% higher peak throughput compared to OLSR in a wireless
mesh network.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07298</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07298</id><submitter>Enzo Rucci</submitter><version version="v1"><date>Sat, 15 May 2021 22:01:37 GMT</date><size>164kb</size><source_type>D</source_type></version><title>Comparison of HPC Architectures for Computing All-Pairs Shortest Paths.
  Intel Xeon Phi KNL vs NVIDIA Pascal</title><authors>Manuel Costanzo and Enzo Rucci and Ulises Costi and Franco Chichizola
  and Marcelo Naiouf</authors><categories>cs.DC</categories><comments>Computer Science - CACIC 2020. CACIC 2020. Communications in Computer
  and Information Science, vol 1409. Springer, Cham</comments><doi>10.1007/978-3-030-75836-3_3</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Today, one of the main challenges for high-performance computing systems is
to improve their performance by keeping energy consumption at acceptable
levels. In this context, a consolidated strategy consists of using accelerators
such as GPUs or many-core Intel Xeon Phi processors. In this work, devices of
the NVIDIA Pascal and Intel Xeon Phi Knights Landing architectures are
described and compared. Selecting the Floyd-Warshall algorithm as a
representative case of graph and memory-bound applications, optimized
implementations were developed to analyze and compare performance and energy
efficiency on both devices. As it was expected, Xeon Phi showed superior when
considering double-precision data. However, contrary to what was considered in
our preliminary analysis, it was found that the performance and energy
efficiency of both devices were comparable using single-precision datatype.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07299</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07299</id><submitter>Eyvind Niklasson</submitter><version version="v1"><date>Sat, 15 May 2021 22:05:46 GMT</date><size>13497kb</size><source_type>D</source_type></version><title>Texture Generation with Neural Cellular Automata</title><authors>Alexander Mordvintsev, Eyvind Niklasson, Ettore Randazzo</authors><categories>cs.AI cs.CV cs.GR</categories><comments>AI for Content Creation Workshop, CVPR 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural Cellular Automata (NCA) have shown a remarkable ability to learn the
required rules to &quot;grow&quot; images, classify morphologies, segment images, as well
as to do general computation such as path-finding. We believe the inductive
prior they introduce lends itself to the generation of textures. Textures in
the natural world are often generated by variants of locally interacting
reaction-diffusion systems. Human-made textures are likewise often generated in
a local manner (textile weaving, for instance) or using rules with local
dependencies (regular grids or geometric patterns). We demonstrate learning a
texture generator from a single template image, with the generation method
being embarrassingly parallel, exhibiting quick convergence and high fidelity
of output, and requiring only some minimal assumptions around the underlying
state manifold. Furthermore, we investigate properties of the learned models
that are both useful and interesting, such as non-stationary dynamics and an
inherent robustness to damage. Finally, we make qualitative claims that the
behaviour exhibited by the NCA model is a learned, distributed, local algorithm
to generate a texture, setting our method apart from existing work on texture
generation. We discuss the advantages of such a paradigm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07302</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07302</id><submitter>Alessandro Lameiras Koerich</submitter><version version="v1"><date>Sat, 15 May 2021 22:33:48 GMT</date><size>700kb</size><source_type>D</source_type></version><title>1D CNN Architectures for Music Genre Classification</title><authors>Safaa Allamy and Alessandro Lameiras Koerich</authors><categories>cs.SD eess.AS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a 1D residual convolutional neural network (CNN)
architecture for music genre classification and compares it with other recent
1D CNN architectures. The 1D CNNs learn a representation and a discriminant
directly from the raw audio signal. Several convolutional layers capture the
time-frequency characteristics of the audio signal and learn various filters
relevant to the music genre recognition task. The proposed approach splits the
audio signal into overlapped segments using a sliding window to comply with the
fixed-length input constraint of the 1D CNNs. As a result, music genre
classification can be carried out on a single audio segment or on the
aggregation of the predictions on several audio segments, which improves the
final accuracy. The performance of the proposed 1D residual CNN is assessed on
a public dataset of 1,000 audio clips. The experimental results have shown that
it achieves 80.93% of mean accuracy in classifying music genres and outperforms
other 1D CNN architectures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07305</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07305</id><submitter>Jun Liu</submitter><version version="v1"><date>Sat, 15 May 2021 22:48:34 GMT</date><size>2136kb</size><source_type>D</source_type></version><title>Distributed Resilient Submodular Action Selection in Adversarial
  Environments</title><authors>Jun Liu, Lifeng Zhou, Pratap Tokekar, and Ryan K. Williams</authors><categories>cs.RO</categories><journal-ref>IEEE Robotics and Automation Letters, 2021</journal-ref><doi>10.1109/LRA.2021.3080629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider a distributed submodular maximization problem for
multi-robot systems when attacked by adversaries. One of the major challenges
for multi-robot systems is to increase resilience against failures or attacks.
This is particularly important for distributed systems under attack as there is
no central point of command that can detect, mitigate, and recover from
attacks. Instead, a distributed multi-robot system must coordinate effectively
to overcome adversarial attacks. In this work, our distributed submodular
action selection problem models a broad set of scenarios where each robot in a
multi-robot system has multiple action selections that may fulfill a global
objective, such as exploration or target tracking. To increase resilience in
this context, we propose a fully distributed algorithm to guide each robot's
action selection when the system is attacked. The proposed algorithm guarantees
performance in a worst-case scenario where up to a portion of the robots
malfunction due to attacks. Importantly, the proposed algorithm is also
consistent, as it is shown to converge to the same solution as a centralized
method. Finally, a distributed resilient multi-robot exploration problem is
presented to confirm the performance of the proposed algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07308</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07308</id><submitter>M. A. Kelly</submitter><version version="v1"><date>Sat, 15 May 2021 22:55:23 GMT</date><size>471kb</size><source_type>D</source_type></version><title>Towards a Predictive Processing Implementation of the Common Model of
  Cognition</title><authors>M. A. Kelly, Alexander Ororbia</authors><categories>cs.AI cs.LG q-bio.NC</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a cognitive architecture that is built from
powerful yet simple neural models. Specifically, we describe an implementation
of the common model of cognition grounded in neural generative coding and
holographic associative memory. The proposed system creates the groundwork for
developing agents that learn continually from diverse tasks as well as model
human performance at larger scales than what is possible with existant
cognitive architectures.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07309</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07309</id><submitter>Christian Glusa</submitter><version version="v1"><date>Sat, 15 May 2021 22:59:20 GMT</date><size>4071kb</size><source_type>D</source_type></version><title>A FETI approach to domain decomposition for meshfree discretizations of
  nonlocal problems</title><authors>Xiao Xu, Christian Glusa, Marta D'Elia, John T. Foster</authors><categories>math.NA cs.NA</categories><report-no>SAND2021-5958 O</report-no><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We propose a domain decomposition method for the efficient simulation of
nonlocal problems. Our approach is based on a multi-domain formulation of a
nonlocal diffusion problem where the subdomains share &quot;nonlocal&quot; interfaces of
the size of the nonlocal horizon. This system of nonlocal equations is first
rewritten in terms of minimization of a nonlocal energy, then discretized with
a meshfree approximation and finally solved via a Lagrange multiplier approach
in a way that resembles the finite element tearing and interconnect method.
Specifically, we propose a distributed projected gradient algorithm for the
solution of the Lagrange multiplier system, whose unknowns determine the
nonlocal interface conditions between subdomains. Several two-dimensional
numerical tests illustrate the strong and weak scalability of our algorithm,
which outperforms the standard approach to the distributed numerical solution
of the problem. This work is the first rigorous numerical study in a
two-dimensional multi-domain setting for nonlocal operators with finite horizon
and, as such, it is a fundamental step towards increasing the use of nonlocal
models in large scale simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07310</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07310</id><submitter>Ting-Jui Chang</submitter><version version="v1"><date>Sat, 15 May 2021 23:02:58 GMT</date><size>2702kb</size><source_type>D</source_type></version><title>Regret Analysis of Distributed Online LQR Control for Unknown LTI
  Systems</title><authors>Ting-Jui Chang and Shahin Shahrampour</authors><categories>math.OC cs.LG cs.SY eess.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:2009.13749</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning has recently opened avenues for rethinking classical optimal
control beyond time-invariant cost metrics, and online controllers are designed
when the performance criteria changes adversarially over time. Inspired by this
line of research, we study the distributed online linear quadratic regulator
(LQR) problem for linear time-invariant (LTI) systems with unknown dynamics.
Consider a multi-agent network where each agent is modeled as a LTI system. The
LTI systems are associated with time-varying quadratic costs that are revealed
sequentially. The goal of the network is to collectively (i) estimate the
unknown dynamics and (ii) compute local control sequences competitive to that
of the best centralized policy in hindsight that minimizes the sum of costs for
all agents. This problem is formulated as a {\it regret} minimization. We
propose a distributed variant of the online LQR algorithm where each agent
computes its system estimate during an exploration stage. The agent then
applies distributed online gradient descent on a semi-definite programming
(SDP) whose feasible set is based on the agent's system estimate. We prove that
the regret bound of our proposed algorithm scales $\tilde{O}(T^{2/3})$,
implying the consensus of the network over time. We also provide simulation
results verifying our theoretical guarantee.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07311</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07311</id><submitter>Yiling Lou</submitter><version version="v1"><date>Sat, 15 May 2021 23:06:03 GMT</date><size>4280kb</size></version><title>How Does Regression Test Selection Affect Program Repair? An Extensive
  Study on 2 Million Patches</title><authors>Yiling Lou, Samuel Benton, Dan Hao, Lu Zhang, Lingming Zhang</authors><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  APR techniques can be extremely time consuming since (1) a large number of
patches can be generated for a given bug, and (2) each patch needs to be
executed on the original tests to ensure its correctness. In the literature,
various techniques (e.g., based on learning, mining, and constraint solving)
have been proposed/studied to reduce the number of patches. However, there is
limited study on the impact of test selection for each patch (e.g., only the
tests affected by the patch need to be executed as the other tests would keep
the same outcomes and can be skipped), and few APR systems actually apply test
selection. Therefore, this paper conducts the first extensive study to
investigate the impact of Regression Test Selection (RTS) on APR. More
specifically, we implemented widely-used RTS techniques at different levels for
12 state-of-the-art APR systems with over 2M patches. Our study reveals various
practical guidelines for future APR, including: (1) the number of patches
widely used for measuring APR efficiency can incur skewed conclusions, and the
use of inconsistent RTS configurations can further skew the conclusion; (2) all
studied RTS techniques can substantially improve APR efficiency and should be
considered in future APR work; (3) method- and statement-level RTS outperform
class-level RTS substantially, and should be preferred; (4) RTS techniques can
substantially outperform state-of-the-art test prioritization techniques for
APR, and combining them can further improve APR efficiency; and (5) traditional
regression test prioritization widely studied in regression testing performs
even better than APR-specific test prioritization when combined with most RTS
techniques. Furthermore, we also present the detailed impact of different patch
categories and patch validation strategies on our findings.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07314</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07314</id><submitter>Aakanksha Naik</submitter><version version="v1"><date>Sat, 15 May 2021 23:34:02 GMT</date><size>5392kb</size><source_type>D</source_type></version><title>STAGE: Tool for Automated Extraction of Semantic Time Cues to Enrich
  Neural Temporal Ordering Models</title><authors>Luke Breitfeller, Aakanksha Naik, Carolyn Rose</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Despite achieving state-of-the-art accuracy on temporal ordering of events,
neural models showcase significant gaps in performance. Our work seeks to fill
one of these gaps by leveraging an under-explored dimension of textual
semantics: rich semantic information provided by explicit textual time cues. We
develop STAGE, a system that consists of a novel temporal framework and a
parser that can automatically extract time cues and convert them into
representations suitable for integration with neural models. We demonstrate the
utility of extracted cues by integrating them with an event ordering model
using a joint BiLSTM and ILP constraint architecture. We outline the
functionality of the 3-part STAGE processing approach, and show two methods of
integrating its representations with the BiLSTM-ILP model: (i) incorporating
semantic cues as additional features, and (ii) generating new constraints from
semantic cues to be enforced in the ILP. We demonstrate promising results on
two event ordering datasets, and highlight important issues in semantic cue
representation and integration for future research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07316</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07316</id><submitter>Ibrahim Sharaf</submitter><version version="v1"><date>Sat, 15 May 2021 23:51:11 GMT</date><size>129kb</size><source_type>D</source_type></version><title>From Masked Language Modeling to Translation: Non-English Auxiliary
  Tasks Improve Zero-shot Spoken Language Understanding</title><authors>Rob van der Goot, Ibrahim Sharaf, Aizhan Imankulova, Ahmet \&quot;Ust\&quot;un,
  Marija Stepanovi\'c, Alan Ramponi, Siti Oryza Khairunnisa, Mamoru Komachi,
  Barbara Plank</authors><categories>cs.CL</categories><comments>To appear in the proceedings of NAACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lack of publicly available evaluation data for low-resource languages
limits progress in Spoken Language Understanding (SLU). As key tasks like
intent classification and slot filling require abundant training data, it is
desirable to reuse existing data in high-resource languages to develop models
for low-resource scenarios. We introduce xSID, a new benchmark for
cross-lingual Slot and Intent Detection in 13 languages from 6 language
families, including a very low-resource dialect. To tackle the challenge, we
propose a joint learning approach, with English SLU training data and
non-English auxiliary tasks from raw text, syntax and translation for transfer.
We study two setups which differ by type and language coverage of the
pre-trained embeddings. Our results show that jointly learning the main tasks
with masked language modeling is effective for slots, while machine translation
transfer works best for intent classification.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07319</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07319</id><submitter>Chengqi Zhao</submitter><version version="v1"><date>Sun, 16 May 2021 00:11:59 GMT</date><size>5563kb</size><source_type>D</source_type></version><title>The Volctrans Neural Speech Translation System for IWSLT 2021</title><authors>Chengqi Zhao and Zhicheng Liu and Jian Tong and Tao Wang and Mingxuan
  Wang and Rong Ye and Qianqian Dong and Jun Cao and Lei Li</authors><categories>cs.CL cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the systems submitted to IWSLT 2021 by the Volctrans
team. We participate in the offline speech translation and text-to-text
simultaneous translation tracks. For offline speech translation, our best
end-to-end model achieves 8.1 BLEU improvements over the benchmark on the
MuST-C test set and is even approaching the results of a strong cascade
solution. For text-to-text simultaneous translation, we explore the best
practice to optimize the wait-k model. As a result, our final submitted systems
exceed the benchmark at around 7 BLEU on the same latency regime. We will
publish our code and model to facilitate both future research works and
industrial applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07320</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07320</id><submitter>Vipul Gupta</submitter><version version="v1"><date>Sun, 16 May 2021 00:15:08 GMT</date><size>641kb</size><source_type>D</source_type></version><title>LocalNewton: Reducing Communication Bottleneck for Distributed Learning</title><authors>Vipul Gupta, Avishek Ghosh, Michal Derezinski, Rajiv Khanna, Kannan
  Ramchandran, Michael Mahoney</authors><categories>cs.DC stat.ML</categories><comments>To be published in Uncertainty in Artificial Intelligence (UAI) 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To address the communication bottleneck problem in distributed optimization
within a master-worker framework, we propose LocalNewton, a distributed
second-order algorithm with local averaging. In LocalNewton, the worker
machines update their model in every iteration by finding a suitable
second-order descent direction using only the data and model stored in their
own local memory. We let the workers run multiple such iterations locally and
communicate the models to the master node only once every few (say L)
iterations. LocalNewton is highly practical since it requires only one
hyperparameter, the number L of local iterations. We use novel matrix
concentration-based techniques to obtain theoretical guarantees for
LocalNewton, and we validate them with detailed empirical evaluation. To
enhance practicability, we devise an adaptive scheme to choose L, and we show
that this reduces the number of local iterations in worker machines between two
model synchronizations as the training proceeds, successively refining the
model quality at the master. Via extensive experiments using several real-world
datasets with AWS Lambda workers and an AWS EC2 master, we show that
LocalNewton requires fewer than 60% of the communication rounds (between master
and workers) and less than 40% of the end-to-end running time, compared to
state-of-the-art algorithms, to reach the same training~loss.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07322</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07322</id><submitter>Max Ehrlich</submitter><version version="v1"><date>Sun, 16 May 2021 00:57:43 GMT</date><size>6641kb</size><source_type>D</source_type></version><title>Unsupervised Super-Resolution of Satellite Imagery for High Fidelity
  Material Label Transfer</title><authors>Arthita Ghosh, Max Ehrlich, Larry Davis, Rama Chellappa</authors><categories>cs.CV cs.LG eess.IV</categories><comments>Published in the proceedings of the 2019 IEEE International
  Geoscience and Remote Sensing Symposium</comments><journal-ref>IGARSS (2019), 5144-5147</journal-ref><doi>10.1109/IGARSS.2019.8900639</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban material recognition in remote sensing imagery is a highly relevant,
yet extremely challenging problem due to the difficulty of obtaining human
annotations, especially on low resolution satellite images. To this end, we
propose an unsupervised domain adaptation based approach using adversarial
learning. We aim to harvest information from smaller quantities of high
resolution data (source domain) and utilize the same to super-resolve low
resolution imagery (target domain). This can potentially aid in semantic as
well as material label transfer from a richly annotated source to a target
domain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07324</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07324</id><submitter>Heather Wilber</submitter><version version="v1"><date>Sun, 16 May 2021 01:08:49 GMT</date><size>1775kb</size><source_type>D</source_type></version><title>Data-driven Algorithms for signal processing with rational functions</title><authors>Heather Wilber, Anil Damle and Alex Townsend</authors><categories>math.NA cs.NA</categories><comments>23 pages, 7 figures</comments><msc-class>41A20, 94A12</msc-class><acm-class>G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rational approximation schemes for reconstructing signals from samples with
poorly separated spectral content are described. These methods are automatic
and adaptive, requiring no tuning or manual parameter selection. Collectively,
they form a framework for fitting trigonometric rational models to data that is
robust to various forms of corruption, including additive Gaussian noise,
perturbed sampling grids, and missing data. Our approach combines a variant of
Prony's method with a modified version of the AAA algorithm. Using
representations in both frequency and time space, a collection of algorithms is
described for adaptively computing with trigonometric rationals. This includes
procedures for differentiation, filtering, convolution, and more.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07326</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07326</id><submitter>Roman Parovik</submitter><version version="v1"><date>Sun, 16 May 2021 01:11:29 GMT</date><size>295kb</size><source_type>D</source_type></version><title>Application of the Adams-Bashfort-Mowlton Method to the Numerical Study
  of Linear Fractional Oscillators Models</title><authors>Parovik Roman</authors><categories>math.NA cs.NA math.DS</categories><msc-class>65L05 (Primary), 34A08 (Secondary)</msc-class><acm-class>G.1.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper presents a numerical analysis of the class of mathematical models
of linear fractional oscillators, which is the Cauchy problem for a
differential equation with derivatives of fractional orders in the sense of
Gerasimov-Caputo. A method based on an explicit nonlocal finite-difference
scheme (ENFDS) and the Adams-Bashfort-Moulton (ABM) method is considered a
numerical analysis tool. An analysis of the errors of the methods is carried
out, and it is shown that the ABM method is more accurate and converges faster
to an exact solution than the ENFDS method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07327</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07327</id><submitter>Hao Wang</submitter><version version="v1"><date>Sun, 16 May 2021 01:12:53 GMT</date><size>3509kb</size></version><title>Que Bian: An Electronic Medical Record Management System on Blockchain</title><authors>Hao Wang</authors><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Medical Record Management System is an important information management
system in healthcare centers and hospitals. Information kept in such systems
need to be clean, correct and tamper-proof. In this paper, we take advantage of
blockchains' tamper-proof and decentralization properties to develop a robust
and secure electronic medical record management system. In particular we choose
HyperLedger Fabric as our underlying technical architecture. HyperLedger Fabric
yields higher throughput and lower latency compared with other blockchains,
which is a perfect candidate for enterprise software development. Our system is
a novel innovation that can serve as an ideal replacement for conventional
Medical Record Management System.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07329</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07329</id><submitter>Yash Kanoria</submitter><version version="v1"><date>Sun, 16 May 2021 01:49:58 GMT</date><size>30kb</size></version><title>Dynamic Matching under Spatial Frictions</title><authors>Yash Kanoria</authors><categories>math.PR cs.DS econ.TH math.OC</categories><msc-class>60D05</msc-class><acm-class>G.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider demand and supply which arise i.i.d. uniformly in the unit
hypercube [0,1]^d in d dimensions, and need to be matched with each other while
minimizing the expected average distance between matched pairs (the &quot;cost&quot;). We
characterize the scaling behavior of the achievable cost in three models as a
function of the dimension d: (i) Static matching of N demand units with N+M
supply units. (ii) A semi-dynamic model where N+M supply units are present
beforehand and N demand units arrive sequentially and must be matched
immediately. (iii) A fully dynamic model where there are always m supply units
present in the system, one supply and one demand unit arrive in each period,
and the demand must be matched immediately. We show that one can achieve nearly
the same cost under the semi-dynamic model as under the static model, despite
uncertainty about the future, and that, under these two models, d=1 is the only
case where cost far exceeds distance to the nearest neighbor (which is
\Theta(1/N^{1/d})) and where adding excess supply M substantially reduces cost
(by smoothing stochastic fluctuations at larger spatial length scales). In the
fully dynamic model, we show that, remarkably, for all d we can achieve a cost
only slightly more than the optimistic distance to the nearest neighbor
\Theta(1/m^{1/d}). Thus, excess supply m reduces cost in the fully dynamic
model for all $d$ by reducing the distance to the nearest neighbor. This is a
fundamentally different phenomenon than that seen in the other two models,
where excess supply reduces cost while leaving the distance to the nearest
neighbor unchanged, only for d=1. Our achievability results are based on
analysis of a certain &quot;Hierarchical Greedy&quot; algorithm which separately handles
stochastic fluctuations at different length scales.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07331</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07331</id><submitter>Haichao Yu</submitter><version version="v1"><date>Sun, 16 May 2021 02:07:44 GMT</date><size>5878kb</size><source_type>D</source_type></version><title>Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain
  Calibration for Network Quantization</title><authors>Haichao Yu, Linjie Yang, Humphrey Shi</authors><categories>cs.LG cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Post-training quantization methods use a set of calibration data to compute
quantization ranges for network parameters and activations. The calibration
data usually comes from the training dataset which could be inaccessible due to
sensitivity of the data. In this work, we want to study such a problem: can we
use out-of-domain data to calibrate the trained networks without knowledge of
the original dataset? Specifically, we go beyond the domain of natural images
to include drastically different domains such as X-ray images, satellite images
and ultrasound images. We find cross-domain calibration leads to surprisingly
stable performance of quantized models on 10 tasks in different image domains
with 13 different calibration datasets. We also find that the performance of
quantized models is correlated with the similarity of the Gram matrices between
the source and calibration domains, which can be used as a criterion to choose
calibration set for better performance. We believe our research opens the door
to borrow cross-domain knowledge for network quantization and compression.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07332</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07332</id><submitter>Virendra Sule</submitter><version version="v1"><date>Sun, 16 May 2021 02:27:56 GMT</date><size>23kb</size></version><title>A Complete algorithm for local inversion of maps: Application to
  Cryptanalysis</title><authors>Virendra Sule</authors><categories>cs.CR</categories><comments>31 pages</comments><acm-class>F.2.1; E.3; I.1.0; I.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a map (function) $F(x):\ftwo^n\rightarrow\ftwo^n$ and a given $y$ in the
image of $F$ the problem of \emph{local inversion} of $F$ is to find all
inverse images $x$ in $\ftwo^n$ such that $y=F(x)$. In Cryptology, such a
problem arises in Cryptanalysis of One way Functions (OWFs). The well known
TMTO attack in Cryptanalysis is a probabilistic algorithm for computing one
solution of local inversion using $O(\sqrt N)$ order computation in offline as
well as online for $N=2^n$. This paper proposes a complete algorithm for
solving the local inversion problem which uses linear complexity for a unique
solution in a periodic orbit. The algorithm is shown to require an offline
computation to solve a hard problem (possibly requiring exponential
computation) and an online computation dependent on $y$ that of repeated
forward evaluation $F(x)$ on points $x$ in $\ff_{2^n}$ which is polynomial time
at each evaluation. However the forward evaluation is repeated at most as many
number of times as the Linear Complexity of the sequence $\{y,F(y),\ldots\}$ to
get one possible solution when this sequence is periodic. All other solutions
are obtained in chains $\{e,F(e),\ldots\}$ for all points $e$ in the Garden of
Eden (GOE) of the map $F$. Hence a solution $x$ exists iff either the former
sequence is periodic or a solution occurs in a chain starting from a point in
GOE. The online computation then turns out to be polynomial time $O(L^k)$ in
the linear complexity $L$ of the sequence to compute one possible solution in a
periodic orbit or $O(l)$ the chain length for a fixed $n$. Hence this is a
complete algorithm for solving the problem of finding all rational solutions
$x$ of the equation $F(x)=y$ for a given $y$ and a map $F$ in $\ff_{2^n}$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07333</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07333</id><submitter>Douglas Zytko</submitter><version version="v1"><date>Sun, 16 May 2021 02:55:31 GMT</date><size>654kb</size></version><title>Computer-Mediated Consent to Sex: The Context of Tinder</title><authors>Douglas Zytko, Nicholas Furlo, Bailey Carlin, Matthew Archer</authors><categories>cs.HC</categories><comments>Pre-print. To appear in Proceedings of the ACM on Human-Computer
  Interaction, Vol. 5, CSCW1</comments><journal-ref>In Proceedings of the ACM on Human-Computer Interaction, Vol. 5,
  CSCW1, Article 189, 2021. ACM, New York, NY, USA</journal-ref><doi>10.1145/3449288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports an interview study about how consent to sexual activity is
computer-mediated. The study's context of online dating is chosen due to the
prevalence of sexual violence, or nonconsensual sexual activity, that is
associated with dating app-use. Participants (n=19) represent a range of gender
identities and sexual orientations, and predominantly used the dating app
Tinder. Findings reveal two computer-mediated consent processes: consent
signaling and affirmative consent. With consent signaling, users employed
Tinder's interface to infer and imply agreement to sex without any explicit
confirmation before making sexual advances in-person. With affirmative consent,
users employed the interface to establish patterns of overt discourse around
sex and consent across online and offline modalities. The paper elucidates
shortcomings of both computer-mediated consent processes that leave users
susceptible to sexual violence and envisions dating apps as potential sexual
violence prevention solutions if deliberately designed to mediate consent
exchange.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07334</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07334</id><submitter>Kenneth Co</submitter><version version="v1"><date>Sun, 16 May 2021 03:01:29 GMT</date><size>3962kb</size><source_type>D</source_type></version><title>Real-time Detection of Practical Universal Adversarial Perturbations</title><authors>Kenneth T. Co, Luis Mu\~noz-Gonz\'alez, Leslie Kanthan, Emil C. Lupu</authors><categories>cs.LG cs.AI cs.CR cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Universal Adversarial Perturbations (UAPs) are a prominent class of
adversarial examples that exploit the systemic vulnerabilities and enable
physically realizable and robust attacks against Deep Neural Networks (DNNs).
UAPs generalize across many different inputs; this leads to realistic and
effective attacks that can be applied at scale. In this paper we propose
HyperNeuron, an efficient and scalable algorithm that allows for the real-time
detection of UAPs by identifying suspicious neuron hyper-activations. Our
results show the effectiveness of HyperNeuron on multiple tasks (image
classification, object detection), against a wide variety of universal attacks,
and in realistic scenarios, like perceptual ad-blocking and adversarial
patches. HyperNeuron is able to simultaneously detect both adversarial mask and
patch UAPs with comparable or better performance than existing UAP defenses
whilst introducing a significantly reduced latency of only 0.86 milliseconds
per image. This suggests that many realistic and practical universal attacks
can be reliably mitigated in real-time, which shows promise for the robust
deployment of machine learning systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07338</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07338</id><submitter>Ming-Kun Xie</submitter><version version="v1"><date>Sun, 16 May 2021 03:24:15 GMT</date><size>44kb</size></version><title>CCMN: A General Framework for Learning with Class-Conditional
  Multi-Label Noise</title><authors>Ming-Kun Xie and Sheng-Jun Huang</authors><categories>cs.LG stat.ML</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Class-conditional noise commonly exists in machine learning tasks, where the
class label is corrupted with a probability depending on its ground-truth. Many
research efforts have been made to improve the model robustness against the
class-conditional noise. However, they typically focus on the single label case
by assuming that only one label is corrupted. In real applications, an instance
is usually associated with multiple labels, which could be corrupted
simultaneously with their respective conditional probabilities. In this paper,
we formalize this problem as a general framework of learning with
Class-Conditional Multi-label Noise (CCMN for short). We establish two unbiased
estimators with error bounds for solving the CCMN problems, and further prove
that they are consistent with commonly used multi-label loss functions.
Finally, a new method for partial multi-label learning is implemented with
unbiased estimator under the CCMN framework. Empirical studies on multiple
datasets and various evaluation metrics validate the effectiveness of the
proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07342</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07342</id><submitter>Lirong Wu</submitter><version version="v1"><date>Sun, 16 May 2021 03:30:03 GMT</date><size>4176kb</size><source_type>D</source_type></version><title>Self-supervised on Graphs: Contrastive, Generative,or Predictive</title><authors>Lirong Wu, Haitao Lin, Zhangyang Gao, Cheng Tan, Stan.Z.Li</authors><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning on graphs has recently achieved remarkable success on a variety
of tasks while such success relies heavily on the massive and carefully labeled
data. However, precise annotations are generally very expensive and
time-consuming. To address this problem, self-supervised learning (SSL) is
emerging as a new paradigm for extracting informative knowledge through
well-designed pretext tasks without relying on manual labels. In this survey,
we extend the concept of SSL, which first emerged in the fields of computer
vision and natural language processing, to present a timely and comprehensive
review of the existing SSL techniques for graph data. Specifically, we divide
existing graph SSL methods into three categories: contrastive, generative, and
predictive. More importantly, unlike many other surveys that only provide a
high-level description of published research, we present an additional
mathematical summary of the existing works in a unified framework. Furthermore,
to facilitate methodological development and empirical comparisons, we also
summarize the commonly used datasets, evaluation metrics, downstream tasks, and
open-source implementations of various algorithms. Finally, we discuss the
technical challenges and potential future directions for improving graph
self-supervised learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07343</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07343</id><submitter>Apurva Badithela</submitter><version version="v1"><date>Sun, 16 May 2021 03:41:32 GMT</date><size>2917kb</size><source_type>D</source_type></version><title>Leveraging Classification Metrics for Quantitative System-Level Analysis
  with Temporal Logic Specifications</title><authors>Apurva Badithela, Tichakorn Wongpiromsarn, Richard M. Murray</authors><categories>eess.SY cs.FL cs.RO cs.SY</categories><comments>This conference paper has been submitted to the 60th IEEE Conference
  on Decision and Control (CDC 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many autonomy applications, performance of perception algorithms is
important for effective planning and control. In this paper, we introduce a
framework for computing the probability of satisfaction of formal system
specifications given a confusion matrix, a statistical average performance
measure for multi-class classification. We define the probability of
satisfaction of a linear temporal logic formula given a specific initial state
of the agent and true state of the environment. Then, we present an algorithm
to construct a Markov chain that represents the system behavior under the
composition of the perception and control components such that the probability
of the temporal logic formula computed over the Markov chain is consistent with
the probability that the temporal logic formula is satisfied by our system. We
illustrate this approach on a simple example of a car with pedestrian on the
sidewalk environment, and compute the probability of satisfaction of safety
requirements for varying parameters of the vehicle. We also illustrate how
satisfaction probability changes with varied precision and recall derived from
the confusion matrix. Based on our results, we identify several opportunities
for future work in developing quantitative system-level analysis that
incorporates perception models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07345</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07345</id><submitter>Shijie Yu</submitter><version version="v1"><date>Sun, 16 May 2021 03:53:55 GMT</date><size>3610kb</size><source_type>D</source_type></version><title>Neighbourhood-guided Feature Reconstruction for Occluded Person
  Re-Identification</title><authors>Shijie Yu and Dapeng Chen and Rui Zhao and Haobin Chen and Yu Qiao</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person images captured by surveillance cameras are often occluded by various
obstacles, which lead to defective feature representation and harm person
re-identification (Re-ID) performance. To tackle this challenge, we propose to
reconstruct the feature representation of occluded parts by fully exploiting
the information of its neighborhood in a gallery image set. Specifically, we
first introduce a visible part-based feature by body mask for each person
image. Then we identify its neighboring samples using the visible features and
reconstruct the representation of the full body by an outlier-removable graph
neural network with all the neighboring samples as input. Extensive experiments
show that the proposed approach obtains significant improvements. In the
large-scale Occluded-DukeMTMC benchmark, our approach achieves 64.2% mAP and
67.6% rank-1 accuracy which outperforms the state-of-the-art approaches by
large margins, i.e.,20.4% and 12.5%, respectively, indicating the effectiveness
of our method on occluded Re-ID problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07346</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07346</id><submitter>Ziyu Ye</submitter><version version="v1"><date>Sun, 16 May 2021 03:55:02 GMT</date><size>5039kb</size><source_type>D</source_type></version><title>Understanding the Effect of Bias in Deep Anomaly Detection</title><authors>Ziyu Ye, Yuxin Chen and Haitao Zheng</authors><categories>cs.LG cs.AI</categories><comments>Accepted at IJCAI '21. Codes available on
  github.com/ZIYU-DEEP/Understanding-Bias-in-Deep-Anomaly-Detection-PyTorch</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly detection presents a unique challenge in machine learning, due to the
scarcity of labeled anomaly data. Recent work attempts to mitigate such
problems by augmenting training of deep anomaly detection models with
additional labeled anomaly samples. However, the labeled data often does not
align with the target distribution and introduces harmful bias to the trained
model. In this paper, we aim to understand the effect of a biased anomaly set
on anomaly detection. Concretely, we view anomaly detection as a supervised
learning task where the objective is to optimize the recall at a given false
positive rate. We formally study the relative scoring bias of an anomaly
detector, defined as the difference in performance with respect to a baseline
anomaly detector. We establish the first finite sample rates for estimating the
relative scoring bias for deep anomaly detection, and empirically validate our
theoretical results on both synthetic and real-world datasets. We also provide
an extensive empirical study on how a biased training anomaly set affects the
anomaly score function and therefore the detection performance on different
anomaly classes. Our study demonstrates scenarios in which the biased anomaly
set can be useful or problematic, and provides a solid benchmark for future
research.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07348</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07348</id><submitter>Dandan Zhang</submitter><version version="v1"><date>Sun, 16 May 2021 04:30:53 GMT</date><size>3208kb</size><source_type>D</source_type></version><title>Explainable Hierarchical Imitation Learning for Robotic Drink Pouring</title><authors>Dandan Zhang, Yu Zheng, Qiang Li, Lei Wei, Dongsheng Zhang, Zhengyou
  Zhang</authors><categories>cs.RO cs.AI</categories><comments>15 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To accurately pour drinks into various containers is an essential skill for
service robots. However, drink pouring is a dynamic process and difficult to
model. Traditional deep imitation learning techniques for implementing
autonomous robotic pouring have an inherent black-box effect and require a
large amount of demonstration data for model training. To address these issues,
an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in
this paper such that a robot can learn high-level general knowledge and execute
low-level actions across multiple drink pouring scenarios. Moreover, with EHIL,
a logical graph can be constructed for task execution, through which the
decision-making process for action generation can be made explainable to users
and the causes of failure can be traced out. Based on the logical graph, the
framework is manipulable to achieve different targets while the adaptability to
unseen scenarios can be achieved in an explainable manner. A series of
experiments have been conducted to verify the effectiveness of the proposed
method. Results indicate that EHIL outperforms the traditional behavior cloning
method in terms of success rate, adaptability, manipulability and
explainability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07350</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07350</id><submitter>Zicheng Zhang</submitter><version version="v1"><date>Sun, 16 May 2021 04:38:46 GMT</date><size>31356kb</size><source_type>D</source_type></version><title>ExSinGAN: Learning an Explainable Generative Model from a Single Image</title><authors>ZiCheng Zhang, CongYing Han, TianDe Guo</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating images from a single sample, as a newly developing branch of image
synthesis, has attracted extensive attention. In this paper, we formulate this
problem as sampling from the conditional distribution of a single image, and
propose a hierarchical framework that simplifies the learning of the intricate
conditional distributions through the successive learning of the distributions
about structure, semantics and texture, making the process of learning and
generation comprehensible. On this basis, we design ExSinGAN composed of three
cascaded GANs for learning an explainable generative model from a given image,
where the cascaded GANs model the distributions about structure, semantics and
texture successively. ExSinGAN is learned not only from the internal patches of
the given image as the previous works did, but also from the external prior
obtained by the GAN inversion technique. Benefiting from the appropriate
combination of internal and external information, ExSinGAN has a more powerful
capability of generation and competitive generalization ability for the image
manipulation tasks compared with prior works.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07351</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07351</id><submitter>Xianyuan Zhan</submitter><version version="v1"><date>Sun, 16 May 2021 05:00:54 GMT</date><size>2280kb</size><source_type>D</source_type></version><title>Model-Based Offline Planning with Trajectory Pruning</title><authors>Xianyuan Zhan, Xiangyu Zhu, Haoran Xu</authors><categories>cs.AI cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offline reinforcement learning (RL) enables learning policies using
pre-collected datasets without environment interaction, which provides a
promising direction to make RL useable in real-world systems. Although recent
offline RL studies have achieved much progress, existing methods still face
many practical challenges in real-world system control tasks, such as
computational restriction during agent training and the requirement of extra
control flexibility. Model-based planning framework provides an attractive
solution for such tasks. However, most model-based planning algorithms are not
designed for offline settings. Simply combining the ingredients of offline RL
with existing methods either provides over-restrictive planning or leads to
inferior performance. We propose a new light-weighted model-based offline
planning framework, namely MOPP, which tackles the dilemma between the
restrictions of offline learning and high-performance planning. MOPP encourages
more aggressive trajectory rollout guided by the behavior policy learned from
data, and prunes out problematic trajectories to avoid potential
out-of-distribution samples. Experimental results show that MOPP provides
competitive performance compared with existing model-based offline planning and
RL approaches, and allows easy adaptation to varying objectives and extra
constraints.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07352</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07352</id><submitter>Roman Parovik</submitter><version version="v1"><date>Sun, 16 May 2021 05:04:26 GMT</date><size>544kb</size><source_type>D</source_type></version><title>Numerical Modeling of Kondratyev's Long Waves Taking into Account
  Heredity</title><authors>Danil Makarov, Roman Parovik</authors><categories>math.NA cs.NA</categories><msc-class>37M05 (Primary), 26A33 (Secondary</msc-class><acm-class>G.1.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper proposes a new mathematical model of economic cycles and crises,
which generalizes the well-known model of Dubovsky S.V. The novelty of the
proposed model lies in taking into account the effect of heredity (memory), as
well as the introduction of harmonic functions responsible for the arrival of
investments in fixed assets and new management technologies in innovation. The
mathematical description is given using the Gerasimov-Caputo fractional
derivatives, which are studied within the framework of the theory of fractional
calculus. The mathematical model was investigated using the numerical method of
Adams-Bashforth-Moulton (ABM), phase trajectories were constructed. It is shown
that the proposed mathematical model can have both regular and chaotic regimes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07354</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07354</id><submitter>Catarina Moreira</submitter><version version="v1"><date>Sun, 16 May 2021 05:24:04 GMT</date><size>2992kb</size><source_type>D</source_type></version><title>Order Effects in Bayesian Updates</title><authors>Catarina Moreira and Jose Acacio de Barros</authors><categories>cs.AI math.PR</categories><journal-ref>In Proceedings of the 43rd Annual Meeting of the Cognitive Science
  Society, 2021</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Order effects occur when judgments about a hypothesis's probability given a
sequence of information do not equal the probability of the same hypothesis
when the information is reversed. Different experiments have been performed in
the literature that supports evidence of order effects.
  We proposed a Bayesian update model for order effects where each question can
be thought of as a mini-experiment where the respondents reflect on their
beliefs. We showed that order effects appear, and they have a simple cognitive
explanation: the respondent's prior belief that two questions are correlated.
  The proposed Bayesian model allows us to make several predictions: (1) we
found certain conditions on the priors that limit the existence of order
effects; (2) we show that, for our model, the QQ equality is not necessarily
satisfied (due to symmetry assumptions); and (3) the proposed Bayesian model
has the advantage of possessing fewer parameters than its quantum counterpart.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07360</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07360</id><submitter>George Grispos</submitter><version version="v1"><date>Sun, 16 May 2021 05:54:24 GMT</date><size>529kb</size></version><title>Investigating Protected Health Information Leakage from Android Medical
  Applications</title><authors>George Grispos and Talon Flynn and William Glisson and Kim-Kwang
  Raymond Choo</authors><categories>cs.CY cs.CR</categories><comments>Presented at the 5th EAI International Conference on Future Access
  Enablers of Ubiquitous and Intelligent Infrastructures (EAI FABULOUS 2021),
  Zagreb, Croatia</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As smartphones and smartphone applications are widely used in a healthcare
context (e.g., remote healthcare), these devices and applications may need to
comply with the Health Insurance Portability and Accountability Act (HIPAA) of
1996. In other words, adequate safeguards to protect the user's sensitive
information (e.g., personally identifiable information and/or medical history)
are required to be enforced on such devices and applications. In this study, we
forensically focus on the potential of recovering residual data from Android
medical applications, with the objective of providing an initial risk
assessment of such applications. Our findings (e.g., documentation of the
artifacts) also contribute to a better understanding of the types and location
of evidential artifacts that can, potentially, be recovered from these
applications in a digital forensic investigation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07362</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07362</id><submitter>Anup Mishra</submitter><version version="v1"><date>Sun, 16 May 2021 05:55:06 GMT</date><size>1960kb</size></version><title>Rate-Splitting Multiple Access for Downlink Multiuser MIMO: Precoder
  Optimization and PHY-Layer Design</title><authors>Anup Mishra, Yijie Mao, Onur Dizdar and Bruno Clerckx</authors><categories>cs.IT eess.SP math.IT</categories><comments>Submitted to journals for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rate-Splitting Multiple Access (RSMA) has recently appeared as a powerful and
robust multiple access and interference management strategy for downlink
Multi-user (MU) multi-antenna communications. In this work, we study the
precoder design problem for RSMA scheme in downlink MU systems with both
perfect and imperfect Channel State Information at the Transmitter (CSIT) and
assess the role and benefits of transmitting multiple common streams. Unlike
existing works which have considered single-antenna receivers (Multiple-Input
Single-Output--MISO), we propose and extend the RSMA framework for
multi-antenna receivers (Multiple-Input Multiple-Output--MIMO) and formulate
the precoder optimization problem with the aim of maximizing the Weighted
Ergodic Sum-Rate (WESR). Precoder optimization is solved using Sample Average
Approximation (SAA) together with the proposed vectorization and Weighted
Minimum Mean Square Error (WMMSE) based approach. Achievable sum-Degree of
Freedom (DoF) of RSMA is derived for the proposed framework as an increasing
function of the number of transmitted common and private streams, which is
further validated by the Ergodic Sum Rate (ESR) performance using Monte Carlo
simulations. Conventional MU-MIMO based on linear precoders and Non-Orthogonal
Multiple Access (NOMA) schemes are considered as baselines. Numerical results
show that with imperfect CSIT, the sum-DoF and ESR performance of RSMA is
superior than that of the two baselines, and is increasing with the number of
transmitted common streams. Moreover, by better managing the interference, RSMA
not only has significant ESR gains over baseline schemes but is more robust to
CSIT inaccuracies, network loads and user deployments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07364</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07364</id><submitter>Yu Shen</submitter><version version="v1"><date>Sun, 16 May 2021 06:13:28 GMT</date><size>23924kb</size><source_type>D</source_type></version><title>BDANet: Multiscale Convolutional Neural Network with Cross-directional
  Attention for Building Damage Assessment from Satellite Images</title><authors>Yu Shen, Sijie Zhu, Taojiannan Yang, Chen Chen, Delu Pan, Jianyu Chen,
  Liang Xiao, Qian Du</authors><categories>cs.CV cs.LG eess.IV</categories><comments>arXiv admin note: text overlap with arXiv:2010.14014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast and effective responses are required when a natural disaster (e.g.,
earthquake, hurricane, etc.) strikes. Building damage assessment from satellite
imagery is critical before relief effort is deployed. With a pair of pre- and
post-disaster satellite images, building damage assessment aims at predicting
the extent of damage to buildings. With the powerful ability of feature
representation, deep neural networks have been successfully applied to building
damage assessment. Most existing works simply concatenate pre- and
post-disaster images as input of a deep neural network without considering
their correlations. In this paper, we propose a novel two-stage convolutional
neural network for Building Damage Assessment, called BDANet. In the first
stage, a U-Net is used to extract the locations of buildings. Then the network
weights from the first stage are shared in the second stage for building damage
assessment. In the second stage, a two-branch multi-scale U-Net is employed as
backbone, where pre- and post-disaster images are fed into the network
separately. A cross-directional attention module is proposed to explore the
correlations between pre- and post-disaster images. Moreover, CutMix data
augmentation is exploited to tackle the challenge of difficult classes. The
proposed method achieves state-of-the-art performance on a large-scale dataset
-- xBD. The code is available at
https://github.com/ShaneShen/BDANet-Building-Damage-Assessment.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07366</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07366</id><submitter>Stephen MacDonell</submitter><version version="v1"><date>Sun, 16 May 2021 06:15:30 GMT</date><size>1965kb</size></version><title>Investigating the Significance of the Bellwether Effect to Improve
  Software Effort Prediction: Further Empirical Study</title><authors>Solomon Mensah, Jacky Keung, Stephen G. MacDonell, Michael Franklin
  Bosu, and Kwabena Ebo Bennin</authors><categories>cs.SE</categories><comments>Journal paper, 23 pages, 8 figures, 9 tables</comments><journal-ref>IEEE Transactions on Reliability 67(3)(2018), pp.1176-1198</journal-ref><doi>10.1109/TR.2018.2839718</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: In addressing how best to estimate how much effort is required to
develop software, a recent study found that using exemplary and recently
completed projects [forming Bellwether moving windows (BMW)] in software effort
prediction (SEP) models leads to relatively improved accuracy. More studies
need to be conducted to determine whether the BMW yields improved accuracy in
general, since different sizing and aging parameters of the BMW are known to
affect accuracy. Objective: To investigate the existence of exemplary projects
(Bellwethers) with defined window size and age parameters, and whether their
use in SEP improves prediction accuracy. Method: We empirically investigate the
moving window assumption based on the theory that the prediction outcome of a
future event depends on the outcomes of prior events. Sampling of Bellwethers
was undertaken using three introduced Bellwether methods (SSPM, SysSam, and
RandSam). The ergodic Markov chain was used to determine the stationarity of
the Bell-wethers. Results: Empirical results show that 1) Bellwethers exist in
SEP and 2) the BMW has an approximate size of 50 to 80 exemplary projects that
should not be more than 2 years old relative to the new projects to be
estimated. Conclusion: The study's results add further weight to the
recommended use of Bellwethers for improved prediction accuracy in SEP.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07367</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07367</id><submitter>Myungjong Kim</submitter><version version="v1"><date>Sun, 16 May 2021 06:23:36 GMT</date><size>553kb</size><source_type>D</source_type></version><title>X-Vectors with Multi-Scale Aggregation for Speaker Diarization</title><authors>Myungjong Kim, Vijendra Raj Apsingekar, Divya Neelagiri</authors><categories>eess.AS cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speaker diarization is the process of labeling different speakers in a speech
signal. Deep speaker embeddings are generally extracted from short speech
segments and clustered to determine the segments belong to same speaker
identity. The x-vector, which embeds segment-level speaker characteristics by
statistically pooling frame-level representations, is one of the most widely
used deep speaker embeddings in speaker diarization. Multi-scale aggregation,
which employs multi-scale representations from different layers, has recently
successfully been used in short duration speaker verification. In this paper,
we investigate a multi-scale aggregation approach in an x-vector embedding
framework for speaker diarization by exploiting multiple statistics pooling
layers from different frame-level layers. Thus, it is expected that x-vectors
with multi-scale aggregation have the potential to capture meaningful speaker
characteristics from short segments, effectively taking advantage of different
information at multiple layers. Experimental evaluation on the CALLHOME dataset
showed that our approach provides substantial improvement over the baseline
x-vectors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07369</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07369</id><submitter>Christian Lex</submitter><version version="v1"><date>Sun, 16 May 2021 06:47:19 GMT</date><size>7kb</size></version><title>Low-Complexity PIR Using Subfield Subcodes</title><authors>Christian J. Lex and Oliver W. Gnilke</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A major drawback of many PIR schemes is the highcomputational cost at the
servers. We present a scheme that usesonly operations in the prime field during
response generation.For binary extension fields this leads to schemes that only
needXOR operations at the servers to calculate the responses. This isachieved
by restricting the queries to a subfield subcode or tracecode. We investigate
possible parameter ranges and focus on theexample of GRS codes and subfield
subcodes of these.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07371</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07371</id><submitter>Gesina Schwalbe</submitter><version version="v1"><date>Sun, 16 May 2021 07:00:27 GMT</date><size>1973kb</size><source_type>D</source_type></version><title>Expressive Explanations of DNNs by Combining Concept Analysis with ILP</title><authors>Johannes Rabold, Gesina Schwalbe, Ute Schmid</authors><categories>cs.LG cs.CV</categories><comments>14 pages, 4 figures; Camera-ready submission to KI2020; The final
  authenticated publication is available online at
  https://doi.org/10.1007/978-3-030-58285-2_11; code available at
  https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020</comments><doi>10.1007/978-3-030-58285-2_11</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Explainable AI has emerged to be a key component for black-box machine
learning approaches in domains with a high demand for reliability or
transparency. Examples are medical assistant systems, and applications
concerned with the General Data Protection Regulation of the European Union,
which features transparency as a cornerstone. Such demands require the ability
to audit the rationale behind a classifier's decision. While visualizations are
the de facto standard of explanations, they come short in terms of
expressiveness in many ways: They cannot distinguish between different
attribute manifestations of visual features (e.g. eye open vs. closed), and
they cannot accurately describe the influence of absence of, and relations
between features. An alternative would be more expressive symbolic surrogate
models. However, these require symbolic inputs, which are not readily available
in most computer vision tasks. In this paper we investigate how to overcome
this: We use inherent features learned by the network to build a global,
expressive, verbal explanation of the rationale of a feed-forward convolutional
deep neural network (DNN). The semantics of the features are mined by a concept
analysis approach trained on a set of human understandable visual concepts. The
explanation is found by an Inductive Logic Programming (ILP) method and
presented as first-order rules. We show that our explanation is faithful to the
original black-box model.
  The code for our experiments is available at
https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07372</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07372</id><submitter>Noam Janco</submitter><version version="v1"><date>Sun, 16 May 2021 07:25:51 GMT</date><size>232kb</size></version><title>An accelerated expectation-maximization for multi-reference alignment</title><authors>Noam Janco and Tamir Bendory</authors><categories>eess.SP cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-reference alignment (MRA) problem entails estimating an image from
multiple noisy and rotated copies of itself. If the noise level is low, one can
reconstruct the image by estimating the missing rotations, aligning the images,
and averaging out the noise. While accurate rotation estimation is impossible
if the noise level is high, the rotations can still be approximated, and thus
can provide indispensable information. In particular, learning the
approximation error can be harnessed for efficient image estimation. In this
paper, we propose a new computational framework, called Synch-EM, that consists
of angular synchronization followed by expectation-maximization (EM). The
synchronization step results in a concentrated distribution of rotations; this
distribution is learned and then incorporated into the EM as a Bayesian prior.
The learned distribution also dramatically reduces the search space, and thus
the computational load, of the EM iterations. We show by extensive numerical
experiments that the proposed framework can significantly accelerate EM for MRA
in high noise levels, occasionally by a few orders of magnitude, without
degrading the reconstruction quality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07377</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07377</id><submitter>Lei Chen</submitter><version version="v1"><date>Sun, 16 May 2021 08:06:22 GMT</date><size>386kb</size><source_type>D</source_type></version><title>Set2setRank: Collaborative Set to Set Ranking for Implicit Feedback
  based Recommendation</title><authors>Lei Chen, Le Wu, Kun Zhang, Richang Hong, Meng Wang</authors><categories>cs.IR cs.AI</categories><comments>The paper is accepted by SIGIR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As users often express their preferences with binary behavior data~(implicit
feedback), such as clicking items or buying products, implicit feedback based
Collaborative Filtering~(CF) models predict the top ranked items a user might
like by leveraging implicit user-item interaction data. For each user, the
implicit feedback is divided into two sets: an observed item set with limited
observed behaviors, and a large unobserved item set that is mixed with negative
item behaviors and unknown behaviors. Given any user preference prediction
model, researchers either designed ranking based optimization goals or relied
on negative item mining techniques for better optimization. Despite the
performance gain of these implicit feedback based models, the recommendation
results are still far from satisfactory due to the sparsity of the observed
item set for each user. To this end, in this paper, we explore the unique
characteristics of the implicit feedback and propose Set2setRank framework for
recommendation. The optimization criteria of Set2setRank are two folds: First,
we design an item to an item set comparison that encourages each observed item
from the sampled observed set is ranked higher than any unobserved item from
the sampled unobserved set. Second, we model set level comparison that
encourages a margin between the distance summarized from the observed item set
and the most &quot;hard&quot; unobserved item from the sampled negative set. Further, an
adaptive sampling technique is designed to implement these two goals. We have
to note that our proposed framework is model-agnostic and can be easily applied
to most recommendation prediction approaches, and is time efficient in
practice. Finally, extensive experiments on three real-world datasets
demonstrate the superiority of our proposed approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07378</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07378</id><submitter>Silvia Gazzola</submitter><version version="v1"><date>Sun, 16 May 2021 08:09:39 GMT</date><size>541kb</size></version><title>Regularization by inexact Krylov methods with applications to blind
  deblurring</title><authors>Silvia Gazzola and Malena Sabat\'e Landman</authors><categories>math.NA cs.NA</categories><msc-class>65F20, 65F22, 65F30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is concerned with the regularization of large-scale discrete
inverse problems by means of inexact Krylov methods. Specifically, we derive
two new inexact Krylov methods that can be efficiently applied to unregularized
or Tikhonov-regularized least squares problems, and we study their theoretical
properties, including links with their exact counterparts and strategies to
monitor the amount of inexactness. We then apply the new methods to separable
nonlinear inverse problems arising in blind deblurring. In this setting
inexactness stems from the uncertainty in the parameters defining the blur,
which may be recovered using a variable projection method leading to an
inner-outer iteration scheme (i.e., one cycle of inner iterations is performed
to solve one linear deblurring subproblem for any intermediate values of the
blurring parameters computed by a nonlinear least squares solver). The new
inexact solvers can naturally handle varying inexact blurring parameters while
solving the linear deblurring subproblems, allowing for a much reduced number
of total iterations and substantial computational savings with respect to their
exact counterparts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07381</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07381</id><submitter>Haoyu Ma</submitter><version version="v1"><date>Sun, 16 May 2021 08:41:30 GMT</date><size>854kb</size><source_type>D</source_type></version><title>Undistillable: Making A Nasty Teacher That CANNOT teach students</title><authors>Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie,
  Zhangyang Wang</authors><categories>cs.LG cs.CR</categories><comments>ICLR 2021(Spotlight). Code is available at
  https://github.com/VITA-Group/Nasty-Teacher</comments><journal-ref>ICLR 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge Distillation (KD) is a widely used technique to transfer knowledge
from pre-trained teacher models to (usually more lightweight) student models.
However, in certain situations, this technique is more of a curse than a
blessing. For instance, KD poses a potential risk of exposing intellectual
properties (IPs): even if a trained machine learning model is released in
'black boxes' (e.g., as executable software or APIs without open-sourcing
code), it can still be replicated by KD through imitating input-output
behaviors. To prevent this unwanted effect of KD, this paper introduces and
investigates a concept called Nasty Teacher: a specially trained teacher
network that yields nearly the same performance as a normal one, but would
significantly degrade the performance of student models learned by imitating
it. We propose a simple yet effective algorithm to build the nasty teacher,
called self-undermining knowledge distillation. Specifically, we aim to
maximize the difference between the output of the nasty teacher and a normal
pre-trained network. Extensive experiments on several datasets demonstrate that
our method is effective on both standard KD and data-free KD, providing the
desirable KD-immunity to model owners for the first time. We hope our
preliminary study can draw more awareness and interest in this new practical
problem of both social and legal importance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07382</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07382</id><submitter>Tianxiang Zhan</submitter><version version="v1"><date>Sun, 16 May 2021 08:41:38 GMT</date><size>620kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 01:01:59 GMT</date><size>620kb</size><source_type>D</source_type></version><title>Uncertainty Measurement of Basic Probability Assignment Integrity Based
  on Approximate Entropy in Evidence Theory</title><authors>Tianxiang Zhan, Yuanpeng He, Hanwen Li, Fuyuan Xiao</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidence theory is that the extension of probability can better deal with
unknowns and inaccurate information. Uncertainty measurement plays a vital role
in both evidence theory and probability theory. Approximate Entropy (ApEn) is
proposed by Pincus to describe the irregularities of complex systems. The more
irregular the time series, the greater the approximate entropy. The ApEn of the
network represents the ability of a network to generate new nodes, or the
possibility of undiscovered nodes. Through the association of network
characteristics and basic probability assignment (BPA) , a measure of the
uncertainty of BPA regarding completeness can be obtained. The main
contribution of paper is to define the integrity of the basic probability
assignment then the approximate entropy of the BPA is proposed to measure the
uncertainty of the integrity of the BPA. The proposed method is based on the
logical network structure to calculate the uncertainty of BPA in evidence
theory. The uncertainty based on the proposed method represents the uncertainty
of integrity of BPA and contributes to the identification of the credibility of
BPA.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07383</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07383</id><submitter>Saksham Bhushan</submitter><version version="v1"><date>Sun, 16 May 2021 08:45:45 GMT</date><size>1239kb</size><source_type>D</source_type></version><title>Dimensioning an Indoor SISO RIS-system: Approximations and Equivalence
  Models</title><authors>Saksham Bhushan, Sai Teja Suggala, Arzad A. Kherani, Sreejith T. V</authors><categories>cs.IT math.IT</categories><comments>4 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We provide closed-form approximations to the performance gain achieved in a
RIS-assisted communication. We then consider a network deployment of RIS and
Transmitter-Receiver pairs and use these approximate expressions to provide
equivalence models which state that the performance of a RIS-equipped network
is similar to the performance of an appropriately spatially scaled system. We
provide a way of assigning available RIS to assist communication between
several transmitter-receiver pairs. Several such approximations are expected to
spawn from this study, with more clarity on structural aspects of the gains
achieved in RIS-assisted communications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07385</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07385</id><submitter>Haruka Asanuma</submitter><version version="v1"><date>Sun, 16 May 2021 09:02:48 GMT</date><size>2128kb</size><source_type>D</source_type></version><title>Statistical Mechanical Analysis of Catastrophic Forgetting in Continual
  Learning with Teacher and Student Networks</title><authors>Haruka Asanuma, Shiro Takagi, Yoshihiro Nagano, Yuki Yoshida, Yasuhiko
  Igarashi, and Masato Okada</authors><categories>stat.ML cs.LG</categories><comments>22 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a computational system continuously learns from an ever-changing
environment, it rapidly forgets its past experiences. This phenomenon is called
catastrophic forgetting. While a line of studies has been proposed with respect
to avoiding catastrophic forgetting, most of the methods are based on intuitive
insights into the phenomenon, and their performances have been evaluated by
numerical experiments using benchmark datasets. Therefore, in this study, we
provide the theoretical framework for analyzing catastrophic forgetting by
using teacher-student learning. Teacher-student learning is a framework in
which we introduce two neural networks: one neural network is a target function
in supervised learning, and the other is a learning neural network. To analyze
continual learning in the teacher-student framework, we introduce the
similarity of the input distribution and the input-output relationship of the
target functions as the similarity of tasks. In this theoretical framework, we
also provide a qualitative understanding of how a single-layer linear learning
neural network forgets tasks. Based on the analysis, we find that the network
can avoid catastrophic forgetting when the similarity among input distributions
is small and that of the input-output relationship of the target functions is
large. The analysis also suggests that a system often exhibits a characteristic
phenomenon called overshoot, which means that even if the learning network has
once undergone catastrophic forgetting, it is possible that the network may
perform reasonably well after further learning of the current task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07387</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07387</id><submitter>Yuhang Zhang</submitter><version version="v1"><date>Sun, 16 May 2021 09:13:56 GMT</date><size>927kb</size><source_type>D</source_type></version><title>Semi-supervised Contrastive Learning with Similarity Co-calibration</title><authors>Yuhang Zhang and Xiaopeng Zhang and Robert.C.Qiu and Jie Li and
  Haohang Xu and Qi Tian</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semi-supervised learning acts as an effective way to leverage massive
unlabeled data. In this paper, we propose a novel training strategy, termed as
Semi-supervised Contrastive Learning (SsCL), which combines the well-known
contrastive loss in self-supervised learning with the cross entropy loss in
semi-supervised learning, and jointly optimizes the two objectives in an
end-to-end way. The highlight is that different from self-training based
semi-supervised learning that conducts prediction and retraining over the same
model weights, SsCL interchanges the predictions over the unlabeled data
between the two branches, and thus formulates a co-calibration procedure, which
we find is beneficial for better prediction and avoid being trapped in local
minimum. Towards this goal, the contrastive loss branch models pairwise
similarities among samples, using the nearest neighborhood generated from the
cross entropy branch, and in turn calibrates the prediction distribution of the
cross entropy branch with the contrastive similarity. We show that SsCL
produces more discriminative representation and is beneficial to few shot
learning. Notably, on ImageNet with ResNet50 as the backbone, SsCL achieves
60.2% and 72.1% top-1 accuracy with 1% and 10% labeled samples, respectively,
which significantly outperforms the baseline, and is better than previous
semi-supervised and self-supervised methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07388</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07388</id><submitter>Maike Meier</submitter><version version="v1"><date>Sun, 16 May 2021 09:20:08 GMT</date><size>3190kb</size><source_type>D</source_type></version><title>Fast randomized numerical rank estimation</title><authors>Maike Meier and Yuji Nakatsukasa</authors><categories>math.NA cs.NA</categories><msc-class>65F55 (primary) 65F99, 68W20, 60B20 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrices with low-rank structure are ubiquitous in scientific computing.
Choosing an appropriate rank is a key step in many computational algorithms
that exploit low-rank structure. However, estimating the rank has been done
largely in an ad-hoc fashion in previous studies. In this work we develop a
randomized algorithm for estimating the numerical rank of a matrix. The
algorithm is based on sketching the matrix with random matrices from both left
and right; the key fact is that with high probability, the sketches preserve
the orders of magnitude of the leading singular values. The rank can hence be
taken to be the number of singular values of the sketch that are larger than
the prescribed threshold. For an $m\times n$ $(m\geq n)$ matrix of numerical
rank $r$, the algorithm runs with complexity $O(mn\log n+r^3)$, or less for
structured matrices. The steps in the algorithm are required as a part of many
low-rank algorithms, so the additional work required to estimate the rank can
be even smaller in practice. Numerical experiments illustrate the speed and
robustness of our rank estimator.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07391</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07391</id><submitter>Kazuya Ueki</submitter><version version="v1"><date>Sun, 16 May 2021 09:43:25 GMT</date><size>521kb</size><source_type>D</source_type></version><title>Survey of Visual-Semantic Embedding Methods for Zero-Shot Image
  Retrieval</title><authors>Kazuya Ueki</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual-semantic embedding is an interesting research topic because it is
useful for various tasks, such as visual question answering (VQA), image-text
retrieval, image captioning, and scene graph generation. In this paper, we
focus on zero-shot image retrieval using sentences as queries and present a
survey of the technological trends in this area. First, we provide a
comprehensive overview of the history of the technology, starting with a
discussion of the early studies of image-to-text matching and how the
technology has evolved over time. In addition, a description of the datasets
commonly used in experiments and a comparison of the evaluation results of each
method are presented. We also introduce the implementation available on github
for use in confirming the accuracy of experiments and for further improvement.
We hope that this survey paper will encourage researchers to further develop
their research on bridging images and languages.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07392</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07392</id><submitter>Wangbin Ding</submitter><version version="v1"><date>Sun, 16 May 2021 09:47:42 GMT</date><size>928kb</size><source_type>D</source_type></version><title>Unsupervised MMRegNet based on Spatially Encoded Gradient Information</title><authors>Wangbin Ding, Lei Li, Xiahai Zhuang, Liqin Huang</authors><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-modality medical images can provide relevant and complementary
anatomical information for a target (organ, tumor or tissue). Registering the
multi-modality images to a common space can fuse these comprehensive
information, and bring convenience for clinical application. Recently, neural
networks have been widely investigated to boost registration methods. However,
it is still challenging to develop a multi-modality registration network due to
the lack of robust criteria for network training. Besides, most existing
registration networks mainly focus on pairwise registration, and can hardly be
applicable for multiple image scenarios. In this work, we propose a
multi-modality registration network (MMRegNet), which can jointly register
multiple images with different modalities to a target image. Meanwhile, we
present spatially encoded gradient information to train the MMRegNet in an
unsupervised manner. The proposed network was evaluated on two datasets, i.e,
MM-WHS 2017 and CHAOS 2019. The results show that the proposed network can
achieve promising performance for cardiac left ventricle and liver registration
tasks. Source code is released publicly on github.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07396</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07396</id><submitter>Henderik Alex Proper</submitter><version version="v1"><date>Sun, 16 May 2021 10:18:26 GMT</date><size>210kb</size></version><title>Developing an Architecture Method Library</title><authors>R.D.T. Janssen, H.A. Proper, H. Bosma, D. Verhoef, S.J.B.A.
  Hoppenbrouwers2</authors><categories>cs.SE cs.DB</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Today, there are millions of professionals worldwide acting as a designer,
architect or engineer in the design, realization, and implementation of
information systems. At this moment there is no well established and clearly
identified body of knowledge that defines their profession in a &quot;standard&quot; way.
In this article, we present the idea of developing an architecture method
library. Such a library could play a pivotal role to further professionalize
the field. The library contains project experiences, reference architectures,
literature, proven methods, tools, etc. Access mechanisms allow the
professional to use this body of knowledge. By giving it an open nature, it can
be filled by professionals from different fields. Feedback mechanisms are
possible to improve the contents of the library, for example by giving feedback
on the method components in the library.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07397</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07397</id><submitter>Ivan Kozitsin</submitter><version version="v1"><date>Sun, 16 May 2021 10:19:35 GMT</date><size>399kb</size></version><title>Face mask perception during the COVID-19 pandemic: an observational
  study of Russian online social network VKontakte</title><authors>Alexander G. Chkhartishvili, Dmitry A. Gubanov, Ivan V. Kozitsin</authors><categories>cs.SI cs.CY</categories><comments>6 pages, 2 tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This cross-sectional study characterizes users' attitudes towards the face
mask requirements introduced by the Russian government as a response to the
COVID-19 pandemic. We study how they relate to other users' characteristics
such as age, gender, and political attitudes. Our results indicate that men and
elder individuals - demographic groups that are most vulnerable to COVID-19 --
underestimate the benefits of wearing face masks. We also discovered that users
in opposition to the Russian government highly approve of this anti-COVID-19
measure -- an oppositionist will approve of the face mask requirements with the
probability of 0.95. For those who support the Russian government, the odds of
approval are merely 0.45.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07398</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07398</id><submitter>Vaibhav Kumar</submitter><version version="v1"><date>Sun, 16 May 2021 10:20:17 GMT</date><size>938kb</size></version><title>On the Secrecy Rate of Downlink NOMA in Underlay Spectrum Sharing with
  Imperfect CSI</title><authors>Vaibhav Kumar, Mark F. Flanagan, Daniel Benevides da Costa, and Le-Nam
  Tran</authors><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures. Invited paper in Workshop 2: Advances in Physical
  Layer Security for 6G Networks in 28TH INTERNATIONAL CONFERENCE ON
  TELECOMMUNICATIONS, London, UK</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present the ergodic sum secrecy rate (ESSR) analysis of an
underlay spectrum sharing non-orthogonal multiple access (NOMA) system. We
consider the scenario where the power transmitted by the secondary transmitter
(ST) is constrained by the peak tolerable interference at multiple primary
receivers (PRs) as well as the maximum transmit power of the ST. The effect of
channel estimation error is also taken into account in our analysis. We derive
exact and asymptotic closed-form expressions for the ESSR of the downlink NOMA
system, and show that the performance can be classified into two distinct
regimes, i.e., it is dictated either by the interference constraint or by the
power constraint. Our results confirm the superiority of the NOMA-based system
over its orthogonal multiple access (OMA) based counterpart. More
interestingly, our results show that NOMA helps in maintaining the secrecy rate
of the strong user while significantly enhancing the secrecy performance of the
weak user as compared to OMA. The correctness of the proposed investigation is
corroborated through Monte Carlo simulation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07400</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07400</id><submitter>Samia Touileb</submitter><version version="v1"><date>Sun, 16 May 2021 10:22:21 GMT</date><size>7166kb</size><source_type>D</source_type></version><title>The interplay between morphological typology and script on a novel
  multi-layer Algerian dialect corpus</title><authors>Samia Touileb and Jeremy Barnes</authors><categories>cs.CL</categories><comments>Accepted at Findings of ACL: ACL2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent years have seen a rise in interest for cross-lingual transfer between
languages with similar typology, and between languages of various scripts.
However, the interplay between morphological typology and difference in script
on cross-lingual transfer is a less studied problem. We explore this interplay
on cross-lingual transfer for two supervised tasks, namely part-of-speech
tagging and sentiment analysis. We introduce a newly annotated corpus of
Algerian user-generated comments comprising parallel annotations of Algerian
written in Latin, Arabic, and code-switched scripts, as well as annotations for
sentiment and topic categories. We perform baseline experiments by fine-tuning
multi-lingual language models. We further explore the effect of script vs.
morphological typology in cross-lingual transfer by fine-tuning multi-lingual
models on languages which are a) morphologically distinct, but use the same
script, b) morphologically similar, but use a distinct script, or c) are
morphologically similar and use the same script. We find there is a delicate
relationship between script and typology for part-of-speech, while sentiment
analysis is less sensitive.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07401</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07401</id><submitter>Carsten Scherer</submitter><version version="v1"><date>Sun, 16 May 2021 10:24:17 GMT</date><size>159kb</size><source_type>D</source_type></version><title>Dissipativity and Integral Quadratic Constraints, Tailored computational
  robustness tests for complex interconnections</title><authors>Carsten Scherer</authors><categories>math.OC cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A central notion in systems theory is dissipativity, which has been
introduced by Jan Willems with the explicit goal of arriving at a fundamental
understanding of the stability properties of feedback interconnections. In
robust control, the framework of integral quadratic constraints (IQCs) builds
on the seminal contributions of Yakubovich and Zames in the 1960's. It provides
a technique for analyzing the stability of an interconnection of some linear
system in feedback with a whole class of systems, also refereed to as
uncertainty.
  In this paper we survey the key ideas of exploiting dissipativity and
integral quadratic constraints for the computational analysis of robust
stability and performance properties of uncertain interconnections in terms of
linear matrix inequalities. In particular for dynamic supply rates, the paper
revolves around the notion of finite-horizon integral quadratic constraints
with a terminal cost. We reveal that this provides a seamless link between the
general IQC theorem and dissipativity theory that has been established only
rather recently.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07402</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07402</id><submitter>Wanli Liu</submitter><version version="v1"><date>Sun, 16 May 2021 10:37:36 GMT</date><size>4355kb</size><source_type>D</source_type></version><title>Is Image Size Important? A Robustness Comparison of Deep Learning
  Methods for Multi-scale Cell Image Classification Tasks: from Convolutional
  Neural Networks to Visual Transformers</title><authors>Wanli Liu, Chen Li, Hongzan Sun, Weiming Hu, Haoyuan Chen, Changhao
  Sun, Marcin Grzegorzek</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cervical cancer is a very common and fatal cancer in women, but it can be
prevented through early examination and treatment. Cytopathology images are
often used to screen for cancer. Then, because of the possibility of artificial
errors due to the large number of this method, the computer-aided diagnosis
system based on deep learning is developed. The image input required by the
deep learning method is usually consistent, but the size of the clinical
medical image is inconsistent. The internal information is lost after resizing
the image directly, so it is unreasonable. A lot of research is to directly
resize the image, and the results are still robust. In order to find a
reasonable explanation, 22 deep learning models are used to process images of
different scales, and experiments are conducted on the SIPaKMeD dataset. The
conclusion is that the deep learning method is very robust to the size changes
of images. This conclusion is also validated on the Herlev dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07403</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07403</id><submitter>Lajos L\'oczi</submitter><version version="v1"><date>Sun, 16 May 2021 10:39:04 GMT</date><size>843kb</size><source_type>D</source_type></version><title>Positivity preservation of implicit discretizations of the advection
  equation</title><authors>Yiannis Hadjimichael, David I. Ketcheson, Lajos L\'oczi</authors><categories>math.NA cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze, from the viewpoint of positivity preservation, certain
discretizations of a fundamental partial differential equation, the
one-dimensional advection equation with periodic boundary condition. The full
discretization is obtained by coupling a finite difference spatial
semi-discretization (the second- and some higher-order centered difference
schemes, or the Fourier spectral collocation method) with an arbitrary
$\theta$-method in time (including the forward and backward Euler methods, and
a second-order method by choosing $\theta\in [0,1]$ suitably). The full
discretization generates a two-parameter family of circulant matrices
$M\in\mathbb{R}^{m\times m}$, where each matrix entry is a rational function in
$\theta$ and $\nu$. Here, $\nu$ denotes the CFL number, being proportional to
the ratio between the temporal and spatial discretization step sizes. The
entrywise non-negativity of the matrix $M$ -- which is equivalent to the
positivity preservation of the fully discrete scheme -- is investigated via
discrete Fourier analysis and also by solving some low-order parametric linear
recursions. We find that positivity preservation of the fully discrete system
is impossible if the number of spatial grid points $m$ is even. However, it
turns out that positivity preservation of the fully discrete system is
recovered for \emph{odd} values of $m$ provided that $\theta\ge 1/2$ and $\nu$
are chosen suitably. These results are interesting since the systems of
ordinary differential equations obtained via the spatial semi-discretizations
studied are \emph{not} positivity preserving.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07404</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07404</id><submitter>Yixuan Li</submitter><version version="v1"><date>Sun, 16 May 2021 10:40:30 GMT</date><size>19991kb</size><source_type>D</source_type></version><title>MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized
  Sports Actions</title><authors>Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, Limin Wang</authors><categories>cs.CV</categories><comments>One track of DeeperAction Workshop@ICCV2021. HomePage:
  https://deeperaction.github.io/multisports/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatio-temporal action detection is an important and challenging problem in
video understanding. The existing action detection benchmarks are limited in
aspects of small numbers of instances in a trimmed video or relatively
low-level atomic actions. This paper aims to present a new multi-person dataset
of spatio-temporal localized sports actions, coined as MultiSports. We first
analyze the important ingredients of constructing a realistic and challenging
dataset for spatio-temporal action detection by proposing three criteria: (1)
motion dependent identification, (2) with well-defined boundaries, (3)
relatively high-level classes. Based on these guidelines, we build the dataset
of Multi-Sports v1.0 by selecting 4 sports classes, collecting around 3200
video clips, and annotating around 37790 action instances with 907k bounding
boxes. Our datasets are characterized with important properties of strong
diversity, detailed annotation, and high quality. Our MultiSports, with its
realistic setting and dense annotations, exposes the intrinsic challenge of
action localization. To benchmark this, we adapt several representative methods
to our dataset and give an in-depth analysis on the difficulty of action
localization in our dataset. We hope our MultiSports can serve as a standard
benchmark for spatio-temporal action detection in the future. Our dataset
website is at https://deeperaction.github.io/multisports/.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07405</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07405</id><submitter>Feng Huang</submitter><version version="v1"><date>Sun, 16 May 2021 10:42:09 GMT</date><size>141kb</size></version><title>Optimal control of robust team stochastic games</title><authors>Feng Huang, Ming Cao, and Long Wang</authors><categories>math.OC cs.AI cs.GT cs.MA cs.SY eess.SY</categories><comments>under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In stochastic dynamic environments, team stochastic games have emerged as a
versatile paradigm for studying sequential decision-making problems of fully
cooperative multi-agent systems. However, the optimality of the derived
policies is usually sensitive to the model parameters, which are typically
unknown and required to be estimated from noisy data in practice. To mitigate
the sensitivity of the optimal policy to these uncertain parameters, in this
paper, we propose a model of &quot;robust&quot; team stochastic games, where players
utilize a robust optimization approach to make decisions. This model extends
team stochastic games to the scenario of incomplete information and meanwhile
provides an alternative solution concept of robust team optimality. To seek
such a solution, we develop a learning algorithm in the form of a Gauss-Seidel
modified policy iteration and prove its convergence. This algorithm, compared
with robust dynamic programming, not only possesses a faster convergence rate,
but also allows for using approximation calculations to alleviate the curse of
dimensionality. Moreover, some numerical simulations are presented to
demonstrate the effectiveness of the algorithm by generalizing the game model
of social dilemmas to sequential robust scenarios.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07407</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07407</id><submitter>Sergei Grudinin</submitter><version version="v1"><date>Sun, 16 May 2021 10:46:44 GMT</date><size>8435kb</size><source_type>D</source_type></version><title>Protein sequence-to-structure learning: Is this the end(-to-end
  revolution)?</title><authors>Elodie Laine, Stephan Eismann, Arne Elofsson, and Sergei Grudinin</authors><categories>q-bio.BM cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The potential of deep learning has been recognized in the protein structure
prediction community for some time, and became indisputable after CASP13. In
CASP14, deep learning has boosted the field to unanticipated levels reaching
near-experimental accuracy. This success comes from advances transferred from
other machine learning areas, as well as methods specifically designed to deal
with protein sequences and structures, and their abstractions. Novel emerging
approaches include (i) geometric learning, i.e. learning on representations
such as graphs, 3D Voronoi tessellations, and point clouds; (ii) pre-trained
protein language models leveraging attention; (iii) equivariant architectures
preserving the symmetry of 3D space; (iv) use of large meta-genome databases;
(v) combinations of protein representations; (vi) and finally truly end-to-end
architectures, i.e. differentiable models starting from a sequence and
returning a 3D structure. Here, we provide an overview and our opinion of the
novel deep learning approaches developed in the last two years and widely used
in CASP14.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07408</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07408</id><submitter>Aryeh Kontorovich</submitter><version version="v1"><date>Sun, 16 May 2021 11:13:13 GMT</date><size>51kb</size><source_type>D</source_type></version><title>Dimension-Free Empirical Entropy Estimation</title><authors>Doron Cohen, Aryeh Kontorovich, Aaron Koolyk, Geoffrey Wolfer</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek an entropy estimator for discrete distributions with fully empirical
accuracy bounds. As stated, this goal is infeasible without some prior
assumptions on the distribution. We discover that a certain information moment
assumption renders the problem feasible. We argue that the moment assumption is
natural and, in some sense, {\em minimalistic} -- weaker than finite support or
tail decay conditions. Under the moment assumption, we provide the first
finite-sample entropy estimates for infinite alphabets, nearly recovering the
known minimax rates. Moreover, we demonstrate that our empirical bounds are
significantly sharper than the state-of-the-art bounds, for various natural
distributions and non-trivial sample regimes. Along the way, we give a
dimension-free analogue of the Cover-Thomas result on entropy continuity (with
respect to total variation distance) for finite alphabets, which may be of
independent interest.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07409</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07409</id><submitter>Roman Parovik</submitter><version version="v1"><date>Sun, 16 May 2021 11:17:28 GMT</date><size>238kb</size><source_type>D</source_type></version><title>Research of the hereditary dynamic Riccati system with modification
  fractional differential operator of Gerasimov-Caputo</title><authors>Dmitriy Tverdyi, Roman Parovik</authors><categories>math.NA cs.NA math.DS</categories><msc-class>37N30 (Primary), 26A33 (Secondary)</msc-class><acm-class>G.1.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the Cauchy problem for the Riccati differential
equation with constant coefficients and a modified Gerasimov-Caputo type
fractional differential operator of variable order. Using Newton's numerical
algorithm, calculation curves are constructed taking into account different
values of the Cauchy problem parameters. The calculation results are compared
with the previously obtained results. The computational accuracy of the
numerical algorithm is investigated. It is shown using the Runge rule that the
computational accuracy tends to the accuracy of the numerical method when
increasing the nodes of the calculated grid.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07411</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07411</id><submitter>Tizian Wenzel</submitter><version version="v1"><date>Sun, 16 May 2021 11:36:25 GMT</date><size>69kb</size></version><title>Analysis of target data-dependent greedy kernel algorithms: Convergence
  rates for $f$-, $f \cdot P$- and $f/P$-greedy</title><authors>Tizian Wenzel, Gabriele Santin, Bernard Haasdonk</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-dependent greedy algorithms in kernel spaces are known to provide fast
converging interpolants, while being extremely easy to implement and efficient
to run. Despite this experimental evidence, no detailed theory has yet been
presented. This situation is unsatisfactory especially when compared to the
case of the data-independent $P$-greedy algorithm, for which optimal
convergence rates are available, despite its performances being usually
inferior to the ones of target data-dependent algorithms.
  In this work we fill this gap by first defining a new scale of greedy
algorithms for interpolation that comprises all the existing ones in a unique
analysis, where the degree of dependency of the selection criterion on the
functional data is quantified by a real parameter. We then prove new
convergence rates where this degree is taken into account and we show that,
possibly up to a logarithmic factor, target data-dependent selection strategies
provide faster convergence.
  In particular, for the first time we obtain convergence rates for target data
adaptive interpolation that are faster than the ones given by uniform points,
without the need of any special assumption on the target function. The rates
are confirmed by a number of examples.
  These results are made possible by a new analysis of greedy algorithms in
general Hilbert spaces.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07419</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07419</id><submitter>Henderik Alex Proper</submitter><version version="v1"><date>Sun, 16 May 2021 12:17:36 GMT</date><size>3386kb</size><source_type>D</source_type></version><title>Work Systems Modeling Library</title><authors>Henderik A. Proper</authors><categories>cs.SE cs.OH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modeling of work systems occurs for all sorts of reasons. Requirements need
to be expressed. A pre-existing situation may need to be charted and analyzed.
Early design decisions may be captured using architecture principles. Detailed
design may be worked out. We all regard these activities as essentially being
forms of modeling. In the work systems modeling library, we consider work
system engineering from a modeling perspective. In the field of work system
engineering, a whole plethora of modeling methods is available to system
engineers and architects. Each of these methods can be used to model some
(aspects) of a domain related to an existing and/or a planned work system. The
aspects may refer to requirements, architecture, design, processing, data, etc,
etc. In other words, these methodes are essentially all intended to model
different aspects of work systems and/or their context. The aim of the work
systems modeling library (WSML) is to bring together methodical knowledge
concerning the modeling of work systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07420</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07420</id><submitter>Martin Zaefferer</submitter><version version="v1"><date>Sun, 16 May 2021 12:38:35 GMT</date><size>211kb</size><source_type>D</source_type></version><title>Resource Planning for Hospitals Under Special Consideration of the
  COVID-19 Pandemic: Optimization and Sensitivity Analysis</title><authors>Thomas Bartz-Beielstein, Marcel Dr\&quot;oscher, Alpar G\&quot;ur, Alexander
  Hinterleitner, Olaf Mersmann, Dessislava Peeva, Lennard Reese, Nicolas
  Rehbach, Frederik Rehbach, Amrita Sen, Aleksandr Subbotin, Martin Zaefferer</authors><categories>cs.AI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crises like the COVID-19 pandemic pose a serious challenge to health-care
institutions. They need to plan the resources required for handling the
increased load, for instance, hospital beds and ventilators. To support the
resource planning of local health authorities from the Cologne region,
BaBSim.Hospital, a tool for capacity planning based on discrete event
simulation, was created. The predictive quality of the simulation is determined
by 29 parameters. Reasonable default values of these parameters were obtained
in detailed discussions with medical professionals. We aim to investigate and
optimize these parameters to improve BaBSim.Hospital. First approaches with
&quot;out-of-the-box&quot; optimization algorithms failed. Implementing a surrogate-based
optimization approach generated useful results in a reasonable time. To
understand the behavior of the algorithm and to get valuable insights into the
fitness landscape, an in-depth sensitivity analysis was performed. The
sensitivity analysis is crucial for the optimization process because it allows
focusing the optimization on the most important parameters. We illustrate how
this reduces the problem dimension without compromising the resulting accuracy.
The presented approach is applicable to many other real-world problems, e.g.,
the development of new elevator systems to cover the last mile or simulation of
student flow in academic study periods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07426</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07426</id><submitter>Romi Banerjee</submitter><version version="v1"><date>Sun, 16 May 2021 12:58:05 GMT</date><size>1117kb</size><source_type>D</source_type></version><title>Curiosity-driven Intuitive Physics Learning</title><authors>Tejas Gaikwad, Romi Banerjee</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Biological infants are naturally curious and try to comprehend their physical
surroundings by interacting, in myriad multisensory ways, with different
objects - primarily macroscopic solid objects - around them. Through their
various interactions, they build hypotheses and predictions, and eventually
learn, infer and understand the nature of the physical characteristics and
behavior of these objects. Inspired thus, we propose a model for
curiosity-driven learning and inference for real-world AI agents. This model is
based on the arousal of curiosity, deriving from observations along
discontinuities in the fundamental macroscopic solid-body physics parameters,
i.e., shape constancy, spatial-temporal continuity, and object permanence. We
use the term body-budget to represent the perceived fundamental properties of
solid objects. The model aims to support the emulation of learning from scratch
followed by substantiation through experience, irrespective of domain, in
real-world AI agents.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07428</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07428</id><submitter>Xianjun Jiao</submitter><version version="v1"><date>Sun, 16 May 2021 13:00:00 GMT</date><size>1103kb</size></version><title>Openwifi CSI fuzzer for authorized sensing and covert channels</title><authors>Xianjun Jiao, Michael Mehari, Wei Liu, Muhammad Aslam, Ingrid Moerman</authors><categories>cs.CR cs.AR cs.NI</categories><comments>Submitted to ACM WiSec 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  CSI (Channel State Information) of WiFi systems has become a hot topic in
recent years. The information contains the environment channel response between
the transmitter and the receiver, so that the people/objects and their movement
in between can be detected by analyzing CSI. CSI is the channel estimation
result in the receiver based on the pre-known training field of the transmitted
signal. CSI information is useful in many cases, but it also brings concerns on
privacy and security. In this paper, we open sourced a CSI fuzzer to enhance
the privacy and security. It is built and embedded into the transmitter of
openwifi, which is an open source full-stack WiFi chip design, to prevent
unauthorized sensing without sacrificing the WiFi link performance. The CSI
fuzzer imposes an artificial channel response to the signal before it leaves
the transmitter, so the CSI extracted at the receiver will indicate the actual
channel response combined with the artificial response. Only the authorized
receiver, that knows the artificial response, can calculate the actual channel
response and perform the CSI sensing. Another potential application of the CSI
fuzzer is covert channels. A set of artificial response patterns can be
predefined between transmitter and receiver, so covert messages can be
delivered via selecting a pattern at the transmitter and recognizing it at the
receiver.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07432</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07432</id><submitter>Chandan Jha Dr.</submitter><version version="v1"><date>Sun, 16 May 2021 13:05:10 GMT</date><size>16098kb</size><source_type>D</source_type></version><title>Zero Aware Configurable Data Encoding by Skipping Transfer for Error
  Resilient Applications</title><authors>Chandan Kumar Jha, Shreyas Singh, Riddhi Thakker, Manu Awasthi and
  Joycee Mekie</authors><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose Zero Aware Configurable Data Encoding by Skipping
Transfer (ZAC-DEST), a data encoding scheme to reduce the energy consumption of
DRAM channels, specifically targeted towards approximate computing and error
resilient applications. ZAC-DEST exploits the similarity between recent data
transfers across channels and information about the error resilience behavior
of applications to reduce on-die termination and switching energy by reducing
the number of 1's transmitted over the channels. ZAC-DEST also provides a
number of knobs for trading off the application's accuracy for energy savings,
and vice versa, and can be applied to both training and inference.
  We apply ZAC-DEST to five machine learning applications. On average, across
all applications and configurations, we observed a reduction of $40$% in
termination energy and $37$% in switching energy as compared to the state of
the art data encoding technique BD-Coder with an average output quality loss of
$10$%. We show that if both training and testing are done assuming the presence
of ZAC-DEST, the output quality of the applications can be improved upto 9
times as compared to when ZAC-DEST is only applied during testing leading to
energy savings during training and inference with increased output quality.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07436</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07436</id><submitter>Wei Cheng</submitter><version version="v1"><date>Sun, 16 May 2021 13:16:38 GMT</date><size>266kb</size><source_type>D</source_type></version><title>Attacking Masked Cryptographic Implementations: Information-Theoretic
  Bounds</title><authors>Wei Cheng, Yi Liu, Sylvain Guilley and Olivier Rioul</authors><categories>cs.IT cs.CR math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Measuring the information leakage is critical for evaluating practical
security of cryptographic devices against side-channel analysis. More
straightforwardly, it is interesting to have an upper bound on success rate of
any attack given a (fixed) number of side-channel measurements. Or conversely,
we wish to derive a lower bound on the number of queries for a given success
rate of optimal attacks. In this paper, we derive several bounds in both
directions by using information-theoretic tools, particularly for cryptographic
implementations protected by masking schemes. We show that a generic upper
bound on the probability of success, irrespective to specific attacks, is
linked to mutual information between side-channel measurements and the secret.
Moreover, our numerical evaluation confirms that, the success rate of optimal
maximum likelihood distinguishers is tightly bounded given a fixed number of
measurements.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07443</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07443</id><submitter>Qin Yang</submitter><version version="v1"><date>Sun, 16 May 2021 14:33:11 GMT</date><size>1685kb</size><source_type>D</source_type></version><title>How Can Robots Trust Each Other? A Relative Needs Entropy Based Trust
  Assessment Models</title><authors>Qin Yang and Ramviyas Parasuraman</authors><categories>cs.MA cs.AI cs.RO</categories><comments>This paper already submitted to the SMC 2021 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation in multi-agent and multi-robot systems can help agents build
various formations, shapes, and patterns presenting corresponding functions and
purposes adapting to different situations. Relationship between agents such as
their spatial proximity and functional similarities could play a crucial role
in cooperation between agents. Trust level between agents is an essential
factor in evaluating their relationships' reliability and stability, much as
people do. This paper proposes a new model called Relative Needs Entropy (RNE)
to assess trust between robotic agents. RNE measures the distance of needs
distribution between individual agents or groups of agents. To exemplify its
utility, we implement and demonstrate our trust model through experiments
simulating a heterogeneous multi-robot grouping task in a persistent urban
search and rescue mission consisting of tasks at two levels of difficulty. The
results suggest that RNE trust-Based grouping of robots can achieve better
performance and adaptability for diverse task execution compared to the
state-of-the-art energy-based or distance-based grouping models.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07444</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07444</id><submitter>Ramakrishnan Raman</submitter><version version="v1"><date>Sun, 16 May 2021 14:38:12 GMT</date><size>1984kb</size></version><title>Knowledge Value Stream Framework For Complex Product Design Decisions</title><authors>Ramakrishnan Raman, Meenakshi D'Souza</authors><categories>cs.SI</categories><comments>31 pages, 25 Figures. A preliminary version of this work was
  presented in 2017 IEEE Technology &amp; Engineering Management Conference
  (TEMSCON), which took place on June 8-10, 2017 in Santa Clara County, CA,
  USA. Subsequently, the work has significantly evolved, expanded in scope and
  progressed to its present form in this submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Product Development value stream includes all the activities, value added and
non value added, for designing and developing a product. It is characterized by
flow of knowledge that drives the decisions, and deals with how the product is
conceptualized, architected and designed by the design teams. The intangible
flow of knowledge is determined by knowledge value stream, which shapes how the
raw concepts and ideas flow into mature knowledge, how the knowledge is
socialized, internalized, and how the knowledge impels the decisions in the
product development value stream. For complex products, the design teams
encounter tough challenges, such as uncertainty and variability, while making
design decisions. This paper proposes a framework for knowledge value stream
for complex product design decisions. The framework encompasses knowledge
cadence and learning cycles as its core elements and incorporates rudiments for
complex product design such as uncertainty, variability and perceptions. It
helps in managing uncertainty and variability and evolving the required
knowledge to drive optimal decisions during the design of complex products. It
advocates a phased model for framework deployment towards establishing and
progressively maturing the knowledge value stream.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07445</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07445</id><submitter>Hyesun Chung</submitter><version version="v1"><date>Sun, 16 May 2021 14:43:51 GMT</date><size>1280kb</size></version><title>Enhancing the Usability of Self-service Kiosks for Older Adults: Effects
  of Using Privacy Partitions and Chairs</title><authors>Hyesun Chung and Woojin Park</authors><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study aimed to evaluate the effects of possible physical design features
of self-service kiosks (SSK), side and back partitions and chairs, on workload
and task performance of older users during a typical SSK task. The study
comparatively evaluated eight physical SSK design alternatives, and younger and
older participants performed a menu ordering task using each physical design
alternative. Older participants showed a large variation in task performance
across the design alternatives indicating stronger impacts of the physical
design features. In particular, sitting significantly reduced task completion
time and workload in multiple dimensions, including time pressure and
frustration. In addition, the use of either side or back partitions reduced
mean ratings of mental demand and effort. The study suggests placing chairs and
either side or back partitions to enhance older adults' user experience. The
use of the proposed physical design recommendations would greatly help them use
SSK more effectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07446</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07446</id><submitter>Prem Talwai</submitter><version version="v1"><date>Sun, 16 May 2021 14:43:54 GMT</date><size>109kb</size></version><title>Sobolev Norm Learning Rates for Conditional Mean Embeddings</title><authors>Prem Talwai, Ali Shameli, David Simchi-Levi</authors><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop novel learning rates for conditional mean embeddings by applying
the theory of interpolation for reproducing kernel Hilbert spaces (RKHS). Our
learning rates demonstrate consistency of the sample estimator under
drastically weaker assumptions than the state-of-the art, allowing the much
broader application of conditional mean embeddings to more complex ML/RL
settings involving infinite dimensional RKHS and continuous state spaces.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07447</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07447</id><submitter>Qin Wang</submitter><version version="v1"><date>Sun, 16 May 2021 14:50:26 GMT</date><size>814kb</size><source_type>D</source_type></version><title>Non-Fungible Token (NFT): Overview, Evaluation, Opportunities and
  Challenges</title><authors>Qin Wang and Rujia Li and Qi Wang and Shiping Chen</authors><categories>cs.CR</categories><comments>Tech Report on NFT</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Non-Fungible Token (NFT) market is mushrooming in the recent couple of
years. The concept of NFT originally comes from a token standard of Ethereum,
aiming to distinguish each token with distinguishable signs. This type of
tokens can be bound with virtual/digital properties as their unique
identifications. With NFTs, all marked properties can be freely traded with
customized values according to their ages, rarity, liquidity, etc. It has
greatly stimulated the prosperity of the decentralized application (DApp)
market. At the time of writing (May 2021), the total money used on completed
NFT sales has reached $34,530,649.86$ USD. The thousandfold return on its
increasing market draws huge attention worldwide. However, the development of
the NFT ecosystem is still in its early stage, and the technologies of NFTs are
pre-mature. Newcomers may get lost in their frenetic evolution due to the lack
of systematic summaries. In this technical report, we explore the NFT
ecosystems in several aspects. We start with an overview of state-of-the-art
NFT solutions, then provide their technical components, protocols, standards,
and desired proprieties. Afterward, we give a security evolution, with
discussions on the perspectives of their design models, opportunities and
challenges. To the best of our knowledge, this is the first systematic study on
the current NFT ecosystems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07451</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07451</id><submitter>Debesh Jha</submitter><version version="v1"><date>Sun, 16 May 2021 15:19:56 GMT</date><size>14010kb</size><source_type>D</source_type></version><title>MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image
  Segmentation</title><authors>Abhishek Srivastava, Debesh Jha, Sukalpa Chanda, Umapada Pal,
  H{\aa}vard D. Johansen, Dag Johansen, Michael A. Riegler, Sharib Ali, P{\aa}l
  Halvorsen</authors><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Methods based on convolutional neural networks have improved the performance
of biomedical image segmentation. However, most of these methods cannot
efficiently segment objects of variable sizes and train on small and biased
datasets, which are common in biomedical use cases. While methods exist that
incorporate multi-scale fusion approaches to address the challenges arising
with variable sizes, they usually use complex models that are more suitable for
general semantic segmentation computer vision problems. In this paper, we
propose a novel architecture called MSRF-Net, which is specially designed for
medical image segmentation tasks. The proposed MSRF-Net is able to exchange
multi-scale features of varying receptive fields using a dual-scale dense
fusion block (DSDF). Our DSDF block can exchange information rigorously across
two different resolution scales, and our MSRF sub-network uses multiple DSDF
blocks in sequence to perform multi-scale fusion. This allows the preservation
of resolution, improved information flow, and propagation of both high- and
low-level features to obtain accurate segmentation maps. The proposed MSRF-Net
allows to capture object variabilities and provides improved results on
different biomedical datasets. Extensive experiments on MSRF-Net demonstrate
that the proposed method outperforms most of the cutting-edge medical image
segmentation state-of-the-art methods. MSRF-Net advances the performance on
four publicly available datasets, and also, MSRF-Net is more generalizable as
compared to state-of-the-art methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07452</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07452</id><submitter>Bai Li</submitter><version version="v1"><date>Sun, 16 May 2021 15:20:36 GMT</date><size>192kb</size><source_type>D</source_type></version><title>How is BERT surprised? Layerwise detection of linguistic anomalies</title><authors>Bai Li, Zining Zhu, Guillaume Thomas, Yang Xu, Frank Rudzicz</authors><categories>cs.CL</categories><comments>ACL 2021 (Long Paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformer language models have shown remarkable ability in detecting when a
word is anomalous in context, but likelihood scores offer no information about
the cause of the anomaly. In this work, we use Gaussian models for density
estimation at intermediate layers of three language models (BERT, RoBERTa, and
XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark.
In lower layers, surprisal is highly correlated to low token frequency, but
this correlation diminishes in upper layers. Next, we gather datasets of
morphosyntactic, semantic, and commonsense anomalies from psycholinguistic
studies; we find that the best performing model RoBERTa exhibits surprisal in
earlier layers when the anomaly is morphosyntactic than when it is semantic,
while commonsense anomalies do not exhibit surprisal at any intermediate layer.
These results suggest that language models employ separate mechanisms to detect
different types of linguistic anomalies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07454</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07454</id><submitter>Lynnette Hui Xian Ng</submitter><version version="v1"><date>Sun, 16 May 2021 15:38:49 GMT</date><size>1056kb</size><source_type>D</source_type></version><title>A Synchronized Action Framework for Responsible Detection of
  Coordination on Social Media</title><authors>Thomas Magelinski, Lynnette Hui Xian Ng, Kathleen M. Carley</authors><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The study of coordinated manipulation of conversations on social media has
become more prevalent as social media's role in amplifying misinformation,
hate, and polarization has come under scrutiny. We discuss the implications of
successful coordination detection algorithms based on shifts of power, and
consider how responsible coordination detection may be carried out through
synchronized action. We then propose a Synchronized Action Framework for
detection of automated coordination through construction and analysis of
multi-view networks. We validate our framework by examining the Reopen America
conversation on Twitter, discovering three coordinated campaigns. We further
investigate covert coordination surrounding the protests and find the task to
be far more complex than examples seen in prior work, demonstrating the need
for our multi-view approach. A cluster of suspicious users is identified and
the activity of three members is detailed. These users amplify protest messages
using the same hashtags at very similar times, though they all focus on
different states. Through this analysis, we emphasize both the potential
usefulness of coordination detection algorithms in investigating amplification,
and the need for careful and responsible deployment of such tools.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07459</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07459</id><submitter>Qin Wang</submitter><version version="v1"><date>Sun, 16 May 2021 15:45:35 GMT</date><size>3746kb</size><source_type>D</source_type></version><title>Formal Security Analysis on dBFT Protocol of NEO</title><authors>Qin Wang and Rujia Li and Shiping Chen and Yang Xiang</authors><categories>cs.CR</categories><comments>Extended version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  NEO is one of the top public chains worldwide. We focus on its backbone
consensus protocol, called delegated Byzantine Fault Tolerance (dBFT). The dBFT
protocol has been adopted by a variety of blockchain systems such as ONT. dBFT
claims to guarantee the security when no more than $f = \lfloor \frac{n}{3}
\rfloor$ nodes are Byzantine, where $n$ is the total number of consensus
participants. However, we identify attacks to break the claimed security. In
this paper, we show our results by providing a security analysis on its dBFT
protocol. First, we evaluate NEO's source code and formally present the
procedures of dBFT via the state machine replication (SMR) model. Next, we
provide a theoretical analysis with two example attacks. These attacks break
the security of dBFT with no more than $f$ nodes. Then, we provide
recommendations on how to fix the system against the identified attacks. The
suggested fixes have been accepted by the NEO official team. Finally, we
further discuss the reasons causing such issues, the relationship with current
permissioned blockchain systems, and the scope of potential influence.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07463</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07463</id><submitter>Mohammed Daoudi</submitter><version version="v1"><date>Sun, 16 May 2021 15:52:29 GMT</date><size>6084kb</size><source_type>D</source_type></version><title>3D to 4D Facial Expressions Generation Guided by Landmarks</title><authors>Naima Otberdout, Claudio Ferrari, Mohamed Daoudi, Stefano Berretti,
  Alberto Del Bimbo</authors><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While deep learning-based 3D face generation has made a progress recently,
the problem of dynamic 3D (4D) facial expression synthesis is less
investigated. In this paper, we propose a novel solution to the following
question: given one input 3D neutral face, can we generate dynamic 3D (4D)
facial expressions from it? To tackle this problem, we first propose a mesh
encoder-decoder architecture (Expr-ED) that exploits a set of 3D landmarks to
generate an expressive 3D face from its neutral counterpart. Then, we extend it
to 4D by modeling the temporal dynamics of facial expressions using a
manifold-valued GAN capable of generating a sequence of 3D landmarks from an
expression label (Motion3DGAN). The generated landmarks are fed into the mesh
encoder-decoder, ultimately producing a sequence of 3D expressive faces. By
decoupling the two steps, we separately address the non-linearity induced by
the mesh deformation and motion dynamics. The experimental results on the CoMA
dataset show that our mesh encoder-decoder guided by landmarks brings a
significant improvement with respect to other landmark-based 3D fitting
approaches, and that we can generate high quality dynamic facial expressions.
This framework further enables the 3D expression intensity to be continuously
adapted from low to high intensity. Finally, we show our framework can be
applied to other tasks, such as 2D-3D facial expression transfer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07464</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07464</id><submitter>Ning Ding</submitter><version version="v1"><date>Sun, 16 May 2021 15:53:17 GMT</date><size>555kb</size><source_type>D</source_type></version><title>Few-NERD: A Few-Shot Named Entity Recognition Dataset</title><authors>Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie,
  Hai-Tao Zheng, Zhiyuan Liu</authors><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted by ACL-IJCNLP 2021, accepted version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, considerable literature has grown up around the theme of few-shot
named entity recognition (NER), but little published benchmark data
specifically focused on the practical and challenging task. Current approaches
collect existing supervised NER datasets and re-organize them to the few-shot
setting for empirical study. These strategies conventionally aim to recognize
coarse-grained entity types with few examples, while in practice, most unseen
entity types are fine-grained. In this paper, we present Few-NERD, a
large-scale human-annotated few-shot NER dataset with a hierarchy of 8
coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238
sentences from Wikipedia, 4,601,160 words are included and each is annotated as
context or a part of a two-level entity type. To the best of our knowledge,
this is the first few-shot NER dataset and the largest human-crafted NER
dataset. We construct benchmark tasks with different emphases to
comprehensively assess the generalization capability of models. Extensive
empirical results and analysis show that Few-NERD is challenging and the
problem requires further research. We make Few-NERD public at
https://ningding97.github.io/fewnerd/.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07465</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07465</id><submitter>Sohil Lal Shrestha</submitter><version version="v1"><date>Sun, 16 May 2021 16:08:03 GMT</date><size>1958kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 14:24:10 GMT</date><size>1943kb</size><source_type>D</source_type></version><title>SLGPT: Using Transfer Learning to Directly Generate Simulink Model Files
  and Find Bugs in the Simulink Toolchain</title><authors>Sohil Lal Shrestha and Christoph Csallner</authors><categories>cs.SE cs.LG</categories><comments>Fixed Grammar and typo</comments><doi>10.1145/3463274.3463806</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Finding bugs in a commercial cyber-physical system (CPS) development tool
such as Simulink is hard as its codebase contains millions of lines of code and
complete formal language specifications are not available. While deep learning
techniques promise to learn such language specifications from sample models,
deep learning needs a large number of training data to work well. SLGPT
addresses this problem by using transfer learning to leverage the powerful
Generative Pre-trained Transformer 2 (GPT-2) model, which has been pre-trained
on a large set of training data. SLGPT adapts GPT-2 to Simulink with both
randomly generated models and models mined from open-source repositories. SLGPT
produced Simulink models that are both more similar to open-source models than
its closest competitor, DeepFuzzSL, and found a super-set of the Simulink
development toolchain bugs found by DeepFuzzSL.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07467</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07467</id><submitter>Michael Yeung</submitter><version version="v1"><date>Sun, 16 May 2021 16:10:32 GMT</date><size>996kb</size></version><title>Advances in Artificial Intelligence to Reduce Polyp Miss Rates during
  Colonoscopy</title><authors>Michael Yeung, Evis Sala, Carola-Bibiane Sch\&quot;onlieb, Leonardo Rundo</authors><categories>eess.IV cs.CV cs.LG</categories><acm-class>I.2.10</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  BACKGROUND AND CONTEXT: Artificial intelligence has the potential to aid
gastroenterologists by reducing polyp miss detection rates during colonoscopy
screening for colorectal cancer.
  NEW FINDINGS: We introduce a new deep neural network architecture, the Focus
U-Net, which achieves state-of-the-art performance for polyp segmentation
across five public datasets containing images of polyps obtained during
colonoscopy.
  LIMITATIONS: The model has been validated on images taken during colonoscopy
but requires validation on live video data to ensure generalisability.
  IMPACT: Once validated on live video data, our polyp segmentation algorithm
could be integrated into colonoscopy practice and assist gastroenterologists by
reducing the number of polyps missed
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07468</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07468</id><submitter>Margarita Grinvald</submitter><version version="v1"><date>Sun, 16 May 2021 16:15:05 GMT</date><size>6513kb</size><source_type>D</source_type></version><title>TSDF++: A Multi-Object Formulation for Dynamic Object Tracking and
  Reconstruction</title><authors>Margarita Grinvald, Federico Tombari, Roland Siegwart, Juan Nieto</authors><categories>cs.CV cs.RO</categories><comments>7 pages, 3 figures. To be published in the 2021 IEEE International
  Conference on Robotics and Automation (ICRA). Code is available at
  https://github.com/ethz-asl/tsdf-plusplus and the accompanying video material
  can be found at https://youtu.be/dSJmoeVasI0</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to simultaneously track and reconstruct multiple objects moving
in the scene is of the utmost importance for robotic tasks such as autonomous
navigation and interaction. Virtually all of the previous attempts to map
multiple dynamic objects have evolved to store individual objects in separate
reconstruction volumes and track the relative pose between them. While simple
and intuitive, such formulation does not scale well with respect to the number
of objects in the scene and introduces the need for an explicit occlusion
handling strategy. In contrast, we propose a map representation that allows
maintaining a single volume for the entire scene and all the objects therein.
To this end, we introduce a novel multi-object TSDF formulation that can encode
multiple object surfaces at any given location in the map. In a multiple
dynamic object tracking and reconstruction scenario, our representation allows
maintaining accurate reconstruction of surfaces even while they become
temporarily occluded by other objects moving in their proximity. We evaluate
the proposed TSDF++ formulation on a public synthetic dataset and demonstrate
its ability to preserve reconstructions of occluded surfaces when compared to
the standard TSDF map representation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07469</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07469</id><submitter>Amirhossein Kardoost</submitter><version version="v1"><date>Sun, 16 May 2021 16:22:38 GMT</date><size>3891kb</size><source_type>D</source_type></version><title>Uncertainty in Minimum Cost Multicuts for Image and Motion Segmentation</title><authors>Amirhossein Kardoost and Margret Keuper</authors><categories>cs.CV cs.AI</categories><comments>Accepted in the 37th Conference on Uncertainty in Artificial
  Intelligence (UAI 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum cost lifted multicut approach has proven practically good
performance in a wide range of applications such as image decomposition, mesh
segmentation, multiple object tracking, and motion segmentation. It addresses
such problems in a graph-based model, where real-valued costs are assigned to
the edges between entities such that the minimum cut decomposes the graph into
an optimal number of segments. Driven by a probabilistic formulation of minimum
cost multicuts, we provide a measure for the uncertainties of the decisions
made during the optimization. We argue that access to such uncertainties is
crucial for many practical applications and conduct an evaluation by means of
sparsifications on three different, widely used datasets in the context of
image decomposition (BSDS-500) and motion segmentation (DAVIS2016 and FBMS59)
in terms of variation of information (VI) and Rand index (RI).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07470</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07470</id><submitter>Stefan Kiefer</submitter><version version="v1"><date>Sun, 16 May 2021 16:24:07 GMT</date><size>7kb</size></version><title>On Complementing Unambiguous Automata and Graphs With Many Cliques and
  Cocliques</title><authors>Emil Indzhev and Stefan Kiefer</authors><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for any unambiguous finite automaton with $n$ states there
exists an unambiguous finite automaton with $\sqrt{n+1} \cdot 2^{n/2}$ states
that recognizes the complement language. This builds and improves upon a
similar result by Jir\'asek et al. [Int. J. Found. Comput. Sci. 29 (5) (2018)].
Our improvement is based on a reduction to and an analysis of a problem from
extremal graph theory: we show that for any graph with $n$ vertices, the
product of the number of its cliques with the number of its cocliques
(independent sets) is bounded by $(n+1) 2^n$.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07472</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07472</id><submitter>Giorgos Stamatelatos</submitter><version version="v1"><date>Sun, 16 May 2021 16:27:46 GMT</date><size>17kb</size></version><title>Lexicographic Enumeration of Set Partitions</title><authors>Giorgos Stamatelatos, Pavlos S. Efraimidis</authors><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we summarize the set partition enumeration problems and
thoroughly explain the algorithms used to solve them. These algorithms iterate
through the partitions in lexicographic order and are easy to understand and
implement in modern high-level programming languages, without recursive
structures and jump logic. We show that they require linear space in respect to
the set cardinality and advance the enumeration in constant amortized time. The
methods discussed in this document are not novel. Our goal is to demonstrate
the process of enumerating set partitions and highlight the ideas behind it.
This work is an aid for learners approaching this enumeration problem and
programmers undertaking the task of implementing it.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07473</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07473</id><submitter>Jonas Kusch</submitter><version version="v1"><date>Sun, 16 May 2021 16:32:01 GMT</date><size>1418kb</size><source_type>D</source_type></version><title>A Realizable Filtered Intrusive Polynomial Moment Method</title><authors>Graham Alldredge, Martin Frank, Jonas Kusch, Ryan McClarren</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusive uncertainty quantification methods for hyperbolic problems exhibit
spurious oscillations at shocks, which leads to a significant reduction of the
overall approximation quality. Furthermore, a challenging task is to preserve
hyperbolicity of the gPC moment system. An intrusive method which guarantees
hyperbolicity is the intrusive polynomial moment (IPM) method, which performs
the gPC expansion on the entropy variables. The method, while still being
subject to oscillations, requires solving a convex optimization problem in
every spatial cell and every time step. The aim of this work is to mitigate
oscillations in the IPM solution by applying filters. Filters reduce
oscillations by damping high order gPC coefficients. Naive filtering, however,
may lead to unrealizable moments, which means that the IPM optimization problem
does not have a solution and the method breaks down. In this paper, we propose
and analyze two separate strategies to guarantee the existence of a solution to
the IPM problem. First, we propose a filter which maintains realizability by
being constructed from an underlying Fokker-Planck equation. Second, we
regularize the IPM optimization problem to be able to cope with non-realizable
gPC coefficients. Consequently, standard filters can be applied to the
regularized IPM method. We demonstrate numerical results for the two strategies
by investigating the Euler equations with uncertain shock structures in one-
and two-dimensional spatial settings. We are able to show a significant
reduction of spurious oscillations by the proposed filters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07474</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07474</id><submitter>Baris Gecer</submitter><version version="v1"><date>Sun, 16 May 2021 16:35:44 GMT</date><size>31182kb</size><source_type>D</source_type></version><title>Fast-GANFIT: Generative Adversarial Network for High Fidelity 3D Face
  Reconstruction</title><authors>Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou</authors><categories>cs.CV</categories><comments>TPAMI camera ready (submitted: 05-May-2020); Check project page:
  https://github.com/barisgecer/GANFit. arXiv admin note: substantial text
  overlap with arXiv:1902.05978</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A lot of work has been done towards reconstructing the 3D facial structure
from single images by capitalizing on the power of Deep Convolutional Neural
Networks (DCNNs). In the recent works, the texture features either correspond
to components of a linear texture space or are learned by auto-encoders
directly from in-the-wild images. In all cases, the quality of the facial
texture reconstruction is still not capable of modeling facial texture with
high-frequency details. In this paper, we take a radically different approach
and harness the power of Generative Adversarial Networks (GANs) and DCNNs in
order to reconstruct the facial texture and shape from single images. That is,
we utilize GANs to train a very powerful facial texture prior \edit{from a
large-scale 3D texture dataset}. Then, we revisit the original 3D Morphable
Models (3DMMs) fitting making use of non-linear optimization to find the
optimal latent parameters that best reconstruct the test image but under a new
perspective. In order to be robust towards initialisation and expedite the
fitting process, we propose a novel self-supervised regression based approach.
We demonstrate excellent results in photorealistic and identity preserving 3D
face reconstructions and achieve for the first time, to the best of our
knowledge, facial texture reconstruction with high-frequency details.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07475</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07475</id><submitter>Seung-Woo Ko</submitter><version version="v1"><date>Sun, 16 May 2021 16:37:26 GMT</date><size>2109kb</size><source_type>D</source_type></version><title>Integrating Geometry-Driven and Data-Driven Positioning via
  Combinatorial Data Augmentation</title><authors>Seung Min Yu, Jihong Park, Sung Mo Kim, and Seung-Woo Ko</authors><categories>cs.IT cs.NI math.IT</categories><comments>submitted to a possible IEEE journal</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Precise positioning has become one core topic in wireless communications by
facilitating candidate techniques of B5G. Nevertheless, most existing
positioning algorithms, categorized into geometric-driven and data-driven
approaches, fail to simultaneously fulfill diversified requirements for
practical use, e.g., accuracy, real-time operation, scalability, maintenance,
etc. This article aims at introducing a new principle, called
\emph{combinatorial data augmentation} (CDA), a catalyst for the two
approaches' tight integration. We first explain the concept of CDA and its
critical advantages over the two standalone approaches. Then, we confirm the
CDA's effectiveness from field experiments based on WiFi round-trip time and
inertial measurement units. Lastly, we present its potential beyond
positioning, expected to play a critical role in B5G.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07476</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07476</id><submitter>Amit Moryossef</submitter><version version="v1"><date>Sun, 16 May 2021 16:37:36 GMT</date><size>6102kb</size><source_type>D</source_type></version><title>Data Augmentation for Sign Language Gloss Translation</title><authors>Amit Moryossef, Kayo Yin, Graham Neubig, Yoav Goldberg</authors><categories>cs.CL</categories><comments>4 pages, 1 page abstract</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sign language translation (SLT) is often decomposed into video-to-gloss
recognition and gloss-to-text translation, where a gloss is a sequence of
transcribed spoken-language words in the order in which they are signed. We
focus here on gloss-to-text translation, which we treat as a low-resource
neural machine translation (NMT) problem. However, unlike traditional
low-resource NMT, gloss-to-text translation differs because gloss-text pairs
often have a higher lexical overlap and lower syntactic overlap than pairs of
spoken languages. We exploit this lexical overlap and handle syntactic
divergence by proposing two rule-based heuristics that generate pseudo-parallel
gloss-text pairs from monolingual spoken language text. By pre-training on the
thus obtained synthetic data, we improve translation from American Sign
Language (ASL) to English and German Sign Language (DGS) to German by up to
3.14 and 2.20 BLEU, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07480</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07480</id><submitter>Dario Paccagnan</submitter><version version="v1"><date>Sun, 16 May 2021 16:45:52 GMT</date><size>518kb</size><source_type>D</source_type></version><title>In Congestion Games, Taxes Achieve Optimal Approximation</title><authors>Dario Paccagnan, Martin Gairing</authors><categories>cs.GT cs.CC math.OC</categories><comments>To appear at the 22nd ACM Conference on Economics &amp; Computation
  (EC'21)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing social cost in atomic congestion games
and show, perhaps surprisingly, that efficiently computed taxation mechanisms
yield the same performance achievable by the best polynomial time algorithm,
even when the latter has full control over the players' actions. It follows
that no other tractable approach geared at incentivizing desirable system
behavior can improve upon this result, regardless of whether it is based on
taxations, coordination mechanisms, information provision, or any other
principle. In short: Judiciously chosen taxes achieve optimal approximation.
  Three technical contributions underpin this conclusion. First, we show that
computing the minimum social cost is NP-hard to approximate within a given
factor depending solely on the admissible resource costs. Second, we design a
tractable taxation mechanism whose efficiency (price of anarchy) matches this
hardness factor, and thus is optimal. As these results extend to coarse
correlated equilibria, any no-regret algorithm inherits the same performances,
allowing us to devise polynomial time algorithms with optimal approximation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07484</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07484</id><submitter>Ioannis Pikoulis</submitter><version version="v1"><date>Sun, 16 May 2021 17:31:59 GMT</date><size>4520kb</size><source_type>D</source_type></version><title>Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional
  Architectures in a Contextual Approach for Video-Based Visual Emotion
  Recognition in the Wild</title><authors>Ioannis Pikoulis, Panagiotis P. Filntisis, Petros Maragos</authors><categories>cs.CV</categories><comments>9 pages, 4 figures, 5 tables, submitted to the 16th IEEE
  International Conference on Automatic Face and Gesture Recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we tackle the task of video-based visual emotion recognition in
the wild. Standard methodologies that rely solely on the extraction of bodily
and facial features often fall short of accurate emotion prediction in cases
where the aforementioned sources of affective information are inaccessible due
to head/body orientation, low resolution and poor illumination. We aspire to
alleviate this problem by leveraging visual context in the form of scene
characteristics and attributes, as part of a broader emotion recognition
framework. Temporal Segment Networks (TSN) constitute the backbone of our
proposed model. Apart from the RGB input modality, we make use of dense Optical
Flow, following an intuitive multi-stream approach for a more effective
encoding of motion. Furthermore, we shift our attention towards skeleton-based
learning and leverage action-centric data as means of pre-training a
Spatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion
recognition. Our extensive experiments on the challenging Body Language Dataset
(BoLD) verify the superiority of our methods over existing approaches, while by
properly incorporating all of the aforementioned modules in a network ensemble,
we manage to surpass the previous best published recognition scores, by a large
margin.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07485</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07485</id><submitter>Maxim Ziatdinov</submitter><version version="v1"><date>Sun, 16 May 2021 17:44:59 GMT</date><size>2369kb</size></version><title>AtomAI: A Deep Learning Framework for Analysis of Image and Spectroscopy
  Data in (Scanning) Transmission Electron Microscopy and Beyond</title><authors>Maxim Ziatdinov, Ayana Ghosh, Tommy Wong, and Sergei V. Kalinin</authors><categories>physics.data-an cond-mat.dis-nn cond-mat.mtrl-sci cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  AtomAI is an open-source software package bridging instrument-specific Python
libraries, deep learning, and simulation tools into a single ecosystem. AtomAI
allows direct applications of the deep convolutional neural networks for atomic
and mesoscopic image segmentation converting image and spectroscopy data into
class-based local descriptors for downstream tasks such as statistical and
graph analysis. For atomically-resolved imaging data, the output is types and
positions of atomic species, with an option for subsequent refinement. AtomAI
further allows the implementation of a broad range of image and spectrum
analysis functions, including invariant variational autoencoders (VAEs). The
latter consists of VAEs with rotational and (optionally) translational
invariance for unsupervised and class-conditioned disentanglement of
categorical and continuous data representations. In addition, AtomAI provides
utilities for mapping structure-property relationships via im2spec and spec2im
type of encoder-decoder models. Finally, AtomAI allows seamless connection to
the first principles modeling with a Python interface, including molecular
dynamics and density functional theory calculations on the inferred atomic
position. While the majority of applications to date were based on atomically
resolved electron microscopy, the flexibility of AtomAI allows straightforward
extension towards the analysis of mesoscopic imaging data once the labels and
feature identification workflows are established/available. The source code and
example notebooks are available at https://github.com/pycroscopy/atomai.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07495</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07495</id><submitter>Mahdiyar Molahasani Majdabadi</submitter><version version="v1"><date>Sun, 16 May 2021 19:03:24 GMT</date><size>1664kb</size><source_type>D</source_type></version><title>Capsule GAN for Prostate MRI Super-Resolution</title><authors>Mahdiyar Molahasani Majdabadi and S. Deivalakshmi and Seokbum Ko</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Prostate cancer is a very common disease among adult men. One in seven
Canadian men is diagnosed with this cancer in their lifetime. Super-Resolution
(SR) can facilitate early diagnosis and potentially save many lives. In this
paper, a robust and accurate model is proposed for prostate MRI SR. The model
is trained on the Prostate-Diagnosis and PROSTATEx datasets. The proposed model
outperformed the state-of-the-art prostate SR model in all similarity metrics
with notable margins. A new task-specific similarity assessment is introduced
as well. A classifier is trained for severe cancer detection and the drop in
the accuracy of this model when dealing with super-resolved images is used for
evaluating the ability of medical detail reconstruction of the SR models. The
proposed SR model is a step towards an efficient and accurate general medical
SR platform.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07497</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07497</id><submitter>David Dice</submitter><version version="v1"><date>Sun, 16 May 2021 19:13:22 GMT</date><size>63kb</size><source_type>D</source_type></version><title>Intra-process Caching and Reuse of Threads</title><authors>Dave Dice and Alex Kogan</authors><categories>cs.DC</categories><acm-class>D.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Creating and destroying threads on modern Linux systems incurs high latency,
absent concurrency, and fails to scale as we increase concurrency. To address
this concern we introduce a process-local cache of idle threads. Specifically,
instead of destroying a thread when it terminates, we cache and then recycle
that thread in the context of subsequent thread creation requests. This
approach shows significant promise in various applications and benchmarks that
create and destroy threads rapidly and illustrates the need for and potential
benefits of improved concurrency infrastructure. With caching, the cost of
creating a new thread drops by almost an order of magnitude. As our experiments
demonstrate, this results in significant performance improvements for multiple
applications that aggressively create and destroy numerous threads.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07499</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07499</id><submitter>Hossein Aghili</submitter><version version="v1"><date>Sun, 16 May 2021 19:25:35 GMT</date><size>17kb</size></version><title>Left Dihedral Codes over Finite Chain Rings</title><authors>H.Aghili and R.Sobhani</authors><categories>cs.IT math.IT math.RA</categories><comments>22 pages, submitted to Discrete Mathematics journal on 15 may 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Let $R$ be a finite commutative chain ring, $D_{2n}$ be the dihedral group of
size $2n$ and $R[D_{2n}]$ be the dihedral group ring. In this paper, we
completely characterize left ideals of $R[D_{2n}]$ (called left $D_{2n}$-codes)
when ${\rm gcd}(char(R),n)=1$. In this way, we explore the structure of some
skew-cyclic codes of length 2 over $R$ and also over $R\times S$, where $S$ is
an isomorphic copy of $R$. As a particular result, we give the structure of
cyclic codes of length 2 over $R$. In the case where $R=\F_{p^m}$ is a Galois
field, we give a classification for left $D_{2N}$-codes over $\F_{p^m}$, for
any positive integer $N$. In both cases we determine dual codes and identify
self-dual ones.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07501</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07501</id><submitter>Mohammad Sayad Haghighi</submitter><version version="v1"><date>Sun, 16 May 2021 19:35:16 GMT</date><size>1511kb</size><source_type>D</source_type></version><title>Analysis of Bitcoin Vulnerability to Bribery Attacks Launched Through
  Large Transactions</title><authors>Ghader Ebrahimpour, Mohammad Sayad Haghighi</authors><categories>cs.CR cs.MA</categories><comments>This work is under review for formal publication</comments><acm-class>G.3; H.2; H.4; E.2; E.3; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin uses blockchain technology to maintain transactions order and
provides probabilistic guarantee to prevent double-spending, assuming that an
attacker's computational power does not exceed %50 of the network power. In
this paper, we design a novel bribery attack and show that this guarantee can
be hugely undermined. Miners are assumed to be rational in this setup and they
are given incentives that are dynamically calculated. In this attack, the
adversary misuses the Bitcoin protocol to bribe miners and maximize their
gained advantage. We will reformulate the bribery attack to propose a general
mathematical foundation upon which we build multiple strategies. We show that,
unlike Whale Attack, these strategies are practical. If the rationality
assumption holds, this shows how vulnerable blockchain-based systems like
Bitcoin are. We suggest a soft fork on Bitcoin to fix this issue at the end.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07505</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07505</id><submitter>David Petrizze</submitter><version version="v1"><date>Sun, 16 May 2021 19:53:46 GMT</date><size>383kb</size><source_type>D</source_type></version><title>Distinguishing Aerial Intruders from Trajectory Data: A Model-Based
  Hypothesis-Testing Approach</title><authors>David Petrizze, Kasra Koorehdavoudi, Mengran Xue, and Sandip Roy</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by security needs in unmanned aerial system (UAS) operations, an
algorithm for identifying airspace intruders (e.g., birds vs. drones) is
developed. The algorithm is structured to use sensed intruder velocity data
from Internet-of-Things platforms together with limited knowledge of physical
models. The identification problem is posed as a statistical hypothesis testing
or detection problem, wherein inertial feedback-controlled objects subject to
stochastic actuation must be distinguished by speed data. The maximum a
posteriori probability detector is obtained, and then is simplified to an
explicit computation based on two points in the sample autocorrelation of the
data. The simplified form allows computationally-friendly implementation of the
algorithm, and simplified learning from archived data. Also, the total
probability of error of the detector is computed and characterized. Simulations
based on synthesized data are presented to illustrate and supplement the formal
analyses.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07508</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07508</id><submitter>Scott Cheng-Hsin Yang</submitter><version version="v1"><date>Sun, 16 May 2021 20:40:23 GMT</date><size>26kb</size></version><title>Abstraction, Validation, and Generalization for Explainable Artificial
  Intelligence</title><authors>Scott Cheng-Hsin Yang, Tomas Folke, and Patrick Shafto</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural network architectures are achieving superhuman performance on an
expanding range of tasks. To effectively and safely deploy these systems, their
decision-making must be understandable to a wide range of stakeholders. Methods
to explain AI have been proposed to answer this challenge, but a lack of theory
impedes the development of systematic abstractions which are necessary for
cumulative knowledge gains. We propose Bayesian Teaching as a framework for
unifying explainable AI (XAI) by integrating machine learning and human
learning. Bayesian Teaching formalizes explanation as a communication act of an
explainer to shift the beliefs of an explainee. This formalization decomposes
any XAI method into four components: (1) the inference to be explained, (2) the
explanatory medium, (3) the explainee model, and (4) the explainer model. The
abstraction afforded by Bayesian Teaching to decompose any XAI method
elucidates the invariances among them. The decomposition of XAI systems enables
modular validation, as each of the first three components listed can be tested
semi-independently. This decomposition also promotes generalization through
recombination of components from different XAI systems, which facilitates the
generation of novel variants. These new variants need not be evaluated one by
one provided that each component has been validated, leading to an exponential
decrease in development time. Finally, by making the goal of explanation
explicit, Bayesian Teaching helps developers to assess how suitable an XAI
system is for its intended real-world use case. Thus, Bayesian Teaching
provides a theoretical framework that encourages systematic, scientific
investigation of XAI.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07510</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07510</id><submitter>Benjamin Townsend</submitter><version version="v1"><date>Sun, 16 May 2021 20:46:29 GMT</date><size>434kb</size><source_type>D</source_type></version><title>Doc2Dict: Information Extraction as Text Generation</title><authors>Benjamin Townsend, Eamon Ito-Fisher, Lily Zhang and Madison May</authors><categories>cs.CL cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Typically, information extraction (IE) requires a pipeline approach: first, a
sequence labeling model is trained on manually annotated documents to extract
relevant spans; then, when a new document arrives, a model predicts spans which
are then post-processed and standardized to convert the information into a
database entry. We replace this labor-intensive workflow with a transformer
language model trained on existing database records to directly generate
structured JSON. Our solution removes the workload associated with producing
token-level annotations and takes advantage of a data source which is generally
quite plentiful (e.g. database records). As long documents are common in
information extraction tasks, we use gradient checkpointing and chunked
encoding to apply our method to sequences of up to 32,000 tokens on a single
GPU. Our Doc2Dict approach is competitive with more complex, hand-engineered
pipelines and offers a simple but effective baseline for document-level
information extraction. We release our Doc2Dict model and code to reproduce our
experiments and facilitate future work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07511</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07511</id><submitter>Carlos E. Budde</submitter><version version="v1"><date>Sun, 16 May 2021 20:52:20 GMT</date><size>273kb</size><source_type>D</source_type></version><title>Efficient Algorithms for Quantitative Attack Tree Analysis</title><authors>Carlos E. Budde, Mari\&quot;elle Stoelinga</authors><categories>cs.CR cs.DS</categories><comments>Public version of CSF'21 paper, including an appendix with all proofs
  of lemmas and theorems</comments><acm-class>F.1.0; F.2.2; G.2.3</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Numerous analysis methods for quantitative attack tree analysis have been
proposed. These algorithms compute relevant security metrics, i.e. performance
indicators that quantify how good the security of a system is, such as the most
likely attack, the cheapest, or the most damaging one. This paper classifies
attack trees in two dimensions: proper trees vs. directed acyclic graphs (i.e.
with shared subtrees); and static vs. dynamic gates. For each class, we propose
novel algorithms that work over a generic attribute domain, encompassing a
large number of concrete security metrics defined on the attack tree semantics.
We also analyse the computational complexity of our methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07512</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07512</id><submitter>Xiao Wang</submitter><version version="v1"><date>Sun, 16 May 2021 20:53:31 GMT</date><size>20017kb</size><source_type>D</source_type></version><title>Substitutional Neural Image Compression</title><authors>Xiao Wang, Wei Jiang, Wei Wang, Shan Liu, Brian Kulis, Peter Chin</authors><categories>cs.CV cs.LG eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We describe Substitutional Neural Image Compression (SNIC), a general
approach for enhancing any neural image compression model, that requires no
data or additional tuning of the trained model. It boosts compression
performance toward a flexible distortion metric and enables bit-rate control
using a single model instance. The key idea is to replace the image to be
compressed with a substitutional one that outperforms the original one in a
desired way. Finding such a substitute is inherently difficult for conventional
codecs, yet surprisingly favorable for neural compression models thanks to
their fully differentiable structures. With gradients of a particular loss
backpropogated to the input, a desired substitute can be efficiently crafted
iteratively. We demonstrate the effectiveness of SNIC, when combined with
various neural compression models and target metrics, in improving compression
quality and performing bit-rate control measured by rate-distortion curves.
Empirical results of control precision and generation speed are also discussed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07513</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07513</id><submitter>Ferdinando Fioretto</submitter><version version="v1"><date>Sun, 16 May 2021 21:04:19 GMT</date><size>962kb</size><source_type>D</source_type></version><title>Decision Making with Differential Privacy under a Fairness Lens</title><authors>Ferdinando Fioretto, Cuong Tran, Pascal Van Hentenryck</authors><categories>cs.AI cs.CY cs.LG</categories><comments>This paper is an extended version of the homonymous one, accepted at
  IJCAI-21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agencies, such as the U.S. Census Bureau, release data sets and statistics
about groups of individuals that are used as input to a number of critical
decision processes. To conform to privacy and confidentiality requirements,
these agencies are often required to release privacy-preserving versions of the
data. This paper studies the release of differentially private data sets and
analyzes their impact on some critical resource allocation tasks under a
fairness perspective. {The paper shows that, when the decisions take as input
differentially private data}, the noise added to achieve privacy
disproportionately impacts some groups over others. The paper analyzes the
reasons for these disproportionate impacts and proposes guidelines to mitigate
these effects. The proposed approaches are evaluated on critical decision
problems that use differentially private census data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07517</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07517</id><submitter>Peter Manohar</submitter><version version="v1"><date>Sun, 16 May 2021 21:28:34 GMT</date><size>46kb</size></version><title>A Stress-Free Sum-of-Squares Lower Bound for Coloring</title><authors>Pravesh K. Kothari and Peter Manohar</authors><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that with high probability over the choice of a random graph $G$
from the Erd\H{o}s-R\'enyi distribution $G(n,1/2)$, a natural
$n^{O(\varepsilon^2 \log n)}$-time, degree $O(\varepsilon^2 \log n)$
sum-of-squares semidefinite program cannot refute the existence of a valid
$k$-coloring of $G$ for $k = n^{1/2 +\varepsilon}$. Our result implies that the
refutation guarantee of the basic semidefinite program (a close variant of the
Lov\'asz theta function) cannot be appreciably improved by a natural $o(\log
n)$-degree sum-of-squares strengthening, and this is tight up to a $n^{o(1)}$
slack in $k$. To the best of our knowledge, this is the first lower bound for
coloring $G(n,1/2)$ for even a single round strengthening of the basic SDP in
any SDP hierarchy.
  Our proof relies on a new variant of instance-preserving non-pointwise
complete reduction within SoS from coloring a graph to finding large
independent sets in it. Our proof is (perhaps surprisingly) short, simple and
does not require complicated spectral norm bounds on random matrices with
dependent entries that have been otherwise necessary in the proofs of many
similar results [BHK+16, HKP+17, KB19, GJJ+20, MRX20].
  Our result formally holds for a constraint system where vertices are allowed
to belong to multiple color classes; we leave the extension to the formally
stronger formulation of coloring, where vertices must belong to unique colors
classes, as an outstanding open problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07518</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07518</id><submitter>Shunhua Jiang</submitter><version version="v1"><date>Sun, 16 May 2021 21:34:04 GMT</date><size>37kb</size></version><title>Near-Optimal Time-Energy Trade-Offs for Deterministic Leader Election</title><authors>Yi-Jun Chang, Ran Duan, Shunhua Jiang</authors><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the energy complexity of the leader election problem in the
single-hop radio network model, where each device has a unique identifier in
$\{1, 2, \ldots, N\}$. Energy is a scarce resource for small battery-powered
devices. For such devices, most of the energy is often spent on communication,
not on computation. To approximate the actual energy cost, the energy
complexity of an algorithm is defined as the maximum over all devices of the
number of time slots where the device transmits or listens. Much progress has
been made in understanding the energy complexity of leader election in radio
networks, but very little is known about the trade-off between time and energy.
  $\textbf{Time-energy trade-off:}$ For any $k \geq \log \log N$, we show that
a leader among at most $n$ devices can be elected deterministically in
$O(n^{1+\epsilon}) + O(k \cdot N^{1/k})$ time and $O(k)$ energy if each device
can simultaneously transmit and listen, where $\epsilon &gt; 0$ is any small
constant. This improves upon the previous $O(N)$-time $O(\log \log N)$-energy
algorithm by Chang et al. [STOC 2017]. We provide lower bounds to show that the
time-energy trade-off of our algorithm is near-optimal.
  $\textbf{Dense instances:}$ For the dense instances where the number of
devices is $n = \Theta(N)$, we design a deterministic leader election algorithm
using only $O(1)$ energy. This improves upon the $O(\log^* N)$-energy algorithm
by Jurdzi\'{n}ski et al. [PODC 2002] and the $O(\alpha(N))$-energy algorithm by
Chang et al. [STOC 2017]. More specifically, we show that the optimal
deterministic energy complexity of leader election is
$\Theta\left(\max\left\{1, \log \frac{N}{n}\right\}\right)$ if the devices
cannot simultaneously transmit and listen, and it is $\Theta\left(\max\left\{1,
\log \log \frac{N}{n}\right\}\right)$ if they can.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07519</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07519</id><submitter>Xiang Deng</submitter><version version="v1"><date>Sun, 16 May 2021 21:38:24 GMT</date><size>5323kb</size><source_type>D</source_type></version><title>Graph-Free Knowledge Distillation for Graph Neural Networks</title><authors>Xiang Deng and Zhongfei Zhang</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Knowledge distillation (KD) transfers knowledge from a teacher network to a
student by enforcing the student to mimic the outputs of the pretrained teacher
on training data. However, data samples are not always accessible in many cases
due to large data sizes, privacy, or confidentiality. Many efforts have been
made on addressing this problem for convolutional neural networks (CNNs) whose
inputs lie in a grid domain within a continuous space such as images and
videos, but largely overlook graph neural networks (GNNs) that handle non-grid
data with different topology structures within a discrete space. The inherent
differences between their inputs make these CNN-based approaches not applicable
to GNNs. In this paper, we propose to our best knowledge the first dedicated
approach to distilling knowledge from a GNN without graph data. The proposed
graph-free KD (GFKD) learns graph topology structures for knowledge transfer by
modeling them with multinomial distribution. We then introduce a gradient
estimator to optimize this framework. Essentially, the gradients w.r.t. graph
structures are obtained by only using GNN forward-propagation without
back-propagation, which means that GFKD is compatible with modern GNN libraries
such as DGL and Geometric. Moreover, we provide the strategies for handling
different types of prior knowledge in the graph data or the GNNs. Extensive
experiments demonstrate that GFKD achieves the state-of-the-art performance for
distilling knowledge from GNNs without training data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07520</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07520</id><submitter>Vladimir Boza</submitter><version version="v1"><date>Sun, 16 May 2021 21:39:17 GMT</date><size>1001kb</size><source_type>D</source_type></version><title>Dynamic Pooling Improves Nanopore Base Calling Accuracy</title><authors>Vladim\'ir Bo\v{z}a, Peter Pere\v{s}\'ini, Bro\v{n}a Brejov\'a,
  Tom\'a\v{s} Vina\v{r}</authors><categories>cs.LG q-bio.GN</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In nanopore sequencing, electrical signal is measured as DNA molecules pass
through the sequencing pores. Translating these signals into DNA bases (base
calling) is a highly non-trivial task, and its quality has a large impact on
the sequencing accuracy. The most successful nanopore base callers to date use
convolutional neural networks (CNN) to accomplish the task.
  Convolutional layers in CNNs are typically composed of filters with constant
window size, performing best in analysis of signals with uniform speed.
However, the speed of nanopore sequencing varies greatly both within reads and
between sequencing runs. Here, we present dynamic pooling, a novel neural
network component, which addresses this problem by adaptively adjusting the
pooling ratio. To demonstrate the usefulness of dynamic pooling, we developed
two base callers: Heron and Osprey. Heron improves the accuracy beyond the
experimental high-accuracy base caller Bonito developed by Oxford Nanopore.
Osprey is a fast base caller that can compete in accuracy with Guppy
high-accuracy mode, but does not require GPU acceleration and achieves a near
real-time speed on common desktop CPUs.
  Availability: https://github.com/fmfi-compbio/osprey,
https://github.com/fmfi-compbio/heron
  Keywords: nanopore sequencing, base calling, convolutional neural networks,
pooling
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07522</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07522</id><submitter>Fredy Vides Fredy</submitter><version version="v1"><date>Sun, 16 May 2021 21:41:34 GMT</date><size>1336kb</size></version><title>Sparse system identification by low-rank approximation</title><authors>Fredy Vides</authors><categories>math.NA cs.NA cs.SY eess.SY math.OC nlin.AO nlin.PS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document, some general results from approximation theory and matrix
analysis with applications to the approximate sparse identification of time
series models and nonlinear discrete-time dynamical systems are presented. The
aforementioned theoretical methods are translated into predictive algorithms
that can be used to approximately simulate the behavior of a given dynamical
system, based on some structured data measured from the system. The
approximation of the state-transition operators determined primarily by
matrices of parameters to be identified based on data measured from a given
system, is approached by proving the existence of low-rank approximations of
submatrices of the trajectory matrices corresponding to the measured data, that
can be used to compute approximate sparse representations of the matrices of
parameters. Prototypical algorithms and numerical implementations of the
aforementioned techniques to the approximate identification and predictive
simulation of time series models with symmetries and nonlinear structured
dynamical systems in theoretical physics, fluid dynamics and weather
forecasting are presented.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07523</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07523</id><submitter>Manoel Horta Ribeiro</submitter><version version="v1"><date>Sun, 16 May 2021 21:47:30 GMT</date><size>1675kb</size><source_type>D</source_type></version><title>Follow the Money: Analyzing @slpng_giants_pt's Strategy to Combat
  Misinformation</title><authors>B\'arbara Gomes Ribeiro, Manoel Horta Ribeiro, Virg\'ilio Almeida,
  Wagner Meira Jr</authors><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2020, the activist movement @sleeping_giants_pt (SGB) made a splash in
Brazil. Similar to its international counterparts, the movement carried
&quot;campaigns&quot; against media outlets spreading misinformation. In those, SGB
targeted companies whose ads were shown in these outlets, publicly asking them
to remove the ads. In this work, we present a careful characterization of SGB's
activism model, analyzing the three campaigns carried by the movement up to
September 2020. We study how successful its complaints were and what factors
are associated with their success, how attention towards the targeted media
outlets progressed, and how online interactions with the companies were
impacted after they were targeted. Leveraging an annotated corpus of SGB's
tweets as well as other data from Twitter and Google Search, we show that SGB's
&quot;campaigns&quot; were largely successful: over 86\% of companies (n=161) responded
positively to SGB's requests, and, for those that responded, we find user
pressure to be negatively correlated with the time companies take to answer
($r$=-0.67; $p$&lt;0.001). Finally, we find that, although changes in the
interactions with companies were transient, the impact in targeted media
outlets endured: all three outlets experienced a significant decrease in
engagement on Twitter and search volume on Google following the start of SGB's
campaigns. Overall, our work suggests that internet-based activism can leverage
the transient attention it captures towards concrete goals to have a
long-lasting impact.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07525</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07525</id><submitter>Tuomas Hakoniemi</submitter><version version="v1"><date>Sun, 16 May 2021 21:50:31 GMT</date><size>18kb</size></version><title>Monomial-size vs. Bit-complexity in Sums-of-Squares and Polynomial
  Calculus</title><authors>Tuomas Hakoniemi</authors><categories>cs.CC cs.LO</categories><comments>To appear in the Proceedings of the 36th Annual ACM/IEEE Symposium on
  Logic in Computer Science (LICS 2021)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we consider the relationship between monomial-size and
bit-complexity in Sums-of-Squares (SOS) in Polynomial Calculus Resolution over
rationals (PCR/$\mathbb{Q}$). We show that there is a set of polynomial
constraints $Q_n$ over Boolean variables that has both SOS and PCR/$\mathbb{Q}$
refutations of degree 2 and thus with only polynomially many monomials, but for
which any SOS or PCR/$\mathbb{Q}$ refutation must have exponential
bit-complexity, when the rational coefficients are represented with their
reduced fractions written in binary.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07526</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07526</id><submitter>Yuping Fan</submitter><version version="v1"><date>Sun, 16 May 2021 21:56:31 GMT</date><size>18843kb</size><source_type>D</source_type></version><title>DRAS-CQSim: A Reinforcement Learning based Framework for HPC Cluster
  Scheduling</title><authors>Yuping Fan and Zhiling Lan</authors><categories>cs.DC cs.AI</categories><journal-ref>Software Impacts 2021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For decades, system administrators have been striving to design and tune
cluster scheduling policies to improve the performance of high performance
computing (HPC) systems. However, the increasingly complex HPC systems combined
with highly diverse workloads make such manual process challenging,
time-consuming, and error-prone. We present a reinforcement learning based HPC
scheduling framework named DRAS-CQSim to automatically learn optimal scheduling
policy. DRAS-CQSim encapsulates simulation environments, agents, hyperparameter
tuning options, and different reinforcement learning algorithms, which allows
the system administrators to quickly obtain customized scheduling policies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07527</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07527</id><submitter>Peter Hegedus Dr</submitter><version version="v1"><date>Sun, 16 May 2021 22:05:48 GMT</date><size>315kb</size><source_type>D</source_type></version><title>Improving Vulnerability Prediction of JavaScript Functions Using Process
  Metrics</title><authors>Tam\'as Viszkok, P\'eter Heged\H{u}s, Rudolf Ferenc</authors><categories>cs.CR cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Due to the growing number of cyber attacks against computer systems, we need
to pay special attention to the security of our software systems. In order to
maximize the effectiveness, excluding the human component from this process
would be a huge breakthrough. The first step towards this is to automatically
recognize the vulnerable parts in our code. Researchers put a lot of effort
into creating machine learning models that could determine if a given piece of
code, or to be more precise, a selected function, contains any vulnerabilities
or not. We aim at improving the existing models, building on previous results
in predicting vulnerabilities at the level of functions in JavaScript code
using the well-known static source code metrics. In this work, we propose to
include several so-called process metrics (e.g., code churn, number of
developers modifying a file, or the age of the changed source code) into the
set of features, and examine how they affect the performance of the
function-level JavaScript vulnerability prediction models. We can confirm that
process metrics significantly improve the prediction power of such models. On
average, we observed a 8.4% improvement in terms of F-measure (from 0.764 to
0.848), 3.5% improvement in terms of precision (from 0.953 to 0.988) and a 6.3%
improvement in terms of recall (from 0.697 to 0.760).
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07529</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07529</id><submitter>Nikita Kurilenko</submitter><version version="v1"><date>Sun, 16 May 2021 22:12:10 GMT</date><size>79kb</size></version><title>Infinitely growing configurations in Emil Post's tag system problem</title><authors>Nikita V. Kurilenko</authors><categories>cs.DM cs.CC</categories><comments>13 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Emil Post's tag system problem is the question of whether or not a tag system
$\{N=3, P(0)=00, P(1)=1101\}$ has a configuration, simulation of which will
never halt or end up in a loop. For the past decades, there were several
attempts to find an answer to this question, including a recent study by
Wolfram (2021), during which the first $2^{84}$ initial configurations were
checked. This paper presents a family of configurations of this type in a form
of strings $a^{n} b c^{m}$, that evolve to $a^{n+1} b c^{m+1}$ after a finite
amount of steps. The proof of this behavior for all non-negative $n$ and $m$ is
described further in a paper as a finite verification procedure, which is
computationally bounded by 20000 iterations of tag. All corresponding code can
be found at
https://github.com/nikitakurilenko/post-tag-infinitely-growing-configurations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07530</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07530</id><submitter>Anna Stakia</submitter><version version="v1"><date>Sun, 16 May 2021 22:20:30 GMT</date><size>7014kb</size><source_type>D</source_type></version><title>Advanced Multi-Variate Analysis Methods for New Physics Searches at the
  Large Hadron Collider</title><authors>Anna Stakia, Tommaso Dorigo, Giovanni Banelli, Daniela Bortoletto,
  Alessandro Casa, Pablo de Castro, Christophe Delaere, Julien Donini, Livio
  Finos, Michele Gallinaro, Andrea Giammanco, Alexander Held, Fabricio
  Jim\'enez Morales, Grzegorz Kotkowski, Seng Pei Liew, Fabio Maltoni, Giovanna
  Menardi, Ioanna Papavergou, Alessia Saggio, Bruno Scarpa, Giles C. Strong,
  Cecilia Tosciri, Jo\~ao Varela, Pietro Vischia, Andreas Weiler</authors><categories>hep-ex cs.LG hep-ph physics.data-an</categories><comments>95 pages, 21 figures, submitted to Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Between the years 2015 and 2019, members of the Horizon 2020-funded
Innovative Training Network named &quot;AMVA4NewPhysics&quot; studied the customization
and application of advanced multivariate analysis methods and statistical
learning tools to high-energy physics problems, as well as developed entirely
new ones. Many of those methods were successfully used to improve the
sensitivity of data analyses performed by the ATLAS and CMS experiments at the
CERN Large Hadron Collider; several others, still in the testing phase, promise
to further improve the precision of measurements of fundamental physics
parameters and the reach of searches for new phenomena. In this paper, the most
relevant new tools, among those studied and developed, are presented along with
the evaluation of their performances.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07531</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07531</id><submitter>Iddo Tzameret</submitter><version version="v1"><date>Sun, 16 May 2021 22:20:55 GMT</date><size>48kb</size></version><title>First-Order Reasoning and Efficient Semi-Algebraic Proofs</title><authors>Fedor Part, Neil Thapen, Iddo Tzameret</authors><categories>cs.LO cs.CC</categories><comments>A preliminary version of this work appears in 36th Ann. Symp. Logic
  Comput. Science (LICS) 2021</comments><acm-class>F.2.2; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semi-algebraic proof systems such as sum-of-squares (SoS) have attracted a
lot of attention recently due to their relation to approximation algorithms:
constant degree semi-algebraic proofs lead to conjecturally optimal
polynomial-time approximation algorithms for important NP-hard optimization
problems. Motivated by the need to allow a more streamlined and uniform
framework for working with SoS proofs than the restrictive propositional level,
we initiate a systematic first-order logical investigation into the kinds of
reasoning possible in algebraic and semi-algebraic proof systems. Specifically,
we develop first-order theories that capture in a precise manner constant
degree algebraic and semi-algebraic proof systems: every statement of a certain
form that is provable in our theories translates into a family of constant
degree polynomial calculus or SoS refutations, respectively; and using a
reflection principle, the converse also holds.
  This places algebraic and semi-algebraic proof systems in the established
framework of bounded arithmetic, while providing theories corresponding to
systems that vary quite substantially from the usual propositional-logic ones.
  We give examples of how our semi-algebraic theory proves statements such as
the pigeonhole principle, we provide a separation between algebraic and
semi-algebraic theories, and we describe initial attempts to go beyond these
theories by introducing extensions that use the inequality symbol, identifying
along the way which extensions lead outside the scope of constant degree SoS.
Moreover, we prove new results for propositional proofs, and specifically
extend Berkholz's dynamic-by-static simulation of polynomial calculus (PC) by
SoS to PC with the radical rule.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07532</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07532</id><submitter>Yang Chen</submitter><version version="v1"><date>Sun, 16 May 2021 22:23:23 GMT</date><size>236kb</size><source_type>D</source_type></version><title>Towards Synthetic Multivariate Time Series Generation for Flare
  Forecasting</title><authors>Yang Chen, Dustin J. Kempton, Azim Ahmadzadeh and Rafal A. Angryk</authors><categories>cs.LG</categories><comments>The 20th International Conference on Artificial Intelligence and Soft
  Computing (ICAISC02021). 13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the limiting factors in training data-driven, rare-event prediction
algorithms is the scarcity of the events of interest resulting in an extreme
imbalance in the data. There have been many methods introduced in the
literature for overcoming this issue; simple data manipulation through
undersampling and oversampling, utilizing cost-sensitive learning algorithms,
or by generating synthetic data points following the distribution of the
existing data. While synthetic data generation has recently received a great
deal of attention, there are real challenges involved in doing so for
high-dimensional data such as multivariate time series. In this study, we
explore the usefulness of the conditional generative adversarial network (CGAN)
as a means to perform data-informed oversampling in order to balance a large
dataset of multivariate time series. We utilize a flare forecasting benchmark
dataset, named SWAN-SF, and design two verification methods to both
quantitatively and qualitatively evaluate the similarity between the generated
minority and the ground-truth samples. We further assess the quality of the
generated samples by training a classical, supervised machine learning
algorithm on synthetic data, and testing the trained model on the unseen, real
data. The results show that the classifier trained on the data augmented with
the synthetic multivariate time series achieves a significant improvement
compared with the case where no augmentation is used. The popular flare
forecasting evaluation metrics, TSS and HSS, report 20-fold and 5-fold
improvements, respectively, indicating the remarkable statistical similarities,
and the usefulness of CGAN-based data generation for complicated tasks such as
flare forecasting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07533</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07533</id><submitter>Richard Jiang</submitter><version version="v1"><date>Sun, 16 May 2021 22:24:37 GMT</date><size>2357kb</size></version><title>Private Facial Diagnosis as an Edge Service for Parkinson's DBS
  Treatment Valuation</title><authors>Richard Jiang, Paul Chazot, Danny Crookes, Ahmed Bouridane and M Emre
  Celebi</authors><categories>cs.AI cs.CR cs.CV</categories><comments>Under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Facial phenotyping has recently been successfully exploited for medical
diagnosis as a novel way to diagnose a range of diseases, where facial
biometrics has been revealed to have rich links to underlying genetic or
medical causes. In this paper, taking Parkinson's Diseases (PD) as a case
study, we proposed an Artificial-Intelligence-of-Things (AIoT) edge-oriented
privacy-preserving facial diagnosis framework to analyze the treatment of Deep
Brain Stimulation (DBS) on PD patients. In the proposed framework, a new
edge-based information theoretically secure framework is proposed to implement
private deep facial diagnosis as a service over a privacy-preserving
AIoT-oriented multi-party communication scheme, where partial homomorphic
encryption (PHE) is leveraged to enable privacy-preserving deep facial
diagnosis directly on encrypted facial patterns. In our experiments with a
collected facial dataset from PD patients, for the first time, we demonstrated
that facial patterns could be used to valuate the improvement of PD patients
undergoing DBS treatment. We further implemented a privacy-preserving deep
facial diagnosis framework that can achieve the same accuracy as the
non-encrypted one, showing the potential of our privacy-preserving facial
diagnosis as an trustworthy edge service for grading the severity of PD in
patients.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07535</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07535</id><submitter>Michail Mylonakis</submitter><version version="v1"><date>Sun, 16 May 2021 22:34:01 GMT</date><size>134kb</size><source_type>D</source_type></version><title>Adaptive Interference Coordination over Channels with Unknown State at
  the Encoder and the Decoder</title><authors>Michail Mylonakis, Photios A. Stavrou, Mikael Skoglund</authors><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We generalize the problem of controlling the interference created to an
external observer while communicating over a discrete memoryless channel (DMC)
which was studied in \cite{serrano:2014}. In particular, we consider the
scenario where the transmission is established over a compound DMC channel with
unknown state at both the encoder and the decoder. Depending on the exact state
$s$ of the channel, we ask for a different level of average precision
$\Delta_s$ on the establishment of the interference coordination with the
external observer. For this set-up, we fully characterize the capacity region.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07536</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07536</id><submitter>Rong Ma</submitter><version version="v1"><date>Sun, 16 May 2021 22:43:20 GMT</date><size>2524kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 01:04:30 GMT</date><size>2524kb</size><source_type>D</source_type></version><title>Theoretical Foundations of t-SNE for Visualizing High-Dimensional
  Clustered Data</title><authors>T. Tony Cai and Rong Ma</authors><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study investigates the theoretical foundations of t-distributed
stochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction
and data visualization method. A novel theoretical framework for the analysis
of t-SNE based on the gradient descent approach is presented. For the early
exaggeration stage of t-SNE, we show its asymptotic equivalence to a power
iteration based on the underlying graph Laplacian, characterize its limiting
behavior, and uncover its deep connection to Laplacian spectral clustering, and
fundamental principles including early stopping as implicit regularization. The
results explain the intrinsic mechanism and the empirical benefits of such a
computational strategy. For the embedding stage of t-SNE, we characterize the
kinematics of the low-dimensional map throughout the iterations, and identify
an amplification phase, featuring the intercluster repulsion and the expansive
behavior of the low-dimensional map. The general theory explains the fast
convergence rate and the exceptional empirical performance of t-SNE for
visualizing clustered data, brings forth the interpretations of the t-SNE
output, and provides theoretical guidance for selecting tuning parameters in
various applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07537</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07537</id><submitter>Wenjin Wang Dr.</submitter><version version="v1"><date>Sun, 16 May 2021 22:45:41 GMT</date><size>3644kb</size><source_type>D</source_type></version><title>Algorithmic Principles of Camera-based Respiratory Motion Extraction</title><authors>Wenjin Wang, Albertus C. den Brinker</authors><categories>cs.CV</categories><comments>Camera based contactless health monitoring</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the respiratory signal from a video based on body motion has been
proposed and recently matured in products for video health monitoring. The core
algorithm for this measurement is the estimation of tiny chest/abdominal
motions induced by respiration, and the fundamental challenge is motion
sensitivity. Though prior arts reported on the validation with real human
subjects, there is no thorough/rigorous benchmark to quantify the sensitivities
and boundary conditions of motion-based core respiratory algorithms that
measure sub-pixel displacement between video frames. In this paper, we designed
a setup with a fully-controllable physical phantom to investigate the essence
of core algorithms, together with a mathematical model incorporating two motion
estimation strategies and three spatial representations, leading to six
algorithmic combinations for respiratory signal extraction. Their promises and
limitations are discussed and clarified via the phantom benchmark. The insights
gained in this paper are intended to improve the understanding and applications
of camera-based respiration measurement in health monitoring.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07540</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07540</id><submitter>Yun Liu</submitter><version version="v1"><date>Sun, 16 May 2021 22:56:06 GMT</date><size>4304kb</size></version><title>Deep learning for detecting pulmonary tuberculosis via chest
  radiography: an international study across 10 countries</title><authors>Sahar Kazemzadeh, Jin Yu, Shahar Jamshy, Rory Pilgrim, Zaid Nabulsi,
  Christina Chen, Neeral Beladia, Charles Lau, Scott Mayer McKinney, Thad
  Hughes, Atilla Kiraly, Sreenivasa Raju Kalidindi, Monde Muyoyeta, Jameson
  Malemela, Ting Shih, Greg S. Corrado, Lily Peng, Katherine Chou, Po-Hsuan
  Cameron Chen, Yun Liu, Krish Eswaran, Daniel Tse, Shravya Shetty, Shruthi
  Prabhakara</authors><categories>eess.IV cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tuberculosis (TB) is a top-10 cause of death worldwide. Though the WHO
recommends chest radiographs (CXRs) for TB screening, the limited availability
of CXR interpretation is a barrier. We trained a deep learning system (DLS) to
detect active pulmonary TB using CXRs from 9 countries across Africa, Asia, and
Europe, and utilized large-scale CXR pretraining, attention pooling, and noisy
student semi-supervised learning. Evaluation was on (1) a combined test set
spanning China, India, US, and Zambia, and (2) an independent mining population
in South Africa. Given WHO targets of 90% sensitivity and 70% specificity, the
DLS's operating point was prespecified to favor sensitivity over specificity.
On the combined test set, the DLS's ROC curve was above all 9 India-based
radiologists, with an AUC of 0.90 (95%CI 0.87-0.92). The DLS's sensitivity
(88%) was higher than the India-based radiologists (75% mean sensitivity),
p&lt;0.001 for superiority; and its specificity (79%) was non-inferior to the
radiologists (84% mean specificity), p=0.004. Similar trends were observed
within HIV positive and sputum smear positive sub-groups, and in the South
Africa test set. We found that 5 US-based radiologists (where TB isn't endemic)
were more sensitive and less specific than the India-based radiologists (where
TB is endemic). The DLS also remained non-inferior to the US-based
radiologists. In simulations, using the DLS as a prioritization tool for
confirmatory testing reduced the cost per positive case detected by 40-80%
compared to using confirmatory testing alone. To conclude, our DLS generalized
to 5 countries, and merits prospective evaluation to assist cost-effective
screening efforts in radiologist-limited settings. Operating point flexibility
may permit customization of the DLS to account for site-specific factors such
as TB prevalence, demographics, clinical resources, and customary practice
patterns.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07542</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07542</id><submitter>Chang Lu</submitter><version version="v1"><date>Sun, 16 May 2021 23:11:11 GMT</date><size>1254kb</size><source_type>D</source_type></version><title>Collaborative Graph Learning with Auxiliary Text for Temporal Event
  Prediction in Healthcare</title><authors>Chang Lu, Chandan K. Reddy, Prithwish Chakraborty, Samantha Kleinberg,
  Yue Ning</authors><categories>cs.LG cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate and explainable health event predictions are becoming crucial for
healthcare providers to develop care plans for patients. The availability of
electronic health records (EHR) has enabled machine learning advances in
providing these predictions. However, many deep learning based methods are not
satisfactory in solving several key challenges: 1) effectively utilizing
disease domain knowledge; 2) collaboratively learning representations of
patients and diseases; and 3) incorporating unstructured text. To address these
issues, we propose a collaborative graph learning model to explore
patient-disease interactions and medical domain knowledge. Our solution is able
to capture structural features of both patients and diseases. The proposed
model also utilizes unstructured text data by employing an attention regulation
strategy and then integrates attentive text features into a sequential learning
process. We conduct extensive experiments on two important healthcare problems
to show the competitive prediction performance of the proposed method compared
with various state-of-the-art models. We also confirm the effectiveness of
learned representations and model interpretability by a set of ablation and
case studies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07543</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07543</id><submitter>Abderrahmen Trichili</submitter><version version="v1"><date>Sun, 16 May 2021 23:20:39 GMT</date><size>5705kb</size><source_type>D</source_type></version><title>Retrofitting FSO Systems in Existing RF Infrastructure: A Non-Zero Sum
  Game Technology</title><authors>Abderrahmen Trichili, Amr Ragheb, Dmitrii Briantcev, Maged A. Esmail,
  Majid Altamimi, Islam Ashry, Boon S. Ooi, Saleh Alshebeili, Mohamed-Slim
  Alouini</authors><categories>cs.ET</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Progress in optical wireless communication (OWC) has unleashed the potential
to transmit data in an ultra-fast manner without incurring large investments
and bulk infrastructure. OWC includes wireless data transmissions in three
optical sub-bands; ultraviolet, visible, and infrared. This paper discusses
installing infrared OWC, known as free space optics (FSO), systems on top of
installed radio frequency (RF) networks for outdoor applications to benefit
from the reliability of RF links and the unlicensed broad optical spectrum, and
the large data rates carried by laser beams propagating in free space. We
equally review commercially available solutions and the hardware requirements
for RF and FSO technology co-existence. The potential of hybrid RF/FSO for
space communication is further discussed. Finally, open problems and future
research directions are presented.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07544</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07544</id><submitter>Jennifer Loe</submitter><version version="v1"><date>Sun, 16 May 2021 23:22:02 GMT</date><size>2121kb</size><source_type>D</source_type></version><title>Experimental Evaluation of Multiprecision Strategies for GMRES on GPUs</title><authors>Jennifer A. Loe, Christian A. Glusa, Ichitaro Yamazaki, Erik G. Boman,
  and Sivasankaran Rajamanickam</authors><categories>math.NA cs.MS cs.NA</categories><comments>Accepted for publication in the IEEE IPDPS Accelerators and Hybrid
  Emerging Systems (AsHES) 11th Workshop, 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support for lower precision computation is becoming more common in
accelerator hardware due to lower power usage, reduced data movement and
increased computational performance. However, computational science and
engineering (CSE) problems require double precision accuracy in several
domains. This conflict between hardware trends and application needs has
resulted in a need for multiprecision strategies at the linear algebra
algorithms level if we want to exploit the hardware to its full potential while
meeting the accuracy requirements. In this paper, we focus on preconditioned
sparse iterative linear solvers, a key kernel in several CSE applications. We
present a study of multiprecision strategies for accelerating this kernel on
GPUs. We seek the best methods for incorporating multiple precisions into the
GMRES linear solver; these include iterative refinement and parallelizable
preconditioners. Our work presents strategies to determine when multiprecision
GMRES will be effective and to choose parameters for a multiprecision iterative
refinement solver to achieve better performance. We use an implementation that
is based on the Trilinos library and employs Kokkos Kernels for performance
portability of linear algebra kernels. Performance results demonstrate the
promise of multiprecision approaches and demonstrate even further improvements
are possible by optimizing low-level kernels.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07547</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07547</id><submitter>Lueling Jia</submitter><version version="v1"><date>Sun, 16 May 2021 23:56:38 GMT</date><size>2747kb</size></version><title>Sparse Spectral-Galerkin Method on An Arbitrary Tetrahedron Using
  Generalized Koornwinder Polynomials</title><authors>Lueling Jia and Huiyuan Li and Zhimin Zhang</authors><categories>math.NA cs.NA</categories><comments>29 pages, 24 figures</comments><msc-class>65N25, 65N35</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper, we propose a sparse spectral-Galerkin approximation scheme for
solving the second-order partial differential equations on an arbitrary
tetrahedron. Generalized Koornwinder polynomials are introduced on the
reference tetrahedron as basis functions with their various recurrence
relations and differentiation properties being explored. The method leads to
well-conditioned and sparse linear systems whose entries can either be
calculated directly by the orthogonality of the generalized Koornwinder
polynomials for differential equations with constant coefficients or be
evaluated efficiently via our recurrence algorithm for problems with variable
coefficients. Clenshaw algorithms for the evaluation of any polynomial in an
expansion of the generalized Koornwinder basis are also designed to boost the
efficiency of the method. Finally, numerical experiments are carried out to
illustrate the effectiveness of the proposed Koornwinder spectral method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07552</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07552</id><submitter>Kailai Xu</submitter><version version="v1"><date>Mon, 17 May 2021 00:27:16 GMT</date><size>1196kb</size><source_type>D</source_type></version><title>Trust Region Method for Coupled Systems of PDE Solvers and Deep Neural
  Networks</title><authors>Kailai Xu, Eric Darve</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physics-informed machine learning and inverse modeling require the solution
of ill-conditioned non-convex optimization problems. First-order methods, such
as SGD and ADAM, and quasi-Newton methods, such as BFGS and L-BFGS, have been
applied with some success to optimization problems involving deep neural
networks in computational engineering inverse problems. However, empirical
evidence shows that convergence and accuracy for these methods remain a
challenge. Our study unveiled at least two intrinsic defects of these methods
when applied to coupled systems of partial differential equations (PDEs) and
deep neural networks (DNNs): (1) convergence is often slow with long plateaus
that make it difficult to determine whether the method has converged or not;
(2) quasi-Newton methods do not provide a sufficiently accurate approximation
of the Hessian matrix; this typically leads to early termination (one of the
stopping criteria of the optimizer is satisfied although the achieved error is
far from minimal). Based on these observations, we propose to use trust region
methods for optimizing coupled systems of PDEs and DNNs. Specifically, we
developed an algorithm for second-order physics constrained learning, an
efficient technique to calculate Hessian matrices based on computational
graphs. We show that trust region methods overcome many of the defects and
exhibit remarkable fast convergence and superior accuracy compared to ADAM,
BFGS, and L-BFGS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07553</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07553</id><submitter>Zheng Zhang</submitter><version version="v1"><date>Mon, 17 May 2021 00:31:37 GMT</date><size>29878kb</size><source_type>D</source_type></version><title>Prototype-supervised Adversarial Network for Targeted Attack of Deep
  Hashing</title><authors>Xunguang Wang, Zheng Zhang, Baoyuan Wu, Fumin Shen, Guangming Lu</authors><categories>cs.CV cs.MM</categories><comments>This paper has been accepted by CVPR 2021, and the related codes
  could be available at https://github.com/xunguangwang/ProS-GAN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its powerful capability of representation learning and high-efficiency
computation, deep hashing has made significant progress in large-scale image
retrieval. However, deep hashing networks are vulnerable to adversarial
examples, which is a practical secure problem but seldom studied in
hashing-based retrieval field. In this paper, we propose a novel
prototype-supervised adversarial network (ProS-GAN), which formulates a
flexible generative architecture for efficient and effective targeted hashing
attack. To the best of our knowledge, this is the first generation-based method
to attack deep hashing networks. Generally, our proposed framework consists of
three parts, i.e., a PrototypeNet, a generator, and a discriminator.
Specifically, the designed PrototypeNet embeds the target label into the
semantic representation and learns the prototype code as the category-level
representative of the target label. Moreover, the semantic representation and
the original image are jointly fed into the generator for a flexible targeted
attack. Particularly, the prototype code is adopted to supervise the generator
to construct the targeted adversarial example by minimizing the Hamming
distance between the hash code of the adversarial example and the prototype
code. Furthermore, the generator is against the discriminator to simultaneously
encourage the adversarial examples visually realistic and the semantic
representation informative. Extensive experiments verify that the proposed
framework can efficiently produce adversarial examples with better targeted
attack performance and transferability over state-of-the-art targeted attack
methods of deep hashing. The related codes could be available at
https://github.com/xunguangwang/ProS-GAN .
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07554</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07554</id><submitter>Laura Blattner</submitter><version version="v1"><date>Mon, 17 May 2021 00:42:26 GMT</date><size>9706kb</size><source_type>D</source_type></version><title>How Costly is Noise? Data and Disparities in Consumer Credit</title><authors>Laura Blattner and Scott Nelson</authors><categories>econ.GN cs.LG q-fin.EC</categories><comments>86 pages, 17 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We show that lenders face more uncertainty when assessing default risk of
historically under-served groups in US credit markets and that this information
disparity is a quantitatively important driver of inefficient and unequal
credit market outcomes. We first document that widely used credit scores are
statistically noisier indicators of default risk for historically under-served
groups. This noise emerges primarily through the explanatory power of the
underlying credit report data (e.g., thin credit files), not through issues
with model fit (e.g., the inability to include protected class in the scoring
model). Estimating a structural model of lending with heterogeneity in
information, we quantify the gains from addressing these information
disparities for the US mortgage market. We find that equalizing the precision
of credit scores can reduce disparities in approval rates and in credit
misallocation for disadvantaged groups by approximately half.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07558</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07558</id><submitter>Debajyoti Halder</submitter><version version="v1"><date>Mon, 17 May 2021 00:55:31 GMT</date><size>3176kb</size><source_type>D</source_type></version><title>fybrrStream: A WebRTC based Efficient and Scalable P2P Live Streaming
  Platform</title><authors>Debajyoti Halder, Prashant Kumar, Saksham Bhushan, and Anand M.
  Baswade</authors><categories>cs.NI cs.MM cs.SI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The demand for streaming media and live video conferencing is at peak and
expected to grow further, thereby the need for low-cost streaming services with
better quality and lower latency is essential. Therefore, in this paper, we
propose a novel peer-to-peer (P2P) live streaming platform, called fybrrStream,
where a logical mesh and physical tree i.e., hybrid topology-based approach is
leveraged for low latency streaming. fybrrStream distributes the load on
participating peers in a hierarchical manner by considering their network
bandwidth, network latency, and node stability. fybrrStream costs as low as the
cost of just hosting a light-weight website and the performance is comparable
to the existing state-of-the-art media streaming services. We evaluated and
tested the proposed fybrrStream platform with real-field experiments using 50+
users spread across India and results obtained show significant improvements in
the live streaming performance over other schemes.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07560</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07560</id><submitter>Varsha Lohani</submitter><version version="v1"><date>Mon, 17 May 2021 01:15:14 GMT</date><size>1651kb</size><source_type>D</source_type></version><title>Dynamic Routing and Spectrum Assignment based on the Availability of
  Consecutive Sub-channels in Flexi-grid Optical Networks</title><authors>Varsha Lohani, Anjali Sharma, and Yatindra Nath Singh</authors><categories>cs.NI cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Using Optical Orthogonal Frequency Multiplexing (O-OFDM), variable bandwidth
channels can be created in Elastic Optical Networks (EON). This allows the use
of spectrum more efficiently by allocating integral multiple of basic bandwidth
slots to the lightpath requests. Consequently, such networks are also called
flexible grid optical networks. It also adds a constraint of keeping all the
allocated slots together when deciding the routes for the requests. This
constraint called the contiguity constraint makes the routing and spectrum
algorithms more challenging. In any network, the lightpath requests will arrive
and depart dynamically and will invariably lead to spectrum fragmentation, and
hence network will have a reduction in maximum possible utilization due to
increased blocking probability. In this paper, we have presented an improvised
RSA algorithm that leads to lesser fragmentation. It is evident from the
results that the presented RSA algorithm uses adaptive parameters to reduce the
blocking probability and fragmentation compared to other algorithms reported in
the recent past.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07561</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07561</id><submitter>Shixiang Tang</submitter><version version="v1"><date>Mon, 17 May 2021 01:15:57 GMT</date><size>1308kb</size><source_type>D</source_type></version><title>Layerwise Optimization by Gradient Decomposition for Continual Learning</title><authors>Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu and Wanli Ouyang</authors><categories>cs.CV cs.LG</categories><comments>cvpr2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks achieve state-of-the-art and sometimes super-human
performance across various domains. However, when learning tasks sequentially,
the networks easily forget the knowledge of previous tasks, known as
&quot;catastrophic forgetting&quot;. To achieve the consistencies between the old tasks
and the new task, one effective solution is to modify the gradient for update.
Previous methods enforce independent gradient constraints for different tasks,
while we consider these gradients contain complex information, and propose to
leverage inter-task information by gradient decomposition. In particular, the
gradient of an old task is decomposed into a part shared by all old tasks and a
part specific to that task. The gradient for update should be close to the
gradient of the new task, consistent with the gradients shared by all old
tasks, and orthogonal to the space spanned by the gradients specific to the old
tasks. In this way, our approach encourages common knowledge consolidation
without impairing the task-specific knowledge. Furthermore, the optimization is
performed for the gradients of each layer separately rather than the
concatenation of all gradients as in previous works. This effectively avoids
the influence of the magnitude variation of the gradients in different layers.
Extensive experiments validate the effectiveness of both gradient-decomposed
optimization and layer-wise updates. Our proposed method achieves
state-of-the-art results on various benchmarks of continual learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07562</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07562</id><submitter>Seong-Gyu Yang</submitter><version version="v1"><date>Mon, 17 May 2021 01:19:01 GMT</date><size>2842kb</size><source_type>D</source_type></version><title>Power-grid stability prediction using transferable machine learnings</title><authors>Seong-Gyu Yang and Beom Jun Kim and Seung-Woo Son and Heetae Kim</authors><categories>physics.soc-ph cs.LG cs.SY eess.SY</categories><comments>10 pages, 6 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Complex network analyses have provided clues to improve power-grid stability
with the help of numerical models. The high computational cost of numerical
simulations, however, has inhibited the approach especially when it deals with
the dynamic properties of power grids such as frequency synchronization. In
this study, we investigate machine learning techniques to estimate the
stability of power grid synchronization. We test three different machine
learning algorithms -- random forest, support vector machine, and artificial
neural network -- training them with two different types of synthetic power
grids consisting of homogeneous and heterogeneous input-power distribution,
respectively. We find that the three machine learning models better predict the
synchronization stability of power-grid nodes when they are trained with the
heterogeneous input-power distribution than the homogeneous one. With the
real-world power grids of Great Britain, Spain, France, and Germany, we also
demonstrate that the machine learning algorithms trained on synthetic power
grids are transferable to the stability prediction of the real-world power
grids, which implies the prospective applicability of machine learning
techniques on power-grid studies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07566</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07566</id><submitter>Hao Xue</submitter><version version="v1"><date>Mon, 17 May 2021 01:27:20 GMT</date><size>543kb</size><source_type>D</source_type></version><title>Exploring Self-Supervised Representation Ensembles for COVID-19 Cough
  Classification</title><authors>Hao Xue and Flora D. Salim</authors><categories>cs.SD cs.LG eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usage of smartphone-collected respiratory sound, trained with deep
learning models, for detecting and classifying COVID-19 becomes popular
recently. It removes the need for in-person testing procedures especially for
rural regions where related medical supplies, experienced workers, and
equipment are limited. However, existing sound-based diagnostic approaches are
trained in a fully supervised manner, which requires large scale well-labelled
data. It is critical to discover new methods to leverage unlabelled respiratory
data, which can be obtained more easily. In this paper, we propose a novel
self-supervised learning enabled framework for COVID-19 cough classification. A
contrastive pre-training phase is introduced to train a Transformer-based
feature encoder with unlabelled data. Specifically, we design a random masking
mechanism to learn robust representations of respiratory sounds. The
pre-trained feature encoder is then fine-tuned in the downstream phase to
perform cough classification. In addition, different ensembles with varied
random masking rates are also explored in the downstream phase. Through
extensive evaluations, we demonstrate that the proposed contrastive
pre-training, the random masking mechanism, and the ensemble architecture
contribute to improving cough classification performance.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07567</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07567</id><submitter>Junge Wang</submitter><version version="v1"><date>Mon, 17 May 2021 01:29:02 GMT</date><size>19kb</size></version><title>Price of Precision in Coded Distributed Matrix Multiplication: A
  Dimensional Analysis</title><authors>Junge Wang, Zhuqing Jia and Syed A. Jafar</authors><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, submitted to ITW2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coded distributed matrix multiplication (CDMM) schemes, such as MatDot codes,
seek efficient ways to distribute matrix multiplication task(s) to a set of $N$
distributed servers so that the answers returned from any $R$ servers are
sufficient to recover the desired product(s). For example, to compute the
product of matrices ${\bf U, V}$, MatDot codes partition each matrix into $p&gt;1$
sub-matrices to create smaller coded computation tasks that reduce the
upload/storage at each server by $1/p$, such that ${\bf UV}$ can be recovered
from the answers returned by any $R=2p-1$ servers. An important concern in CDMM
is to reduce the recovery threshold $R$ for a given storage/upload constraint.
Recently, Jeong et al. introduced Approximate MatDot (AMD) codes that are shown
to improve the recovery threshold by a factor of nearly $2$, from $2p-1$ to
$p$. A key observation that motivates our work is that the storage/upload
required for approximate computing depends not only on the dimensions of the
(coded) sub-matrices that are assigned to each server, but also on their
precision levels -- a critical aspect that is not explored by Jeong et al. Our
main contribution is a dimensional analysis of AMD codes inspired by the
Generalized Degrees of Freedom (GDoF) framework previously developed for
wireless networks, which indicates that for the same upload/storage, once the
precision levels of the task assignments are accounted for, AMD codes
surprisingly fall short in all aspects to even the trivial replication scheme
which assigns the full computation task to every server. Indeed, the trivial
replication scheme has a much better recovery threshold of $1$, better download
cost, better computation cost, and much better encoding/decoding (none
required) complexity than AMD codes. The dimensional analysis is supported by
simple numerical experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07569</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07569</id><submitter>Shuvendu Lahiri</submitter><version version="v1"><date>Mon, 17 May 2021 01:38:31 GMT</date><size>1741kb</size><source_type>D</source_type></version><title>DeepMerge: Learning to Merge Programs</title><authors>Elizabeth Dinella, Todd Mytkowicz, Alexey Svyatkovskiy, Christian
  Bird, Mayur Naik, Shuvendu K. Lahiri</authors><categories>cs.SE</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Program merging is ubiquitous in modern software development. Although
commonly used in most version control systems, text-based merge algorithms are
prone to producing spurious merge conflicts: they report a conflict even when
program changes do not interfere with each other semantically. Spurious merge
conflicts are costly to development as the need for manual intervention stalls
modern continuous integration pipelines. We propose a novel data-driven
approach to identify and resolve spurious merge conflicts with a
sequence-to-sequence machine learning model. We realize our approach in a tool
DeepMerge that uses a novel combination of (i) an edit-aware embedding of merge
inputs and (ii) a variation of pointer networks to construct resolutions from
input segments. We also propose an algorithm to extract ground truth manual
resolutions from a code corpus and employ it to curate a dataset comprising
10,729 non-trivial resolutions in Javascript programs. Our evaluation shows
that DeepMerge can predict correct resolutions with high precision ($72$%) and
modest recall ($34$%) on the dataset overall, and high recall ($78$%) on merges
comprising of upto 3 lines that comprise $24$% of the dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07571</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07571</id><submitter>Yohan Jo</submitter><version version="v1"><date>Mon, 17 May 2021 01:41:39 GMT</date><size>689kb</size><source_type>D</source_type></version><title>Classifying Argumentative Relations Using Logical Mechanisms and
  Argumentation Schemes</title><authors>Yohan Jo, Seojin Bang, Chris Reed, Eduard Hovy</authors><categories>cs.CL</categories><comments>To Appear in TACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While argument mining has achieved significant success in classifying
argumentative relations between statements (support, attack, and neutral), we
have a limited computational understanding of logical mechanisms that
constitute those relations. Most recent studies rely on black-box models, which
are not as linguistically insightful as desired. On the other hand, earlier
studies use rather simple lexical features, missing logical relations between
statements. To overcome these limitations, our work classifies argumentative
relations based on four logical and theory-informed mechanisms between two
statements, namely (i) factual consistency, (ii) sentiment coherence, (iii)
causal relation, and (iv) normative relation. We demonstrate that our
operationalization of these logical mechanisms classifies argumentative
relations without directly training on data labeled with the relations,
significantly better than several unsupervised baselines. We further
demonstrate that these mechanisms also improve supervised classifiers through
representation learning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07574</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07574</id><submitter>Jianzhi Lou</submitter><version version="v1"><date>Mon, 17 May 2021 01:49:02 GMT</date><size>5638kb</size><source_type>D</source_type></version><title>SoundFence: Securing Ultrasonic Sensors in Vehicles Using Physical-Layer
  Defense</title><authors>Jianzhi Lou, Qiben Yan, Qing Hui, Huacheng Zeng</authors><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous vehicles (AVs), equipped with numerous sensors such as camera,
LiDAR, radar, and ultrasonic sensor, are revolutionizing the transportation
industry. These sensors are expected to sense reliable information from a
physical environment, facilitating the critical decision-making process of the
AVs. Ultrasonic sensors, which detect obstacles in a short distance, play an
important role in assisted parking and blind spot detection events. However,
due to their weak security level, ultrasonic sensors are particularly
vulnerable to signal injection attacks, when the attackers inject malicious
acoustic signals to create fake obstacles and intentionally mislead the
vehicles to make wrong decisions with disastrous aftermath. In this paper, we
systematically analyze the attack model of signal injection attacks toward
moving vehicles. By considering the potential threats, we propose SoundFence, a
physical-layer defense system which leverages the sensors' signal processing
capability without requiring any additional equipment. SoundFence verifies the
benign measurement results and detects signal injection attacks by analyzing
sensor readings and the physical-layer signatures of ultrasonic signals. Our
experiment with commercial sensors shows that SoundFence detects most (more
than 95%) of the abnormal sensor readings with very few false alarms, and it
can also accurately distinguish the real echo from injected signals to identify
injection attacks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07576</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07576</id><submitter>Yuxin Wu</submitter><version version="v1"><date>Mon, 17 May 2021 01:58:15 GMT</date><size>5795kb</size><source_type>D</source_type></version><title>Rethinking &quot;Batch&quot; in BatchNorm</title><authors>Yuxin Wu, Justin Johnson</authors><categories>cs.CV</categories><comments>Tech report</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  BatchNorm is a critical building block in modern convolutional neural
networks. Its unique property of operating on &quot;batches&quot; instead of individual
samples introduces significantly different behaviors from most other operations
in deep learning. As a result, it leads to many hidden caveats that can
negatively impact model's performance in subtle ways. This paper thoroughly
reviews such problems in visual recognition tasks, and shows that a key to
address them is to rethink different choices in the concept of &quot;batch&quot; in
BatchNorm. By presenting these caveats and their mitigations, we hope this
review can help researchers use BatchNorm more effectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07578</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07578</id><submitter>Abhiram Bhaskar Kakarla</submitter><version version="v1"><date>Mon, 17 May 2021 02:28:36 GMT</date><size>392kb</size></version><title>Towards Novel Multipath Data Scheduling For Future IoT Systems: A Survey</title><authors>Abhiram Bhaskar Kakarla</authors><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the initial years of its inception, the Internet was widely used for
transferring data packets between users and respective data sources by using IP
addresses. With the advancements in technology, the Internet has been used to
share data within several small and resource-constrained devices connected in
billions to create the framework for the so-called Internet of Things (IoT).
These systems were known for the presentation of a large quantum of data
emerging within these devices. On the flip side, these devices are known to
impose huge overheads on the IoT network. Therefore, it was essential to
develop solutions concerning different network-related problems as a part of
IoT networking. In this paper, we review these challenges emerge in routing,
congestion, energy conservation, scalability, heterogeneity, reliability,
security, and quality of service (QoS). This can be leverage to use the
available network optimally. As part of this research work, a detailed survey
is to be conducted on the network optimization process within IoT, as presented
in another research. Owing to the advances in wireless networking, relevant
Internet-of-Things (IoT) devices were equipped with several elements, including
multiple network access interfaces. The adoption of multipath TCP (MPTCP)
technology would improve the total throughput of data transmission. On the
other hand, leveraging traditional MPTCP path management algorithms lead to
other problems in data transport areas along with even buffer blockage. This
shall lead to massive issues in areas of reduction of transmission performance
across the entire IoT network. To this end, we develop a novel multipath
algorithm that would efficiently manage the data transport in an intelligently
scheduled and seamless manner using multiple wireless/wireline paths.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07579</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07579</id><submitter>Paulo Santos</submitter><version version="v1"><date>Mon, 17 May 2021 02:33:45 GMT</date><size>3843kb</size><source_type>D</source_type></version><title>Monitoring electrical systems data-network equipment by means ofFuzzy
  and Paraconsistent Annotated Logic</title><authors>Hyghor Miranda Cortes, Paulo Eduardo Santos, Joao Inacio da Silva
  Filho</authors><categories>cs.AI cs.SY eess.SY</categories><comments>38 pages; 14 figures; Under submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The constant increase in the amount and complexity of information obtained
from IT data networkelements, for its correct monitoring and management, is a
reality. The same happens to data net-works in electrical systems that provide
effective supervision and control of substations and hydro-electric plants.
Contributing to this fact is the growing number of installations and new
environmentsmonitored by such data networks and the constant evolution of the
technologies involved. This sit-uation potentially leads to incomplete and/or
contradictory data, issues that must be addressed inorder to maintain a good
level of monitoring and, consequently, management of these systems. Inthis
paper, a prototype of an expert system is developed to monitor the status of
equipment of datanetworks in electrical systems, which deals with
inconsistencies without trivialising the inferences.This is accomplished in the
context of the remote control of hydroelectric plants and substationsby a
Regional Operation Centre (ROC). The expert system is developed with algorithms
definedupon a combination of Fuzzy logic and Paraconsistent Annotated Logic
with Annotation of TwoValues (PAL2v) in order to analyse uncertain signals and
generate the operating conditions (faulty,normal, unstable or inconsistent /
indeterminate) of the equipment that are identified as importantfor the remote
control of hydroelectric plants and substations. A prototype of this expert
systemwas installed on a virtualised server with CLP500 software (from the
EFACEC manufacturer) thatwas applied to investigate scenarios consisting of a
Regional (Brazilian) Operation Centre, with aGeneric Substation and a Generic
Hydroelectric Plant, representing a remote control environment.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07581</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07581</id><submitter>Pin-Yu Chen</submitter><version version="v1"><date>Mon, 17 May 2021 02:39:22 GMT</date><size>81366kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 04:02:06 GMT</date><size>81256kb</size><source_type>D</source_type></version><title>Vision Transformers are Robust Learners</title><authors>Sayak Paul and Pin-Yu Chen</authors><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformers, composed of multiple self-attention layers, hold strong
promises toward a generic learning primitive applicable to different data
modalities, including the recent breakthroughs in computer vision achieving
state-of-the-art (SOTA) standard accuracy with better parameter efficiency.
Since self-attention helps a model systematically align different components
present inside the input data, it leaves grounds to investigate its performance
under model robustness benchmarks. In this work, we study the robustness of the
Vision Transformer (ViT) against common corruptions and perturbations,
distribution shifts, and natural adversarial examples. We use six different
diverse ImageNet datasets concerning robust classification to conduct a
comprehensive performance comparison of ViT models and SOTA convolutional
neural networks (CNNs), Big-Transfer. Through a series of six systematically
designed experiments, we then present analyses that provide both quantitative
and qualitative indications to explain why ViTs are indeed more robust
learners. For example, with fewer parameters and similar dataset and
pre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A
which is 4.3x higher than a comparable variant of BiT. Our analyses on image
masking, Fourier spectrum sensitivity, and spread on discrete cosine energy
spectrum reveal intriguing properties of ViT attributing to improved
robustness. Code for reproducing our experiments is available here:
https://git.io/J3VO0.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07582</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07582</id><submitter>Alsharif Abuadbba Dr</submitter><version version="v1"><date>Mon, 17 May 2021 02:42:54 GMT</date><size>3478kb</size><source_type>D</source_type></version><title>RAIDER: Reinforcement-aided Spear Phishing Detector</title><authors>Keelan Evans, Alsharif Abuadbba, Mohiuddin Ahmed, Tingmin Wu, Mike
  Johnstone, Surya Nepal</authors><categories>cs.CR cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spear Phishing is a harmful cyber-attack facing business and individuals
worldwide. Considerable research has been conducted recently into the use of
Machine Learning (ML) techniques to detect spear-phishing emails. ML-based
solutions may suffer from zero-day attacks; unseen attacks unaccounted for in
the training data. As new attacks emerge, classifiers trained on older data are
unable to detect these new varieties of attacks resulting in increasingly
inaccurate predictions. Spear Phishing detection also faces scalability
challenges due to the growth of the required features which is proportional to
the number of the senders within a receiver mailbox. This differs from
traditional phishing attacks which typically perform only a binary
classification between phishing and benign emails. Therefore, we devise a
possible solution to these problems, named RAIDER: Reinforcement AIded Spear
Phishing DEtectoR. A reinforcement-learning based feature evaluation system
that can automatically find the optimum features for detecting different types
of attacks. By leveraging a reward and penalty system, RAIDER allows for
autonomous features selection. RAIDER also keeps the number of features to a
minimum by selecting only the significant features to represent phishing emails
and detect spear-phishing attacks. After extensive evaluation of RAIDER over
11,000 emails and across 3 attack scenarios, our results suggest that using
reinforcement learning to automatically identify the significant features could
reduce the dimensions of the required features by 55% in comparison to existing
ML-based systems. It also improves the accuracy of detecting spoofing attacks
by 4% from 90% to 94%. In addition, RAIDER demonstrates reasonable detection
accuracy even against a sophisticated attack named Known Sender in which
spear-phishing emails greatly resemble those of the impersonated sender.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07583</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07583</id><submitter>Ziqiang Shi</submitter><version version="v1"><date>Mon, 17 May 2021 02:46:15 GMT</date><size>9705kb</size><source_type>D</source_type></version><title>It$\hat{\text{o}}$TTS and It$\hat{\text{o}}$Wave: Linear Stochastic
  Differential Equation Is All You Need For Audio Generation</title><authors>Shoule Wu and Ziqiang Shi</authors><categories>cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose to unify the two aspects of voice synthesis, namely
text-to-speech (TTS) and vocoder, into one framework based on a pair of forward
and reverse-time linear stochastic differential equations (SDE). The solutions
of this SDE pair are two stochastic processes, one of which turns the
distribution of mel spectrogram (or wave), that we want to generate, into a
simple and tractable distribution. The other is the generation procedure that
turns this tractable simple signal into the target mel spectrogram (or wave).
The model that generates mel spectrogram is called It$\hat{\text{o}}$TTS, and
the model that generates wave is called It$\hat{\text{o}}$Wave.
It$\hat{\text{o}}$TTS and It$\hat{\text{o}}$Wave use the Wiener process as a
driver to gradually subtract the excess signal from the noise signal to
generate realistic corresponding meaningful mel spectrogram and audio
respectively, under the conditional inputs of original text or mel spectrogram.
The results of the experiment show that the mean opinion scores (MOS) of
It$\hat{\text{o}}$TTS and It$\hat{\text{o}}$Wave can exceed the current
state-of-the-art methods, reached 3.925$\pm$ 0.160 and 4.35$\pm$ 0.115
respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07584</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07584</id><submitter>Md Ashiqur Rahman</submitter><version version="v1"><date>Mon, 17 May 2021 02:58:36 GMT</date><size>612kb</size><source_type>D</source_type></version><title>On Data-centric Forwarding in Mobile Ad-hoc Networks: Baseline Design
  and Simulation Analysis</title><authors>Md Ashiqur Rahman, Beichuan Zhang</authors><categories>cs.NI</categories><comments>Accepted at The 30th International Conference on Computer
  Communications and Networks (ICCCN 2021)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IP networking deals with end-to-end communication where the network layer
routing protocols maintain the reachability from one address to another.
However, challenging environments, such as mobile ad-hoc networks or MANETs,
lead to frequent path failures and changes between the sender and receiver,
incurring higher packet loss. The obligatory route setup and maintenance of a
device-to-device stable path in MANETs incur significant data retrieval delay
and transmission overhead. Such overhead exaggerates the packet loss manifold.
  Named Data Networking (NDN) can avoid such delays and overhead and
significantly improve the overall network performance. It does so with direct
application-controlled named-data retrieval from any node in a network instead
of reaching a specific IP address with protocol message exchange. However,
existing works lack any explicit or systematic analysis to justify such claims.
Our work analyzes the core NDN and IP architectures in a MANET at a baseline
level. The extensive simulations show that NDN, when applied correctly, yields
much lower data retrieval latency than IP and can lower the network
transmission overhead in most cases. As a result, NDN's stateful forwarder can
significantly increase the retrieval rate, offering a better trade-off at the
network layer. Such performance comes from its caching, built-in multicast, and
request aggregation without requiring an IP-like separate routing control
plane.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07585</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07585</id><submitter>Yujuan Ding</submitter><version version="v1"><date>Mon, 17 May 2021 03:02:04 GMT</date><size>512kb</size><source_type>D</source_type></version><version version="v2"><date>Tue, 18 May 2021 01:05:22 GMT</date><size>512kb</size><source_type>D</source_type></version><title>Leveraging Two Types of Global Graph for Sequential Fashion
  Recommendation</title><authors>Yujuan Ding, Yunshan Ma, Wai Keung Wong, Tat-Seng Chua</authors><categories>cs.IR cs.MM</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Sequential fashion recommendation is of great significance in online fashion
shopping, which accounts for an increasing portion of either fashion retailing
or online e-commerce. The key to building an effective sequential fashion
recommendation model lies in capturing two types of patterns: the personal
fashion preference of users and the transitional relationships between adjacent
items. The two types of patterns are usually related to user-item interaction
and item-item transition modeling respectively. However, due to the large sets
of users and items as well as the sparse historical interactions, it is
difficult to train an effective and efficient sequential fashion recommendation
model. To tackle these problems, we propose to leverage two types of global
graph, i.e., the user-item interaction graph and item-item transition graph, to
obtain enhanced user and item representations by incorporating higher-order
connections over the graphs. In addition, we adopt the graph kernel of LightGCN
for the information propagation in both graphs and propose a new design for
item-item transition graph. Extensive experiments on two established sequential
fashion recommendation datasets validate the effectiveness and efficiency of
our approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07592</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07592</id><submitter>Yutong Li</submitter><version version="v1"><date>Mon, 17 May 2021 03:50:51 GMT</date><size>6751kb</size><source_type>D</source_type></version><title>Dermoscopic Image Classification with Neural Style Transfer</title><authors>Yutong Li, Ruoqing Zhu, Annie Qu and Mike Yeh</authors><categories>eess.IV cs.CV cs.LG</categories><comments>32 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Skin cancer, the most commonly found human malignancy, is primarily diagnosed
visually via dermoscopic analysis, biopsy, and histopathological examination.
However, unlike other types of cancer, automated image classification of skin
lesions is deemed more challenging due to the irregularity and variability in
the lesions' appearances. In this work, we propose an adaptation of the Neural
Style Transfer (NST) as a novel image pre-processing step for skin lesion
classification problems. We represent each dermoscopic image as the style image
and transfer the style of the lesion onto a homogeneous content image. This
transfers the main variability of each lesion onto the same localized region,
which allows us to integrate the generated images together and extract latent,
low-rank style features via tensor decomposition. We train and cross-validate
our model on a dermoscopic data set collected and preprocessed from the
International Skin Imaging Collaboration (ISIC) database. We show that the
classification performance based on the extracted tensor features using the
style-transferred images significantly outperforms that of the raw images by
more than 10%, and is also competitive with well-studied, pre-trained CNN
models through transfer learning. Additionally, the tensor decomposition
further identifies latent style clusters, which may provide clinical
interpretation and insights.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07593</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07593</id><submitter>Peter Karkus</submitter><version version="v1"><date>Mon, 17 May 2021 03:54:34 GMT</date><size>4668kb</size><source_type>D</source_type></version><title>Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation</title><authors>Peter Karkus, Shaojun Cai, David Hsu</authors><categories>cs.CV cs.AI cs.LG cs.RO stat.ML</categories><comments>CVPR 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Simultaneous localization and mapping (SLAM) remains challenging for a number
of downstream applications, such as visual robot navigation, because of rapid
turns, featureless walls, and poor camera quality. We introduce the
Differentiable SLAM Network (SLAM-net) along with a navigation architecture to
enable planar robot navigation in previously unseen indoor environments.
SLAM-net encodes a particle filter based SLAM algorithm in a differentiable
computation graph, and learns task-oriented neural network components by
backpropagating through the SLAM algorithm. Because it can optimize all model
components jointly for the end-objective, SLAM-net learns to be robust in
challenging conditions. We run experiments in the Habitat platform with
different real-world RGB and RGB-D datasets. SLAM-net significantly outperforms
the widely adapted ORB-SLAM in noisy conditions. Our navigation architecture
with SLAM-net improves the state-of-the-art for the Habitat Challenge 2020
PointNav task by a large margin (37% to 64% success). Project website:
http://sites.google.com/view/slamnet
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07595</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07595</id><submitter>Xinxin Liu</submitter><version version="v1"><date>Mon, 17 May 2021 03:57:14 GMT</date><size>36kb</size></version><title>A Complete Axiomatisation for Divergence Preserving Branching Congruence
  of Finite-State Behaviours</title><authors>Xinxin Liu and Tingting Yu</authors><categories>cs.LO</categories><comments>13 pages. To be published in LICS2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present an equational inference system for finite-state expressions, and
prove that the system is sound and complete with respect to divergence
preserving branching congruence, closing a problem that has been open since
1993. The inference system refines Rob van Glabbeek's simple and elegant
complete axiomatisation for branching bisimulation congruence of finite-state
behaviours by joining four simple axioms after dropping one axiom which is
unsound under the more refined divergence sensitive semantics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07596</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07596</id><submitter>Zhepei Wang</submitter><version version="v1"><date>Mon, 17 May 2021 03:57:33 GMT</date><size>1560kb</size><source_type>D</source_type></version><title>Sound Event Detection with Adaptive Frequency Selection</title><authors>Zhepei Wang, Jonah Casebeer, Adam Clemmitt, Efthymios Tzinis, Paris
  Smaragdis</authors><categories>cs.SD eess.AS</categories><comments>Submitted to IEEE Workshop on Applications of Signal Processing to
  Audio and Acoustics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present HIDACT, a novel network architecture for adaptive
computation for efficiently recognizing acoustic events. We evaluate the model
on a sound event detection task where we train it to adaptively process
frequency bands. The model learns to adapt to the input without requesting all
frequency sub-bands provided. It can make confident predictions within fewer
processing steps, hence reducing the amount of computation. Experimental
results show that HIDACT has comparable performance to baseline models with
more parameters and higher computational complexity. Furthermore, the model can
adjust the amount of computation based on the data and computational budget.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07597</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07597</id><submitter>Zhenzhong Chen</submitter><version version="v1"><date>Mon, 17 May 2021 04:00:33 GMT</date><size>6251kb</size><source_type>D</source_type></version><title>Collaborative Variational Bandwidth Auto-encoder for Recommender Systems</title><authors>Yaochen Zhu and Zhenzhong Chen</authors><categories>cs.IR cs.DB</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Collaborative filtering has been widely adopted by modern recommender systems
to discover user preferences based on their past behaviors. However, the
observed interactions for different users are usually unbalanced, which leads
to high uncertainty in the collaborative embeddings of users with sparse
ratings, thereby severely degenerating the recommendation performance.
Consequently, more efforts have been dedicated to the hybrid recommendation
strategy where user/item features are utilized as auxiliary information to
address the sparsity problem. However, since these features contain rich
multimodal patterns and most of them are irrelevant to the recommendation
purpose, excessive reliance on these features will make the model difficult to
generalize. To address the above two challenges, we propose a VBAE for
recommendation. VBAE models both the collaborative and the user feature
embeddings as Gaussian random variables inferred via deep neural networks to
capture non-linear similarities between users based on their ratings and
features. Furthermore, VBAE establishes an information regulation mechanism by
introducing a user-dependent channel variable where the bandwidth is determined
by the information already contained in the observed ratings to dynamically
control the amount of information allowed to be accessed from the corresponding
user features. The user-dependent channel variable alleviates the uncertainty
problem when the ratings are sparse while avoids unnecessary dependence of the
model on noisy user features simultaneously. Codes and datasets are released at
https://github.com/yaochenzhu/vbae.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07599</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07599</id><submitter>Feng Bao</submitter><version version="v1"><date>Mon, 17 May 2021 04:03:29 GMT</date><size>3401kb</size><source_type>D</source_type></version><title>Disentangled Variational Information Bottleneck for Multiview
  Representation Learning</title><authors>Feng Bao</authors><categories>cs.LG cs.CV cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Multiview data contain information from multiple modalities and have
potentials to provide more comprehensive features for diverse machine learning
tasks. A fundamental question in multiview analysis is what is the additional
information brought by additional views and can quantitatively identify this
additional information. In this work, we try to tackle this challenge by
decomposing the entangled multiview features into shared latent representations
that are common across all views and private representations that are specific
to each single view. We formulate this feature disentanglement in the framework
of information bottleneck and propose disentangled variational information
bottleneck (DVIB). DVIB explicitly defines the properties of shared and private
representations using constrains from mutual information. By deriving
variational upper and lower bounds of mutual information terms, representations
are efficiently optimized. We demonstrate the shared and private
representations learned by DVIB well preserve the common labels shared between
two views and unique labels corresponding to each single view, respectively.
DVIB also shows comparable performance in classification task on images with
corruptions. DVIB implementation is available at
https://github.com/feng-bao-ucsf/DVIB.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07603</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07603</id><submitter>Weiming Zhuang</submitter><version version="v1"><date>Mon, 17 May 2021 04:15:55 GMT</date><size>3398kb</size><source_type>D</source_type></version><title>EasyFL: A Low-code Federated Learning Platform For Dummies</title><authors>Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang</authors><categories>cs.DC cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Academia and industry have developed several platforms to support the popular
privacy-preserving distributed learning method -- Federated Learning (FL).
However, these platforms are complex to use and require a deep understanding of
FL, which imposes high barriers to entry for beginners, limits the productivity
of data scientists, and compromises deployment efficiency. In this paper, we
propose the first low-code FL platform, EasyFL, to enable users with various
levels of expertise to experiment and prototype FL applications with little
coding. We achieve this goal while ensuring great flexibility for customization
by unifying simple API design, modular design, and granular training flow
abstraction. With only a few lines of code, EasyFL empowers them with many
out-of-the-box functionalities to accelerate experimentation and deployment.
These practical functionalities are heterogeneity simulation, distributed
training optimization, comprehensive tracking, and seamless deployment. They
are proposed based on challenges identified in the proposed FL life cycle. Our
implementations show that EasyFL requires only three lines of code to build a
vanilla FL application, at least 10x lesser than other platforms. Besides, our
evaluations demonstrate that EasyFL expedites training by 1.5x. It also
improves the efficiency of experiments and deployment. We believe that EasyFL
will increase the productivity of data scientists and democratize FL to wider
audiences.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07605</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07605</id><submitter>Shenghao Yang</submitter><version version="v1"><date>Mon, 17 May 2021 04:23:26 GMT</date><size>1660kb</size><source_type>D</source_type></version><title>Utility Maximization for Multihop Wireless Networks Employing BATS Codes</title><authors>Yanyan Dong, Sheng Jin, Yanzuo Chen, Shenghao Yang and Hoover H. F.
  Yin</authors><categories>cs.IT cs.NI math.IT</categories><comments>This paper was presented in part at 2020 IEEE International
  Conference on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BATS codes are a class of efficient random linear network coding variation
that has been studied for multihop wireless networks mostly in scenarios of a
single communication flow. Towards sophisticated multi-flow network
communications, we formulate a network utility maximization (NUM) problem that
jointly optimizes the BATS code parameters of all the flows and network
scheduling. The NUM problem adopts a batch-wise packet loss model that can be
obtained from the network local statistics without any constraints on packet
loss patterns. Moreover, the NUM problem allows a different number of recoded
packets to be transmitted for different batches in a flow, which is called
adaptive recoding. Due to both the non-convex objective and the BATS
code-related variables, the algorithms developed for the existing flow
optimization problems can not be directed applied to solve our NUM problem. We
introduce a two-step algorithm for solving the NUM problem, where the first
step solves the problem with nonadaptive recoding schemes, and the second step
optimizes adaptive recoding hop-by-hop from upstream to downstream in each
flow. We perform various numerical evaluations and simulations to verify the
effectiveness and efficiency of the algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07606</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07606</id><submitter>Weiming Zhuang</submitter><version version="v1"><date>Mon, 17 May 2021 04:24:25 GMT</date><size>11224kb</size><source_type>D</source_type></version><title>Towards Unsupervised Domain Adaptation for Deep Face Recognition under
  Privacy Constraints via Federated Learning</title><authors>Weiming Zhuang, Xin Gan, Yonggang Wen, Xuesen Zhang, Shuai Zhang,
  Shuai Yi</authors><categories>cs.CV cs.AI cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised domain adaptation has been widely adopted to generalize models
for unlabeled data in a target domain, given labeled data in a source domain,
whose data distributions differ from the target domain. However, existing works
are inapplicable to face recognition under privacy constraints because they
require sharing sensitive face images between two domains. To address this
problem, we propose a novel unsupervised federated face recognition approach
(FedFR). FedFR improves the performance in the target domain by iteratively
aggregating knowledge from the source domain through federated learning. It
protects data privacy by transferring models instead of raw data between
domains. Besides, we propose a new domain constraint loss (DCL) to regularize
source domain training. DCL suppresses the data volume dominance of the source
domain. We also enhance a hierarchical clustering algorithm to predict pseudo
labels for the unlabeled target domain accurately. To this end, FedFR forms an
end-to-end training pipeline: (1) pre-train in the source domain; (2) predict
pseudo labels by clustering in the target domain; (3) conduct
domain-constrained federated learning across two domains. Extensive experiments
and analysis on two newly constructed benchmarks demonstrate the effectiveness
of FedFR. It outperforms the baseline and classic methods in the target domain
by over 4% on the more realistic benchmark. We believe that FedFR will shed
light on applying federated learning to more computer vision tasks under
privacy constraints.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07608</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07608</id><submitter>Aimin Hou</submitter><version version="v1"><date>Mon, 17 May 2021 04:26:40 GMT</date><size>783kb</size></version><title>Hamiltonian Cycle Problem is in P</title><authors>Aimin Hou</authors><categories>cs.DS</categories><comments>29 pages, 6 figures, FOCS2021</comments><msc-class>68Qxx</msc-class><acm-class>F.1.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we present the first deterministic polynomial time algorithm
for determining the existence of a Hamiltonian cycle and finding a Hamiltonian
cycle in general graphs. Our algorithm can also resolve the Hamiltonian path
problem in the traceable graphs. The space complexity of our algorithm is
O(n^4). The time complexity are theoretically O(n^5*d^2) on average and
O(n^6*d^2) in the worst case respectively, where d is the maximum degree of
vertex. With parallel computing, the space complexity can be improved to O(n^3)
and the time complexity to O(n^3*d^2) on average and O(n^4*d^2) in the worst
case. We construct the corresponding path hologram transformed from the
original graph and compute the path set, which is a collection of segment sets
consisting of all the vertices located on the same segment layer among all the
longest basic paths, of every vertex with dynamic programming. The path
hologram is a multi-segment graph with the vertex &lt;u, k&gt; where u is a vertex
and k is the segment layer of u in the path hologram. To ensure each path
stored in the path set is legal and each segment set of the path set contains
only valid vertices, the key strategy of our method is the &quot;consecutive&quot;
deleting-replenishing operations recursively on the left/right action field of
a vertex, respectively. The greatest contribution of our method is the path set
in which the (n-1)! paths can be stored in O(n^2) space for a complete graph of
order n. In fact, our algorithm can be directly applied to the original graph.
Besides, our algorithm can deal with the finite general graphs including
undirected, directed, and mixed. As a well-known problem in NPC, the
Hamiltonian Cycle Problem can be now resolved practically in deterministic
polynomial time, so this problem is in P and we prove that the conjecture of
P=NP holds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07609</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07609</id><submitter>Hoover H. F. Yin</submitter><version version="v1"><date>Mon, 17 May 2021 04:55:45 GMT</date><size>35kb</size></version><title>Intrablock Interleaving for Batched Network Coding with Blockwise
  Adaptive Recoding</title><authors>Hoover H. F. Yin, Ka Hei Ng, Allen Z. Zhong, Raymond W. Yeung,
  Shenghao Yang, Ian Y. Y. Chan</authors><categories>cs.IT cs.NI math.IT</categories><comments>submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Batched network coding (BNC) is a low-complexity solution to network
transmission in feedbackless multi-hop packet networks with packet loss. BNC
encodes the source data into batches of packets. As a network coding scheme,
the intermediate nodes perform recoding on the received packets instead of just
forwarding them. Blockwise adaptive recoding (BAR) is a recoding strategy which
can enhance the throughput and adapt real-time changes in the incoming channel
condition. In wireless applications, in order to combat burst packet loss,
interleavers can be applied for BNC in a hop-by-hop manner. In particular, a
batch-stream interleaver that permutes packets across blocks can be applied
with BAR to further boost the throughput. However, the previously proposed
minimal communication protocol for BNC only supports permutation of packets
within a block, called intrablock interleaving, and so it is not compatible
with the batch-stream interleaver. In this paper, we design an intrablock
interleaver for BAR that is backward compatible with the aforementioned minimal
protocol, so that the throughput can be enhanced without upgrading all the
existing devices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07610</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07610</id><submitter>Maya Ramchandran</submitter><version version="v1"><date>Mon, 17 May 2021 04:58:29 GMT</date><size>362kb</size><source_type>D</source_type></version><title>Cross-Cluster Weighted Forests</title><authors>Maya Ramchandran, Rajarshi Mukherjee, and Giovanni Parmigiani</authors><categories>stat.ML cs.LG</categories><comments>20 pages, 6 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Adapting machine learning algorithms to better handle the presence of natural
clustering or batch effects within training datasets is imperative across a
wide variety of biological applications. This article considers the effect of
ensembling Random Forest learners trained on clusters within a single dataset
with heterogeneity in the distribution of the features. We find that
constructing ensembles of forests trained on clusters determined by algorithms
such as k-means results in significant improvements in accuracy and
generalizability over the traditional Random Forest algorithm. We denote our
novel approach as the Cross-Cluster Weighted Forest, and examine its robustness
to various data-generating scenarios and outcome models. Furthermore, we
explore the influence of the data-partitioning and ensemble weighting
strategies on conferring the benefits of our method over the existing paradigm.
Finally, we apply our approach to cancer molecular profiling and gene
expression datasets that are naturally divisible into clusters and illustrate
that our approach outperforms classic Random Forest. Code and supplementary
material are available at https://github.com/m-ramchandran/cross-cluster.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07612</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07612</id><submitter>Xiaoyu Fan</submitter><version version="v1"><date>Mon, 17 May 2021 05:05:24 GMT</date><size>13807kb</size><source_type>D</source_type></version><title>PPCA: Privacy-preserving Principal Component Analysis Using Secure
  Multiparty Computation(MPC)</title><authors>Xiaoyu Fan, Guosai Wang, Kun Chen, Xu He, Wei Xu</authors><categories>cs.CR cs.DS</categories><comments>11 pages, 3 figures, 5 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Privacy-preserving data mining has become an important topic. People have
built several multi-party-computation (MPC)-based frameworks to provide
theoretically guaranteed privacy, the poor performance of real-world algorithms
have always been a challenge. Using Principal Component Analysis (PCA) as an
example, we show that by considering the unique performance characters of the
MPC platform, we can design highly effective algorithm-level optimizations,
such as replacing expensive operators and batching up. We achieve about
200$\times$ performance boost over existing privacy-preserving PCA algorithms
with the same level of privacy guarantee. Also, using real-world datasets, we
show that by combining multi-party data, we can achieve better training
results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07614</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07614</id><submitter>Hoover H. F. Yin</submitter><version version="v1"><date>Mon, 17 May 2021 05:18:48 GMT</date><size>30kb</size></version><title>A Unified Adaptive Recoding Framework for Batched Network Coding</title><authors>Hoover H. F. Yin, Bin Tang, Ka Hei Ng, Shenghao Yang, Xishi Wang,
  Qiaoqiao Zhou</authors><categories>cs.IT cs.NI math.IT</categories><comments>submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Batched network coding is a variation of random linear network coding which
has low computational and storage costs. In order to adapt random fluctuations
in the number of erasures in individual batches, it is not optimal to recode
and transmit the same number of packets for all batches. Different distributed
optimization models, which are called adaptive recoding schemes, were
formulated for this purpose. The key component of these optimization problems
is the expected value of the rank distribution of a batch at the next network
node, which is also known as the expected rank. In this paper, we put forth a
unified adaptive recoding framework. We show that the expected rank functions
are concave when the packet loss pattern is a stationary stochastic process
regardless of the field size, which covers but not limited to independent
packet loss and burst packet loss. Under this concavity assumption, we show
that there always exists a solution which not only can minimize the randomness
on the number of recoded packets but also can tolerate rank distribution errors
due to inaccurate measurements or limited precision of the machine. To obtain
such an optimal solution, we propose tuning schemes that can turn any feasible
solution into a desired optimal solution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07615</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07615</id><submitter>Haoran Li</submitter><version version="v1"><date>Mon, 17 May 2021 05:30:41 GMT</date><size>2960kb</size><source_type>D</source_type></version><title>Federated Knowledge Graphs Embedding</title><authors>Hao Peng, Haoran Li, Yangqiu Song, Vincent Zheng, Jianxin Li</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel decentralized scalable learning framework,
Federated Knowledge Graphs Embedding (FKGE), where embeddings from different
knowledge graphs can be learnt in an asynchronous and peer-to-peer manner while
being privacy-preserving. FKGE exploits adversarial generation between pairs of
knowledge graphs to translate identical entities and relations of different
domains into near embedding spaces. In order to protect the privacy of the
training data, FKGE further implements a privacy-preserving neural network
structure to guarantee no raw data leakage. We conduct extensive experiments to
evaluate FKGE on 11 knowledge graphs, demonstrating a significant and
consistent improvement in model quality with at most 17.85% and 7.90% increases
in performance on triple classification and link prediction tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07617</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07617</id><submitter>Hoover H. F. Yin</submitter><version version="v1"><date>Mon, 17 May 2021 05:42:10 GMT</date><size>327kb</size><source_type>D</source_type></version><title>BAR: Blockwise Adaptive Recoding for Batched Network Coding</title><authors>Hoover H. F. Yin, Shenghao Yang, Qiaoqiao Zhou, Lily M. L. Yung, Ka
  Hei Ng</authors><categories>cs.IT cs.NI math.IT</categories><comments>submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-hop networks become popular network topologies in various emerging
Internet of things applications. Batched network coding (BNC) is a solution to
reliable communications in such networks with packet loss. By grouping packets
into small batches and restricting recoding to the packets belonging to the
same batch, BNC has a much smaller computational and storage requirements at
the intermediate nodes compared with a direct application of random linear
network coding. In this paper, we propose a practical recoding scheme called
blockwise adaptive recoding (BAR) which learns the latest channel knowledge
from short observations so that BAR can adapt to the fluctuation of channel
conditions. We focus on investigating practical concerns such as the design of
efficient BAR algorithms. We also design and investigate feedback schemes for
BAR under imperfect feedback systems. Our numerical evaluations show that BAR
has significant throughput gain for small batch size compared with the existing
baseline recoding scheme. More importantly, this gain is insensitive to
inaccurate channel knowledge. This encouraging result suggests that BAR is
suitable to be realized in practice as the exact channel model and its
parameters could be unknown and subject to change from time to time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07618</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07618</id><submitter>Kaustav Chatterjee</submitter><version version="v1"><date>Mon, 17 May 2021 05:49:53 GMT</date><size>4086kb</size><source_type>D</source_type></version><title>Dissipation of Oscillation Energy and Distribution of Damping Power in a
  Multimachine Power System: A Small-signal Analysis</title><authors>Kaustav Chatterjee and Nilanjan Ray Chaudhuri</authors><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper revisits the concept of damping torque in a multimachine power
system and its relation to the dissipation of oscillation energy in synchronous
machine windings. As a multimachine extension of an existing result on a
single-machine-infinite-bus (SMIB) system, we show that the total damping power
for a mode stemming from the interaction of electromagnetic torques and rotor
speeds is equal to the sum of average power dissipations in the generator
windings corresponding to the modal oscillation. Further, counter-intuitive to
the SMIB result, we demonstrate that, although the equality holds on an
aggregate, such is not the case for individual machines in an interconnected
system. To that end, distribution factors are derived for expressing the
average damping power of each generator as a linear combination of average
powers of modal energy dissipation in the windings of all machines in the
system. These factors represent the distribution of damping power in a
multimachine system. The results are validated on IEEE 4-machine and 16-machine
test systems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07620</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07620</id><submitter>Xuesu Xiao</submitter><version version="v1"><date>Mon, 17 May 2021 05:56:37 GMT</date><size>10852kb</size><source_type>D</source_type></version><title>APPL: Adaptive Planner Parameter Learning</title><authors>Xuesu Xiao, Zizhao Wang, Zifan Xu, Bo Liu, Garrett Warnell, Gauraang
  Dhamankar, Anirudh Nair, Peter Stone</authors><categories>cs.RO</categories><comments>arXiv admin note: text overlap with arXiv:2011.00400</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While current autonomous navigation systems allow robots to successfully
drive themselves from one point to another in specific environments, they
typically require extensive manual parameter re-tuning by human robotics
experts in order to function in new environments. Furthermore, even for just
one complex environment, a single set of fine-tuned parameters may not work
well in different regions of that environment. These problems prohibit reliable
mobile robot deployment by non-expert users. As a remedy, we propose Adaptive
Planner Parameter Learning (APPL), a machine learning framework that can
leverage non-expert human interaction via several modalities -- including
teleoperated demonstrations, corrective interventions, and evaluative feedback
-- and also unsupervised reinforcement learning to learn a parameter policy
that can dynamically adjust the parameters of classical navigation systems in
response to changes in the environment. APPL inherits safety and explainability
from classical navigation systems while also enjoying the benefits of machine
learning, i.e., the ability to adapt and improve from experience. We present a
suite of individual APPL methods and also a unifying cycle-of-learning scheme
that combines all the proposed methods in a framework that can improve
navigation performance through continual, iterative human interaction and
simulation training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07621</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07621</id><submitter>Sho Inoue Mr.</submitter><version version="v1"><date>Mon, 17 May 2021 05:58:33 GMT</date><size>29603kb</size><source_type>D</source_type></version><title>Style-Restricted GAN: Multi-Modal Translation with Style Restriction
  Using Generative Adversarial Networks</title><authors>Sho Inoue and Tad Gonsalves</authors><categories>cs.CV</categories><comments>18 pages, 13 figures, 6 tables; This paper is submitted to IEEE
  Access; Our implementation is available at
  https://github.com/shinshoji01/Style-Restricted_GAN</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Unpaired image-to-image translation using Generative Adversarial Networks
(GAN) is successful in converting images among multiple domains. Moreover,
recent studies have shown a way to diversify the outputs of the generator.
However, since there are no restrictions on how the generator diversifies the
results, it is likely to translate some unexpected features. In this paper, we
propose Style-Restricted GAN (SRGAN), a novel approach to transfer input images
into different domains' with different styles, changing the exclusively
class-related features. Additionally, instead of KL divergence loss, we adopt 3
new losses to restrict the distribution of the encoded features: batch KL
divergence loss, correlation loss, and histogram imitation loss. The study
reports quantitative as well as qualitative results with Precision, Recall,
Density, and Coverage. The proposed 3 losses lead to the enhancement of the
level of diversity compared to the conventional KL loss. In particular, SRGAN
is found to be successful in translating with higher diversity and without
changing the class-unrelated features in the CelebA face dataset. Our
implementation is available at
https://github.com/shinshoji01/Style-Restricted_GAN.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07622</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07622</id><submitter>Ting-Wei Wu</submitter><version version="v1"><date>Mon, 17 May 2021 06:02:17 GMT</date><size>98kb</size><source_type>D</source_type></version><title>Ensemble-based Transfer Learning for Low-resource Machine Translation
  Quality Estimation</title><authors>Ting-Wei Wu, Yung-An Hsieh, Yi-Chieh Liu</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Quality Estimation (QE) of Machine Translation (MT) is a task to estimate the
quality scores for given translation outputs from an unknown MT system.
However, QE scores for low-resource languages are usually intractable and hard
to collect. In this paper, we focus on the Sentence-Level QE Shared Task of the
Fifth Conference on Machine Translation (WMT20), but in a more challenging
setting. We aim to predict QE scores of given translation outputs when barely
none of QE scores of that paired languages are given during training. We
propose an ensemble-based predictor-estimator QE model with transfer learning
to overcome such QE data scarcity challenge by leveraging QE scores from other
miscellaneous languages and translation results of targeted languages. Based on
the evaluation results, we provide a detailed analysis of how each of our
extension affects QE models on the reliability and the generalization ability
to perform transfer learning under multilingual tasks. Finally, we achieve the
best performance on the ensemble model combining the models pretrained by
individual languages as well as different levels of parallel trained corpus
with a Pearson's correlation of 0.298, which is 2.54 times higher than
baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07623</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07623</id><submitter>Jiwei Li</submitter><version version="v1"><date>Mon, 17 May 2021 06:03:56 GMT</date><size>265kb</size></version><title>Sentence Similarity Based on Contexts</title><authors>Xiaofei Sun, Yuxian Meng, Xiang Ao, Fei Wu, Tianwei Zhang, Jiwei Li
  and Chun Fan</authors><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing methods to measure sentence similarity are faced with two
challenges: (1) labeled datasets are usually limited in size, making them
insufficient to train supervised neural models; (2) there is a training-test
gap for unsupervised language modeling (LM) based models to compute semantic
scores between sentences, since sentence-level semantics are not explicitly
modeled at training. This results in inferior performances in this task. In
this work, we propose a new framework to address these two issues. The proposed
framework is based on the core idea that the meaning of a sentence should be
defined by its contexts, and that sentence similarity can be measured by
comparing the probabilities of generating two sentences given the same context.
The proposed framework is able to generate high-quality, large-scale dataset
with semantic similarity scores between two sentences in an unsupervised
manner, with which the train-test gap can be largely bridged. Extensive
experiments show that the proposed framework achieves significant performance
boosts over existing baselines under both the supervised and unsupervised
settings across different datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07624</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07624</id><submitter>Fengbin Zhu</submitter><version version="v1"><date>Mon, 17 May 2021 06:12:06 GMT</date><size>7275kb</size><source_type>D</source_type></version><title>TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and
  Textual Content in Finance</title><authors>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang,
  Jiancheng Lv, Fuli Feng and Tat-Seng Chua</authors><categories>cs.CL cs.AI</categories><comments>Accepted by ACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hybrid data combining both tabular and textual content (e.g., financial
reports) are quite pervasive in the real world. However, Question Answering
(QA) over such hybrid data is largely neglected in existing research. In this
work, we extract samples from real financial reports to build a new large-scale
QA dataset containing both Tabular And Textual data, named TAT-QA, where
numerical reasoning is usually required to infer the answer, such as addition,
subtraction, multiplication, division, counting, comparison/sorting, and the
compositions. We further propose a novel QA model termed TAGOP, which is
capable of reasoning over both tables and text. It adopts sequence tagging to
extract relevant cells from the table along with relevant spans from the text
to infer their semantics, and then applies symbolic reasoning over them with a
set of aggregation operators to arrive at the final answer. TAGOPachieves 58.0%
inF1, which is an 11.1% absolute increase over the previous best baseline
model, according to our experiments on TAT-QA. But this result still lags far
behind performance of expert human, i.e.90.8% in F1. It is demonstrated that
our TAT-QA is very challenging and can serve as a benchmark for training and
testing powerful QA models that address hybrid form data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07625</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07625</id><submitter>Kamala Gajurel</submitter><version version="v1"><date>Mon, 17 May 2021 06:15:35 GMT</date><size>1276kb</size><source_type>D</source_type></version><title>A Fine-Grained Visual Attention Approach for Fingerspelling Recognition
  in the Wild</title><authors>Kamala Gajurel, Cuncong Zhong and Guanghui Wang</authors><categories>cs.CV</categories><comments>7 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Fingerspelling in sign language has been the means of communicating technical
terms and proper nouns when they do not have dedicated sign language gestures.
Automatic recognition of fingerspelling can help resolve communication barriers
when interacting with deaf people. The main challenges prevalent in
fingerspelling recognition are the ambiguity in the gestures and strong
articulation of the hands. The automatic recognition model should address high
inter-class visual similarity and high intra-class variation in the gestures.
Most of the existing research in fingerspelling recognition has focused on the
dataset collected in a controlled environment. The recent collection of a
large-scale annotated fingerspelling dataset in the wild, from social media and
online platforms, captures the challenges in a real-world scenario. In this
work, we propose a fine-grained visual attention mechanism using the
Transformer model for the sequence-to-sequence prediction task in the wild
dataset. The fine-grained attention is achieved by utilizing the change in
motion of the video frames (optical flow) in sequential context-based attention
along with a Transformer encoder model. The unsegmented continuous video
dataset is jointly trained by balancing the Connectionist Temporal
Classification (CTC) loss and the maximum-entropy loss. The proposed approach
can capture better fine-grained attention in a single iteration. Experiment
evaluations show that it outperforms the state-of-the-art approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07627</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07627</id><submitter>Subhankar Ghosh</submitter><version version="v1"><date>Mon, 17 May 2021 06:18:36 GMT</date><size>3789kb</size><source_type>D</source_type></version><title>Shared and Private VAEs with Generative Replay for Continual Learning</title><authors>Subhankar Ghosh</authors><categories>cs.CV cs.LG</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continual learning tries to learn new tasks without forgetting previously
learned ones. In reality, most of the existing artificial neural network(ANN)
models fail, while humans do the same by remembering previous works throughout
their life. Although simply storing all past data can alleviate the problem, it
needs large memory and often infeasible in real-world applications where last
data access is limited. We hypothesize that the model that learns to solve each
task continually has some task-specific properties and some task-invariant
characteristics. We propose a hybrid continual learning model that is more
suitable in real case scenarios to address the issues that has a task-invariant
shared variational autoencoder and T task-specific variational autoencoders.
Our model combines generative replay and architectural growth to prevent
catastrophic forgetting. We show our hybrid model effectively avoids forgetting
and achieves state-of-the-art results on visual continual learning benchmarks
such as MNIST, Permuted MNIST(QMNIST), CIFAR100, and miniImageNet datasets. We
discuss results on a few more datasets, such as SVHN, Fashion-MNIST, EMNIST,
and CIFAR10.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07630</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07630</id><submitter>Andr\'e Artelt</submitter><version version="v1"><date>Mon, 17 May 2021 06:33:58 GMT</date><size>45kb</size><source_type>D</source_type></version><title>Convex optimization for actionable \&amp; plausible counterfactual
  explanations</title><authors>Andr\'e Artelt and Barbara Hammer</authors><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transparency is an essential requirement of machine learning based decision
making systems that are deployed in real world. Often, transparency of a given
system is achieved by providing explanations of the behavior and predictions of
the given system. Counterfactual explanations are a prominent instance of
particular intuitive explanations of decision making systems. While a lot of
different methods for computing counterfactual explanations exist, only very
few work (apart from work from the causality domain) considers feature
dependencies as well as plausibility which might limit the set of possible
counterfactual explanations.
  In this work we enhance our previous work on convex modeling for computing
counterfactual explanations by a mechanism for ensuring actionability and
plausibility of the resulting counterfactual explanations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07632</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07632</id><submitter>Jun Yang</submitter><version version="v1"><date>Mon, 17 May 2021 06:43:38 GMT</date><size>509kb</size></version><title>Dual-Stage Low-Complexity Reconfigurable Speech Enhancement</title><authors>Jun Yang and Nico Brailovsky</authors><categories>eess.AS cs.SD eess.SP</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a dual-stage, low complexity, and reconfigurable
technique to enhance the speech contaminated by various types of noise sources.
Driven by input data and audio contents, the proposed dual-stage speech
enhancement approach performs a coarse and fine processing in the first-stage
and second-stage, respectively. In this paper, we demonstrate that the proposed
speech enhancement solution significantly enhances the metrics of 3-fold
QUality Evaluation of Speech in Telecommunication (3QUEST) consisting of speech
mean-opinion-score (SMOS) and noise MOS (NMOS) for near-field and far-field
applications. Moreover, the proposed speech enhancement approach greatly
improves both the signal-to-noise ratio (SNR) and subjective listening
experience. For comparisons, the traditional speech enhancement methods reduce
the SMOS although they increase NMOS and SNR. In addition, the proposed speech
enhancement scheme can be easily adopted in both capture path and speech render
path for speech communication and conferencing systems, and voice-trigger
applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07634</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07634</id><submitter>Sunil Kumar Maurya</submitter><version version="v1"><date>Mon, 17 May 2021 06:46:01 GMT</date><size>1348kb</size><source_type>D</source_type></version><title>Improving Graph Neural Networks with Simple Architecture Design</title><authors>Sunil Kumar Maurya, Xin Liu and Tsuyoshi Murata</authors><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Neural Networks have emerged as a useful tool to learn on the data by
applying additional constraints based on the graph structure. These graphs are
often created with assumed intrinsic relations between the entities. In recent
years, there have been tremendous improvements in the architecture design,
pushing the performance up in various prediction tasks. In general, these
neural architectures combine layer depth and node feature aggregation steps.
This makes it challenging to analyze the importance of features at various hops
and the expressiveness of the neural network layers. As different graph
datasets show varying levels of homophily and heterophily in features and class
label distribution, it becomes essential to understand which features are
important for the prediction tasks without any prior information. In this work,
we decouple the node feature aggregation step and depth of graph neural network
and introduce several key design strategies for graph neural networks. More
specifically, we propose to use softmax as a regularizer and &quot;Soft-Selector&quot; of
features aggregated from neighbors at different hop distances; and
&quot;Hop-Normalization&quot; over GNN layers. Combining these techniques, we present a
simple and shallow model, Feature Selection Graph Neural Network (FSGNN), and
show empirically that the proposed model outperforms other state of the art GNN
models and achieves up to 64% improvements in accuracy on node classification
tasks. Moreover, analyzing the learned soft-selection parameters of the model
provides a simple way to study the importance of features in the prediction
tasks. Finally, we demonstrate with experiments that the model is scalable for
large graphs with millions of nodes and billions of edges.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07635</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07635</id><submitter>Lakshman Balasubramanian</submitter><version version="v1"><date>Mon, 17 May 2021 06:48:15 GMT</date><size>7376kb</size><source_type>D</source_type></version><title>Open-set Recognition based on the Combination of Deep Learning and
  Ensemble Method for Detecting Unknown Traffic Scenarios</title><authors>Lakshman Balasubramanian, Friedrich Kruber, Michael Botsch and Ke Deng</authors><categories>cs.CV</categories><comments>Accepted for IEEE Intelligent Vehicles 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An understanding and classification of driving scenarios are important for
testing and development of autonomous driving functionalities. Machine learning
models are useful for scenario classification but most of them assume that data
received during the testing are from one of the classes used in the training.
This assumption is not true always because of the open environment where
vehicles operate. This is addressed by a new machine learning paradigm called
open-set recognition. Open-set recognition is the problem of assigning test
samples to one of the classes used in training or to an unknown class. This
work proposes a combination of Convolutional Neural Networks (CNN) and Random
Forest (RF) for open set recognition of traffic scenarios. CNNs are used for
the feature generation and the RF algorithm along with extreme value theory for
the detection of known and unknown classes. The proposed solution is featured
by exploring the vote patterns of trees in RF instead of just majority voting.
By inheriting the ensemble nature of RF, the vote pattern of all trees combined
with extreme value theory is shown to be well suited for detecting unknown
classes. The proposed method has been tested on the highD and OpenTraffic
datasets and has demonstrated superior performance in various aspects compared
to existing solutions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07636</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07636</id><submitter>Sauptik Dhar</submitter><version version="v1"><date>Mon, 17 May 2021 06:48:25 GMT</date><size>1157kb</size><source_type>D</source_type></version><title>DOC3-Deep One Class Classification using Contradictions</title><authors>Sauptik Dhar, Bernardo Gonzalez Torres</authors><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Deep Learning, Anomaly Detection, Visual Inspection, Learning from
  Contradictions, Outlier Exposure, 18 pages, 14 tables, 6 Figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper introduces the notion of learning from contradictions (a.k.a
Universum learning) for deep one class classification problems. We formalize
this notion for the widely adopted one class large-margin loss, and propose the
Deep One Class Classification using Contradictions (DOC3) algorithm. We show
that learning from contradictions incurs lower generalization error by
comparing the Empirical Radamacher Complexity (ERC) of DOC3 against its
traditional inductive learning counterpart. Our empirical results demonstrate
the efficacy of DOC3 algorithm achieving &gt; 30% for CIFAR-10 and &gt;50% for MV-Tec
AD data sets in test AUCs compared to its inductive learning counterpart and in
many cases improving the state-of-the-art in anomaly detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07637</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07637</id><submitter>Pengyang Li</submitter><version version="v1"><date>Mon, 17 May 2021 06:49:29 GMT</date><size>607kb</size><source_type>D</source_type></version><title>Class-Incremental Few-Shot Object Detection</title><authors>Pengyang Li, Yanan Li and Donghui Wang</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional detection networks usually need abundant labeled training
samples, while humans can learn new concepts incrementally with just a few
examples. This paper focuses on a more challenging but realistic
class-incremental few-shot object detection problem (iFSD). It aims to
incrementally transfer the model for novel objects from only a few annotated
samples without catastrophically forgetting the previously learned ones. To
tackle this problem, we propose a novel method LEAST, which can transfer with
Less forgetting, fEwer training resources, And Stronger Transfer capability.
Specifically, we first present the transfer strategy to reduce unnecessary
weight adaptation and improve the transfer capability for iFSD. On this basis,
we then integrate the knowledge distillation technique using a less
resource-consuming approach to alleviate forgetting and propose a novel
clustering-based exemplar selection process to preserve more discriminative
features previously learned. Being a generic and effective method, LEAST can
largely improve the iFSD performance on various benchmarks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07639</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07639</id><submitter>Lakshman Balasubramanian</submitter><version version="v1"><date>Mon, 17 May 2021 06:54:59 GMT</date><size>728kb</size><source_type>D</source_type></version><title>Traffic Scenario Clustering by Iterative Optimisation of Self-Supervised
  Networks Using a Random Forest Activation Pattern Similarity</title><authors>Lakshman Balasubramanian, Jonas Wurst, Michael Botsch and Ke Deng</authors><categories>cs.CV</categories><comments>Accepted for IEEE Intelligent Vehicles 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic scenario categorisation is an essential component of automated
driving, for e.\,g., in motion planning algorithms and their validation.
Finding new relevant scenarios without handcrafted steps reduce the required
resources for the development of autonomous driving dramatically. In this work,
a method is proposed to address this challenge by introducing a clustering
technique based on a novel data-adaptive similarity measure, called Random
Forest Activation Pattern (RFAP) similarity. The RFAP similarity is generated
using a tree encoding scheme in a Random Forest algorithm. The clustering
method proposed in this work takes into account that there are labelled
scenarios available and the information from the labelled scenarios can help to
guide the clustering of unlabelled scenarios. It consists of three steps.
First, a self-supervised Convolutional Neural Network~(CNN) is trained on all
available traffic scenarios using a defined self-supervised objective. Second,
the CNN is fine-tuned for classification of the labelled scenarios. Third,
using the labelled and unlabelled scenarios an iterative optimisation procedure
is performed for clustering. In the third step at each epoch of the iterative
optimisation, the CNN is used as a feature generator for an unsupervised Random
Forest. The trained forest, in turn, provides the RFAP similarity to adapt
iteratively the feature generation process implemented by the CNN. Extensive
experiments and ablation studies have been done on the highD dataset. The
proposed method shows superior performance compared to baseline clustering
techniques.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07644</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07644</id><submitter>Manthan Patel</submitter><version version="v1"><date>Mon, 17 May 2021 07:12:40 GMT</date><size>26516kb</size><source_type>D</source_type></version><title>Collaborative Mapping of Archaeological Sites using multiple UAVs</title><authors>Manthan Patel, Aditya Bandopadhyay and Aamir Ahmad</authors><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  UAVs have found an important application in archaeological mapping. Majority
of the existing methods employ an offline method to process the data collected
from an archaeological site. They are time-consuming and computationally
expensive. In this paper, we present a multi-UAV approach for faster mapping of
archaeological sites. Employing a team of UAVs not only reduces the mapping
time by distribution of coverage area, but also improves the map accuracy by
exchange of information. Through extensive experiments in a realistic
simulation (AirSim), we demonstrate the advantages of using a collaborative
mapping approach. We then create the first 3D map of the Sadra Fort, a 15th
Century Fort located in Gujarat, India using our proposed method. Additionally,
we present two novel archaeological datasets recorded in both simulation and
real-world to facilitate research on collaborative archaeological mapping. For
the benefit of the community, we make the AirSim simulation environment, as
well as the datasets publicly available.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07645</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07645</id><submitter>Giorgos Kordopatis-Zilos Mr.</submitter><version version="v1"><date>Mon, 17 May 2021 07:18:43 GMT</date><size>8736kb</size></version><title>Leveraging EfficientNet and Contrastive Learning for Accurate
  Global-scale Location Estimation</title><authors>Giorgos Kordopatis-Zilos, Panagiotis Galopoulos, Symeon Papadopoulos,
  Ioannis Kompatsiaris</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of global-scale image geolocation,
proposing a mixed classification-retrieval scheme. Unlike other methods that
strictly tackle the problem as a classification or retrieval task, we combine
the two practices in a unified solution leveraging the advantages of each
approach with two different modules. The first leverages the EfficientNet
architecture to assign images to a specific geographic cell in a robust way.
The second introduces a new residual architecture that is trained with
contrastive learning to map input images to an embedding space that minimizes
the pairwise geodesic distance of same-location images. For the final location
estimation, the two modules are combined with a search-within-cell scheme,
where the locations of most similar images from the predicted geographic cell
are aggregated based on a spatial clustering scheme. Our approach demonstrates
very competitive performance on four public datasets, achieving new
state-of-the-art performance in fine granularity scales, i.e., 15.0% at 1km
range on Im2GPS3k.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07646</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07646</id><submitter>Ling Cheng</submitter><version version="v1"><date>Mon, 17 May 2021 07:20:36 GMT</date><size>11215kb</size><source_type>D</source_type></version><title>On Decentralization of Bitcoin: An Asset Perspective</title><authors>Ling Cheng, Feida Zhu, Huiwen Liu and Chunyan Miao</authors><categories>cs.CR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its advent in 2009, Bitcoin, a cryptography-enabled peer-to-peer
digital payment system, has been gaining increasing attention from both
academia and industry. An effort designed to overcome a cluster of bottlenecks
inherent in existing centralized financial systems, Bitcoin has always been
championed by the crypto community as an example of the spirit of
decentralization. While the decentralized nature of Bitcoin's Proof-of-Work
consensus algorithm has often been discussed in great detail, no systematic
study has so far been conducted to quantitatively measure the degree of
decentralization of Bitcoin from an asset perspective -- How decentralized is
Bitcoin as a financial asset? We present in this paper the first systematic
investigation of the degree of decentralization for Bitcoin based on its entire
transaction history. We proposed both static and dynamic analysis of Bitcoin
transaction network with quantifiable decentralization measures developed based
on network analysis and market efficiency study. Case studies are also
conducted to demonstrate the effectiveness of our proposed metrics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07647</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07647</id><submitter>Yi Wei</submitter><version version="v1"><date>Mon, 17 May 2021 07:29:55 GMT</date><size>2655kb</size><source_type>D</source_type></version><title>FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle
  Detection</title><authors>Yi Wei, Shang Su, Jiwen Lu, Jie Zhou</authors><categories>cs.CV</categories><comments>Accepted to ICRA 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of weakly supervised 3D vehicle
detection. Conventional methods for 3D object detection need vast amounts of
manually labelled 3D data as supervision signals. However, annotating large
datasets requires huge human efforts, especially for 3D area. To tackle this
problem, we propose frustum-aware geometric reasoning (FGR) to detect vehicles
in point clouds without any 3D annotations. Our method consists of two stages:
coarse 3D segmentation and 3D bounding box estimation. For the first stage, a
context-aware adaptive region growing algorithm is designed to segment objects
based on 2D bounding boxes. Leveraging predicted segmentation masks, we develop
an anti-noise approach to estimate 3D bounding boxes in the second stage.
Finally 3D pseudo labels generated by our method are utilized to train a 3D
detector. Independent of any 3D groundtruth, FGR reaches comparable performance
with fully supervised methods on the KITTI dataset. The findings indicate that
it is able to accurately detect objects in 3D space with only 2D bounding boxes
and sparse point clouds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07648</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07648</id><submitter>Jieting Luo</submitter><version version="v1"><date>Mon, 17 May 2021 07:32:43 GMT</date><size>2539kb</size><source_type>D</source_type></version><title>A Formal Framework for Reasoning about Agents' Independence in
  Self-organizing Multi-agent Systems</title><authors>Jieting Luo, Beishui Liao, John-Jules Meyer</authors><categories>cs.AI cs.LO cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-organization is a process where a stable pattern is formed by the
cooperative behavior between parts of an initially disordered system without
external control or influence. It has been introduced to multi-agent systems as
an internal control process or mechanism to solve difficult problems
spontaneously. However, because a self-organizing multi-agent system has
autonomous agents and local interactions between them, it is difficult to
predict the behavior of the system from the behavior of the local agents we
design. This paper proposes a logic-based framework of self-organizing
multi-agent systems, where agents interact with each other by following their
prescribed local rules. The dependence relation between coalitions of agents
regarding their contributions to the global behavior of the system is reasoned
about from the structural and semantic perspectives. We show that the
computational complexity of verifying such a self-organizing multi-agent system
remains close to the domain of standard ATL. We then combine our framework with
graph theory to decompose a system into different coalitions located in
different layers, which allows us to verify agents' full contributions more
efficiently. The resulting information about agents' full contributions allows
us to understand the complex link between local agent behavior and system level
behavior in a self-organizing multi-agent system. Finally, we show how we can
use our framework to model a constraint satisfaction problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07653</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07653</id><submitter>Roza Goscien</submitter><version version="v1"><date>Mon, 17 May 2021 08:00:55 GMT</date><size>706kb</size><source_type>D</source_type></version><title>Traffic-Aware Service Relocation in Cloud-Oriented Elastic Optical
  Networks</title><authors>R\'o\.za Go\'scie\'n</authors><categories>cs.NI cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we study problem of efficient service relocation (i.e.,
changing assigned data center for a selected client node) in elastic optical
networks (EONs) in order to increase network performance (measured by the
volume of accepted traffic). To this end, we first propose novel traffic model
for cloud ready transport networks. The model takes into account four flow
types (i.e., city-to-city, city-to-data center, data center-to-data center and
data center-to-data center) while the flow characteristics are based on real
economical and geographical parameters of the cities related to network nodes.
Then, we propose dedicated flow allocation algorithm that can be supported by
the service relocation process. We also introduce 21 different relocation
policies, which use three types of data for decision making - network
topological characteristics, rejection history and traffic prediction.
Eventually, we perform extensive numerical experiments in order to: (i) tune
proposed optimization approaches and (ii) evaluate and compare their efficiency
and select the best one. The results of the investigation prove high efficiency
of the proposed policies. The propoerly designed relocation policy allowed to
allocate up to 3% more traffic (compared to the allocation without that
policy). The results also reveal that the most efficient relocation policy
bases its decisions on two types of data simultaneously - the rejection history
and traffic prediction.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07654</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07654</id><submitter>Jiwei Li</submitter><version version="v1"><date>Mon, 17 May 2021 08:03:48 GMT</date><size>273kb</size><source_type>D</source_type></version><title>Dependency Parsing as MRC-based Span-Span Prediction</title><authors>Leilei Gan, Yuxian Meng, Kun Kuang, Xiaofei Sun, Chun Fan, Fei Wu and
  Jiwei Li</authors><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Higher-order methods for dependency parsing can partially but not fully
addresses the issue that edges in dependency tree should be constructed at the
text span/subtree level rather than word level. % This shortcoming can cause an
incorrect span covered the corresponding tree rooted at a certain word though
the word is correctly linked to its head. In this paper, we propose a new
method for dependency parsing to address this issue. The proposed method
constructs dependency trees by directly modeling span-span (in other words,
subtree-subtree) relations. It consists of two modules: the {\it text span
proposal module} which proposes candidate text spans, each of which represents
a subtree in the dependency tree denoted by (root, start, end); and the {\it
span linking module}, which constructs links between proposed spans. We use the
machine reading comprehension (MRC) framework as the backbone to formalize the
span linking module in an MRC setup, where one span is used as a query to
extract the text span/subtree it should be linked to. The proposed method comes
with the following merits: (1) it addresses the fundamental problem that edges
in a dependency tree should be constructed between subtrees; (2) the MRC
framework allows the method to retrieve missing spans in the span proposal
stage, which leads to higher recall for eligible spans. Extensive experiments
on the PTB, CTB and Universal Dependencies (UD) benchmarks demonstrate the
effectiveness of the proposed method. We are able to achieve new SOTA
performances on PTB and UD benchmarks, and competitive performances to previous
SOTA models on the CTB dataset. Code is available at
https://github.com/ShannonAI/mrc-for-dependency-parsing.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07656</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07656</id><submitter>Mengdi Wang</submitter><version version="v1"><date>Mon, 17 May 2021 08:10:22 GMT</date><size>46458kb</size><source_type>D</source_type></version><title>Thin-Film Smoothed Particle Hydrodynamics Fluid</title><authors>Mengdi Wang, Yitong Deng, Xiangxin Kong, Aditya H. Prasad, Shiying
  Xiong, Bo Zhu</authors><categories>physics.flu-dyn cs.GR</categories><comments>SIGGRAPH 2021 Technical Paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a particle-based method to simulate thin-film fluid that jointly
facilitates aggressive surface deformation and vigorous tangential flows. We
build our dynamics model from the surface tension driven Navier-Stokes equation
with the dimensionality reduced using the asymptotic lubrication theory and
customize a set of differential operators based on the weakly compressible
Smoothed Particle Hydrodynamics (SPH) for evolving pointset surfaces. The key
insight is that the compressible nature of SPH, which is unfavorable in its
typical usage, is helpful in our application to co-evolve the thickness,
calculate the surface tension, and enforce the fluid incompressibility on a
thin film. In this way, we are able to two-way couple the surface deformation
with the in-plane flows in a physically based manner. We can simulate complex
vortical swirls, fingering effects due to Rayleigh-Taylor instability,
capillary waves, Newton's interference fringes, and the Marangoni effect on
liberally deforming surfaces by presenting both realistic visual results and
numerical validations. The particle-based nature of our system also enables it
to conveniently handle topology changes and codimension transitions, allowing
us to marry the thin-film simulation with a wide gamut of 3D phenomena, such as
pinch-off of unstable catenoids, dripping under gravity, merging of droplets,
as well as bubble rupture.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07659</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07659</id><submitter>Nikita Bhandari</submitter><version version="v1"><date>Mon, 17 May 2021 08:15:41 GMT</date><size>350kb</size></version><title>Comparison of machine learning and deep learning techniques in promoter
  prediction across diverse species</title><authors>Nikita Bhandari, Satyajeet Khare, Rahee Walambe, Ketan Kotecha</authors><categories>q-bio.GN cs.LG</categories><comments>17 pages, 4 figures, 4 tables</comments><journal-ref>PeerJ Comput. Sci. 7:e365 (2021)</journal-ref><doi>10.7717/peerj-cs.365</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gene promoters are the key DNA regulatory elements positioned around the
transcription start sites and are responsible for regulating gene transcription
process. Various alignment-based, signal-based and content-based approaches are
reported for the prediction of promoters. However, since all promoter sequences
do not show explicit features, the prediction performance of these techniques
is poor. Therefore, many machine learning and deep learning models have been
proposed for promoter prediction. In this work, we studied methods for vector
encoding and promoter classification using genome sequences of three distinct
higher eukaryotes viz. yeast (Saccharomyces cerevisiae), A. thaliana (plant)
and human (Homo sapiens). We compared one-hot vector encoding method with
frequency-based tokenization (FBT) for data pre-processing on 1-D Convolutional
Neural Network (CNN) model. We found that FBT gives a shorter input dimension
reducing the training time without affecting the sensitivity and specificity of
classification. We employed the deep learning techniques, mainly CNN and
recurrent neural network with Long Short Term Memory (LSTM) and random forest
(RF) classifier for promoter classification at k-mer sizes of 2, 4 and 8. We
found CNN to be superior in classification of promoters from non-promoter
sequences (binary classification) as well as species-specific classification of
promoter sequences (multiclass classification). In summary, the contribution of
this work lies in the use of synthetic shuffled negative dataset and
frequency-based tokenization for pre-processing. This study provides a
comprehensive and generic framework for classification tasks in genomic
applications and can be extended to various classification problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07660</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07660</id><submitter>Etienne David</submitter><version version="v1"><date>Mon, 17 May 2021 08:18:30 GMT</date><size>963kb</size></version><title>Global Wheat Head Dataset 2021: an update to improve the benchmarking
  wheat head localization with more diversity</title><authors>Etienne DAVID, Mario Serouart, Daniel Smith, Simon Madec, Kaaviya
  Velumani, Shouyang Liu, Xu Wang, Francisco Pinto Espinosa, Shahameh Shafiee,
  Izzat S. A. Tahir, Hisashi Tsujimoto, Shuhei Nasuda, Bangyou Zheng, Norbert
  Kichgessner, Helge Aasen, Andreas Hund, Pouria Sadhegi-Tehran, Koichi
  Nagasawa, Goro Ishikawa, S\'ebastien Dandrifosse, Alexis Carlier, Benoit
  Mercatoris, Ken Kuroki, Haozhou Wang, Masanori Ishii, Minhajul A. Badhon,
  Curtis Pozniak, David Shaner LeBauer, Morten Lilimo, Jesse Poland, Scott
  Chapman, Benoit de Solan, Fr\'ed\'eric Baret, Ian Stavness, Wei Guo</authors><categories>cs.CV</categories><comments>8 pages, 2 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Global Wheat Head Detection (GWHD) dataset was created in 2020 and has
assembled 193,634 labelled wheat heads from 4,700 RGB images acquired from
various acquisition platforms and 7 countries/institutions. With an associated
competition hosted in Kaggle, GWHD has successfully attracted attention from
both the computer vision and agricultural science communities. From this first
experience in 2020, a few avenues for improvements have been identified,
especially from the perspective of data size, head diversity and label
reliability. To address these issues, the 2020 dataset has been reexamined,
relabeled, and augmented by adding 1,722 images from 5 additional countries,
allowing for 81,553 additional wheat heads to be added. We would hence like to
release a new version of the Global Wheat Head Detection (GWHD) dataset in
2021, which is bigger, more diverse, and less noisy than the 2020 version. The
GWHD 2021 is now publicly available at http://www.global-wheat.com/ and a new
data challenge has been organized on AIcrowd to make use of this updated
dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07663</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07663</id><submitter>Sophie Rain</submitter><version version="v1"><date>Mon, 17 May 2021 08:26:14 GMT</date><size>1693kb</size><source_type>D</source_type></version><title>Summing Up Smart Transitions</title><authors>Neta Elad, Sophie Rain, Neil Immerman, Laura Kov\'acs and Mooly Sagiv</authors><categories>cs.LO</categories><comments>This submission is an extended version of the CAV 2021 paper &quot;Summing
  Up Smart Transitions&quot;, by N. Elad, S. Rain, N. Immerman, L. Kov\'acs and M.
  Sagiv</comments><acm-class>F.3.1; F.4.1</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Some of the most significant high-level properties of currencies are the sums
of certain account balances. Properties of such sums can ensure the integrity
of currencies and transactions. For example, the sum of balances should not be
changed by a transfer operation. Currencies manipulated by code present a
verification challenge to mathematically prove their integrity by reasoning
about computer programs that operate over them, e.g., in Solidity. The ability
to reason about sums is essential: even the simplest ERC-20 token standard of
the Ethereum community provides a way to access the total supply of balances.
  Unfortunately, reasoning about code written against this interface is
non-trivial: the number of addresses is unbounded, and establishing global
invariants like the preservation of the sum of the balances by operations like
transfer requires higher-order reasoning. In particular, automated reasoners do
not provide ways to specify summations of arbitrary length.
  In this paper, we present a generalization of first-order logic which can
express the unbounded sum of balances. We prove the decidablity of one of our
extensions and the undecidability of a slightly richer one. We introduce
first-order encodings to automate reasoning over software transitions with
summations. We demonstrate the applicability of our results by using SMT
solvers and first-order provers for validating the correctness of common
transitions in smart contracts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07666</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07666</id><submitter>Daniel Schuster</submitter><version version="v1"><date>Mon, 17 May 2021 08:29:43 GMT</date><size>3638kb</size><source_type>D</source_type></version><title>Cortado---An Interactive Tool for Data-Driven Process Discovery and
  Modeling</title><authors>Daniel Schuster, Sebastiaan J. van Zelst, Wil M. P. van der Aalst</authors><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process mining aims to diagnose and improve operational processes. Process
mining techniques allow analyzing the event data generated and recorded during
the execution of (business) processes to gain valuable insights. Process
discovery is a key discipline in process mining that comprises the discovery of
process models on the basis of the recorded event data. Most process discovery
algorithms work in a fully automated fashion. Apart from adjusting their
configuration parameters, conventional process discovery algorithms offer
limited to no user interaction, i.e., we either edit the discovered process
model by hand or change the algorithm's input by, for instance, filtering the
event data. However, recent work indicates that the integration of domain
knowledge in (semi-)automated process discovery algorithms often enhances the
quality of the process models discovered. Therefore, this paper introduces
Cortado, a novel process discovery tool that leverages domain knowledge while
incrementally discovering a process model from given event data. Starting from
an initial process model, Cortado enables the user to incrementally add new
process behavior to the process model under construction in a visual and
intuitive manner. As such, Cortado unifies the world of manual process modeling
with that of automated process discovery.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07667</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07667</id><submitter>Bin Zhao</submitter><version version="v1"><date>Mon, 17 May 2021 08:36:10 GMT</date><size>4623kb</size><source_type>D</source_type></version><title>AudioVisual Video Summarization</title><authors>Bin Zhao, Maoguo Gong, Xuelong Li</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio and vision are two main modalities in video data. Multimodal learning,
especially for audiovisual learning, has drawn considerable attention recently,
which can boost the performance of various computer vision tasks. However, in
video summarization, existing approaches just exploit the visual information
while neglect the audio information. In this paper, we argue that the audio
modality can assist vision modality to better understand the video content and
structure, and further benefit the summarization process. Motivated by this, we
propose to jointly exploit the audio and visual information for the video
summarization task, and develop an AudioVisual Recurrent Network (AVRN) to
achieve this. Specifically, the proposed AVRN can be separated into three
parts: 1) the two-stream LSTM is utilized to encode the audio and visual
feature sequentially by capturing their temporal dependency. 2) the audiovisual
fusion LSTM is employed to fuse the two modalities by exploring the latent
consistency between them. 3) the self-attention video encoder is adopted to
capture the global dependency in the video. Finally, the fused audiovisual
information, and the integrated temporal and global dependencies are jointly
used to predict the video summary. Practically, the experimental results on the
two benchmarks, \emph{i.e.,} SumMe and TVsum, have demonstrated the
effectiveness of each part, and the superiority of AVRN compared to those
approaches just exploiting visual information for video summarization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07668</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07668</id><submitter>Alexander von Rohr</submitter><version version="v1"><date>Mon, 17 May 2021 08:36:18 GMT</date><size>75kb</size><source_type>D</source_type></version><title>Probabilistic robust linear quadratic regulators with Gaussian processes</title><authors>Alexander von Rohr, Matthias Neumann-Brosig, Sebastian Trimpe</authors><categories>eess.SY cs.LG cs.SY</categories><comments>to be published in the proceedings of the 3rd Conference on Learning
  for Dynamics and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic models such as Gaussian processes (GPs) are powerful tools to
learn unknown dynamical systems from data for subsequent use in control design.
While learning-based control has the potential to yield superior performance in
demanding applications, robustness to uncertainty remains an important
challenge. Since Bayesian methods quantify uncertainty of the learning results,
it is natural to incorporate these uncertainties into a robust design. In
contrast to most state-of-the-art approaches that consider worst-case
estimates, we leverage the learning method's posterior distribution in the
controller synthesis. The result is a more informed and, thus, more efficient
trade-off between performance and robustness. We present a novel controller
synthesis for linearized GP dynamics that yields robust controllers with
respect to a probabilistic stability margin. The formulation is based on a
recently proposed algorithm for linear quadratic control synthesis, which we
extend by giving probabilistic robustness guarantees in the form of credibility
bounds for the system's stability.Comparisons to existing methods based on
worst-case and certainty-equivalence designs reveal superior performance and
robustness properties of the proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07669</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07669</id><submitter>Shenghao Yang</submitter><version version="v1"><date>Mon, 17 May 2021 08:38:23 GMT</date><size>166kb</size></version><title>Capacity Scalability of Line Networks with Batched Codes</title><authors>Shenghao Yang, Jie Wang, Yanyan Dong and Yiheng Zhang</authors><categories>cs.IT math.IT</categories><comments>This paper was presented in part at ISIT 2019 and 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of line networks with buffer size constraints is an open, but
practically important problem. In this paper, we consider line networks formed
by discrete memoryless channels, and study the achievable rate of batched
codes, which include several existing coding schemes studied in literature for
line networks as special cases. A batched code has an outer code and an inner
code. The outer code encodes the information messages into batches, each of
which is a group of coded symbols, while the inner code performs recoding on
the symbols belonging to the same batch at all nodes that forwarding a batch.
Batched codes enable us to impose various block-length and buffer size
constraints by tuning coding parameters. Using a technique that captures the
communication bottleneck of line networks, we derive upper bounds on the
achievable rates of batched codes as functions of line network length for
several coding parameter sets. We also discuss specific recoding schemes that
can achieve rates of the same order of the line network length for these coding
parameter sets. Our results shed light on how to design large multi-hop network
communications in practical scenarios using a unified coding framework instead
of the traditional layered approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07671</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07671</id><submitter>Petra Posedel \v{S}imovi\'c</submitter><version version="v1"><date>Mon, 17 May 2021 08:40:34 GMT</date><size>67kb</size><source_type>D</source_type></version><title>Classifying variety of customer's online engagement for churn prediction
  with mixed-penalty logistic regression</title><authors>Petra Posedel \v{S}imovi\'c, Davor Horvatic, Edward W. Sun</authors><categories>stat.ML cs.LG econ.EM</categories><comments>28 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using big data to analyze consumer behavior can provide effective
decision-making tools for preventing customer attrition (churn) in customer
relationship management (CRM). Focusing on a CRM dataset with several different
categories of factors that impact customer heterogeneity (i.e., usage of
self-care service channels, duration of service, and responsiveness to
marketing actions), we provide new predictive analytics of customer churn rate
based on a machine learning method that enhances the classification of logistic
regression by adding a mixed penalty term. The proposed penalized logistic
regression can prevent overfitting when dealing with big data and minimize the
loss function when balancing the cost from the median (absolute value) and mean
(squared value) regularization. We show the analytical properties of the
proposed method and its computational advantage in this research. In addition,
we investigate the performance of the proposed method with a CRM data set (that
has a large number of features) under different settings by efficiently
eliminating the disturbance of (1) least important features and (2) sensitivity
from the minority (churn) class. Our empirical results confirm the expected
performance of the proposed method in full compliance with the common
classification criteria (i.e., accuracy, precision, and recall) for evaluating
machine learning methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07672</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07672</id><submitter>Chae Eun Lee</submitter><version version="v1"><date>Mon, 17 May 2021 08:42:19 GMT</date><size>4554kb</size><source_type>D</source_type></version><title>Voxel-level Siamese Representation Learning for Abdominal Multi-Organ
  Segmentation</title><authors>Chae Eun Lee, Minyoung Chung, Yeong-Gil Shin</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works in medical image segmentation have actively explored various
deep learning architectures or objective functions to encode high-level
features from volumetric data owing to limited image annotations. However, most
existing approaches tend to ignore cross-volume global context and define
context relations in the decision space. In this work, we propose a novel
voxel-level Siamese representation learning method for abdominal multi-organ
segmentation to improve representation space. The proposed method enforces
voxel-wise feature relations in the representation space for leveraging limited
datasets more comprehensively to achieve better performance. Inspired by recent
progress in contrastive learning, we suppressed voxel-wise relations from the
same class to be projected to the same point without using negative samples.
Moreover, we introduce a multi-resolution context aggregation method that
aggregates features from multiple hidden layers, which encodes both the global
and local contexts for segmentation. Our experiments on the multi-organ dataset
outperformed the existing approaches by 2% in Dice score coefficient. The
qualitative visualizations of the representation spaces demonstrate that the
improvements were gained primarily by a disentangled feature space.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07673</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07673</id><submitter>Bin Zhao</submitter><version version="v1"><date>Mon, 17 May 2021 08:44:34 GMT</date><size>5085kb</size><source_type>D</source_type></version><title>EA-Net: Edge-Aware Network for Flow-based Video Frame Interpolation</title><authors>Bin Zhao and Xuelong Li</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video frame interpolation can up-convert the frame rate and enhance the video
quality. In recent years, although the interpolation performance has achieved
great success, image blur usually occurs at the object boundaries owing to the
large motion. It has been a long-standing problem, and has not been addressed
yet. In this paper, we propose to reduce the image blur and get the clear shape
of objects by preserving the edges in the interpolated frames. To this end, the
proposed Edge-Aware Network (EA-Net) integrates the edge information into the
frame interpolation task. It follows an end-to-end architecture and can be
separated into two stages, \emph{i.e.}, edge-guided flow estimation and
edge-protected frame synthesis. Specifically, in the flow estimation stage,
three edge-aware mechanisms are developed to emphasize the frame edges in
estimating flow maps, so that the edge-maps are taken as the auxiliary
information to provide more guidance to boost the flow accuracy. In the frame
synthesis stage, the flow refinement module is designed to refine the flow map,
and the attention module is carried out to adaptively focus on the
bidirectional flow maps when synthesizing the intermediate frames. Furthermore,
the frame and edge discriminators are adopted to conduct the adversarial
training strategy, so as to enhance the reality and clarity of synthesized
frames. Experiments on three benchmarks, including Vimeo90k, UCF101 for
single-frame interpolation and Adobe240-fps for multi-frame interpolation, have
demonstrated the superiority of the proposed EA-Net for the video frame
interpolation task.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07674</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07674</id><submitter>Andrea Cossu</submitter><version version="v1"><date>Mon, 17 May 2021 08:49:01 GMT</date><size>128kb</size></version><title>Continual Learning with Echo State Networks</title><authors>Andrea Cossu, Davide Bacciu, Antonio Carta, Claudio Gallicchio,
  Vincenzo Lomonaco</authors><categories>cs.LG cs.AI</categories><comments>In review at ESANN 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continual Learning (CL) refers to a learning setup where data is non
stationary and the model has to learn without forgetting existing knowledge.
The study of CL for sequential patterns revolves around trained recurrent
networks. In this work, instead, we introduce CL in the context of Echo State
Networks (ESNs), where the recurrent component is kept fixed. We provide the
first evaluation of catastrophic forgetting in ESNs and we highlight the
benefits in using CL strategies which are not applicable to trained recurrent
models. Our results confirm the ESN as a promising model for CL and open to its
use in streaming scenarios.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07684</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07684</id><submitter>Rancy El Nmeir</submitter><version version="v1"><date>Mon, 17 May 2021 09:04:11 GMT</date><size>59kb</size><source_type>D</source_type></version><title>Quantization-based approximation of reflected BSDEs with extended upper
  bounds for recursive quantization</title><authors>Rancy El Nmeir and Gilles Pag\`es</authors><categories>math.PR cs.NA math.NA</categories><comments>40 pages, 1 figure, 3 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We establish upper bounds for the $L^p$-quantization error, p in (1, 2+d),
induced by the recursive Markovian quantization of a d-dimensional diffusion
discretized via the Euler scheme. We introduce a hybrid recursive quantization
scheme, easier to implement in the high-dimensional framework, and establish
upper bounds to the corresponding $L^p$-quantization error. To take advantage
of these extensions, we propose a time discretization scheme and a recursive
quantization-based discretization scheme associated to a reflected Backward
Stochastic Differential Equation and estimate $L^p$-error bounds induced by the
space approximation. We will explain how to numerically compute the solution of
the reflected BSDE relying on the recursive quantization and compare it to
other types of quantization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07688</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07688</id><submitter>Yuejia Xiang</submitter><version version="v1"><date>Mon, 17 May 2021 09:18:56 GMT</date><size>2195kb</size><source_type>D</source_type></version><title>OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph
  Embedding</title><authors>Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Zhenxi Lin, Yefeng
  Zheng</authors><categories>cs.CL cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semantic embedding has been widely investigated for aligning knowledge graph
(KG) entities. Current methods have explored and utilized the graph structure,
the entity names and attributes, but ignore the ontology (or ontological
schema) which contains critical meta information such as classes and their
membership relationships with entities. In this paper, we propose an
ontology-guided entity alignment method named OntoEA, where both KGs and their
ontologies are jointly embedded, and the class hierarchy and the class
disjointness are utilized to avoid false mappings. Extensive experiments on
seven public and industrial benchmarks have demonstrated the state-of-the-art
performance of OntoEA and the effectiveness of the ontologies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07691</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07691</id><submitter>Anubhav Singh</submitter><version version="v1"><date>Mon, 17 May 2021 09:21:48 GMT</date><size>1177kb</size><source_type>D</source_type></version><title>Approximate Novelty Search</title><authors>Anubhav Singh, Nir Lipovetzky, Miquel Ramirez, Javier Segovia-Aguas</authors><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Width-based search algorithms seek plans by prioritizing states according to
a suitably defined measure of novelty, that maps states into a set of novelty
categories. Space and time complexity to evaluate state novelty is known to be
exponential on the cardinality of the set. We present novel methods to obtain
polynomial approximations of novelty and width-based search. First, we
approximate novelty computation via random sampling and Bloom filters, reducing
the runtime and memory footprint. Second, we approximate the best-first search
using an adaptive policy that decides whether to forgo the expansion of nodes
in the open list. These two techniques are integrated into existing width-based
algorithms, resulting in new planners that perform significantly better than
other state-of-the-art planners over benchmarks from the International Planning
Competitions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07692</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07692</id><submitter>Christopher Battarbee</submitter><version version="v1"><date>Mon, 17 May 2021 09:24:46 GMT</date><size>12kb</size></version><title>Cryptanalysis of Semidirect Product Key Exchange Using Matrices Over
  Non-Commutative Rings</title><authors>Christopher Battarbee, Delaram Kahrobaei and Siamak F. Shahandashti</authors><categories>cs.CR</categories><comments>11 pages</comments><acm-class>E.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It was recently demonstrated that the Matrix Action Key Exchange (MAKE)
algorithm, a new type of key exchange protocol using the semidirect product of
matrix groups, is vulnerable to a linear algebraic attack if the matrices are
over a commutative ring. In this note, we establish conditions under which
protocols using matrices over a non-commutative ring are also vulnerable to
this attack. We then demonstrate that group rings $R[G]$ used in
arXiv:1304.6572, where $R$ is a commutative ring and $G$ is a non-abelian
group, are examples of non-commutative rings that satisfy these conditions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07693</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07693</id><submitter>Joe Watson</submitter><version version="v1"><date>Mon, 17 May 2021 09:27:12 GMT</date><size>761kb</size><source_type>D</source_type></version><title>Stochastic Control through Approximate Bayesian Input Inference</title><authors>Joe Watson, Hany Abdulsamad, Rolf Findeisen and Jan Peters</authors><categories>cs.LG cs.RO cs.SY eess.SY</categories><comments>Submitted to Transactions on Automatic Control Special Issue:
  Learning and Control. This work has been submitted to the IEEE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal control under uncertainty is a prevailing challenge in control, due
to the difficulty in producing tractable solutions for the stochastic
optimization problem. By framing the control problem as one of input
estimation, advanced approximate inference techniques can be used to handle the
statistical approximations in a principled and practical manner. Analyzing the
Gaussian setting, we present a solver capable of several stochastic control
methods, and was found to be superior to popular baselines on nonlinear
simulated tasks. We draw connections that relate this inference formulation to
previous approaches for stochastic optimal control, and outline several
advantages that this inference view brings due to its statistical nature.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07698</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07698</id><submitter>Casper Hansen</submitter><version version="v1"><date>Mon, 17 May 2021 09:34:03 GMT</date><size>5259kb</size><source_type>D</source_type></version><title>Automatic Fake News Detection: Are Models Learning to Reason?</title><authors>Casper Hansen and Christian Hansen and Lucas Chaves Lima</authors><categories>cs.CL cs.CY cs.LG</categories><comments>Accepted at ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most fact checking models for automatic fake news detection are based on
reasoning: given a claim with associated evidence, the models aim to estimate
the claim veracity based on the supporting or refuting content within the
evidence. When these models perform well, it is generally assumed to be due to
the models having learned to reason over the evidence with regards to the
claim. In this paper, we investigate this assumption of reasoning, by exploring
the relationship and importance of both claim and evidence. Surprisingly, we
find on political fact checking datasets that most often the highest
effectiveness is obtained by utilizing only the evidence, as the impact of
including the claim is either negligible or harmful to the effectiveness. This
highlights an important problem in what constitutes evidence in existing
approaches for automatic fake news detection.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07703</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07703</id><submitter>Antonio Fern\'andez Peralta</submitter><version version="v1"><date>Mon, 17 May 2021 09:39:37 GMT</date><size>3755kb</size><source_type>D</source_type></version><title>The effect of algorithmic bias and network structure on coexistence,
  consensus, and polarization of opinions</title><authors>Antonio F. Peralta, J\'anos Kert\'esz, Gerardo I\~niguez</authors><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individuals of modern societies share ideas and participate in collective
processes within a pervasive, variable, and mostly hidden ecosystem of content
filtering technologies that determine what information we see online. Despite
the impact of these algorithms on daily life and society, little is known about
their effect on information transfer and opinion formation. It is thus unclear
to what extent algorithmic bias has a harmful influence on collective
decision-making, such as a tendency to polarize debate. Here we introduce a
general theoretical framework to systematically link models of opinion
dynamics, social network structure, and content filtering. We showcase the
flexibility of our framework by exploring a family of binary-state opinion
dynamics models where information exchange lies in a spectrum from pairwise to
group interactions. All models show an opinion polarization regime driven by
algorithmic bias and modular network structure. The role of content filtering
is, however, surprisingly nuanced; for pairwise interactions it leads to
polarization, while for group interactions it promotes coexistence of opinions.
This allows us to pinpoint which social interactions are robust against
algorithmic bias, and which ones are susceptible to bias-enhanced opinion
polarization. Our framework gives theoretical ground for the development of
heuristics to tackle harmful effects of online bias, such as information
bottlenecks, echo chambers, and opinion radicalization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07704</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07704</id><submitter>Bar Sadeh</submitter><version version="v1"><date>Mon, 17 May 2021 09:46:11 GMT</date><size>24kb</size></version><title>Bounds on the Capacity of PIR over Graphs</title><authors>Bar Sadeh and Yujie Gu and Itzhak Tamo</authors><categories>cs.IT math.IT</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In the private information retrieval (PIR) problem, a user wants to retrieve
a file from a database without revealing any information about the desired
file's identity to the servers that store the database. In this paper, we study
the PIR capacity of a graph-based replication system, in which each file is
stored on two distinct servers according to an underlying graph. This paper
aims to provide upper and lower bounds to the PIR capacity of graphs via
various graph properties. In particular, we provide several upper bounds on the
PIR capacity that apply to all graphs. We further improve the bounds for
specific graph families (which turn out to be tight in certain cases) by
utilizing the underlying graph structure. For the lower bounds, we establish
optimal rate PIR retrieval schemes for star graphs via edge-coloring
techniques. Lastly, we provide an improved PIR scheme for complete graphs,
which implies an improved general lower bound on all graphs' PIR capacity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07706</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07706</id><submitter>Xu Ma</submitter><version version="v1"><date>Mon, 17 May 2021 09:48:15 GMT</date><size>2149kb</size><source_type>D</source_type></version><title>Towards a Better Tradeoff between Effectiveness and Efficiency in
  Pre-Ranking: A Learnable Feature Selection based Approach</title><authors>Xu Ma, Pengjie Wang, Hui Zhao, Shaoguo Liu, Chuhan Zhao, Wei Lin,
  Kuang-Chih Lee, Jian Xu, Bo Zheng</authors><categories>cs.IR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-world search, recommendation, and advertising systems, the
multi-stage ranking architecture is commonly adopted. Such architecture usually
consists of matching, pre-ranking, ranking, and re-ranking stages. In the
pre-ranking stage, vector-product based models with representation-focused
architecture are commonly adopted to account for system efficiency. However, it
brings a significant loss to the effectiveness of the system. In this paper, a
novel pre-ranking approach is proposed which supports complicated models with
interaction-focused architecture. It achieves a better tradeoff between
effectiveness and efficiency by utilizing the proposed learnable Feature
Selection method based on feature Complexity and variational Dropout (FSCD).
Evaluations in a real-world e-commerce sponsored search system for a search
engine demonstrate that utilizing the proposed pre-ranking, the effectiveness
of the system is significantly improved. Moreover, compared to the systems with
conventional pre-ranking models, an identical amount of computational resource
is consumed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07711</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07711</id><submitter>Spyridon Mastorakis</submitter><version version="v1"><date>Mon, 17 May 2021 10:05:08 GMT</date><size>1842kb</size><source_type>D</source_type></version><title>Hash-MAC-DSDV: Mutual Authentication for Intelligent IoT-Based
  Cyber-Physical Systems</title><authors>Muhammad Adil and Mian Ahmad Jan and Spyridon Mastorakis and Houbing
  Song and Muhammad Mohsin Jadoon and Safia Abbas and Ahmed Farouk</authors><categories>cs.CR cs.NI</categories><comments>Accepted by the IEEE Internet of Things Journal. The copyright is
  with the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-Physical Systems (CPS) connected in the form of Internet of Things
(IoT) are vulnerable to various security threats, due to the
infrastructure-less deployment of IoT devices. Device-to-Device (D2D)
authentication of these networks ensures the integrity, authenticity, and
confidentiality of information in the deployed area. The literature suggests
different approaches to address security issues in CPS technologies. However,
they are mostly based on centralized techniques or specific system deployments
with higher cost of computation and communication. It is therefore necessary to
develop an effective scheme that can resolve the security problems in CPS
technologies of IoT devices. In this paper, a lightweight Hash-MAC-DSDV (Hash
Media Access Control Destination Sequence Distance Vector) routing scheme is
proposed to resolve authentication issues in CPS technologies, connected in the
form of IoT networks. For this purpose, a CPS of IoT devices (multi-WSNs) is
developed from the local-chain and public chain, respectively. The proposed
scheme ensures D2D authentication by the Hash-MAC-DSDV mutual scheme, where the
MAC addresses of individual devices are registered in the first phase and
advertised in the network in the second phase. The proposed scheme allows
legitimate devices to modify their routing table and unicast the one-way hash
authentication mechanism to transfer their captured data from source towards
the destination. Our evaluation results demonstrate that Hash- MAC-DSDV
outweighs the existing schemes in terms of attack detection, energy consumption
and communication metrics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07715</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07715</id><submitter>Kelei He</submitter><version version="v1"><date>Mon, 17 May 2021 10:11:45 GMT</date><size>3252kb</size><source_type>D</source_type></version><title>Cross-Modality Brain Tumor Segmentation via Bidirectional
  Global-to-Local Unsupervised Domain Adaptation</title><authors>Kelei He, Wen Ji, Tao Zhou, Zhuoyuan Li, Jing Huo, Xin Zhang, Yang
  Gao, Dinggang Shen, Bing Zhang, and Junfeng Zhang</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Accurate segmentation of brain tumors from multi-modal Magnetic Resonance
(MR) images is essential in brain tumor diagnosis and treatment. However, due
to the existence of domain shifts among different modalities, the performance
of networks decreases dramatically when training on one modality and performing
on another, e.g., train on T1 image while performing on T2 image, which is
often required in clinical applications. This also prohibits a network from
being trained on labeled data and then transferred to unlabeled data from a
different domain. To overcome this, unsupervised domain adaptation (UDA)
methods provide effective solutions to alleviate the domain shift between
labeled source data and unlabeled target data. In this paper, we propose a
novel Bidirectional Global-to-Local (BiGL) adaptation framework under a UDA
scheme. Specifically, a bidirectional image synthesis and segmentation module
is proposed to segment the brain tumor using the intermediate data
distributions generated for the two domains, which includes an image-to-image
translator and a shared-weighted segmentation network. Further, a
global-to-local consistency learning module is proposed to build robust
representation alignments in an integrated way. Extensive experiments on a
multi-modal brain MR benchmark dataset demonstrate that the proposed method
outperforms several state-of-the-art unsupervised domain adaptation methods by
a large margin, while a comprehensive ablation study validates the
effectiveness of each key component. The implementation code of our method will
be released at \url{https://github.com/KeleiHe/BiGL}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07720</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07720</id><submitter>Dimitri Kartsaklis</submitter><version version="v1"><date>Mon, 17 May 2021 10:32:18 GMT</date><size>772kb</size><source_type>D</source_type></version><title>A CCG-Based Version of the DisCoCat Framework</title><authors>Richie Yeung, Dimitri Kartsaklis</authors><categories>cs.CL math.CT</categories><comments>SemSpace 2021: Semantic Spaces at the Intersection of NLP, Physics,
  and Cognitive Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the DisCoCat model (Coecke et al., 2010) has been proved a valuable
tool for studying compositional aspects of language at the level of semantics,
its strong dependency on pregroup grammars poses important restrictions: first,
it prevents large-scale experimentation due to the absence of a pregroup
parser; and second, it limits the expressibility of the model to context-free
grammars. In this paper we solve these problems by reformulating DisCoCat as a
passage from Combinatory Categorial Grammar (CCG) to a category of semantics.
We start by showing that standard categorial grammars can be expressed as a
biclosed category, where all rules emerge as currying/uncurrying the identity;
we then proceed to model permutation-inducing rules by exploiting the symmetry
of the compact closed category encoding the word meaning. We provide a proof of
concept for our method, converting &quot;Alice in Wonderland&quot; into DisCoCat form, a
corpus that we make available to the community.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07722</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07722</id><submitter>Wadii Boulila Prof.</submitter><version version="v1"><date>Mon, 17 May 2021 10:40:49 GMT</date><size>304kb</size></version><title>Microservices in IoT Security: Current Solutions, Research Challenges,
  and Future Directions</title><authors>Maha Driss, Daniah Hasan, Wadii Boulila, Jawad Ahmad</authors><categories>cs.CR cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, the Internet of Things (IoT) technology has led to the
emergence of multiple smart applications in different vital sectors including
healthcare, education, agriculture, energy management, etc. IoT aims to
interconnect several intelligent devices over the Internet such as sensors,
monitoring systems, and smart appliances to control, store, exchange, and
analyze collected data. The main issue in IoT environments is that they can
present potential vulnerabilities to be illegally accessed by malicious users,
which threatens the safety and privacy of gathered data. To face this problem,
several recent works have been conducted using microservices-based architecture
to minimize the security threats and attacks related to IoT data. By employing
microservices, these works offer extensible, reusable, and reconfigurable
security features. In this paper, we aim to provide a survey about
microservices-based approaches for securing IoT applications. This survey will
help practitioners understand ongoing challenges and explore new and promising
research opportunities in the IoT security field. To the best of our knowledge,
this paper constitutes the first survey that investigates the use of
microservices technology for securing IoT applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07725</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07725</id><submitter>Derya Malak</submitter><version version="v1"><date>Mon, 17 May 2021 10:52:54 GMT</date><size>8038kb</size><source_type>D</source_type></version><title>Distributed Computation over MAC via Kolmogorov-Arnold Representation</title><authors>Derya Malak and Ali Tajer</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kolmogorov's representation theorem provides a framework for decomposing any
arbitrary real-valued, multivariate, and continuous function into a two-layer
nested superposition of a finite number of functions. The functions at these
two layers, are referred to as the inner and outer functions with the key
property that the design of the inner functions is independent of that of the
original function of interest to be computed. This brings modularity and
universality to the design of the inner function, and subsequently, a part of
computation. This paper capitalizes on such modularity and universality in
functional representation to propose two frameworks for distributed computation
over the additive multiple access channels (MACs). In the first framework, each
source encodes the inner representations and sends them over the additive MAC.
Subsequently, the receiver computes the outer functions to compute the function
of interest. Transmitting the values of the inner functions instead of the
messages directly leads to compression gains. In the second approach, in order
to further increase the compression rate, the framework aims to also bring
computing the outer functions to the source sites. Specifically, each source
employs a graph-coloring-based approach to perform joint functional compression
of the inner and the outer functions, which may attain further compression
savings over the former. These modular encoding schemes provide an exact
representation in the asymptotic regime and the non-asymptotic regime.
Contrasting these with the baseline model where sources directly transmit data
over MAC, we observe gains. To showcase the gains of these two frameworks and
their discrepancies, they are applied to a number of commonly used computations
in distributed systems, e.g., computing products, $\ell_m$-norms, polynomial
functions, extremum values of functions, and affine transformations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07727</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07727</id><submitter>Andrea Fronzetti Colladon PhD</submitter><version version="v1"><date>Mon, 17 May 2021 10:54:23 GMT</date><size>1477kb</size></version><title>Using social network and semantic analysis to analyze online travel
  forums and forecast tourism demand</title><authors>A Fronzetti Colladon, B Guardabascio, R Innarella</authors><categories>econ.EM cs.CL cs.SI</categories><acm-class>J.4; H.4.0; I.2.7</acm-class><journal-ref>Decision Support Systems 123, 113075 (2019)</journal-ref><doi>10.1016/j.dss.2019.113075</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Forecasting tourism demand has important implications for both policy makers
and companies operating in the tourism industry. In this research, we applied
methods and tools of social network and semantic analysis to study
user-generated content retrieved from online communities which interacted on
the TripAdvisor travel forum. We analyzed the forums of 7 major European
capital cities, over a period of 10 years, collecting more than 2,660,000
posts, written by about 147,000 users. We present a new methodology of analysis
of tourism-related big data and a set of variables which could be integrated
into traditional forecasting models. We implemented Factor Augmented
Autoregressive and Bridge models with social network and semantic variables
which often led to a better forecasting performance than univariate models and
models based on Google Trend data. Forum language complexity and the
centralization of the communication network, i.e. the presence of eminent
contributors, were the variables that contributed more to the forecasting of
international airport arrivals.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07729</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07729</id><submitter>Vinicius Luiz Santos Silva</submitter><version version="v1"><date>Mon, 17 May 2021 10:56:53 GMT</date><size>2736kb</size><source_type>D</source_type></version><title>Data Assimilation Predictive GAN (DA-PredGAN): applied to determine the
  spread of COVID-19</title><authors>Vinicius L S Silva, Claire E Heaney, Yaqi Li, Christopher C Pain</authors><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the novel use of a generative adversarial network (GAN) (i) to
make predictions in time (PredGAN) and (ii) to assimilate measurements
(DA-PredGAN). In the latter case, we take advantage of the natural adjoint-like
properties of generative models and the ability to simulate forwards and
backwards in time. GANs have received much attention recently, after achieving
excellent results for their generation of realistic-looking images. We wish to
explore how this property translates to new applications in computational
modelling and to exploit the adjoint-like properties for efficient data
assimilation. To predict the spread of COVID-19 in an idealised town, we apply
these methods to a compartmental model in epidemiology that is able to model
space and time variations. To do this, the GAN is set within a reduced-order
model (ROM), which uses a low-dimensional space for the spatial distribution of
the simulation states. Then the GAN learns the evolution of the low-dimensional
states over time. The results show that the proposed methods can accurately
predict the evolution of the high-fidelity numerical simulation, and can
efficiently assimilate observed data and determine the corresponding model
parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07730</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07730</id><submitter>Drishti Jain</submitter><version version="v1"><date>Mon, 17 May 2021 10:58:35 GMT</date><size>1417kb</size></version><title>The State of Infodemic on Twitter</title><authors>Drishti Jain (1), Tavpritesh Sethi (1) ((1) Indraprastha Institute of
  Information Technology)</authors><categories>cs.SI cs.LG</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Following the wave of misinterpreted, manipulated and malicious information
growing on the Internet, the misinformation surrounding COVID-19 has become a
paramount issue. In the context of the current COVID-19 pandemic, social media
posts and platforms are at risk of rumors and misinformation in the face of the
serious uncertainty surrounding the virus itself. At the same time, the
uncertainty and new nature of COVID-19 means that other unconfirmed information
that may appear &quot;rumored&quot; may be an important indicator of the behavior and
impact of this new virus. Twitter, in particular, has taken a center stage in
this storm where Covid-19 has been a much talked about subject. We have
presented an exploratory analysis of the tweets and the users who are involved
in spreading misinformation and then delved into machine learning models and
natural language processing techniques to identify if a tweet contains
misinformation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07731</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07731</id><submitter>Alexander P. Kartun-Giles MSci PhD</submitter><version version="v1"><date>Mon, 17 May 2021 10:59:13 GMT</date><size>2341kb</size><source_type>D</source_type></version><title>Connectivity of 1d random geometric graphs</title><authors>Alexander P. Kartun-Giles, Kostas Koufos, Nicolas Privault</authors><categories>math.CO cond-mat.stat-mech cs.SI math.PR physics.soc-ph</categories><comments>27 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A 1d random geometric graph (1d RGG) is built by joining a random sample of
$n$ points from an interval of the real line with probability $p$. We count the
number of $k$-hop paths between two vertices of the graph in the case where the
space is the 1d interval $[0,1]$. We show how the $k$-hop path count between
two vertices at Euclidean distance $|x-y|$ is in bijection with the volume
enclosed by a uniformly random $d$-dimensional lattice path joining the corners
of a $(k-1)$-dimensional hyperrectangular lattice. We are able to provide the
probability generating function and distribution of this $k$-hop path count as
a sum over lattice paths, incorporating the idea of restricted integer
partitions with limited number of parts. We therefore demonstrate and describe
an important link between spatial random graphs, and lattice path
combinatorics, where the $d$-dimensional lattice paths correspond to spatial
permutations of the geometric points on the line.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07733</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07733</id><submitter>Julian Rasch</submitter><version version="v1"><date>Mon, 17 May 2021 11:05:59 GMT</date><size>909kb</size><source_type>D</source_type></version><title>Knowledge State Networks for Effective Skill Assessment in Atomic
  Learning</title><authors>Julian Rasch and David Middelbeck</authors><categories>cs.LG</categories><comments>submitted to JEDM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to introduce a new framework for fast and effective
knowledge state assessments in the context of personalized, skill-based online
learning. We use knowledge state networks - specific neural networks trained on
assessment data of previous learners - to predict the full knowledge state of
other learners from only partial information about their skills. In combination
with a matching assessment strategy for asking discriminative questions we
demonstrate that our approach leads to a significant speed-up of the assessment
process - in terms of the necessary number of assessment questions - in
comparison to standard assessment designs. In practice, the presented methods
enable personalized, skill-based online learning also for skill ontologies of
very fine granularity without deteriorating the associated learning experience
by a lengthy assessment process.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07734</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07734</id><submitter>Stefan Hetzl</submitter><version version="v1"><date>Mon, 17 May 2021 11:06:16 GMT</date><size>38kb</size></version><title>Induction and Skolemization in saturation theorem proving</title><authors>Stefan Hetzl, Jannik Vierling</authors><categories>math.LO cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider a typical integration of induction in saturation-based theorem
provers and investigate the effects of Skolem symbols occurring in the
induction formulas. In a practically relevant setting we establish a
Skolem-free characterization of refutation in saturation-based proof systems
with induction. Finally, we use this characterization to obtain unprovability
results for a concrete saturation-based induction prover.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07736</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07736</id><submitter>Changpeng Shao</submitter><version version="v1"><date>Mon, 17 May 2021 11:11:52 GMT</date><size>1320kb</size></version><title>A deterministic Kaczmarz algorithm for solving linear systems</title><authors>Changpeng Shao</authors><categories>math.NA cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a deterministic Kaczmarz method for solving linear systems
$A\x=\b$ with $A$ nonsingular. Instead of using orthogonal projections, we use
reflections in the original Kaczmarz iterative method. This generates a series
of points on an $n$-sphere $\S$ centered at the solution $\x_*=A^{-1}\b$. We
show that these points are nicely distributed on $\S$. Taking the average of
several points will lead to an effective approximation to the solution. We will
show how to choose these points efficiently. The numerical tests show that in
practice this deterministic scheme converges much faster than we expected and
can beat the (block) randomized Kaczmarz methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07737</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07737</id><submitter>Jeffrey Galkowski</submitter><version version="v1"><date>Mon, 17 May 2021 11:13:19 GMT</date><size>289kb</size><source_type>D</source_type></version><title>Perfectly-matched-layer truncation is exponentially accurate at high
  frequency</title><authors>Jeffrey Galkowski, David Lafontaine, Euan A. Spence</authors><categories>math.AP cs.NA math.NA math.SP</categories><comments>39 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wide variety of scattering problems including scattering by
Dirichlet, Neumann, and penetrable obstacles. We show that for any fixed
perfectly-matched-layer (PML) width and a steep-enough scaling angle, the PML
solution is exponentially close, both in frequency and the tangent of the
scaling angle, to the true scattering solution. Moreover, for a fixed scaling
angle and large enough PML width, the PML solution is exponentially close to
the true scattering solution in both frequency and the PML width. In fact, the
exponential bound holds with rate of decay $c(w\tan\theta -C) k$ where $w$ is
the PML width and $\theta$ is the scaling angle. More generally, the results of
the paper hold in the framework of black-box scattering under the assumption of
an exponential bound on the norm of the cutoff resolvent, thus including
problems with strong trapping. These are the first results on the exponential
accuracy of PML at high-frequency with non-trivial scatterers.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07739</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07739</id><submitter>Akash Kumar</submitter><version version="v1"><date>Mon, 17 May 2021 11:20:14 GMT</date><size>2187kb</size><source_type>D</source_type></version><title>Toward the Design of Fault-Tolerance- and Peak- Power-Aware Multi-Core
  Mixed-Criticality Systems</title><authors>Behnaz Ranjbar, Ali Hosseinghorban, Mohammad Salehi, Alireza Ejlali
  and Akash Kumar</authors><categories>cs.DC</categories><comments>This is an extended version of the paper accepted to be published in
  IEEE TCAD 2021 which includes Appendices A-E</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mixed-Criticality (MC) systems have recently been devised to address the
requirements of real-time systems in industrial applications, where the system
runs tasks with different criticality levels on a single platform. In some
workloads, a high-critically task might overrun and overload the system, or a
fault can occur during the execution. However, these systems must be
fault-tolerant and guarantee the correct execution of all high-criticality
tasks by their deadlines to avoid catastrophic consequences, in any situation.
Furthermore, in these MC systems, the peak power consumption of the system may
increase, especially in an overload situation and exceed the processor Thermal
Design Power (TDP) constraint. This may cause generating heat beyond the
cooling capacity, resulting the system stop to avoid excessive heat and halting
the processor. In this paper, we propose a technique for dependent
dual-criticality tasks in fault-tolerant multi-core MC systems to manage peak
power consumption and temperature. The technique develops a tree of possible
task mapping and scheduling at design-time to cover all possible scenarios and
reduce the low-criticality task drop rate in the high-criticality mode. At
run-time, the system exploits the tree to select a proper schedule according to
fault occurrences and criticality mode changes. Experimental results show that
the average task schedulability is 74.14% on average for the proposed method,
while the peak power consumption and maximum temperature are improved by 16.65%
and 14.9 C on average, respectively, compared to a recent work. In addition,
for a real-life application, our method reduces the peak power and maximum
temperature by up to 20.06% and 5 C, respectively, compared to a
state-of-the-art approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07740</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07740</id><submitter>Andrea Fronzetti Colladon PhD</submitter><version version="v1"><date>Mon, 17 May 2021 11:25:14 GMT</date><size>519kb</size></version><title>Personality correlates of key roles in informal advice networks</title><authors>E. Battistoni and A. Fronzetti Colladon</authors><categories>physics.soc-ph cs.SI</categories><journal-ref>Learning and Individual Differences 34, 63-69 (2014)</journal-ref><doi>10.1016/j.lindif.2014.05.007</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Prior research has emphasised the importance of informal advice networks for
knowledge sharing and peer learning. We use Social Network Analysis to detect
individuals who play a strategic role in advice networks. Even if roles have
been extensively described, how to identify people within them is still an open
issue. Furthermore, we investigate whether an association between key players
and the big five personality traits exists, by means of nonparametric
statistics. To achieve this, we present a case study which involves roughly 180
university students. We found 21 of them playing a key role. Results give
evidence of significant associations between key positions and
Conscientiousness, Neuroticism and Agreeableness; whereas no evidence is found
for a relationship with Extraversion or Openness to Experience. Consistently,
personality emerges as a relevant indicator for predicting people who are more
likely to play a strategic role, even when connection patterns are unknown.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07741</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07741</id><submitter>Michael Murray</submitter><version version="v1"><date>Mon, 17 May 2021 11:30:46 GMT</date><size>8107kb</size><source_type>D</source_type></version><title>Activation function design for deep networks: linearity and effective
  initialisation</title><authors>Michael Murray, Vinayak Abrol, Jared Tanner</authors><categories>cs.LG</categories><comments>33 pages, 10 figures, paper code and scripts are hosted at
  https://github.com/Cross-Caps/AFLI</comments><msc-class>68T07</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The activation function deployed in a deep neural network has great influence
on the performance of the network at initialisation, which in turn has
implications for training. In this paper we study how to avoid two problems at
initialisation identified in prior works: rapid convergence of pairwise input
correlations, and vanishing and exploding gradients. We prove that both these
problems can be avoided by choosing an activation function possessing a
sufficiently large linear region around the origin, relative to the bias
variance $\sigma_b^2$ of the network's random initialisation. We demonstrate
empirically that using such activation functions leads to tangible benefits in
practice, both in terms test and training accuracy as well as training time.
Furthermore, we observe that the shape of the nonlinear activation outside the
linear region appears to have a relatively limited impact on training. Finally,
our results also allow us to train networks in a new hyperparameter regime,
with a much larger bias variance than has previously been possible.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07743</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07743</id><submitter>Anastasis Kratsios</submitter><version version="v1"><date>Mon, 17 May 2021 11:34:09 GMT</date><size>161kb</size></version><title>Universal Regular Conditional Distributions via Probability
  Measure-Valued Deep Neural Models</title><authors>Anastasis Kratsios</authors><categories>cs.LG cs.NE math.MG math.PR stat.ML</categories><comments>21 Pages + 21 Page Appendix, 7 Tables, 94 References</comments><msc-class>68T07, 28A50, 49Q22, 54C65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a general framework for explicitly constructing
universal deep neural models with inputs from a complete, separable, and
locally-compact metric space $\mathcal{X}$ and outputs in the Wasserstein-1
$\mathcal{P}_1(\mathcal{Y})$ space over a complete and separable metric space
$\mathcal{Y}$. We find that any model built using the proposed framework is
dense in the space $C(\mathcal{X},\mathcal{P}_1(\mathcal{Y}))$ of continuous
functions from $\mathcal{X}$ to $\mathcal{P}_1(\mathcal{Y})$ in the
corresponding uniform convergence on compacts topology, quantitatively. We
identify two methods in which the curse of dimensionality can be broken. The
first approach constructs subsets of
$C(\mathcal{X},\mathcal{P}_1(\mathcal{Y}))$ consisting of functions that can be
efficiently approximated. In the second approach, given any fixed $f \in
C(\mathcal{X},\mathcal{P}_1(\mathcal{Y}))$, we build non-trivial subsets of
$\mathcal{X}$ on which $f$ can be efficiently approximated. The results are
applied to three open problems lying at the interface of applied probability
and computational learning theory. We find that the proposed models can
approximate any regular conditional distribution of a $\mathcal{Y}$-valued
random element $Y$ depending on an $\mathcal{X}$-valued random element $X$,
with arbitrarily high probability. The proposed models are also shown to be
capable of generically expressing the aleatoric uncertainty present in most
randomized machine learning models. The proposed framework is used to derive an
affirmative answer to the open conjecture of Bishop (1994); namely: mixture
density networks are generic regular conditional distributions. Numerical
experiments are performed in the contexts of extreme learning machines,
randomized DNNs, and heteroscedastic regression.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07745</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07745</id><submitter>Alessandro Bosso</submitter><version version="v1"><date>Mon, 17 May 2021 11:38:49 GMT</date><size>1371kb</size></version><title>Low-Input Accurate Periodic Motion of an Underactuated Mechanism: Mass
  Distribution and Nonlinear Spring Shaping</title><authors>Andrea Tilli, Elena Ruggiano, Alessandro Bosso, Alessandro Samor\`i</authors><categories>eess.SY cs.SY</categories><comments>accepted for presentation at the 2021 IEEE/ASME International
  Conference on Advanced Intelligent Mechatronics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a control-oriented structural design approach for a 2-DOF
underactuated mechanical system, with the purpose of generating an optimal
oscillatory behavior of the end-effector. To achieve the desired periodic
motion, we propose to adjust the dynamic response of the mechanism by selecting
its mass distribution and the characteristic of a nonlinear spring. In
particular, we introduce a two-step optimization strategy to shape the system's
zero dynamics, obtained via input-output linearization. The first part of the
procedure aims to minimize the root-mean-square value of the input torque by
optimizing the mechanism's mass distribution. In this context, we show that a
perfect matching with the desired trajectory can be reached by assuming the
ability to design an arbitrary shape of the system's elastic properties. Then,
in order to favor a simpler physical implementation of the structure, we
dedicate the second optimization step to the piecewise linear approximation of
the previously defined stiffness characteristic. The proposed procedure is
finally tested in detailed numerical simulations, confirming its effectiveness
in generating a complex and efficient periodic motion.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07749</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07749</id><submitter>Andrea Fronzetti Colladon PhD</submitter><version version="v1"><date>Mon, 17 May 2021 11:49:30 GMT</date><size>601kb</size></version><title>Studying the association of online brand importance with museum
  visitors: An application of the semantic brand score</title><authors>A. Fronzetti Colladon, F. Grippa, R. Innarella</authors><categories>cs.CL cs.SI econ.GN physics.soc-ph q-fin.EC</categories><acm-class>I.2.7; J.4; H.4.0</acm-class><journal-ref>Tourism Management Perspectives 33, 100588 (2020)</journal-ref><doi>10.1016/j.tmp.2019.100588</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper explores the association between brand importance and growth in
museum visitors. We analyzed 10 years of online forum discussions and applied
the Semantic Brand Score (SBS) to assess the brand importance of five European
Museums. Our Naive Bayes and regression models indicate that variations in the
combined dimensions of the SBS (prevalence, diversity and connectivity) are
aligned with changes in museum visitors. Results suggest that, in order to
attract more visitors, museum brand managers should focus on increasing the
volume of online posting and the richness of information generated by users
around the brand, rather than controlling for the posts' overall positivity or
negativity.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07751</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07751</id><submitter>Ruibo Li</submitter><version version="v1"><date>Mon, 17 May 2021 11:53:58 GMT</date><size>2053kb</size><source_type>D</source_type></version><title>HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs
  and Position-aware Flow Embedding</title><authors>Ruibo Li, Guosheng Lin, Tong He, Fayao Liu, Chunhua Shen</authors><categories>cs.CV</categories><comments>Accepted to CVPR2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scene flow in 3D point clouds plays an important role in understanding
dynamic environments. Although significant advances have been made by deep
neural networks, the performance is far from satisfactory as only per-point
translational motion is considered, neglecting the constraints of the rigid
motion in local regions. To address the issue, we propose to introduce the
motion consistency to force the smoothness among neighboring points. In
addition, constraints on the rigidity of the local transformation are also
added by sharing unique rigid motion parameters for all points within each
local region. To this end, a high-order CRFs based relation module (Con-HCRFs)
is deployed to explore both point-wise smoothness and region-wise rigidity. To
empower the CRFs to have a discriminative unary term, we also introduce a
position-aware flow estimation module to be incorporated into the Con-HCRFs.
Comprehensive experiments on FlyingThings3D and KITTI show that our proposed
framework (HCRF-Flow) achieves state-of-the-art performance and significantly
outperforms previous approaches substantially.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07752</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07752</id><submitter>Bencheng Yan</submitter><version version="v1"><date>Mon, 17 May 2021 11:56:04 GMT</date><size>2593kb</size><source_type>D</source_type></version><title>Explicit Semantic Cross Feature Learning via Pre-trained Graph Neural
  Networks for CTR Prediction</title><authors>Feng Li, Bencheng Yan, Qingqing Long, Pengjie Wang, Wei Lin, Jian Xu
  and Bo Zheng</authors><categories>cs.AI cs.IR cs.LG</categories><comments>SIGIR 2021, 5 pages; The first two authors contributed equally to
  this work; Pengjie Wang gave a lot of guidance in this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross features play an important role in click-through rate (CTR) prediction.
Most of the existing methods adopt a DNN-based model to capture the cross
features in an implicit manner. These implicit methods may lead to a
sub-optimized performance due to the limitation in explicit semantic modeling.
Although traditional statistical explicit semantic cross features can address
the problem in these implicit methods, it still suffers from some challenges,
including lack of generalization and expensive memory cost. Few works focus on
tackling these challenges. In this paper, we take the first step in learning
the explicit semantic cross features and propose Pre-trained Cross Feature
learning Graph Neural Networks (PCF-GNN), a GNN based pre-trained model aiming
at generating cross features in an explicit fashion. Extensive experiments are
conducted on both public and industrial datasets, where PCF-GNN shows
competence in both performance and memory-efficiency in various tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07753</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07753</id><submitter>Kimiaki Shirahama</submitter><version version="v1"><date>Mon, 17 May 2021 11:57:02 GMT</date><size>317kb</size></version><title>Generic Itemset Mining Based on Reinforcement Learning</title><authors>Kazuma Fujioka and Kimiaki Shirahama</authors><categories>cs.DB cs.LG</categories><comments>12 pages, 4 figures</comments><acm-class>H.2.8; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest problems in itemset mining is the requirement of
developing a data structure or algorithm, every time a user wants to extract a
different type of itemsets. To overcome this, we propose a method, called
Generic Itemset Mining based on Reinforcement Learning (GIM-RL), that offers a
unified framework to train an agent for extracting any type of itemsets. In
GIM-RL, the environment formulates iterative steps of extracting a target type
of itemsets from a dataset. At each step, an agent performs an action to add or
remove an item to or from the current itemset, and then obtains from the
environment a reward that represents how relevant the itemset resulting from
the action is to the target type. Through numerous trial-and-error steps where
various rewards are obtained by diverse actions, the agent is trained to
maximise cumulative rewards so that it acquires the optimal action policy for
forming as many itemsets of the target type as possible. In this framework, an
agent for extracting any type of itemsets can be trained as long as a reward
suitable for the type can be defined. The extensive experiments on mining high
utility itemsets, frequent itemsets and association rules show the general
effectiveness and one remarkable potential (agent transfer) of GIM-RL. We hope
that GIM-RL opens a new research direction towards learning-based itemset
mining.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07754</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07754</id><submitter>Xinjian Luo</submitter><version version="v1"><date>Mon, 17 May 2021 11:58:16 GMT</date><size>9607kb</size><source_type>D</source_type></version><title>A Fusion-Denoising Attack on InstaHide with Data Augmentation</title><authors>Xinjian Luo, Xiaokui Xiao, Yuncheng Wu, Juncheng Liu, Beng Chin Ooi</authors><categories>cs.CR cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  InstaHide is a state-of-the-art mechanism for protecting private training
images in collaborative learning. It works by mixing multiple private images
and modifying them in such a way that their visual features are no longer
distinguishable to the naked eye, without significantly degrading the accuracy
of training. In recent work, however, Carlini et al. show that it is possible
to reconstruct private images from the encrypted dataset generated by
InstaHide, by exploiting the correlations among the encrypted images.
Nevertheless, Carlini et al.'s attack relies on the assumption that each
private image is used without modification when mixing up with other private
images. As a consequence, it could be easily defeated by incorporating data
augmentation into InstaHide. This leads to a natural question: is InstaHide
with data augmentation secure?
  This paper provides a negative answer to the above question, by present an
attack for recovering private images from the outputs of InstaHide even when
data augmentation is present. The basic idea of our attack is to use a
comparative network to identify encrypted images that are likely to correspond
to the same private image, and then employ a fusion-denoising network for
restoring the private image from the encrypted ones, taking into account the
effects of data augmentation. Extensive experiments demonstrate the
effectiveness of the proposed attack in comparison to Carlini et al.'s attack.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07758</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07758</id><submitter>Wang-Zhou Dai</submitter><version version="v1"><date>Mon, 17 May 2021 12:10:26 GMT</date><size>53kb</size><source_type>D</source_type></version><title>Automated Biodesign Engineering by Abductive Meta-Interpretive Learning</title><authors>Wang-Zhou Dai, Liam Hallett, Stephen H. Muggleton, Geoff S. Baldwin</authors><categories>cs.AI cs.LG</categories><comments>Accepted by SSS-21 (AAAI Spring Symposium Series 2021), Artificial
  Intelligence for Synthetic Biology (AI4Synbio) track</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The application of Artificial Intelligence (AI) to synthetic biology will
provide the foundation for the creation of a high throughput automated platform
for genetic design, in which a learning machine is used to iteratively optimise
the system through a design-build-test-learn (DBTL) cycle. However, mainstream
machine learning techniques represented by deep learning lacks the capability
to represent relational knowledge and requires prodigious amounts of annotated
training data. These drawbacks strongly restrict AI's role in synthetic biology
in which experimentation is inherently resource and time intensive. In this
work, we propose an automated biodesign engineering framework empowered by
Abductive Meta-Interpretive Learning ($Meta_{Abd}$), a novel machine learning
approach that combines symbolic and sub-symbolic machine learning, to further
enhance the DBTL cycle by enabling the learning machine to 1) exploit domain
knowledge and learn human-interpretable models that are expressed by formal
languages such as first-order logic; 2) simultaneously optimise the structure
and parameters of the models to make accurate numerical predictions; 3) reduce
the cost of experiments and effort on data annotation by actively generating
hypotheses and examples. To verify the effectiveness of $Meta_{Abd}$, we have
modelled a synthetic dataset for the production of proteins from a three gene
operon in a microbial host, which represents a common synthetic biology
problem.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07761</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07761</id><submitter>Victor Lopez</submitter><version version="v1"><date>Mon, 17 May 2021 12:13:36 GMT</date><size>91kb</size></version><title>Efficient Off-Policy Q-Learning for Data-Based Discrete-Time LQR
  Problems</title><authors>Victor G. Lopez, Mohammad Alsalti and Matthias A. M\&quot;uller</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces and analyzes an improved Q-learning algorithm for
discrete-time linear time-invariant systems. The proposed method does not
require any knowledge of the system dynamics, and it enjoys significant
efficiency advantages over other data-based optimal control methods in the
literature. This algorithm can be fully executed off-line, as it does not
require to apply the current estimate of the optimal input to the system as in
on-policy algorithms. It is shown that a persistently exciting input, defined
from an easily tested matrix rank condition, guarantees the convergence of the
algorithm. Moreover, the method avoids the use of linear matrix inequalities
(LMIs) for control design, decreasing the corresponding computational
complexity. A data-based method is proposed to design the initial stabilizing
feedback gain that the algorithm requires. Robustness of the algorithm in the
presence of noisy measurements is analyzed. Both theoretical and simulation
comparisons are performed to show the advantages of this algorithm against
other model-free control design methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07762</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07762</id><submitter>Federico Milano</submitter><version version="v1"><date>Mon, 17 May 2021 12:14:01 GMT</date><size>72kb</size></version><title>A Geometrical Interpretation of Frequency</title><authors>Federico Milano</authors><categories>math.DG cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The letter provides a geometrical interpretation of frequency in electric
circuits. According to this interpretation, the frequency is defined as a
multivector with symmetric and antisymmetric components. The conventional
definition of frequency is shown to be a special case of the proposed
theoretical framework. Several examples serve to show the features, generality
as well as practical aspects of the proposed approach.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07763</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07763</id><submitter>Bill Cassidy</submitter><version version="v1"><date>Mon, 17 May 2021 12:15:01 GMT</date><size>776kb</size><source_type>D</source_type></version><title>A Cloud-based Deep Learning Framework for Remote Detection of Diabetic
  Foot Ulcers</title><authors>Bill Cassidy, Neil D. Reeves, Joseph M. Pappachan, Naseer Ahmad,
  Samantha Haycocks, David Gillespie, Moi Hoon Yap</authors><categories>cs.LG cs.CV</categories><comments>10 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research proposes a mobile and cloud-based framework for the automatic
detection of diabetic foot ulcers and conducts an investigation of its
performance. The system uses a cross-platform mobile framework which enables
the deployment of mobile apps to multiple platforms using a single TypeScript
code base. A deep convolutional neural network was deployed to a cloud-based
platform where the mobile app could send photographs of patient's feet for
inference to detect the presence of diabetic foot ulcers. The functionality and
usability of the system were tested in two clinical settings: Salford Royal NHS
Foundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. The
benefits of the system, such as the potential use of the app by patients to
identify and monitor their condition are discussed.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07768</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07768</id><submitter>Massih-Reza Amini</submitter><version version="v1"><date>Mon, 17 May 2021 12:19:22 GMT</date><size>169kb</size><source_type>D</source_type></version><title>Self-Learning for Received Signal Strength Map Reconstruction with
  Neural Architecture Search</title><authors>Aleksandra Malkova, Loic Pauletto, Christophe Villien, Benoit Denis,
  Massih-Reza Amini</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a Neural Network (NN) model based on Neural
Architecture Search (NAS) and self-learning for received signal strength (RSS)
map reconstruction out of sparse single-snapshot input measurements, in the
case where data-augmentation by side deterministic simulations cannot be
performed. The approach first finds an optimal NN architecture and
simultaneously train the deduced model over some ground-truth measurements of a
given (RSS) map. These ground-truth measurements along with the predictions of
the model over a set of randomly chosen points are then used to train a second
NN model having the same architecture. Experimental results show that signal
predictions of this second model outperforms non-learning based interpolation
state-of-the-art techniques and NN models with no architecture search on five
large-scale maps of RSS measurements.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07769</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07769</id><submitter>Federico Milano</submitter><version version="v1"><date>Mon, 17 May 2021 12:25:09 GMT</date><size>1678kb</size><source_type>D</source_type></version><title>Complex Frequency</title><authors>Federico Milano</authors><categories>eess.SY cs.SY math.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper introduces the concept of complex frequency. The imaginary part of
the complex frequency is the variation with respect of a synchronous reference
of the local bus frequency as commonly defined in power system studies. The
real part is defined based on the variation of the voltage magnitude. The
latter term is crucial for the correct interpretation and analysis of the
variation of the frequency at each bus of the network. The paper also develops
a set of differential equations that describe the link between complex powers
and complex frequencies at network buses in transient conditions. No
simplifications are assumed except for constant elements of the network
admittance matrix. A variety of analytical and numerical examples show the
applications and potentials of the proposed concept.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07770</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07770</id><submitter>Martin Vohralik</submitter><version version="v1"><date>Mon, 17 May 2021 12:25:34 GMT</date><size>227kb</size><source_type>D</source_type></version><title>$p$-robust equilibrated flux reconstruction in ${\boldsymbol
  H}(\mathrm{curl})$ based on local minimizations. Application to a posteriori
  analysis of the curl-curl problem</title><authors>Th\'eophile Chaumont-Frelet, Martin Vohral\'ik</authors><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a local construction of H(curl)-conforming piecewise polynomials
satisfying a prescribed curl constraint. We start from a piecewise polynomial
not contained in the H(curl) space but satisfying a suitable orthogonality
property. The procedure employs minimizations in vertex patches and the outcome
is, up to a generic constant independent of the underlying polynomial degree,
as accurate as the best-approximations over the entire local versions of
H(curl). This allows to design guaranteed, fully computable, constant-free, and
polynomial-degree-robust a posteriori error estimates of Prager-Synge type for
N\'ed\'elec finite element approximations of the curl-curl problem. A
divergence-free decomposition of a divergence-free H(div)-conforming piecewise
polynomial, relying on over-constrained minimizations in Raviart-Thomas spaces,
is the key ingredient. Numerical results illustrate the theoretical
developments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07771</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07771</id><submitter>Maria Naumcheva</submitter><version version="v1"><date>Mon, 17 May 2021 12:27:30 GMT</date><size>184kb</size><source_type>D</source_type></version><title>Deep Learning Models in Software Requirements Engineering</title><authors>Maria Naumcheva</authors><categories>cs.SE cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Requirements elicitation is an important phase of any software project: the
errors in requirements are more expensive to fix than the errors introduced at
later stages of software life cycle. Nevertheless, many projects do not devote
sufficient time to requirements. Automated requirements generation can improve
the quality of software projects. In this article we have accomplished the
first step of the research on this topic: we have applied the vanilla sentence
autoencoder to the sentence generation task and evaluated its performance. The
generated sentences are not plausible English and contain only a few meaningful
words. We believe that applying the model to a larger dataset may produce
significantly better results. Further research is needed to improve the quality
of generated data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07773</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07773</id><submitter>Ram\'on Feenstra</submitter><version version="v1"><date>Mon, 17 May 2021 12:36:12 GMT</date><size>466kb</size></version><title>Spanish philosophers perceptions of pay to publish and open access:
  books versus journals, more than a financial dilemma</title><authors>Ramon A. Feenstra and Emilio Delgado Lopez-Cozar</authors><categories>cs.DL</categories><comments>20 pages, 3 tables, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study examines habits and perceptions related to pay to publish and open
access practices in fields that have attracted little research to date:
philosophy and ethics. The study is undertaken in the Spanish context, where
the culture of publication and the book and journal publishing industry has
some specific characteristics with regard to paying to publish, such as not
offering open access distribution of books published for a fee. The study draws
on data from a survey of 201 researchers, a public debate with 26 researchers,
and 14 in-depth interviews. The results reveal some interesting insights on the
criteria researchers apply when selecting publishers and journals for their
work, the extent of paying to publish (widespread in the case of books and
modest for journals) and the debates that arise over the effects it has on
manuscript review and unequal access to resources to cover publication fees.
Data on the extent of open access and the researchers views on dissemination of
publicly funded research are also presented.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07775</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07775</id><submitter>Xiangmeng Wang</submitter><version version="v1"><date>Mon, 17 May 2021 12:38:59 GMT</date><size>9489kb</size><source_type>D</source_type></version><title>Be Causal: De-biasing Social Network Confounding in Recommendation</title><authors>Qian Li, Xiangmeng Wang, Guandong Xu</authors><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recommendation systems, the existence of the missing-not-at-random (MNAR)
problem results in the selection bias issue, degrading the recommendation
performance ultimately. A common practice to address MNAR is to treat missing
entries from the so-called ``exposure'' perspective, i.e., modeling how an item
is exposed (provided) to a user. Most of the existing approaches use heuristic
models or re-weighting strategy on observed ratings to mimic the
missing-at-random setting. However, little research has been done to reveal how
the ratings are missing from a causal perspective. To bridge the gap, we
propose an unbiased and robust method called DENC (De-bias Network Confounding
in Recommendation) inspired by confounder analysis in causal inference. In
general, DENC provides a causal analysis on MNAR from both the inherent factors
(e.g., latent user or item factors) and auxiliary network's perspective.
Particularly, the proposed exposure model in DENC can control the social
network confounder meanwhile preserves the observed exposure information. We
also develop a deconfounding model through the balanced representation learning
to retain the primary user and item features, which enables DENC generalize
well on the rating prediction. Extensive experiments on three datasets validate
that our proposed model outperforms the state-of-the-art baselines.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07776</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07776</id><submitter>Julien Girard-Satabin</submitter><version version="v1"><date>Mon, 17 May 2021 12:40:51 GMT</date><size>500kb</size><source_type>D</source_type></version><title>DISCO Verification: Division of Input Space into COnvex polytopes for
  neural network verification</title><authors>Julien Girard-Satabin (LIST, TAU), Aymeric Varasse (LIST), Marc
  Schoenauer (TAU), Guillaume Charpiat (TAU), Zakaria Chihani (LIST)</authors><categories>cs.AI cs.NE cs.PF</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impressive results of modern neural networks partly come from their non
linear behaviour. Unfortunately, this property makes it very difficult to apply
formal verification tools, even if we restrict ourselves to networks with a
piecewise linear structure. However, such networks yields subregions that are
linear and thus simpler to analyse independently. In this paper, we propose a
method to simplify the verification problem by operating a partitionning into
multiple linear subproblems. To evaluate the feasibility of such an approach,
we perform an empirical analysis of neural networks to estimate the number of
linear regions, and compare them to the bounds currently known. We also present
the impact of a technique aiming at reducing the number of linear regions
during training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07782</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07782</id><submitter>B\'erenger Bramas</submitter><version version="v1"><date>Mon, 17 May 2021 12:48:54 GMT</date><size>284kb</size><source_type>D</source_type></version><title>A fast vectorized sorting implementation based on the ARM scalable
  vector extension (SVE)</title><authors>B\'erenger Bramas</authors><categories>cs.DC</categories><comments>https://gitlab.inria.fr/bramas/arm-sve-sort</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The way developers implement their algorithms and how these implementations
behave on modern CPUs are governed by the design and organization of these. The
vectorization units (SIMD) are among the few CPUs' parts that can and must be
explicitly controlled. In the HPC community, the x86 CPUs and their
vectorization instruction sets were de-facto the standard for decades. Each new
release of an instruction set was usually a doubling of the vector length
coupled with new operations. Each generation was pushing for adapting and
improving previous implementations. The release of the ARM scalable vector
extension (SVE) changed things radically for several reasons. First, we expect
ARM processors to equip many supercomputers in the next years. Second, SVE's
interface is different in several aspects from the x86 extensions as it
provides different instructions, uses a predicate to control most operations,
and has a vector size that is only known at execution time. Therefore, using
SVE opens new challenges on how to adapt algorithms including the ones that are
already well-optimized on x86. In this paper, we port a hybrid sort based on
the well-known Quicksort and Bitonic-sort algorithms. We use a Bitonic sort to
process small partitions/arrays and a vectorized partitioning implementation to
divide the partitions. We explain how we use the predicates and how we manage
the non-static vector size. We explain how we efficiently implement the sorting
kernels. Our approach only needs an array of O(log N) for the recursive calls
in the partitioning phase, both in the sequential and in the parallel case. We
test the performance of our approach on a modern ARMv8.2 and assess the
different layers of our implementation by sorting/partitioning integers, double
floating-point numbers, and key/value pairs of integers. Our approach is faster
than the GNU C++ sort algorithm by a speedup factor of 4 on average.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07784</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07784</id><submitter>Alexandros Dimopoulos</submitter><version version="v1"><date>Mon, 17 May 2021 12:50:17 GMT</date><size>91kb</size></version><title>Multi-output, multi-level, multi-gate design using non-linear
  programming</title><authors>A. C. Dimopoulos, C. Pavlatos, G. Papakonstantinou</authors><categories>cs.AR</categories><comments>14 pages, 5 figues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using logic gates is the traditional way of designing logic circuits.
However, most of the minimization algorithms concern a limited set of gates
(complete sets), like sum of products, exclusive-or sum of products, NAND
gates, NOR gates e.t.c.. In this paper, a method is proposed for minimizing
multi-output Boolean functions using any kind of two-input gates although it
can easily be extended to multi-input gates. The method is based on non-linear
mixed integer programming. The experimental results show that the method gives
the same or better results compared to other methods available in the
literature. However, other methods do not ensure that they produce the minimal
solution, while the main advantages of the proposed method are that it does
guarantee minimality and it can also handle Boolean functions for incompletely
specified functions. The method is general enough and can easily be extended to
more complicated design modules than just basic gates.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07789</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07789</id><submitter>Lukas Drees</submitter><version version="v1"><date>Mon, 17 May 2021 13:00:01 GMT</date><size>9416kb</size><source_type>D</source_type></version><title>Temporal Prediction and Evaluation of Brassica Growth in the Field using
  Conditional Generative Adversarial Networks</title><authors>Lukas Drees, Laura Verena Junker-Frohn, Jana Kierdorf, Ribana Roscher</authors><categories>cs.CV cs.LG cs.NE</categories><comments>38 pages, 10 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Farmers frequently assess plant growth and performance as basis for making
decisions when to take action in the field, such as fertilization, weed
control, or harvesting. The prediction of plant growth is a major challenge, as
it is affected by numerous and highly variable environmental factors. This
paper proposes a novel monitoring approach that comprises high-throughput
imaging sensor measurements and their automatic analysis to predict future
plant growth. Our approach's core is a novel machine learning-based growth
model based on conditional generative adversarial networks, which is able to
predict the future appearance of individual plants. In experiments with RGB
time-series images of laboratory-grown Arabidopsis thaliana and field-grown
cauliflower plants, we show that our approach produces realistic, reliable, and
reasonable images of future growth stages. The automatic interpretation of the
generated images through neural network-based instance segmentation allows the
derivation of various phenotypic traits that describe plant growth.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07791</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07791</id><submitter>Boris Bonev</submitter><version version="v1"><date>Mon, 17 May 2021 13:02:01 GMT</date><size>5814kb</size></version><title>A hierarchical preconditioner for wave problems in quasilinear
  complexity</title><authors>Boris Bonev and Jan S. Hesthaven</authors><categories>math.NA cs.DS cs.NA</categories><msc-class>65F08, 15A23, 65F30, 65F50</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The paper introduces a novel, hierarchical preconditioner based on nested
dissection and hierarchical matrix compression. The preconditioner is intended
for continuous and discontinuous Galerkin formulations of elliptic problems. We
exploit the property that Schur complements arising in such problems can be
well approximated by hierarchical matrices. An approximate factorization can be
computed matrix-free and in a (quasi-)linear number of operations. The nested
dissection is specifically designed to aid the factorization process using
hierarchical matrices. We demonstrate the viability of the preconditioner on a
range of 2D problems, including the Helmholtz equation and the elastic wave
equation. Throughout all tests, including wave phenomena with high wavenumbers,
the generalized minimal residual method (GMRES) with the proposed
preconditioner converges in a very low number of iterations. We demonstrate
that this is due to the hierarchical nature of our approach which makes the
high wavenumber limit manageable.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07795</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07795</id><submitter>Gopi Ramena</submitter><version version="v1"><date>Mon, 17 May 2021 13:06:23 GMT</date><size>4948kb</size><source_type>D</source_type></version><title>STRIDE : Scene Text Recognition In-Device</title><authors>Rachit S Munjal, Arun D Prabhu, Nikhil Arora, Sukumar Moharana, Gopi
  Ramena</authors><categories>cs.CV</categories><comments>accepted in IJCNN 2021</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Optical Character Recognition (OCR) systems have been widely used in various
applications for extracting semantic information from images. To give the user
more control over their privacy, an on-device solution is needed. The current
state-of-the-art models are too heavy and complex to be deployed on-device. We
develop an efficient lightweight scene text recognition (STR) system, which has
only 0.88M parameters and performs real-time text recognition. Attention
modules tend to boost the accuracy of STR networks but are generally slow and
not optimized for device inference. So, we propose the use of convolution
attention modules to the text recognition networks, which aims to provide
channel and spatial attention information to the LSTM module by adding very
minimal computational cost. It boosts our word accuracy on ICDAR 13 dataset by
almost 2\%. We also introduce a novel orientation classifier module, to support
the simultaneous recognition of both horizontal and vertical text. The proposed
model surpasses on-device metrics of inference time and memory footprint and
achieves comparable accuracy when compared to the leading commercial and other
open-source OCR engines. We deploy the system on-device with an inference speed
of 2.44 ms per word on the Exynos 990 chipset device and achieve an accuracy of
88.4\% on ICDAR-13 dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07796</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07796</id><submitter>David Glowacki</submitter><version version="v1"><date>Mon, 17 May 2021 13:07:32 GMT</date><size>5121kb</size></version><title>Dissolving yourself in connection to others: shared experiences of ego
  attenuation and connectedness during group VR experiences can be comparable
  to psychedelics</title><authors>David R. Glowacki, Rhoslyn Roebuck Williams, Olivia M. Maynard, James
  E. Pike, Rachel Freire, Mark D. Wonnacott, Mike Chatziapostolou</authors><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With a growing body of research highlighting the therapeutic potential of
experiential phenomenology which diminishes egoic identity and increases one's
sense of connectedness, there is significant interest in how to elicit such
'self-transcendent experiences' (STEs) in laboratory contexts. Psychedelic
drugs (YDs) have proven particularly effective in this respect, producing
subjective phenomenology which reliably elicits intense STEs. With virtual
reality (VR) emerging as a powerful tool for constructing new perceptual
environments, we describe a VR framework called 'Isness-distributed' (Isness-D)
which harnesses the unique affordances of distributed multi-person VR to blur
conventional self-other boundaries. Within Isness-D, groups of participants
co-habit a shared virtual space, collectively experiencing their bodies as
luminous energetic essences with diffuse spatial boundaries. It enables moments
of 'energetic coalescence', a new class of embodied phenomenological
intersubjective experience where bodies can fluidly merge, enabling
participants to have an experience of including multiple others within their
self-representation. To evaluate Isness-D, we adopted a citizen science
approach, coordinating an international network of Isness-D 'nodes'. We
analyzed the results (N = 58) using 4 different self-report scales previously
applied to analyze subjective YD phenomenology (the inclusion of community in
self scale, ego-dissolution inventory, communitas scale, and the MEQ30 mystical
experience questionnaire). Despite the complexities associated with a
distributed experiment like this, the Isness-D scores on all 4 scales were
statistically indistinguishable from recently published YD studies,
demonstrating that distributed VR can be used to design intersubjective STEs
where people dissolve their sense of self in the connection to others.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07797</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07797</id><submitter>Taro Langner</submitter><version version="v1"><date>Mon, 17 May 2021 13:12:20 GMT</date><size>2711kb</size><source_type>D</source_type></version><title>Deep regression for uncertainty-aware and interpretable analysis of
  large-scale body MRI</title><authors>Taro Langner, Robin Strand, H{\aa}kan Ahlstr\&quot;om, Joel Kullberg</authors><categories>eess.IV cs.CV cs.LG</categories><comments>Presented at the Swedish Symposium on Deep Learning 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large-scale medical studies such as the UK Biobank examine thousands of
volunteer participants with medical imaging techniques. Combined with the vast
amount of collected metadata, anatomical information from these images has the
potential for medical analyses at unprecedented scale. However, their
evaluation often requires manual input and long processing times, limiting the
amount of reference values for biomarkers and other measurements available for
research. Recent approaches with convolutional neural networks for regression
can perform these evaluations automatically. On magnetic resonance imaging
(MRI) data of more than 40,000 UK Biobank subjects, these systems can estimate
human age, body composition and more. This style of analysis is almost entirely
data-driven and no manual intervention or guidance with manually segmented
ground truth images is required. The networks often closely emulate the
reference method that provided their training data and can reach levels of
agreement comparable to the expected variability between established medical
gold standard techniques. The risk of silent failure can be individually
quantified by predictive uncertainty obtained from a mean-variance criterion
and ensembling. Saliency analysis furthermore enables an interpretation of the
underlying relevant image features and showed that the networks learned to
correctly target specific organs, limbs, and regions of interest.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07799</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07799</id><submitter>Mona Fuhrl\&quot;ander</submitter><version version="v1"><date>Mon, 17 May 2021 13:14:49 GMT</date><size>11kb</size></version><title>Efficient yield optimization with limited gradient information</title><authors>Mona Fuhrl\&quot;ander and Sebastian Sch\&quot;ops</authors><categories>cs.CE</categories><msc-class>60G15, 60H35, 65K99, 78M31,</msc-class><acm-class>G.1.1; G.1.2; G.1.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work an efficient strategy for yield optimization with uncertain and
deterministic optimization variables is presented. The gradient based adaptive
Newton-Monte Carlo method is modified, such that it can handle variables with
(uncertain parameters) and without (deterministic parameters) analytical
gradient information. This mixed strategy is numerically compared to derivative
free approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07800</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07800</id><submitter>Lin Wu</submitter><version version="v1"><date>Mon, 17 May 2021 13:14:52 GMT</date><size>4812kb</size><source_type>D</source_type></version><title>Multi-modal Visual Place Recognition in Dynamics-Invariant Perception
  Space</title><authors>Lin Wu, Teng Wang, Changyin Sun</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Visual place recognition is one of the essential and challenging problems in
the fields of robotics. In this letter, we for the first time explore the use
of multi-modal fusion of semantic and visual modalities in dynamics-invariant
space to improve place recognition in dynamic environments. We achieve this by
first designing a novel deep learning architecture to generate the static
semantic segmentation and recover the static image directly from the
corresponding dynamic image. We then innovatively leverage the
spatial-pyramid-matching model to encode the static semantic segmentation into
feature vectors. In parallel, the static image is encoded using the popular
Bag-of-words model. On the basis of the above multi-modal features, we finally
measure the similarity between the query image and target landmark by the joint
similarity of their semantic and visual codes. Extensive experiments
demonstrate the effectiveness and robustness of the proposed approach for place
recognition in dynamic environments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07804</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07804</id><submitter>Juliana Ferreira J</submitter><version version="v1"><date>Mon, 17 May 2021 13:18:57 GMT</date><size>257kb</size></version><title>Designer-User Communication for XAI: An epistemological approach to
  discuss XAI design</title><authors>Juliana Jansen Ferreira and Mateus Monteiro</authors><categories>cs.AI cs.HC</categories><comments>ACM CHI Workshop on Operationalizing Human-Centered Perspectives in
  Explainable AI at CHI 2021. 6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Artificial Intelligence is becoming part of any technology we use nowadays.
If the AI informs people's decisions, the explanation about AI's outcomes,
results, and behavior becomes a necessary capability. However, the discussion
of XAI features with various stakeholders is not a trivial task. Most of the
available frameworks and methods for XAI focus on data scientists and ML
developers as users. Our research is about XAI for end-users of AI systems. We
argue that we need to discuss XAI early in the AI-system design process and
with all stakeholders. In this work, we aimed at investigating how to
operationalize the discussion about XAI scenarios and opportunities among
designers and developers of AI and its end-users. We took the Signifying
Message as our conceptual tool to structure and discuss XAI scenarios. We
experiment with its use for the discussion of a healthcare AI-System.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07806</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07806</id><submitter>Jiawei Jiang</submitter><version version="v1"><date>Mon, 17 May 2021 13:19:23 GMT</date><size>4411kb</size><source_type>D</source_type></version><title>Towards Demystifying Serverless Machine Learning Training</title><authors>Jiawei Jiang, Shaoduo Gan, Yue Liu, Fanlin Wang, Gustavo Alonso, Ana
  Klimovic, Ankit Singla, Wentao Wu, Ce Zhang</authors><categories>cs.DC cs.LG</categories><doi>10.1145/3448016.3459240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The appeal of serverless (FaaS) has triggered a growing interest on how to
use it in data-intensive applications such as ETL, query processing, or machine
learning (ML). Several systems exist for training large-scale ML models on top
of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results
in terms of their performance and relative advantage over &quot;serverful&quot;
infrastructures (IaaS). In this paper we present a systematic, comparative
study of distributed ML training over FaaS and IaaS. We present a design space
covering design choices such as optimization algorithms and synchronization
protocols, and implement a platform, LambdaML, that enables a fair comparison
between FaaS and IaaS. We present experimental results using LambdaML, and
further develop an analytic model to capture cost/performance tradeoffs that
must be considered when opting for a serverless infrastructure. Our results
indicate that ML training pays off in serverless only for models with efficient
(i.e., reduced) communication and that quickly converge. In general, FaaS can
be much faster but it is never significantly cheaper than IaaS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07809</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07809</id><submitter>Andrey Ignatov</submitter><version version="v1"><date>Mon, 17 May 2021 13:20:35 GMT</date><size>5364kb</size><source_type>D</source_type></version><title>Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021
  Challenge: Report</title><authors>Andrey Ignatov, Cheng-Ming Chiang, Hsien-Kai Kuo, Anastasia Sycheva,
  Radu Timofte, Min-Hung Chen, Man-Yu Lee, Yu-Syuan Xu, Yu Tseng, Shusong Xu,
  Jin Guo, Chao-Hung Chen, Ming-Chun Hsyu, Wen-Chia Tsai, Chao-Wei Chen,
  Grigory Malivenko, Minsu Kwon, Myungje Lee, Jaeyoon Yoo, Changbeom Kang,
  Shinjo Wang, Zheng Shaolong, Hao Dejun, Xie Fen, Feng Zhuang, Yipeng Ma,
  Jingyang Peng, Tao Wang, Fenglong Song, Chih-Chung Hsu, Kwan-Lin Chen,
  Mei-Hsuang Wu, Vishal Chudasama, Kalpesh Prajapati, Heena Patel, Anjali
  Sarvaiya, Kishor Upla, Kiran Raja, Raghavendra Ramachandra, Christoph Busch,
  Etienne de Stoutz</authors><categories>eess.IV cs.CV cs.LG</categories><comments>Mobile AI 2021 Workshop and Challenges:
  https://ai-benchmark.com/workshops/mai/2021/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the quality of mobile cameras starts to play a crucial role in modern
smartphones, more and more attention is now being paid to ISP algorithms used
to improve various perceptual aspects of mobile photos. In this Mobile AI
challenge, the target was to develop an end-to-end deep learning-based image
signal processing (ISP) pipeline that can replace classical hand-crafted ISPs
and achieve nearly real-time performance on smartphone NPUs. For this, the
participants were provided with a novel learned ISP dataset consisting of
RAW-RGB image pairs captured with the Sony IMX586 Quad Bayer mobile sensor and
a professional 102-megapixel medium format camera. The runtime of all models
was evaluated on the MediaTek Dimensity 1000+ platform with a dedicated AI
processing unit capable of accelerating both floating-point and quantized
neural networks. The proposed solutions are fully compatible with the above NPU
and are capable of processing Full HD photos under 60-100 milliseconds while
achieving high fidelity results. A detailed description of all models developed
in this challenge is provided in this paper.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07811</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07811</id><submitter>Alexander Bauer</submitter><version version="v1"><date>Wed, 28 Apr 2021 07:02:24 GMT</date><size>1322kb</size></version><title>Mundus vult decipi, ergo decipiatur: Visual Communication of Uncertainty
  in Election Polls</title><authors>Alexander Bauer, Andr\'e Klima, Jana Gau{\ss}, Hannah K\&quot;umpel,
  Andreas Bender, Helmut K\&quot;uchenhoff</authors><categories>cs.CY stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Election poll reporting often focuses on mean values and only subordinately
discusses the underlying uncertainty. Subsequent interpretations are too often
phrased as certain. Moreover, media coverage rarely adequately takes into
account the differences between now- and forecasts. These challenges were
ubiquitous in the context of the 2016 and 2020 U.S. presidential elections, but
are also present in multi-party systems like Germany. We discuss potential
sources of bias in nowcasting and forecasting and review the current standards
in the visual presentation of survey-based nowcasts. Concepts are presented to
attenuate the issue of falsely perceived accuracy. We discuss multiple visual
presentation techniques for central aspects in poll reporting. One key idea is
the use of Probabilities of Events instead of party shares. The presented ideas
offer modern and improved ways to communicate (changes in) the electoral mood
for the general media.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07812</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07812</id><submitter>Mehdi Dadvar</submitter><version version="v1"><date>Wed, 28 Apr 2021 05:35:45 GMT</date><size>1780kb</size></version><title>Contemporary Research Trends in Response Robotics</title><authors>Mehdi Dadvar, Soheil Habibian</authors><categories>cs.CY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multidisciplinary nature of response robotics has brought about a
diversified research community with extended expertise. Motivated by the recent
accelerated rate of publications in the field, this paper analyzes the
technical content, statistics, and implications of the literature from
bibliometric standpoints. The aim is to study the global progress of response
robotics research and identify the contemporary trends. To that end, we
investigated the collaboration mapping together with the citation network to
formally recognize impactful and contributing authors, publications, sources,
institutions, funding agencies, and countries. We found how natural and
human-made disasters contributed to forming productive regional research
communities, while there are communities that only view response robotics as an
application of their research. Furthermore, through an extensive discussion on
the bibliometric results, we elucidated the philosophy behind research priority
shifts in response robotics and presented our deliberations on future research
directions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07814</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07814</id><submitter>Joana Castellar J.A.C Castellar</submitter><version version="v1"><date>Tue, 27 Apr 2021 15:24:00 GMT</date><size>1992kb</size></version><title>Nature-based solutions in the urban context: terminology, classification
  and scoring for urban challenges and ecosystem services</title><authors>Joana.A.C. Castellar, Lucia .A. Popartan, Josep Pueyo-Ros, Natasa
  Atanasova, Gunter Langergraber, Ina Saumel, Lluis Corominas, Joaquim Comas,
  Vicenc Acuna</authors><categories>cs.CY physics.soc-ph</categories><journal-ref>Science of the Total Environment 779 (2021) 146237 Contents</journal-ref><doi>10.1016/j.scitotenv.2021.146237</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The concept of Nature-Based Solutions (NBS) has emerged to foster sustainable
development by transversally addressing social, economic, and environmental
urban challenges. However, there is still a considerable lack of agreement on
the conceptualization of NBS, especially concerning typologies, nomenclature,
and performance assessments in terms of ecosystem services (ES) and urban
challenges (UC). Therefore, this article consolidates the knowledge from 4
European projects to set a path for a common understanding of NBS and thus,
facilitate their mainstreaming.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07815</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07815</id><submitter>Niclas Boehmer</submitter><version version="v1"><date>Mon, 17 May 2021 13:28:39 GMT</date><size>505kb</size><source_type>D</source_type></version><title>Putting a Compass on the Map of Elections</title><authors>Niclas Boehmer, Robert Bredereck, Piotr Faliszewski, Rolf Niedermeier,
  Stanis{\l}aw Szufa</authors><categories>cs.GT econ.EM</categories><comments>Accepted to IJCAI 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Szufa et al. [AAMAS 2020] presented a &quot;map of elections&quot; that
visualizes a set of 800 elections generated from various statistical cultures.
While similar elections are grouped together on this map, there is no obvious
interpretation of the elections' positions. We provide such an interpretation
by introducing four canonical &quot;extreme&quot; elections, acting as a compass on the
map. We use them to analyze both a dataset provided by Szufa et al. and a
number of real-life elections. In effect, we find a new variant of the Mallows
model and show that it captures real-life scenarios particularly well.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07825</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07825</id><submitter>Andrey Ignatov</submitter><version version="v1"><date>Mon, 17 May 2021 13:34:15 GMT</date><size>2677kb</size><source_type>D</source_type></version><title>Real-Time Quantized Image Super-Resolution on Mobile NPUs, Mobile AI
  2021 Challenge: Report</title><authors>Andrey Ignatov, Radu Timofte, Maurizio Denna, Abdel Younes, Andrew
  Lek, Mustafa Ayazoglu, Jie Liu, Zongcai Du, Jiaming Guo, Xueyi Zhou, Hao Jia,
  Youliang Yan, Zexin Zhang, Yixin Chen, Yunbo Peng, Yue Lin, Xindong Zhang,
  Hui Zeng, Kun Zeng, Peirong Li, Zhihuang Liu, Shiqi Xue, Shengpeng Wang</authors><categories>eess.IV cs.CV cs.LG</categories><comments>Mobile AI 2021 Workshop and Challenges:
  https://ai-benchmark.com/workshops/mai/2021/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image super-resolution is one of the most popular computer vision problems
with many important applications to mobile devices. While many solutions have
been proposed for this task, they are usually not optimized even for common
smartphone AI hardware, not to mention more constrained smart TV platforms that
are often supporting INT8 inference only. To address this problem, we introduce
the first Mobile AI challenge, where the target is to develop an end-to-end
deep learning-based image super-resolution solutions that can demonstrate a
real-time performance on mobile or edge NPUs. For this, the participants were
provided with the DIV2K dataset and trained quantized models to do an efficient
3X image upscaling. The runtime of all models was evaluated on the Synaptics
VS680 Smart Home board with a dedicated NPU capable of accelerating quantized
neural networks. The proposed solutions are fully compatible with all major
mobile AI accelerators and are capable of reconstructing Full HD images under
40-60 ms while achieving high fidelity results. A detailed description of all
models developed in the challenge is provided in this paper.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07826</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07826</id><submitter>Jamal Al Qundus</submitter><version version="v1"><date>Mon, 17 May 2021 13:34:45 GMT</date><size>1421kb</size><source_type>D</source_type></version><title>TopicsRanksDC: Distance-based Topic Ranking applied on Two-Class Data</title><authors>Malik Yousef, Jamal Al Qundus, Silvio Peikert, and Adrian Paschke</authors><categories>cs.IR cs.AI cs.LG</categories><comments>10 pages, 5 figures</comments><journal-ref>International Conference on Database and Expert Systems
  Applications DEXA 2020: Database and Expert Systems Applications pp 11-21</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we introduce a novel approach named TopicsRanksDC for topics
ranking based on the distance between two clusters that are generated by each
topic. We assume that our data consists of text documents that are associated
with two-classes. Our approach ranks each topic contained in these text
documents by its significance for separating the two-classes. Firstly, the
algorithm detects topics using Latent Dirichlet Allocation (LDA). The words
defining each topic are represented as two clusters, where each one is
associated with one of the classes. We compute four distance metrics, Single
Linkage, Complete Linkage, Average Linkage and distance between the centroid.
We compare the results of LDA topics and random topics. The results show that
the rank for LDA topics is much higher than random topics. The results of
TopicsRanksDC tool are promising for future work to enable search engines to
suggest related topics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07829</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07829</id><submitter>Yuchen Zhong</submitter><version version="v1"><date>Mon, 17 May 2021 13:41:47 GMT</date><size>3303kb</size><source_type>D</source_type></version><title>Compressed Communication for Distributed Training: Adaptive Methods and
  System</title><authors>Yuchen Zhong, Cong Xie, Shuai Zheng, Haibin Lin</authors><categories>cs.DC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication overhead severely hinders the scalability of distributed
machine learning systems. Recently, there has been a growing interest in using
gradient compression to reduce the communication overhead of the distributed
training. However, there is little understanding of applying gradient
compression to adaptive gradient methods. Moreover, its performance benefits
are often limited by the non-negligible compression overhead. In this paper, we
first introduce a novel adaptive gradient method with gradient compression. We
show that the proposed method has a convergence rate of
$\mathcal{O}(1/\sqrt{T})$ for non-convex problems. In addition, we develop a
scalable system called BytePS-Compress for two-way compression, where the
gradients are compressed in both directions between workers and parameter
servers. BytePS-Compress pipelines the compression and decompression on CPUs
and achieves a high degree of parallelism. Empirical evaluations show that we
improve the training time of ResNet50, VGG16, and BERT-base by 5.0%, 58.1%,
23.3%, respectively, without any accuracy loss with 25 Gb/s networking.
Furthermore, for training the BERT models, we achieve a compression rate of
333x compared to the mixed-precision training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07830</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07830</id><submitter>Suman Saha</submitter><version version="v1"><date>Mon, 17 May 2021 13:42:09 GMT</date><size>10886kb</size><source_type>D</source_type></version><title>Learning to Relate Depth and Semantics for Unsupervised Domain
  Adaptation</title><authors>Suman Saha, Anton Obukhov, Danda Pani Paudel, Menelaos Kanakis, Yuhua
  Chen, Stamatios Georgoulis, Luc Van Gool</authors><categories>cs.CV cs.LG</categories><comments>Accepted at CVPR 2021</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We present an approach for encoding visual task relationships to improve
model performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic
segmentation and monocular depth estimation are shown to be complementary
tasks; in a multi-task learning setting, a proper encoding of their
relationships can further improve performance on both tasks. Motivated by this
observation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes
task dependencies between the semantic and depth predictions. To capture the
cross-task relationships, we propose a neural network architecture that
contains task-specific and cross-task refinement heads. Furthermore, we propose
an Iterative Self-Learning (ISL) training scheme, which exploits semantic
pseudo-labels to provide extra supervision on the target domain. We
experimentally observe improvements in both tasks' performance because the
complementary information present in these tasks is better captured.
Specifically, we show that: (1) our approach improves performance on all tasks
when they are complementary and mutually dependent; (2) the CTRL helps to
improve both semantic segmentation and depth estimation tasks performance in
the challenging UDA setting; (3) the proposed ISL training scheme further
improves the semantic segmentation performance. The implementation is available
at https://github.com/susaha/ctrl-uda.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07831</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07831</id><submitter>Hangcheng Dong</submitter><version version="v1"><date>Mon, 17 May 2021 13:43:37 GMT</date><size>2877kb</size><source_type>D</source_type></version><title>How to Explain Neural Networks: A perspective of data space division</title><authors>Hangcheng Dong, Bingguo Liu, Fengdong Chen, Dong Ye and Guodong Liu</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Interpretability of intelligent algorithms represented by deep learning has
been yet an open problem. We discuss the shortcomings of the existing
explainable method based on the two attributes of explanation, which are called
completeness and explicitness. Furthermore, we point out that a model that
completely relies on feed-forward mapping is extremely easy to cause
inexplicability because it is hard to quantify the relationship between this
mapping and the final model. Based on the perspective of the data space
division, the principle of complete local interpretable model-agnostic
explanations (CLIMEP) is proposed in this paper. To study the classification
problems, we further discussed the equivalence of the CLIMEP and the decision
boundary. As a matter of fact, it is also difficult to implementation of
CLIMEP. To tackle the challenge, motivated by the fact that a fully-connected
neural network (FCNN) with piece-wise linear activation functions (PWLs) can
partition the input space into several linear regions, we extend this result to
arbitrary FCNNs by the strategy of linearizing the activation functions.
Applying this technique to solving classification problems, it is the first
time that the complete decision boundary of FCNNs has been able to be obtained.
Finally, we propose the DecisionNet (DNet), which divides the input space by
the hyper-planes of the decision boundary. Hence, each linear interval of the
DNet merely contains samples of the same label. Experiments show that the
surprising model compression efficiency of the DNet with an arbitrary
controlled precision.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07837</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07837</id><submitter>Rohith Mekala</submitter><version version="v1"><date>Wed, 28 Apr 2021 21:35:43 GMT</date><size>1263kb</size></version><title>An Experimental Analysis of Work-Life Balance Among The Employees using
  Machine Learning Classifiers</title><authors>Karampudi Radha, Mekala Rohith</authors><categories>cs.CY cs.LG</categories><comments>10 pages, 16 figures, Published with International Journal of
  Computer Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  69(4):39-48, April 2021. ISSN: 2231-2803</journal-ref><doi>10.14445/22312803/IJCTT-V69I4P108</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Researchers today have found out the importance of Artificial Intelligence,
and Machine Learning in our daily lives, as well as they can be used to improve
the quality of our lives as well as the cities and nations alike. An example of
this is that it is currently speculated that ML can provide ways to relieve
workers as it can predict effective working schedules and patterns which
increase the efficiency of the workers. Ultimately this is leading to a
Work-Life Balance for the workers. But how is this possible? It is practically
possible with the Machine Learning algorithms to predict, calculate the factors
affecting the feelings of the worker's work-life balance. In order to actually
do this, a sizeable amount of 12,756 people's data has been taken under
consideration. Upon analysing the data and calculating under various factors,
we have found out the correlation of various factors and WLB(Work-Life Balance
in short). There are some factors that have to be taken into serious
consideration as they play a major role in WLB. We have trained 80% of our data
with Random Forest Classifier, SVM and Naive Bayes algorithms. Upon testing,
the algorithms predict the WLB with 71.5% as the best accuracy.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07838</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07838</id><submitter>Yang Lu</submitter><version version="v1"><date>Thu, 29 Apr 2021 02:25:30 GMT</date><size>1244kb</size></version><title>Digital Resistance during COVID-19: A Workflow Management System of
  Contactless Purchasing and Its Empirical Study of Customer Acceptance</title><authors>Yang Lu</authors><categories>cs.CY cs.SY eess.SY</categories><comments>5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The COVID-19 pandemic has stimulated the shift of work and life from the
physical to a more digital format. To survive and thrive, companies have
integrated more digital-enabled elements into their businesses to facilitate
resilience, by avoiding potential close physical contact. Following Design
Science Research Methodology (DSRM), this paper builds a workflow management
system for contactless digital resilience when customers are purchasing in a
store. Customer behavior, in coping with digital resilience against COVID-19,
is illustrated and empirically tested, using a derivative model in which the
constructs are from classical theories. Data was collected from individual
customers via the Internet, and 247 completed questionnaires were examined.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07839</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07839</id><submitter>Prabath Abeysiriwardana</submitter><version version="v1"><date>Thu, 29 Apr 2021 15:08:41 GMT</date><size>711kb</size></version><title>&quot;Connected Researches&quot; in &quot;Smart Lab Bubble&quot;: A Lifeline for Commercial
  Agriculture in &quot;New Normal&quot;</title><authors>Prabath Chaminda Abeysiriwardana, Udith K. Jayasinghe-Mudalige, Saluka
  R. Kodituwakku</authors><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Research in commercial agriculture is the best strategy that can be adopted
by a country to keep on track of the second sustainable goal -- zero hunger by
2030. Analyzing the drawbacks of present research environment and find
solutions through digital intervention would be ideal solution to de-isolate
the research out come in light of disruptions caused by the Covid pandemic. The
performance of the research institutes is not expected to remain the same and
would prefer to be stagnated at a lower level. The right evacuation plan that
could be worked out by establishing connected research through the digital
solution and followed by digitally endorsed performance monitoring and
evaluation would be saviour for keeping the research in commercial agriculture
live at this pandemic. This paper will discuss what are the problems in
carrying out research in commercial agriculture and propose a conceptual model
to connect research beyond physical presence by digital transformations in
organization design of research institutes in light of Covid-19. Further,
digitally endorsed performance measurements and evaluation is envisaged in a
digitally empowered connected lab complex -- &quot;Smart Lab Bubble&quot; that is further
facilitated through policy measures. The connected lab complex called the
&quot;Smart Lab Bubble&quot; concept we present here could be viewed or applied in
different perspectives to engineer the real need of the time for the
sustainability of research in commercial agriculture. Further, it could be
adopted in research in other life science areas.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07841</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07841</id><submitter>Bolu John Folayan Dr.</submitter><version version="v1"><date>Thu, 29 Apr 2021 18:00:04 GMT</date><size>308kb</size></version><title>Post-war Civil War Propaganda Techniques and Media Spins in Nigeria and
  Journalism Practice</title><authors>Bolu John Folayan, Olumide Samuel Ogunjobi, Prosper Zannu, Taiwo
  Ajibolu Balofin</authors><categories>cs.CY cs.MM physics.soc-ph</categories><doi>10.24297/jssr.v17i.8993</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In public relations and political communication, a spin is a form of
propaganda achieved through knowingly presenting a biased interpretation of an
event or issues. It is also the act of presenting narratives to influence
public opinion about events, people or and ideas. In war time, various forms of
spins are employed by antagonists to push their brigades to victory and wear
out the opponents. During the Nigerian civil war, quite a number of these spins
were dominant for example GOWON (Go On With One Nigeria); On Aburi We Stand, O
Le Ku Ija Ore. Post-war years presented different spins and fifty years after
the war, different spins continue to push emerging narratives (e.g.
marginalization, restructuring. This paper investigates and analyzes the
different propaganda techniques and spins in the narratives of the Nigerian
civil in the past five years through a content analysis of three national
newspapers: The Nigerian Tribune, Daily Trust and Sun Newspapers. Findings
confirm that propaganda and spins are not limited to war time, but are actively
deployed in peace time. This development places additional challenge on
journalists to uphold the canons of balance, truth and fairness in reporting
sensitive national issues.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07844</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07844</id><submitter>David Leslie</submitter><version version="v1"><date>Fri, 30 Apr 2021 17:23:07 GMT</date><size>1231kb</size></version><title>Does &quot;AI&quot; stand for augmenting inequality in the era of covid-19
  healthcare?</title><authors>David Leslie, Anjali Mazumder, Aidan Peppin, Maria Wolters and Alexa
  Hagerty</authors><categories>cs.CY cs.LG</categories><journal-ref>bmj, 372 (2021)</journal-ref><doi>10.1136/bmj.n304</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Among the most damaging characteristics of the covid-19 pandemic has been its
disproportionate effect on disadvantaged communities. As the outbreak has
spread globally, factors such as systemic racism, marginalisation, and
structural inequality have created path dependencies that have led to poor
health outcomes. These social determinants of infectious disease and
vulnerability to disaster have converged to affect already disadvantaged
communities with higher levels of economic instability, disease exposure,
infection severity, and death. Artificial intelligence (AI) technologies are an
important part of the health informatics toolkit used to fight contagious
disease. AI is well known, however, to be susceptible to algorithmic biases
that can entrench and augment existing inequality. Uncritically deploying AI in
the fight against covid-19 thus risks amplifying the pandemic's adverse effects
on vulnerable groups, exacerbating health inequity. In this paper, we claim
that AI systems can introduce or reflect bias and discrimination in three ways:
in patterns of health discrimination that become entrenched in datasets, in
data representativeness, and in human choices made during the design,
development, and deployment of these systems. We highlight how the use of AI
technologies threaten to exacerbate the disparate effect of covid-19 on
marginalised, under-represented, and vulnerable groups, particularly black,
Asian, and other minoritised ethnic people, older populations, and those of
lower socioeconomic status. We conclude that, to mitigate the compounding
effects of AI on inequalities associated with covid-19, decision makers,
technology developers, and health officials must account for the potential
biases and inequities at all stages of the AI process.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07845</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07845</id><submitter>Yasir K{\i}l{\i}\c{c}</submitter><version version="v1"><date>Sat, 1 May 2021 06:00:51 GMT</date><size>1001kb</size></version><title>Shared data granularity: A latent dimension of privacy scoring over
  online social networks</title><authors>Yasir Kilic</authors><categories>cs.CR cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Privacy scoring aims at measuring the privacy violation risk of a user over
an online social network (OSN). Existing work in the field rely on possibly
biased or emotional survey data and focus only on personel purpose OSNs like
Facebook. In contrast to existing work, in this thesis, we work with real-world
OSN data collected from LinkedIn, the most popular professional-purpose OSN
(ProOSN). Towards this end, we developed an extensive crawler to collect all
relevant profile data of 5,389 LinkedIn users, modelled these data using both
relational and graph databases and quantitatively analyzed all privacy risk
scoring methods in the literature. Additionally, we propose a novel scoring
method that consider the granularity of data an OSN user shares on her profile
page. Extensive experimental evaluation of existing and proposed scoring
methods indicates the effectiveness of the proposed solution.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07847</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07847</id><submitter>Beatrice Alex</submitter><version version="v1"><date>Mon, 3 May 2021 09:38:26 GMT</date><size>12379kb</size><source_type>D</source_type></version><title>The Online Pivot: Lessons Learned from Teaching a Text and Data Mining
  Course in Lockdown, Enhancing online Teaching with Pair Programming and
  Digital Badges</title><authors>Beatrice Alex, Clare Llewellyn, Pawel Michal Orzechowski, Maria
  Boutchkova</authors><categories>cs.CY cs.CL</categories><comments>11 pages, 4 figures, to appear in the Proceedings of the Fifth
  Workshop on Teaching NLP @ NAACL 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we provide an account of how we ported a text and data mining
course online in summer 2020 as a result of the COVID-19 pandemic and how we
improved it in a second pilot run. We describe the course, how we adapted it
over the two pilot runs and what teaching techniques we used to improve
students' learning and community building online. We also provide information
on the relentless feedback collected during the course which helped us to adapt
our teaching from one session to the next and one pilot to the next. We discuss
the lessons learned and promote the use of innovative teaching techniques
applied to the digital such as digital badges and pair programming in break-out
rooms for teaching Natural Language Processing courses to beginners and
students with different backgrounds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07850</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07850</id><submitter>Manex Agirrezabal</submitter><version version="v1"><date>Tue, 4 May 2021 11:21:03 GMT</date><size>461kb</size><source_type>D</source_type></version><title>The Flipped Classroom model for teaching Conditional Random Fields in an
  NLP course</title><authors>Manex Agirrezabal</authors><categories>cs.CY cs.AI cs.CL</categories><comments>Accepted to the 5th Workshop on Teaching NLP at NAACL-HLT 2021</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this article, we show and discuss our experience in applying the flipped
classroom method for teaching Conditional Random Fields in a Natural Language
Processing course. We present the activities that we developed together with
their relationship to a cognitive complexity model (Bloom's taxonomy). After
this, we provide our own reflections and expectations of the model itself.
Based on the evaluation got from students, it seems that students learn about
the topic and also that the method is rewarding for some students.
Additionally, we discuss some shortcomings and we propose possible solutions to
them. We conclude the paper with some possible future work.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07852</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07852</id><submitter>Bryce Goodman</submitter><version version="v1"><date>Tue, 4 May 2021 22:56:04 GMT</date><size>411kb</size></version><title>Hard Choices and Hard Limits for Artificial Intelligence</title><authors>Bryce Goodman</authors><categories>cs.CY cs.AI cs.LG</categories><acm-class>K.4</acm-class><doi>10.1145/3461702.3462539</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Artificial intelligence (AI) is supposed to help us make better choices. Some
of these choices are small, like what route to take to work, or what music to
listen to. Others are big, like what treatment to administer for a disease or
how long to sentence someone for a crime. If AI can assist with these big
decisions, we might think it can also help with hard choices, cases where
alternatives are neither better, worse nor equal but on a par. The aim of this
paper, however, is to show that this view is mistaken: the fact of parity shows
that there are hard limits on AI in decision making and choices that AI cannot,
and should not, resolve.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07854</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07854</id><submitter>Filipo Sharevski</submitter><version version="v1"><date>Thu, 6 May 2021 15:26:48 GMT</date><size>168kb</size><source_type>D</source_type></version><title>&quot;Hey Alexa, What do You Know About the COVID-19 Vaccine?&quot; --
  (Mis)perceptions of Mass Immunization Among Voice Assistant Users</title><authors>Filipo Sharevski, Anna Slowinski, Peter Jachim, Emma Pieroni</authors><categories>cs.CY cs.CR cs.HC</categories><comments>arXiv admin note: text overlap with arXiv:2104.04077</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we analyzed the perceived accuracy of COVID-19 vaccine
information spoken back by Amazon Alexa. Unlike social media, Amazon Alexa
doesn't apply soft moderation to unverified content, allowing for use of
third-party malicious skills to arbitrarily phrase COVID-19 vaccine
information. The results from a 210-participant study suggest that a
third-party malicious skill could successful reduce the perceived accuracy
among the users of information as to who gets the vaccine first, vaccine
testing, and the side effects of the vaccine. We also found that the
vaccine-hesitant participants are drawn to pessimistically rephrased Alexa
responses focused on the downsides of the mass immunization. We discuss
solutions for soft moderation against misperception-inducing or altogether
COVID-19 misinformation malicious third-party skills.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07855</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07855</id><submitter>A Mallikarjuna Reddy dr</submitter><version version="v1"><date>Fri, 7 May 2021 07:35:23 GMT</date><size>595kb</size></version><title>An Extensive Analytical Approach on Human Resources using Random Forest
  Algorithm</title><authors>Swarajya lakshmi v papineni, A.Mallikarjuna Reddy, Sudeepti
  yarlagadda, Snigdha Yarlagadda, Haritha Akkinen</authors><categories>cs.CY cs.AI</categories><doi>10.14445/22315381/IJETT-V69I5P217</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The current job survey shows that most software employees are planning to
change their job role due to high pay for recent jobs such as data scientists,
business analysts and artificial intelligence fields. The survey also indicated
that work life imbalances, low pay, uneven shifts and many other factors also
make employees think about changing their work life. In this paper, for an
efficient organisation of the company in terms of human resources, the proposed
system designed a model with the help of a random forest algorithm by
considering different employee parameters. This helps the HR department retain
the employee by identifying gaps and helping the organisation to run smoothly
with a good employee retention ratio. This combination of HR and data science
can help the productivity, collaboration and well-being of employees of the
organisation. It also helps to develop strategies that have an impact on the
performance of employees in terms of external and social factors.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07856</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07856</id><submitter>Edward Simmons</submitter><version version="v1"><date>Fri, 7 May 2021 10:08:47 GMT</date><size>136kb</size></version><title>Correlations Between Learning Environments and Dropout Intention</title><authors>Edward Simmons</authors><categories>cs.CY</categories><doi>10.13140/RG.2.2.28550.50245</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This research is comparing learning environments to students dropout
intentions. While using statistics I looked at data and the correlations
between two articles to see how the two studies looked side to side. Learning
environments and dropout intentions can both have vary effects on students.
They can both determine if a student does well, or bad in school especially
math.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07857</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07857</id><submitter>Abbas Najafizadeh</submitter><version version="v1"><date>Fri, 7 May 2021 15:16:22 GMT</date><size>50kb</size></version><title>An Enterprise Architecture Framework for E-learning</title><authors>Abbas Najafizadeh, Maryam Saadati, S. Mahdi Jamei, S. Shervin
  Ostadzadeh</authors><categories>cs.CY</categories><comments>E-learning; Enterprise Architecture; Framework; Information System;
  Zachman Framework</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  With a trend toward becoming more and more information and communication
based, learning services and processes were also evolved. E-learning comprises
all forms of electronically supported learning and teaching. The information
and communication systems serve as a fundamental role to implement these
learning processes. In the typical information-driven organizations, the
E-learning is part of a much larger platform for applications and data that
extends across the Internet and intranet/extranet. In this respect, E-learning
has brought about an inevitable tendency to lunge towards organizing their
information based activities in a comprehensive way. Building an Enterprise
Architecture (EA) undoubtedly serves as a fundamental concept to accomplish
this goal. In this paper, we propose an EA for E-learning information systems.
The presented framework helps developers to design and justify completely
integrated learning and teaching processes and information systems which
results in improved pedagogical success rate.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07858</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07858</id><submitter>Mahbub Hasan</submitter><version version="v1"><date>Fri, 7 May 2021 16:41:46 GMT</date><size>94kb</size><source_type>D</source_type></version><title>Optimal Group Formulation Using Machine Learning</title><authors>Mahbub Hasan and Al-Emran</authors><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Group formation itself a perplexing process. Over the decade of time
education and others disciple has improved imminently but optimal group
formation in educational system is still struggling. Our research focus on to
create optimal group in a class of any institute. In this research we use
Simulated Annealing (SA) for best group formation based on the previous
academic record. We generally create an arbitrary cluster first then optimise
using SA. Our model has significant success rate over a large number of
datasets. This research will play a pioneer role in group formations in the
academic and related researches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07869</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07869</id><submitter>Andrey Ignatov</submitter><version version="v1"><date>Mon, 17 May 2021 14:06:21 GMT</date><size>17826kb</size><source_type>D</source_type></version><title>Fast and Accurate Camera Scene Detection on Smartphones</title><authors>Angeline Pouget, Sidharth Ramesh, Maximilian Giang, Ramithan
  Chandrapalan, Toni Tanner, Moritz Prussing, Radu Timofte, Andrey Ignatov</authors><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AI-powered automatic camera scene detection mode is nowadays available in
nearly any modern smartphone, though the problem of accurate scene prediction
has not yet been addressed by the research community. This paper for the first
time carefully defines this problem and proposes a novel Camera Scene Detection
Dataset (CamSDD) containing more than 11K manually crawled images belonging to
30 different scene categories. We propose an efficient and NPU-friendly CNN
model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset
and achieves more than 200 FPS on the recent mobile SoCs. An additional
in-the-wild evaluation of the obtained solution is performed to analyze its
performance and limitation in the real-world scenarios. The dataset and
pre-trained models used in this paper are available on the project website.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07870</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07870</id><submitter>Ashitha Ganapathy</submitter><version version="v1"><date>Mon, 17 May 2021 14:07:35 GMT</date><size>318kb</size></version><title>Cybernetics and the Future of Work</title><authors>Ashitha Ganapathy, Michael Timothy Bennett</authors><categories>cs.CY</categories><comments>Copyright 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The disruption caused by the pandemic has called into question industrial
norms and created an opportunity to reimagine the future of work. We discuss
how this period of opportunity may be leveraged to bring about a future in
which the workforce thrives rather than survives. Any coherent plan of such
breadth must address the interaction of multiple technological, social,
economic, and environmental systems. A shared language that facilitates
communication across disciplinary boundaries can bring together stakeholders
and facilitate a considered response. The origin story of cybernetics and the
ideas posed therein serve to illustrate how we may better understand present
complex challenges, to create a future of work that places human values at its
core.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07876</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07876</id><submitter>Reda Mastouri</submitter><version version="v1"><date>Mon, 10 May 2021 18:31:45 GMT</date><size>836kb</size></version><title>The challenges and realities of retailing in a COVID-19 world:
  Identifying trending and Vital During Crisis keywords during Covid-19 using
  Machine Learning (Austria as a case study)</title><authors>Reda Mastouri Et Al., Joseph Gilkey</authors><categories>cs.CY cs.AI</categories><comments>easychair, ENSIAS Rabat, Morocco. Saint Peter's University, NJ- USA</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  From global pandemics to geopolitical turmoil, leaders in logistics, product
allocation, procurement and operations are facing increasing difficulty with
safeguarding their organizations against supply chain vulnerabilities. It is
recommended to opt for forecasting against trending based benchmark because
auditing a future forecast puts more focus on seasonality. The forecasting
models provide with end-to-end, real time oversight of the entire supply chain,
while utilizing predictive analytics and artificial intelligence to identify
potential disruptions before they occur. By combining internal and external
data points, coming up with an AI-enabled modelling engine can greatly reduce
risk by helping retail companies proactively respond to supply and demand
variability. This research paper puts focus on creating an ingenious way to
tackle the impact of COVID19 on Supply chain, product allocation, trending and
seasonality.
  Key words: Supply chain, covid-19, forecasting, coronavirus, manufacturing,
seasonality, trending, retail.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07877</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07877</id><submitter>Vyacheslav Yukalov</submitter><version version="v1"><date>Mon, 10 May 2021 16:58:38 GMT</date><size>12kb</size></version><title>Quantum Uncertainty in Decision Theory</title><authors>V.I. Yukalov</authors><categories>cs.AI quant-ph</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An approach is presented treating decision theory as a probabilistic theory
based on quantum techniques. Accurate definitions are given and thorough
analysis is accomplished for the quantum probabilities describing the choice
between separate alternatives, sequential alternatives characterizing
conditional quantum probabilities, and behavioral quantum probabilities taking
into account rational-irrational duality of decision making. The comparison
between quantum and classical probabilities is explained. The analysis
demonstrates that quantum probabilities serve as an essentially more powerful
tool of characterizing various decision-making situations including the
influence of psychological behavioral effects.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07878</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07878</id><submitter>Gargi Joshi Miss</submitter><version version="v1"><date>Mon, 17 May 2021 14:17:49 GMT</date><size>1011kb</size></version><version version="v2"><date>Tue, 18 May 2021 11:53:33 GMT</date><size>1051kb</size></version><title>A Review on Explainability in Multimodal Deep Neural Nets</title><authors>Gargi Joshi, Rahee Walambe, Ketan Kotecha</authors><categories>cs.AI cs.CV</categories><comments>24 pages 6 figures</comments><journal-ref>in IEEE Access, vol. 9, pp. 59800-59821, 2021</journal-ref><doi>10.1109/ACCESS.2021.3070212.</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Artificial Intelligence techniques powered by deep neural nets have achieved
much success in several application domains, most significantly and notably in
the Computer Vision applications and Natural Language Processing tasks.
Surpassing human-level performance propelled the research in the applications
where different modalities amongst language, vision, sensory, text play an
important role in accurate predictions and identification. Several multimodal
fusion methods employing deep learning models are proposed in the literature.
Despite their outstanding performance, the complex, opaque and black-box nature
of the deep neural nets limits their social acceptance and usability. This has
given rise to the quest for model interpretability and explainability, more so
in the complex tasks involving multimodal AI methods. This paper extensively
reviews the present literature to present a comprehensive survey and commentary
on the explainability in multimodal deep neural nets, especially for the vision
and language tasks. Several topics on multimodal AI and its applications for
generic domains have been covered in this paper, including the significance,
datasets, fundamental building blocks of the methods and techniques,
challenges, applications, and future trends in this domain
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07879</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07879</id><submitter>Reza Vaezi</submitter><version version="v1"><date>Wed, 12 May 2021 15:53:44 GMT</date><size>285kb</size></version><title>Conscious AI</title><authors>Hadi Esmaeilzadeh and Reza Vaezi</authors><categories>cs.AI cs.CL cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in artificial intelligence (AI) have achieved human-scale
speed and accuracy for classification tasks. In turn, these capabilities have
made AI a viable replacement for many human activities that at their core
involve classification, such as basic mechanical and analytical tasks in
low-level service jobs. Current systems do not need to be conscious to
recognize patterns and classify them. However, for AI to progress to more
complicated tasks requiring intuition and empathy, it must develop capabilities
such as metathinking, creativity, and empathy akin to human self-awareness or
consciousness. We contend that such a paradigm shift is possible only through a
fundamental shift in the state of artificial intelligence toward consciousness,
a shift similar to what took place for humans through the process of natural
selection and evolution. As such, this paper aims to theoretically explore the
requirements for the emergence of consciousness in AI. It also provides a
principled understanding of how conscious AI can be detected and how it might
be manifested in contrast to the dominant paradigm that seeks to ultimately
create machines that are linguistically indistinguishable from humans.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07881</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07881</id><submitter>Benjamin Rech</submitter><version version="v1"><date>Thu, 13 May 2021 10:05:56 GMT</date><size>489kb</size></version><title>Plattformen und neue Technologien im Journalismus: Ergebnisse einer
  Online-Befragung von Journalistinnen und Journalisten in Deutschland</title><authors>Benjamin Rech, Matthias Meyer</authors><categories>cs.CY</categories><comments>in German</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In an online survey in December 2020, 385 journalists in Germany were
surveyed about platforms in journalism and about their frequency of use and
willingness to adopt emerging technologies. Journalists have a commitment to
publish on a journalism platform on a full-time basis. Freelancers have a
higher commitment than employed journalists. A platform subscription model is
rated more attractive than advertising for a platform. Employed journalists on
the other hand consider advertising more attractive than freelance journalists.
For German journalists it is important that the platform is developed in Europe
or Germany and that it sets high standards on data protection. Multimedia forms
and interactive elements are used occasionally, often or always. Stories or
Reels are predominantly not used. AI software as well as editorial analytics
are rarely or never used. Apart from stories or reels, journalists intend to
use multimedia forms and interactive elements more often in the future. They
are receptive to software for research process documentation as well as to the
analysis of indicators of their own publications. Software as a support for
text production, image selection or headline suggestions is mostly rejected.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07882</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07882</id><submitter>Max Hahn-Klimroth</submitter><version version="v1"><date>Thu, 13 May 2021 10:52:46 GMT</date><size>878kb</size><source_type>D</source_type></version><title>Efficient and accurate group testing via Belief Propagation: an
  empirical study</title><authors>AminCoja-Oghlan, Max Hahn-Klimroth, Philipp Loick, Manuel Penschuck</authors><categories>cs.AI cs.DM cs.IT math.IT stat.ML</categories><msc-class>05C80, 60B20, 68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The group testing problem asks for efficient pooling schemes and algorithms
that allow to screen moderately large numbers of samples for rare infections.
The goal is to accurately identify the infected samples while conducting the
least possible number of tests. Exploring the use of techniques centred around
the Belief Propagation message passing algorithm, we suggest a new test design
that significantly increases the accuracy of the results. The new design comes
with Belief Propagation as an efficient inference algorithm. Aiming for results
on practical rather than asymptotic problem sizes, we conduct an experimental
study.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07889</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07889</id><submitter>Jiayi Chen</submitter><version version="v1"><date>Mon, 17 May 2021 14:22:58 GMT</date><size>4194kb</size><source_type>D</source_type></version><title>HetMAML: Task-Heterogeneous Model-Agnostic Meta-Learning for Few-Shot
  Learning Across Modalities</title><authors>Jiayi Chen, Aidong Zhang</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of existing gradient-based meta-learning approaches to few-shot learning
assume that all tasks have the same input feature space. However, in the real
world scenarios, there are many cases that the input structures of tasks can be
different, that is, different tasks may vary in the number of input modalities
or the data structure of each modality. Existing gradient-based approaches
cannot handle such heterogeneous task distribution (HTD) as different types of
tasks only share partial meta-parameters. In this paper, we propose HetMAML, a
task-heterogeneous meta-agnostic meta-learning framework that can generalize
not only common meta-parameters shared across different types of tasks but also
type-specific meta-parameters. Specifically, we design a multi-channel backbone
module that encodes the input of each type of tasks into the same length
sequence of modality-specific embeddings. Then, we propose a task-aware
multimodal encoder which can automatically take into account the context of
task-specific input structures and adaptively project the heterogeneous input
spaces to the same lower-dimensional concept space. The extensive experiments
on five task-heterogeneous datasets demonstrate that our HetMAML successfully
captures both type-specific and shared meta-parameters across heterogeneous
tasks which fast adapt to all types of new tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07892</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07892</id><submitter>Rak-Kyeong Seong</submitter><version version="v1"><date>Mon, 17 May 2021 16:40:17 GMT</date><size>2242kb</size><source_type>D</source_type></version><title>Topology for Substrate Routing in Semiconductor Package Design</title><authors>Rak-Kyeong Seong, Jaeho Yang, Sang-Hoon Han</authors><categories>cs.CG</categories><comments>24 pages, 22 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new signal routing method for solving routing
problems that occur in the design process of semiconductor package substrates.
Our work uses a topological transformation of the layers of the package
substrate in order to simplify the routing problem into a problem of connecting
points on a circle with non-intersecting straight line segments. The circle,
which we call the Circular Frame, is a polygonal schema, which is originally
used in topology to study the topological structure of 2-manifolds. We show
through experiments that our new routing method based on the Circular Frame
competes with certain grid-based routing algorithms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07893</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07893</id><submitter>Konstantin Zimenko</submitter><version version="v1"><date>Fri, 14 May 2021 10:19:42 GMT</date><size>167kb</size><source_type>D</source_type></version><title>Adaptive Finite-time and Fixed-time Control Design using Output
  Stability Conditions</title><authors>Konstantin Zimenko, Denis Efimov, Andrey Polyakov</authors><categories>math.OC cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The present paper provides a sufficient condition to ensure output
finite-time and fixed-time stability. Comparing with analogous researches the
proposed result is less restrictive and obtained for a wider class of systems.
The presented output stability condition is used for adaptive control design,
where the state vector of a plant is extended by adjustable control parameters.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07894</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07894</id><submitter>Stephanie Kirmse</submitter><version version="v1"><date>Fri, 14 May 2021 10:58:17 GMT</date><size>10086kb</size><source_type>D</source_type></version><title>Topology-optimization based design of multi-degree-of-freedom compliant
  mechanisms (mechanisms with multiple pseudo-mobility)</title><authors>Stephanie Kirmse, Lucio Flavio Campanile, and Alexander Hasse
  (corresponding author)</authors><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Unlike conventional mechanisms, compliant mechanisms produce the desired
deformations by exploiting elastic strain and do not need, therefore, moving
parts. The number of degrees of freedom of a conventional mechanism, also
called mobility, is the number of independent coordinates needed to define a
configuration of the mechanism. Due to the different operating principle, such
definition of degree of freedom or mobility cannot be directly applied to
compliant mechanisms. While those terms are not able to denote a property of a
given compliant mechanism, they are meaningful when applied to the design of a
compliant mechanism. Compliant mechanisms are, however, mostly seen as elastic
structures, for which the term degree of freedom is used in a different
meaning. In order to avoid ambiguities, the term pseudo-mobility (already
introduced in previous published work) will be used to denote the number of
scalar parameters needed to identify one single desired deformation, i.e. one
single deformation for which the compliant mechanism is designed. Many
synthesis approaches exist for compliant mechanisms with single pseudo-mobility
(commonly referred to as &quot;single degree of freedom mechanisms&quot;). In the case of
compliant mechanisms with multiple pseudo-mobility (multiple-degree of freedom
mechanisms), only synthesis approaches for relatively simple mechanisms exist
so far, while systems for more complex tasks like shape adaptation are not
covered. In addition, only certain cases of transverse loads are included in
the synthesis with these approaches. In this paper, a novel optimization
algorithm is presented that addresses these two shortcomings. The algorithm is
tested on a simple mechanism with one translation and one rotation kinematic
degree of freedom, a compliant parallel mechanism for pure translation and a
shape-adaptive structure.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07898</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07898</id><submitter>Ruben Rodriguez Torrado</submitter><version version="v1"><date>Mon, 17 May 2021 14:29:08 GMT</date><size>1318kb</size><source_type>D</source_type></version><title>Physics-informed attention-based neural network for solving non-linear
  partial differential equations</title><authors>Ruben Rodriguez-Torrado, Pablo Ruiz, Luis Cueto-Felgueroso, Michael
  Cerny Green, Tyler Friesen, Sebastien Matringe and Julian Togelius</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Physics-Informed Neural Networks (PINNs) have enabled significant
improvements in modelling physical processes described by partial differential
equations (PDEs). PINNs are based on simple architectures, and learn the
behavior of complex physical systems by optimizing the network parameters to
minimize the residual of the underlying PDE. Current network architectures
share some of the limitations of classical numerical discretization schemes
when applied to non-linear differential equations in continuum mechanics. A
paradigmatic example is the solution of hyperbolic conservation laws that
develop highly localized nonlinear shock waves. Learning solutions of PDEs with
dominant hyperbolic character is a challenge for current PINN approaches, which
rely, like most grid-based numerical schemes, on adding artificial dissipation.
Here, we address the fundamental question of which network architectures are
best suited to learn the complex behavior of non-linear PDEs. We focus on
network architecture rather than on residual regularization. Our new
methodology, called Physics-Informed Attention-based Neural Networks, (PIANNs),
is a combination of recurrent neural networks and attention mechanisms. The
attention mechanism adapts the behavior of the deep neural network to the
non-linear features of the solution, and break the current limitations of
PINNs. We find that PIANNs effectively capture the shock front in a hyperbolic
model problem, and are capable of providing high-quality solutions inside and
beyond the training set.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07900</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07900</id><submitter>Kazuma Tsuji</submitter><version version="v1"><date>Mon, 17 May 2021 14:32:45 GMT</date><size>344kb</size><source_type>D</source_type></version><title>Acceleration of the kernel herding algorithm by improved gradient
  approximation</title><authors>Kazuma Tsuji and Ken'ichiro Tanaka</authors><categories>math.NA cs.NA stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel herding is a method used to construct quadrature formulas in a
reproducing kernel Hilbert space. Although there are some advantages of kernel
herding, such as numerical stability of quadrature and effective outputs of
nodes and weights, the convergence speed of worst-case integration error is
slow in comparison to other quadrature methods. To address this problem, we
propose two improved versions of the kernel herding algorithm. The fundamental
concept of both algorithms involves approximating negative gradients with a
positive linear combination of vertex directions. We analyzed the convergence
and validity of both algorithms theoretically; in particular, we showed that
the approximation of negative gradients directly influences the convergence
speed. In addition, we confirmed the accelerated convergence of the worst-case
integration error with respect to the number of nodes and computational time
through numerical experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07901</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07901</id><submitter>Nanyang Yang</submitter><version version="v1"><date>Mon, 17 May 2021 14:32:47 GMT</date><size>4851kb</size><source_type>D</source_type></version><title>Multi-object Tracking with Tracked Object Bounding Box Association</title><authors>Nanyang Yang, Yi Wang and Lap-Pui Chau</authors><categories>cs.CV</categories><comments>6 pages, accepted paper at ICME workshop 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CenterTrack tracking algorithm achieves state-of-the-art tracking
performance using a simple detection model and single-frame spatial offsets to
localize objects and predict their associations in a single network. However,
this joint detection and tracking method still suffers from high identity
switches due to the inferior association method. To reduce the high number of
identity switches and improve the tracking accuracy, in this paper, we propose
to incorporate a simple tracked object bounding box and overlapping prediction
based on the current frame onto the CenterTrack algorithm. Specifically, we
propose an Intersection over Union (IOU) distance cost matrix in the
association step instead of simple point displacement distance. We evaluate our
proposed tracker on the MOT17 test dataset, showing that our proposed method
can reduce identity switches significantly by 22.6% and obtain a notable
improvement of 1.5% in IDF1 compared to the original CenterTrack's under the
same tracklet lifetime. The source code is released at
https://github.com/Nanyangny/CenterTrack-IOU.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07902</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07902</id><submitter>David \'Alvarez Robert</submitter><version version="v1"><date>Mon, 17 May 2021 14:32:54 GMT</date><size>2541kb</size><source_type>D</source_type></version><title>Advanced Synchronization Techniques for Task-based Runtime Systems</title><authors>David \'Alvarez, Kevin Sala, Marcos Maro\~nas, Aleix Roca, Vicen\c{c}
  Beltran</authors><categories>cs.DC</categories><comments>14 pages, 11 figures. Published in the 26th ACM SIGPLAN Symposium on
  Principles and Practice of Parallel Programming (PPoPP'21)</comments><journal-ref>Proceedings of the 26th ACM SIGPLAN Symposium on Principles and
  Practice of Parallel Programming (2021) 334-347</journal-ref><doi>10.1145/3437801.3441601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task-based programming models like OmpSs-2 and OpenMP provide a flexible
data-flow execution model to exploit dynamic, irregular and nested parallelism.
Providing an efficient implementation that scales well with small granularity
tasks remains a challenge, and bottlenecks can manifest in several runtime
components. In this paper, we analyze the limiting factors in the scalability
of a task-based runtime system and propose individual solutions for each of the
challenges, including a wait-free dependency system and a novel scalable
scheduler design based on delegation. We evaluate how the optimizations impact
the overall performance of the runtime, both individually and in combination.
We also compare the resulting runtime against state of the art OpenMP
implementations, showing equivalent or better performance, especially for
fine-grained tasks.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07903</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07903</id><submitter>Nils Holzenberger</submitter><version version="v1"><date>Mon, 17 May 2021 14:33:02 GMT</date><size>5329kb</size><source_type>D</source_type></version><title>Factoring Statutory Reasoning as Language Understanding Challenges</title><authors>Nils Holzenberger and Benjamin Van Durme</authors><categories>cs.CL</categories><comments>18 pages, 3 figures. To appear in ACL 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statutory reasoning is the task of determining whether a legal statute,
stated in natural language, applies to the text description of a case. Prior
work introduced a resource that approached statutory reasoning as a monolithic
textual entailment problem, with neural baselines performing nearly at-chance.
To address this challenge, we decompose statutory reasoning into four types of
language-understanding challenge problems, through the introduction of concepts
and structure found in Prolog programs. Augmenting an existing benchmark, we
provide annotations for the four tasks, and baselines for three of them. Models
for statutory reasoning are shown to benefit from the additional structure,
improving on prior baselines. Further, the decomposition into subtasks
facilitates finer-grained model diagnostics and clearer incremental progress.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07906</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07906</id><submitter>Sen Zhan</submitter><version version="v1"><date>Mon, 17 May 2021 14:36:13 GMT</date><size>1509kb</size><source_type>D</source_type></version><title>Distributionally Robust Chance-Constrained Flexibility Planning for
  Integrated Energy System</title><authors>Sen Zhan, Peng Hou, Guangya Yang</authors><categories>eess.SY cs.SY</categories><comments>Msc thesis at DTU, submitted to IJEPES</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Inflexible combined heat and power (CHP) plants and uncertain wind power
production result in excess power in distribution networks, which leads to
inverse power flow challenging grid operations. Power-to-X facilities such as
electrolysers and electric boilers can offer extra flexibility to the
integrated energy system. In this regard, we aim to jointly determine the
optimal Power-to-X facility sizing and integrated energy system operations in
this study. To account for wind power uncertainties, a distributionally robust
chance-constrained model is developed to characterize wind power uncertainties
using ambiguity sets. Linear decision rules are applied to analytically express
real-time recourse actions when uncertainties are exposed, which allows the
propagation of wind power uncertainties to gas and heat systems. Accordingly,
the developed three-stage distributionally robust chance-constrained model is
converted into a computationally tractable single-stage mixed-integer conic
model. A case study validates the effectiveness of introducing the electrolyser
and electric boiler into the integrated energy system, with respect to the
decreased system cost, expanded CHP plant flexibility and reduced inverse power
flow. The developed distributionally robust optimization model exhibits better
effectiveness and robustness compared to a chance-constrained optimization
model assuming wind forecast errors follow Gaussian distribution. Detailed
profit analysis reveals that although the overall system cost is minimized, the
profit is distributed unevenly across various stakeholders in the system.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07909</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07909</id><submitter>Ning Xie</submitter><version version="v1"><date>Mon, 17 May 2021 14:45:38 GMT</date><size>80kb</size><source_type>D</source_type></version><title>Application of Deep Self-Attention in Knowledge Tracing</title><authors>Junhao Zeng, Qingchun Zhang, Ning Xie, Bochun Yang</authors><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of intelligent tutoring system has greatly influenced the way
students learn and practice, which increases their learning efficiency. The
intelligent tutoring system must model learners' mastery of the knowledge
before providing feedback and advices to learners, so one class of algorithm
called &quot;knowledge tracing&quot; is surely important. This paper proposed Deep
Self-Attentive Knowledge Tracing (DSAKT) based on the data of PTA, an online
assessment system used by students in many universities in China, to help these
students learn more efficiently. Experimentation on the data of PTA shows that
DSAKT outperforms the other models for knowledge tracing an improvement of AUC
by 2.1% on average, and this model also has a good performance on the ASSIST
dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07911</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07911</id><submitter>Kuan Xu</submitter><version version="v1"><date>Mon, 17 May 2021 14:49:54 GMT</date><size>87kb</size><source_type>D</source_type></version><title>SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising</title><authors>Kuan Xuan, Yongbo Wang, Yongliang Wang, Zujie Wen, Yang Dong</authors><categories>cs.CL stat.ML</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In text-to-SQL task, seq-to-seq models often lead to sub-optimal performance
due to limitations in their architecture. In this paper, we present a simple
yet effective approach that adapts transformer-based seq-to-seq model to robust
text-to-SQL generation. Instead of inducing constraint to decoder or reformat
the task as slot-filling, we propose to train seq-to-seq model with Schema
aware Denoising (SeaD), which consists of two denoising objectives that train
model to either recover input or predict output from two novel erosion and
shuffle noises. These denoising objectives acts as the auxiliary tasks for
better modeling the structural data in S2S generation. In addition, we improve
and propose a clause-sensitive execution guided (EG) decoding strategy to
overcome the limitation of EG decoding for generative model. The experiments
show that the proposed method improves the performance of seq-to-seq model in
both schema linking and grammar correctness and establishes new
state-of-the-art on WikiSQL benchmark. The results indicate that the capacity
of vanilla seq-to-seq architecture for text-to-SQL may have been
under-estimated.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07914</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07914</id><submitter>Yan Bai</submitter><version version="v1"><date>Mon, 17 May 2021 14:55:08 GMT</date><size>4575kb</size><source_type>D</source_type></version><title>Large-Scale Unsupervised Person Re-Identification with Contrastive
  Learning</title><authors>Weiquan Huang, Yan Bai, Qiuyu Ren, Xinbo Zhao, Ming Feng and Yin Wang</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing public person Re-Identification~(ReID) datasets are small in modern
terms because of labeling difficulty. Although unlabeled surveillance video is
abundant and relatively easy to obtain, it is unclear how to leverage these
footage to learn meaningful ReID representations. In particular, most existing
unsupervised and domain adaptation ReID methods utilize only the public
datasets in their experiments, with labels removed. In addition, due to small
data sizes, these methods usually rely on fine tuning by the unlabeled training
data in the testing domain to achieve good performance. Inspired by the recent
progress of large-scale self-supervised image classification using contrastive
learning, we propose to learn ReID representation from large-scale unlabeled
surveillance video alone. Assisted by off-the-shelf pedestrian detection tools,
we apply the contrastive loss at both the image and the tracklet levels.
Together with a principal component analysis step using camera labels freely
available, our evaluation using a large-scale unlabeled dataset shows far
superior performance among unsupervised methods that do not use any training
data in the testing domain. Furthermore, the accuracy improves with the data
size and therefore our method has great potential with even larger and more
diversified datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07917</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07917</id><submitter>Giulia Cisotto</submitter><version version="v1"><date>Mon, 17 May 2021 14:57:13 GMT</date><size>254kb</size><source_type>D</source_type></version><title>CNN-based Approaches For Cross-Subject Classification in Motor Imagery:
  From The State-of-The-Art to DynamicNet</title><authors>Alberto Zancanaro, Giulia Cisotto, Jo\~ao Ruivo Paulo, Gabriel Pires,
  and Urbano J. Nunes</authors><categories>cs.LG cs.CV cs.HC cs.NE eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motor imagery (MI)-based brain-computer interface (BCI) systems are being
increasingly employed to provide alternative means of communication and control
for people suffering from neuro-motor impairments, with a special effort to
bring these systems out of the controlled lab environments. Hence, accurately
classifying MI from brain signals, e.g., from electroencephalography (EEG), is
essential to obtain reliable BCI systems. However, MI classification is still a
challenging task, because the signals are characterized by poor SNR, high
intra-subject and cross-subject variability. Deep learning approaches have
started to emerge as valid alternatives to standard machine learning
techniques, e.g., filter bank common spatial pattern (FBCSP), to extract
subject-independent features and to increase the cross-subject classification
performance of MI BCI systems. In this paper, we first present a review of the
most recent studies using deep learning for MI classification, with particular
attention to their cross-subject performance. Second, we propose DynamicNet, a
Python-based tool for quick and flexible implementations of deep learning
models based on convolutional neural networks. We show-case the potentiality of
DynamicNet by implementing EEGNet, a well-established architecture for
effective EEG classification. Finally, we compare its performance with FBCSP in
a 4-class MI classification over public datasets. To explore its cross-subject
classification ability, we applied three different cross-validation schemes.
From our results, we demonstrate that DynamicNet-implemented EEGNet outperforms
FBCSP by about 25%, with a statistically significant difference when
cross-subject validation schemes are applied.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07921</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07921</id><submitter>Gencer Sumbul</submitter><version version="v1"><date>Mon, 17 May 2021 15:00:31 GMT</date><size>134kb</size><source_type>D</source_type></version><title>BigEarthNet-MM: A Large Scale Multi-Modal Multi-Label Benchmark Archive
  for Remote Sensing Image Classification and Retrieval</title><authors>Gencer Sumbul, Arne de Wall, Tristan Kreuziger, Filipe Marcelino, Hugo
  Costa, Pedro Benevides, M\'ario Caetano, Beg\&quot;um Demir, Volker Markl</authors><categories>cs.CV</categories><comments>The paper is under review. Our code is available online at
  https://git.tu-berlin.de/rsim/BigEarthNet-MM_19-classes_models. arXiv admin
  note: substantial text overlap with arXiv:2001.06372</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents the multi-modal BigEarthNet (BigEarthNet-MM) benchmark
archive made up of 590,326 pairs of Sentinel-1 and Sentinel-2 image patches to
support the deep learning (DL) studies in multi-modal multi-label remote
sensing (RS) image retrieval and classification. Each pair of patches in
BigEarthNet-MM is annotated with multi-labels provided by the CORINE Land Cover
(CLC) map of 2018 based on its thematically most detailed Level-3 class
nomenclature. Our initial research demonstrates that some CLC classes are
challenging to be accurately described by only considering (single-date)
BigEarthNet-MM images. In this paper, we also introduce an alternative
class-nomenclature as an evolution of the original CLC labels to address this
problem. This is achieved by interpreting and arranging the CLC Level-3
nomenclature based on the properties of BigEarthNet-MM images in a new
nomenclature of 19 classes. In our experiments, we show the potential of
BigEarthNet-MM for multi-modal multi-label image retrieval and classification
problems by considering several state-of-the-art DL models. We also demonstrate
that the DL models trained from scratch on BigEarthNet-MM outperform those
pre-trained on ImageNet, especially in relation to some complex classes,
including agriculture and other vegetated and natural environments. We make all
the data and the DL models publicly available at https://bigearth.net, offering
an important resource to support studies on multi-modal image scene
classification and retrieval problems in RS.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07922</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07922</id><submitter>Carlos Beltr\'an</submitter><version version="v1"><date>Mon, 17 May 2021 15:00:36 GMT</date><size>7kb</size></version><title>How well-conditioned can the eigenvalue problem be?</title><authors>Carlos Beltr\'an, Laurent B\'etermin, Peter Grabner, Stefan
  Steinerberger</authors><categories>math.NA cs.NA math.CA</categories><comments>6 pages, 2 figures</comments><msc-class>65F15 (Primary), 31C20 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The condition number for eigenvalue computations is a well--studied
  quantity. But how small can we expect it to be? Namely, which is a perfectly
  conditioned matrix w.r.t. eigenvalue computations? In this note we answer
  this question with exact first order asymptotic.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07924</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07924</id><submitter>Yuan Cao</submitter><version version="v1"><date>Mon, 17 May 2021 15:01:38 GMT</date><size>20kb</size></version><title>Construction and enumeration of left dihedral codes satisfying certain
  duality properties</title><authors>Yuan Cao, Yonglin Cao, Fanghui Ma</authors><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb{F}_{q}$ be the finite field of $q$ elements and let
$D_{2n}=\langle x,y\mid x^n=1, y^2=1, yxy=x^{n-1}\rangle$ be the dihedral group
of order $n$. Left ideals of the group algebra $\mathbb{F}_{q}[D_{2n}]$ are
known as left dihedral codes over $\mathbb{F}_{q}$ of length $2n$, and
abbreviated as left $D_{2n}$-codes. Let ${\rm gcd}(n,q)=1$. In this paper, we
give an explicit representation for the Euclidean hull of every left
$D_{2n}$-code over $\mathbb{F}_{q}$. On this basis, we determine all distinct
Euclidean LCD codes and Euclidean self-orthogonal codes which are left
$D_{2n}$-codes over $\mathbb{F}_{q}$. In particular, we provide an explicit
representation and a precise enumeration for these two subclasses of left
$D_{2n}$-codes and self-dual left $D_{2n}$-codes,
  respectively. Moreover, we give a direct and simple method for determining
the encoder (generator matrix) of any left $D_{2n}$-code over $\mathbb{F}_{q}$,
and present several numerical examples to illustrative our applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07925</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07925</id><submitter>R\&quot;udiger Verf\&quot;urth</submitter><version version="v1"><date>Mon, 17 May 2021 15:02:02 GMT</date><size>14kb</size></version><title>Quasi-monotonicity and Robust Localization with Continuous Piecewise
  Polynomials</title><authors>Francesca Tantardini, R\&quot;udiger Verf\&quot;urth</authors><categories>math.NA cs.NA</categories><comments>12 pages, 5 figures</comments><msc-class>41A15, 41A63, 41A05, 65N30, 65N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the energy norm arising from elliptic problems with discontinuous
piecewise constant diffusion. We prove that under the quasi-monotonicity
property on the diffusion coefficient, the best approximation error with
continuous piecewise polynomials is equivalent to the $\ell_2$-sum of best
errors on elements, in the spirit of A. Veeser for the $H^1$-seminorm. If the
quasi-monotonicity is violated, counterexamples show that a robust localization
does not hold in general, neither on elements, nor on pairs of adjacent
elements, nor on stars of elements sharing a common vertex.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07926</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07926</id><submitter>YueFeng Chen</submitter><version version="v1"><date>Mon, 17 May 2021 15:04:15 GMT</date><size>835kb</size><source_type>D</source_type></version><title>Rethinking the Design Principles of Robust Vision Transformer</title><authors>Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Shaokai Ye, Yuan He,
  Hui Xue</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances on Vision Transformers (ViT) have shown that
self-attention-based networks, which take advantage of long-range dependencies
modeling ability, surpassed traditional convolution neural networks (CNNs) in
most vision tasks. To further expand the applicability for computer vision,
many improved variants are proposed to re-design the Transformer architecture
by considering the superiority of CNNs, i.e., locality, translation invariance,
for better performance. However, these methods only consider the standard
accuracy or computation cost of the model. In this paper, we rethink the design
principles of ViTs based on the robustness. We found some design components
greatly harm the robustness and generalization ability of ViTs while some
others are beneficial. By combining the robust design components, we propose
Robust Vision Transformer (RVT). RVT is a new vision transformer, which has
superior performance and strong robustness. We further propose two new
plug-and-play techniques called position-aware attention rescaling and
patch-wise augmentation to train our RVT. The experimental results on ImageNet
and six robustness benchmarks show the advanced robustness and generalization
ability of RVT compared with previous Transformers and state-of-the-art CNNs.
Our RVT-S* also achieves Top-1 rank on multiple robustness leaderboards
including ImageNet-C and ImageNet-Sketch. The code will be available at
https://github.com/vtddggg/Robust-Vision-Transformer.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07930</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07930</id><submitter>Senthil Yogamani</submitter><version version="v1"><date>Mon, 17 May 2021 15:10:00 GMT</date><size>32908kb</size><source_type>D</source_type></version><title>Pseudo-Label Ensemble-based Semi-supervised Learning for Handling Noisy
  Soiling Segmentation Annotations</title><authors>Michal Uricar, Ganesh Sistu, Lucie Yahiaoui and Senthil Yogamani</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manual annotation of soiling on surround view cameras is a very challenging
and expensive task. The unclear boundary for various soiling categories like
water drops or mud particles usually results in a large variance in the
annotation quality. As a result, the models trained on such poorly annotated
data are far from being optimal. In this paper, we focus on handling such noisy
annotations via pseudo-label driven ensemble model which allow us to quickly
spot problematic annotations and in most cases also sufficiently fixing them.
We train a soiling segmentation model on both noisy and refined labels and
demonstrate significant improvements using the refined annotations. It also
illustrates that it is possible to effectively refine lower cost coarse
annotations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07933</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07933</id><submitter>Sarah Perrin</submitter><version version="v1"><date>Mon, 17 May 2021 15:17:36 GMT</date><size>8202kb</size><source_type>D</source_type></version><title>Mean Field Games Flock! The Reinforcement Learning Way</title><authors>Sarah Perrin, Mathieu Lauri\`ere, Julien P\'erolat, Matthieu Geist,
  Romuald \'Elie, Olivier Pietquin</authors><categories>cs.MA cs.AI</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We present a method enabling a large number of agents to learn how to flock,
which is a natural behavior observed in large populations of animals. This
problem has drawn a lot of interest but requires many structural assumptions
and is tractable only in small dimensions. We phrase this problem as a Mean
Field Game (MFG), where each individual chooses its acceleration depending on
the population behavior. Combining Deep Reinforcement Learning (RL) and
Normalizing Flows (NF), we obtain a tractable solution requiring only very weak
assumptions. Our algorithm finds a Nash Equilibrium and the agents adapt their
velocity to match the neighboring flock's average one. We use Fictitious Play
and alternate: (1) computing an approximate best response with Deep RL, and (2)
estimating the next population distribution with NF. We show numerically that
our algorithm learn multi-group or high-dimensional flocking with obstacles.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07936</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07936</id><submitter>Narges Mehran</submitter><version version="v1"><date>Mon, 17 May 2021 15:22:57 GMT</date><size>223kb</size><source_type>D</source_type></version><title>A Two-Sided Matching Model for Data Stream Processing in the Cloud-Fog
  Continuum</title><authors>Narges Mehran, Dragi Kimovski, Radu Prodan</authors><categories>cs.DC</categories><comments>7 figures</comments><doi>10.1109/CCGrid51090.2021.00061</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Latency-sensitive and bandwidth-intensive stream processing applications are
dominant traffic generators over the Internet network. A stream consists of a
continuous sequence of data elements, which require processing in nearly
real-time. To improve communication latency and reduce the network congestion,
Fog computing complements the Cloud services by moving the computation towards
the edge of the network. Unfortunately, the heterogeneity of the new Cloud-Fog
continuum raises important challenges related to deploying and executing data
stream applications. We explore in this work a two-sided stable matching model
called Cloud-Fog to data stream application matching (CODA) for deploying a
distributed application represented as a workflow of stream processing
microservices on heterogeneous Cloud-Fog computing resources. In CODA, the
application microservices rank the continuum resources based on their
microservice stream processing time, while resources rank the stream processing
microservices based on their residual bandwidth. A stable many-to-one matching
algorithm assigns microservices to resources based on their mutual preferences,
aiming to optimize the complete stream processing time on the application side,
and the total streaming traffic on the resource side. We evaluate the CODA
algorithm using simulated and real-world Cloud-Fog scenarios. We achieved 11 to
45% lower stream processing time and 1.3 to 20% lower streaming traffic
compared to related state-of-the-art approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07937</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07937</id><submitter>Paul Kantor</submitter><version version="v1"><date>Mon, 17 May 2021 15:23:12 GMT</date><size>2672kb</size></version><title>Confidence Assertions in Cyber-Security for an Information-Sharing
  Environment</title><authors>Paul B. Kantor, Dennis E. Egan, Jonathan Bullinger, Katie McKeon,
  James Wojtowicz</authors><categories>cs.CR</categories><comments>50pp. CCICADA Technical Report</comments><report-no>CCICADA-TR/2017-001</report-no><msc-class>68M25</msc-class><acm-class>K.4.0</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Information sharing is vital in resisting cyberattacks, and the volume and
severity of these attacks is increasing very rapidly. Therefore responders must
triage incoming warnings in deciding how to act. This study asked a very
specific question: &quot;how can the addition of confidence information to alerts
and warnings improve overall resistance to cyberattacks.&quot; We sought, in
particular, to identify current practices, and if possible, to identify some
&quot;best practices.&quot; The research involved literature review and interviews with
subject matter experts at every level from system administrators to persons who
develop broad principles of policy. An innovative Modified Online Delphi Panel
technique was used to elicit judgments and recommendations from experts who
were able to speak with each other and vote anonymously to rank proposed
practices.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07938</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07938</id><submitter>Vincenzo Suriani</submitter><version version="v1"><date>Mon, 17 May 2021 15:24:23 GMT</date><size>7262kb</size><source_type>D</source_type></version><title>RoSmEEry: Robotic Simulated Environment for Evaluation and Benchmarking
  of Semantic Mapping Algorithms</title><authors>Sara Kaszuba, Sandeep Reddy Sabbella, Vincenzo Suriani, Francesco
  Riccio, Daniele Nardi</authors><categories>cs.RO</categories><comments>Published at ARMS2021 (Autonomous Robots and Multirobot Systems),
  Workshop at AAMAS 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Human-robot interaction requires a common understanding of the operational
environment, which can be provided by a representation that blends geometric
and symbolic knowledge: a semantic map. Through a semantic map the robot can
interpret user commands by grounding them to its sensory observations. Semantic
mapping is the process that builds such a representation. Despite being
fundamental to enable cognition and high-level reasoning in robotics, semantic
mapping is a challenging task due to generalization to different scenarios and
sensory data types. In fact, it is difficult to obtain a rich and accurate
semantic map of the environment and of the objects therein. Moreover, to date,
there are no frameworks that allow for a comparison of the performance in
building semantic maps for a given environment. To tackle these issues we
design RoSmEEry, a novel framework based on the Gazebo simulator, where we
introduce an accessible and ready-to-use methodology for a systematic
evaluation of semantic mapping algorithms. We release our framework, as an
open-source package, with multiple simulation environments with the aim to
provide a general set-up to quantitatively measure the performances in
acquiring semantic knowledge about the environment.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07940</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07940</id><submitter>Peng Liang</submitter><version version="v1"><date>Mon, 17 May 2021 15:27:19 GMT</date><size>1240kb</size></version><title>Mining Architecture Tactics and Quality Attributes Knowledge in Stack
  Overflow</title><authors>Tingting Bi and Peng Liang and Antony Tang and Xin Xia</authors><categories>cs.SE</categories><comments>Preprint accepted for publication in Journal of Systems and Software,
  2021</comments><doi>10.1016/j.jss.2021.111005</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Context: Architecture Tactics (ATs) are architectural building blocks that
provide general architectural solutions for addressing Quality Attributes (QAs)
issues. Mining and analyzing QA-AT knowledge can help the software architecture
community better understand architecture design. However, manually capturing
and mining this knowledge is labor-intensive and difficult. Objective: Using
Stack Overflow (SO) as our source, our main goals are to effectively mine such
knowledge; and to have some sense of how developers use ATs with respect to QA
concerns from related discussions. Methods: We applied a semi-automatic
dictionary-based mining approach to extract the QA-AT posts in SO. With the
mined QA-AT posts, we identified the relationships between ATs and QAs.
Results: Our approach allow us to mine QA-AT knowledge effectively with an
F-measure of 0.865 and Performance of 82.2%. Using this mining approach, we are
able to discover architectural synonyms of QAs and ATs used by designers, from
which we discover how developers apply ATs to address quality requirements.
Conclusions: We make two contributions in this work: First, we demonstrated a
semi-automatic approach to mine ATs and QAs from SO posts; Second, we
identified little-known design relationships between QAs and ATs and grouped
architectural design considerations to aid architects make architecture tactics
design decisions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07944</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07944</id><submitter>Yunfei Chu</submitter><version version="v1"><date>Mon, 17 May 2021 15:33:25 GMT</date><size>14902kb</size><source_type>D</source_type></version><title>TCL: Transformer-based Dynamic Graph Modelling via Contrastive Learning</title><authors>Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang,
  Xiaofeng He, Le Song, Jingren Zhou, Hongxia Yang</authors><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dynamic graph modeling has recently attracted much attention due to its
extensive applications in many real-world scenarios, such as recommendation
systems, financial transactions, and social networks. Although many works have
been proposed for dynamic graph modeling in recent years, effective and
scalable models are yet to be developed. In this paper, we propose a novel
graph neural network approach, called TCL, which deals with the
dynamically-evolving graph in a continuous-time fashion and enables effective
dynamic node representation learning that captures both the temporal and
topology information. Technically, our model contains three novel aspects.
First, we generalize the vanilla Transformer to temporal graph learning
scenarios and design a graph-topology-aware transformer. Secondly, on top of
the proposed graph transformer, we introduce a two-stream encoder that
separately extracts representations from temporal neighborhoods associated with
the two interaction nodes and then utilizes a co-attentional transformer to
model inter-dependencies at a semantic level. Lastly, we are inspired by the
recently developed contrastive learning and propose to optimize our model by
maximizing mutual information (MI) between the predictive representations of
two future interaction nodes. Benefiting from this, our dynamic representations
can preserve high-level (or global) semantics about interactions and thus is
robust to noisy interactions. To the best of our knowledge, this is the first
attempt to apply contrastive learning to representation learning on dynamic
graphs. We evaluate our model on four benchmark datasets for interaction
prediction and experiment results demonstrate the superiority of our model.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07946</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07946</id><submitter>Federico Mason</submitter><version version="v1"><date>Mon, 17 May 2021 15:34:00 GMT</date><size>2528kb</size><source_type>D</source_type></version><title>Using Distributed Reinforcement Learning for Resource Orchestration in a
  Network Slicing Scenario</title><authors>Federico Mason, Gianfranco Nencioni, Andrea Zanella</authors><categories>cs.MA</categories><comments>14 pages, 11 figures, 4 tables. This paper is under review at IEEE
  Transaction on Networking. Copyright IEEE 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Network Slicing (NS) paradigm enables the partition of physical and
virtual resources among multiple logical networks, possibly managed by
different tenants. In such a scenario, network resources need to be dynamically
allocated according to the slices' requirements. In this paper, we attack the
above problem by exploiting a Deep Reinforcement Learning approach. Our
framework is based on a distributed architecture, where multiple agents
cooperate towards a common goal. The agents' training is carried out following
the Advantage Actor Critic algorithm, which allows to handle continuous action
spaces. By means of extensive simulations, we show that our approach yields
better performance than both a static allocation of system resources and an
efficient empirical strategy. At the same time, the proposed system ensures
high adaptability to different scenarios without the need for additional
training.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07948</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07948</id><submitter>David Lawrence</submitter><version version="v1"><date>Wed, 28 Apr 2021 13:51:42 GMT</date><size>1420kb</size><source_type>D</source_type></version><title>AI Enabled Data Quality Monitoring with Hydra</title><authors>Thomas Britton, David Lawrence, Kishansingh Rajput</authors><categories>cs.CY</categories><comments>vCHEP21</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data quality monitoring is critical to all experiments impacting the quality
of any physics results. Traditionally, this is done through an alarm system,
which detects low level faults, leaving higher level monitoring to human crews.
Artificial Intelligence is beginning to find its way into scientific
applications, but comes with difficulties, relying on the acquisition of new
skill sets, either through education or acquisition, in data science. This
paper will discuss the development and deployment of the Hydra monitoring
system in production at Gluex. It will show how &quot;off-the-shelf&quot; technologies
can be rapidly developed, as well as discuss what sociological hurdles must be
overcome to successfully deploy such a system. Early results from production
running of Hydra will also be shared as well as a future outlook for
development of Hydra.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07949</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07949</id><submitter>Abhijit Suresh</submitter><version version="v1"><date>Thu, 29 Apr 2021 20:45:02 GMT</date><size>5138kb</size><source_type>D</source_type></version><title>Using Transformers to Provide Teachers with Personalized Feedback on
  their Classroom Discourse: The TalkMoves Application</title><authors>Abhijit Suresh, Jennifer Jacobs, Vivian Lai, Chenhao Tan, Wayne Ward,
  James H. Martin, Tamara Sumner</authors><categories>cs.CY cs.CL</categories><comments>Presented at the AAAI 2021 Spring Symposium on Artificial
  Intelligence for K-12 Education</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  TalkMoves is an innovative application designed to support K-12 mathematics
teachers to reflect on, and continuously improve their instructional practices.
This application combines state-of-the-art natural language processing
capabilities with automated speech recognition to automatically analyze
classroom recordings and provide teachers with personalized feedback on their
use of specific types of discourse aimed at broadening and deepening classroom
conversations about mathematics. These specific discourse strategies are
referred to as &quot;talk moves&quot; within the mathematics education community and
prior research has documented the ways in which systematic use of these
discourse strategies can positively impact student engagement and learning. In
this article, we describe the TalkMoves application's cloud-based
infrastructure for managing and processing classroom recordings, and its
interface for providing teachers with feedback on their use of talk moves
during individual teaching episodes. We present the series of model
architectures we developed, and the studies we conducted, to develop our
best-performing, transformer-based model (F1 = 79.3%). We also discuss several
technical challenges that need to be addressed when working with real-world
speech and language data from noisy K-12 classrooms.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07951</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07951</id><submitter>Levent Guvenc</submitter><version version="v1"><date>Sat, 8 May 2021 23:51:00 GMT</date><size>1033kb</size></version><title>Pedestrian Path Modification Mobile Tool for COVID-19 Social Distancing
  for Use in Multi-Modal Trip Navigation</title><authors>Sukru Yaren Gelbal, Mustafa Ridvan Cantas, Bilin Aksun-Guvenc, Levent
  Guvenc</authors><categories>cs.CY cs.RO</categories><comments>8 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The novel Corona virus pandemic is one of the biggest worldwide problems
right now. While hygiene and wearing masks make up a large portion of the
currently suggested precautions by the Centers for Disease Control and
Prevention (CDC) and World Health Organization (WHO), social distancing is
another and arguably the most important precaution that would protect people
since the airborne virus is easily transmitted through the air. Social
distancing while walking outside, can be more effective, if pedestrians know
locations of each other and even better if they know locations of people who
are possible carriers. With this information, they can change their routes
depending on the people walking nearby or they can stay away from areas that
contain or have recently contained crowds. This paper presents a mobile device
application that would be a very beneficial tool for social distancing during
Coronavirus Disease 2019 (COVID-19). The application works, synced close to
real-time, in a networking fashion with all users obtaining their locations and
drawing a virtual safety bubble around them. These safety bubbles are used with
the constant velocity pedestrian model to predict possible future social
distancing violations and warn the user with sound and vibration. Moreover, it
takes into account the virus staying airborne for a certain time, hence,
creating time-decaying non-safe areas in the past trajectories of the users.
The mobile app generates collision free paths for navigating around the
undesired locations for the pedestrian mode of transportation when used as part
of a multi-modal trip planning app. Results are applicable to other modes of
transportation also. Features and the methods used for implementation are
discussed in the paper. The application is tested using previously collected
real pedestrian walking data in a realistic environment.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07952</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07952</id><submitter>Yuanpeng He</submitter><version version="v1"><date>Wed, 12 May 2021 12:41:57 GMT</date><size>3392kb</size><source_type>D</source_type></version><title>MMGET: A Markov model for generalized evidence theory</title><authors>Yuanpeng He</authors><categories>cs.AI</categories><comments>20 pages,24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real life, lots of information merges from time to time. To appropriately
describe the actual situations, lots of theories have been proposed. Among
them, Dempster-Shafer evidence theory is a very useful tool in managing
uncertain information. To better adapt to complex situations of open world, a
generalized evidence theory is designed. However, everything occurs in sequence
and owns some underlying relationships with each other. In order to further
embody the details of information and better conforms to situations of real
world, a Markov model is introduced into the generalized evidence theory which
helps extract complete information volume from evidence provided. Besides, some
numerical examples is offered to verify the correctness and rationality of the
proposed method.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07957</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07957</id><submitter>Daniel Tanneberg</submitter><version version="v1"><date>Mon, 17 May 2021 15:37:32 GMT</date><size>7930kb</size><source_type>D</source_type></version><title>Evolutionary Training and Abstraction Yields Algorithmic Generalization
  of Neural Computers</title><authors>Daniel Tanneberg, Elmar Rueckert, Jan Peters</authors><categories>cs.NE cs.AI cs.LG stat.ML</categories><comments>Nature Machine Intelligence</comments><journal-ref>Nature Machine Intelligence, Vol. 2, December 2020, 753-763</journal-ref><doi>10.1038/s42256-020-00255-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key feature of intelligent behaviour is the ability to learn abstract
strategies that scale and transfer to unfamiliar problems. An abstract strategy
solves every sample from a problem class, no matter its representation or
complexity -- like algorithms in computer science. Neural networks are powerful
models for processing sensory data, discovering hidden patterns, and learning
complex functions, but they struggle to learn such iterative, sequential or
hierarchical algorithmic strategies. Extending neural networks with external
memories has increased their capacities in learning such strategies, but they
are still prone to data variations, struggle to learn scalable and transferable
solutions, and require massive training data. We present the Neural Harvard
Computer (NHC), a memory-augmented network based architecture, that employs
abstraction by decoupling algorithmic operations from data manipulations,
realized by splitting the information flow and separated modules. This
abstraction mechanism and evolutionary training enable the learning of robust
and scalable algorithmic solutions. On a diverse set of 11 algorithms with
varying complexities, we show that the NHC reliably learns algorithmic
solutions with strong generalization and abstraction: perfect generalization
and scaling to arbitrary task configurations and complexities far beyond seen
during training, and being independent of the data representation and the task
domain.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07959</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07959</id><submitter>Kiran Tomlinson</submitter><version version="v1"><date>Mon, 17 May 2021 15:39:02 GMT</date><size>153kb</size><source_type>D</source_type></version><title>Choice Set Confounding in Discrete Choice</title><authors>Kiran Tomlinson, Johan Ugander, and Austin R. Benson</authors><categories>cs.LG cs.SI econ.EM</categories><comments>12 pages, accepted to KDD 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard methods in preference learning involve estimating the parameters of
discrete choice models from data of selections (choices) made by individuals
from a discrete set of alternatives (the choice set). While there are many
models for individual preferences, existing learning methods overlook how
choice set assignment affects the data. Often, the choice set itself is
influenced by an individual's preferences; for instance, a consumer choosing a
product from an online retailer is often presented with options from a
recommender system that depend on information about the consumer's preferences.
Ignoring these assignment mechanisms can mislead choice models into making
biased estimates of preferences, a phenomenon that we call choice set
confounding; we demonstrate the presence of such confounding in widely-used
choice datasets.
  To address this issue, we adapt methods from causal inference to the discrete
choice setting. We use covariates of the chooser for inverse probability
weighting and/or regression controls, accurately recovering individual
preferences in the presence of choice set confounding under certain
assumptions. When such covariates are unavailable or inadequate, we develop
methods that take advantage of structured choice set assignment to improve
prediction. We demonstrate the effectiveness of our methods on real-world
choice data, showing, for example, that accounting for choice set confounding
makes choices observed in hotel booking and commute transportation more
consistent with rational utility-maximization.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07960</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07960</id><submitter>Martin Zaefferer</submitter><version version="v1"><date>Mon, 17 May 2021 15:40:42 GMT</date><size>969kb</size><source_type>D</source_type></version><title>Behavior-based Neuroevolutionary Training in Reinforcement Learning</title><authors>J\&quot;org Stork, Martin Zaefferer, Nils Eisler, Patrick Tichelmann,
  Thomas Bartz-Beielstein, A. E. Eiben</authors><categories>cs.NE cs.AI</categories><doi>10.1145/3449726.3463171</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In addition to their undisputed success in solving classical optimization
problems, neuroevolutionary and population-based algorithms have become an
alternative to standard reinforcement learning methods. However, evolutionary
methods often lack the sample efficiency of standard value-based methods that
leverage gathered state and value experience. If reinforcement learning for
real-world problems with significant resource cost is considered, sample
efficiency is essential. The enhancement of evolutionary algorithms with
experience exploiting methods is thus desired and promises valuable insights.
This work presents a hybrid algorithm that combines topology-changing
neuroevolutionary optimization with value-based reinforcement learning. We
illustrate how the behavior of policies can be used to create distance and loss
functions, which benefit from stored experiences and calculated state values.
They allow us to model behavior and perform a directed search in the behavior
space by gradient-free evolutionary algorithms and surrogate-based
optimization. For this purpose, we consolidate different methods to generate
and optimize agent policies, creating a diverse population. We exemplify the
performance of our algorithm on standard benchmarks and a purpose-built
real-world problem. Our results indicate that combining methods can enhance the
sample efficiency and learning speed for evolutionary approaches.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07961</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07961</id><submitter>Alan Wang</submitter><version version="v1"><date>Mon, 17 May 2021 15:42:28 GMT</date><size>1267kb</size><source_type>D</source_type></version><title>Joint Optimization of Hadamard Sensing and Reconstruction in Compressed
  Sensing Fluorescence Microscopy</title><authors>Alan Q. Wang, Aaron K. LaViolette, Leo Moon, Chris Xu, and Mert R.
  Sabuncu</authors><categories>eess.IV cs.CV</categories><comments>Accepted at MICCAI 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Compressed sensing fluorescence microscopy (CS-FM) proposes a scheme whereby
less measurements are collected during sensing and reconstruction is performed
to recover the image. Much work has gone into optimizing the sensing and
reconstruction portions separately. We propose a method of jointly optimizing
both sensing and reconstruction end-to-end under a total measurement
constraint, enabling learning of the optimal sensing scheme concurrently with
the parameters of a neural network-based reconstruction network. We train our
model on a rich dataset of confocal, two-photon, and wide-field microscopy
images comprising of a variety of biological samples. We show that our method
outperforms several baseline sensing schemes and a regularized regression
reconstruction algorithm.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07962</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07962</id><submitter>Hritam Basak</submitter><version version="v1"><date>Mon, 17 May 2021 15:43:59 GMT</date><size>707kb</size><source_type>D</source_type></version><title>DFENet: A Novel Dimension Fusion Edge Guided Network for Brain MRI
  Segmentation</title><authors>Hritam Basak, Rukhshanda Hussain, Ajay Rana</authors><categories>eess.IV cs.CV</categories><comments>Submitted at SN Computer Science</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid increment of morbidity of brain stroke in the last few years have
been a driving force towards fast and accurate segmentation of stroke lesions
from brain MRI images. With the recent development of deep-learning,
computer-aided and segmentation methods of ischemic stroke lesions have been
useful for clinicians in early diagnosis and treatment planning. However, most
of these methods suffer from inaccurate and unreliable segmentation results
because of their inability to capture sufficient contextual features from the
MRI volumes. To meet these requirements, 3D convolutional neural networks have
been proposed, which, however, suffer from huge computational requirements. To
mitigate these problems, we propose a novel Dimension Fusion Edge-guided
network (DFENet) that can meet both of these requirements by fusing the
features of 2D and 3D CNNs. Unlike other methods, our proposed network uses a
parallel partial decoder (PPD) module for aggregating and upsampling selected
features, rich in important contextual information. Additionally, we use an
edge-guidance and enhanced mixing loss for constantly supervising and
improvising the learning process of the network. The proposed method is
evaluated on publicly available Anatomical Tracings of Lesions After Stroke
(ATLAS) dataset, resulting in mean DSC, IoU, Precision and Recall values of
0.5457, 0.4015, 0.6371, and 0.4969 respectively. The results, when compared to
other state-of-the-art methods, outperforms them by a significant margin.
Therefore, the proposed model is robust, accurate, superior to the existing
methods, and can be relied upon for biomedical applications.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07963</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07963</id><submitter>Carolina Urz\'ua-Torres</submitter><version version="v1"><date>Mon, 17 May 2021 15:44:20 GMT</date><size>770kb</size><source_type>D</source_type></version><title>Full operator preconditioning and the accuracy of solving linear systems</title><authors>Stephan Mohr and Yuji Nakatsukasa and Carolina Urz\'ua-Torres</authors><categories>math.NA cs.NA</categories><msc-class>65N12, 65F35</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unless special conditions apply, the attempt to solve ill-conditioned systems
of linear equations with standard numerical methods leads to uncontrollably
high numerical error. Often, such systems arise from the discretization of
operator equations with a large number of discrete variables. In this paper we
show that the accuracy can be improved significantly if the equation is
transformed before discretization, a process we call full operator
preconditioning (FOP). It bears many similarities with traditional
preconditioning for iterative methods but, crucially, transformations are
applied at the operator level. We show that while condition-number improvements
from traditional preconditioning generally do not improve the accuracy of the
solution, FOP can. A number of topics in numerical analysis can be interpreted
as implicitly employing FOP; we highlight (i) Chebyshev interpolation in
polynomial approximation, and (ii) Olver-Townsend's spectral method, both of
which produce solutions of dramatically improved accuracy over a naive problem
formulation. In addition, we propose a FOP preconditioner based on integration
for the solution of fourth-order differential equations with the finite-element
method, showing the resulting linear system is well-conditioned regardless of
the discretization size, and demonstrate its error-reduction capabilities on
several examples. This work shows that FOP can improve accuracy beyond the
standard limit for both direct and iterative methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07965</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07965</id><submitter>Arpita Biswas</submitter><version version="v1"><date>Mon, 17 May 2021 15:44:55 GMT</date><size>1329kb</size><source_type>D</source_type></version><title>Learn to Intervene: An Adaptive Learning Policy for Restless Bandits in
  Application to Preventive Healthcare</title><authors>Arpita Biswas, Gaurav Aggarwal, Pradeep Varakantham, Milind Tambe</authors><categories>cs.LG cs.AI</categories><comments>To appear in the 30th International Joint Conference on Artificial
  Intelligence (IJCAI 2021)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In many public health settings, it is important for patients to adhere to
health programs, such as taking medications and periodic health checks.
Unfortunately, beneficiaries may gradually disengage from such programs, which
is detrimental to their health. A concrete example of gradual disengagement has
been observed by an organization that carries out a free automated call-based
program for spreading preventive care information among pregnant women. Many
women stop picking up calls after being enrolled for a few months. To avoid
such disengagements, it is important to provide timely interventions. Such
interventions are often expensive and can be provided to only a small fraction
of the beneficiaries. We model this scenario as a restless multi-armed bandit
(RMAB) problem, where each beneficiary is assumed to transition from one state
to another depending on the intervention. Moreover, since the transition
probabilities are unknown a priori, we propose a Whittle index based Q-Learning
mechanism and show that it converges to the optimal solution. Our method
improves over existing learning-based methods for RMABs on multiple benchmarks
from literature and also on the maternal healthcare dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07966</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07966</id><submitter>Anand Santhanakrishnan</submitter><version version="v1"><date>Mon, 17 May 2021 15:46:48 GMT</date><size>1028kb</size><source_type>D</source_type></version><title>A Game-Theoretic Analysis of Competitive Editing in Wikipedia:
  Contributors' Effort to Influence Articles and the Community's Attempt to
  Ensure Neutrality</title><authors>Santhanakrishnan Anand, Ofer Arazy, Narayan B. Mandayam and Oded Nov</authors><categories>cs.GT</categories><comments>37 pages, 6 figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer production, such as the collaborative authoring of Wikipedia articles,
involves both cooperation and competition between contributors, and we focus on
the latter. As individuals, contributors compete to align Wikipedia articles
with their personal perspectives. As a community, they work collectively to
ensure a neutral point of view (NPOV). We study the interplay between
individuals' competition and the community's endeavor to ensure neutrality. We
develop a two-level game-theoretic model, modeling the interactions of
ownership-motivated individuals and neutrality-seeking communal mechanisms as a
Stackelberg game. We present our model's predictions regarding the relation
between contributors' effort (i.e. typical size of edit) and benefits (i.e. the
portion of the article they eventually ``own''). We validate the model's
prediction through an empirical analysis, by studying the interactions of
219,811 distinct contributors that co-produced 864 Wikipedia articles over a
decade. The analysis and empirical results suggest that contributors who make
large edits (``creators'') eventually lose the article's ownership to those who
refine the articles and typically make smaller edits (``curators''). Whereas
neutrality-seeking mechanisms are essential for ensuring that ownership is not
concentrated within a small number of contributors, our findings suggest that
the burden of excessive governance may deter contributors from participating.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07968</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07968</id><submitter>David Rhodes</submitter><version version="v1"><date>Mon, 17 May 2021 15:48:22 GMT</date><size>2703kb</size><source_type>D</source_type></version><title>A New Vertex Connectivity Metric</title><authors>David L. Rhodes, Breanna N. Johnson</authors><categories>cs.DS cs.DM</categories><comments>9 pages, 3 figures, 4 tables</comments><acm-class>G.2.2; G.2.3; F.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A new metric for quantifying pairwise vertex connectivity in graphs is
defined and an implementation presented. While general in nature, it features a
combination of input features well-suited for social networks, including
applicability to directed or undirected graphs, weighted edges, and computes
using the impact from all-paths between the vertices. Moreover, the $O(V+E)$
method is applicable to large graphs. Comparisons with other techniques are
included.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07975</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07975</id><submitter>Iain Mackie</submitter><version version="v1"><date>Mon, 17 May 2021 15:58:44 GMT</date><size>842kb</size><source_type>D</source_type></version><title>How Deep is your Learning: the DL-HARD Annotated Deep Learning Dataset</title><authors>Iain Mackie and Jeffery Dalton and Andrew Yates</authors><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Learning Hard (DL-HARD) is a new annotated dataset designed to more
effectively evaluate neural ranking models on complex topics. It builds on TREC
Deep Learning (DL) topics by extensively annotating them with question intent
categories, answer types, wikified entities, topic categories, and result type
metadata from a commercial web search engine. Based on this data, we introduce
a framework for identifying challenging queries. DL-HARD contains fifty topics
from the official DL 2019/2020 evaluation benchmark, half of which are newly
and independently assessed. We perform experiments using the official submitted
runs to DL on DL-HARD and find substantial differences in metrics and the
ranking of participating systems. Overall, DL-HARD is a new resource that
promotes research on neural ranking methods by focusing on challenging and
complex topics.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07979</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07979</id><submitter>Patrick Sol\'e</submitter><version version="v1"><date>Mon, 17 May 2021 16:02:55 GMT</date><size>12kb</size></version><title>Designs, permutations, and projective planes</title><authors>Minjia Shi, XiaoXiao Li, Patrick Sol\'e</authors><categories>math.CO cs.DM</categories><comments>12 pages. arXiv admin note: text overlap with arXiv:2102.08276</comments><msc-class>Primary 05E35, Secondary O5E20, 05E24</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A notion of $t$-designs in the symmetric group on $n$ letters was introduced
by Godsil in 1988. In particular $t$-transitive sets of permutations form a
$t$-design. We derive lower bounds for $t=1$ and $t=2$ by a power moment
method. For general $n,$ and $t\le n/2,$ we give a linear programming lower
bound involving Charlier polynomials. For specific $n$ and $t=2,$ this bound is
strong enough to show that sharply transitive $2$-designs do not exist for many
composite integers $n.$ This implies the non-existence of projective planes of
order $10,15,18,24,26,28,33,35.$ In general, based on numerical evidence, we
conjecture that $t$-designs have size $n(n-1)\dots (n-t+1),$ or larger.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07983</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07983</id><submitter>Ayantha Randika Ponnamperuma Arachchige</submitter><version version="v1"><date>Mon, 17 May 2021 16:09:15 GMT</date><size>2988kb</size><source_type>D</source_type></version><title>Unknown-box Approximation to Improve Optical Character Recognition
  Performance</title><authors>Ayantha Randika, Nilanjan Ray, Xiao Xiao, Allegra Latimer</authors><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical character recognition (OCR) is a widely used pattern recognition
application in numerous domains. There are several feature-rich,
general-purpose OCR solutions available for consumers, which can provide
moderate to excellent accuracy levels. However, accuracy can diminish with
difficult and uncommon document domains. Preprocessing of document images can
be used to minimize the effect of domain shift. In this paper, a novel approach
is presented for creating a customized preprocessor for a given OCR engine.
Unlike the previous OCR agnostic preprocessing techniques, the proposed
approach approximates the gradient of a particular OCR engine to train a
preprocessor module. Experiments with two datasets and two OCR engines show
that the presented preprocessor is able to improve the accuracy of the OCR up
to 46% from the baseline by applying pixel-level manipulations to the document
image. The implementation of the proposed method and the enhanced public
datasets are available for download.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07985</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07985</id><submitter>Franziska Boenisch</submitter><version version="v1"><date>Mon, 17 May 2021 16:10:54 GMT</date><size>195kb</size><source_type>D</source_type></version><title>Gradient Masking and the Underestimated Robustness Threats of
  Differential Privacy in Deep Learning</title><authors>Franziska Boenisch, Philip Sperl, Konstantin B\&quot;ottinger</authors><categories>cs.CR cs.AI cs.LG</categories><acm-class>I.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An important problem in deep learning is the privacy and security of neural
networks (NNs). Both aspects have long been considered separately. To date, it
is still poorly understood how privacy enhancing training affects the
robustness of NNs. This paper experimentally evaluates the impact of training
with Differential Privacy (DP), a standard method for privacy preservation, on
model vulnerability against a broad range of adversarial attacks. The results
suggest that private models are less robust than their non-private
counterparts, and that adversarial examples transfer better among DP models
than between non-private and private ones. Furthermore, detailed analyses of DP
and non-DP models suggest significant differences between their gradients.
Additionally, this work is the first to observe that an unfavorable choice of
parameters in DP training can lead to gradient masking, and, thereby, results
in a wrong sense of security.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07986</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07986</id><submitter>David Montero</submitter><version version="v1"><date>Mon, 17 May 2021 16:10:58 GMT</date><size>29942kb</size></version><version version="v2"><date>Tue, 18 May 2021 07:15:56 GMT</date><size>28592kb</size></version><title>Learning to Automatically Catch Potholes in Worldwide Road Scene Images</title><authors>J. Javier Yebes, David Montero, Ignacio Arriola</authors><categories>cs.CV cs.LG eess.IV</categories><comments>in IEEE Intelligent Transportation Systems Magazine</comments><acm-class>I.4.8</acm-class><doi>10.1109/MITS.2019.2926370</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Among several road hazards that are present in any paved way in the world,
potholes are one of the most annoying and also involving higher maintenance
costs. There exists an increasing interest on the automated detection of these
hazards enabled by technological and research progress. Our research work
tackled the challenge of pothole detection from images of real world road
scenes. The main novelty resides on the application of the latest progress in
AI to learn the visual appearance of potholes. We built a large dataset of
images with pothole annotations. They contained road scenes from different
cities in the world, taken with different cameras, vehicles and viewpoints
under varied environmental conditions. Then, we fine-tuned four different
object detection models based on Faster R-CNN and SSD deep neural networks. We
achieved high average precision and the pothole detector was tested on the
Nvidia DrivePX2 platform with GPGPU capability, which can be embedded on
vehicles. Moreover, it was deployed on a real vehicle to notify the detected
potholes to a given IoT platform as part of AUTOPILOT H2020 project.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07996</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07996</id><submitter>Fatema Hasan</submitter><version version="v1"><date>Mon, 17 May 2021 16:22:43 GMT</date><size>39kb</size></version><title>Learning User Embeddings from Temporal Social Media Data: A Survey</title><authors>Fatema Hasan, Kevin S. Xu, James R. Foulds, Shimei Pan</authors><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User-generated data on social media contain rich information about who we
are, what we like and how we make decisions. In this paper, we survey
representative work on learning a concise latent user representation (a.k.a.
user embedding) that can capture the main characteristics of a social media
user. The learned user embeddings can later be used to support different
downstream user analysis tasks such as personality modeling, suicidal risk
assessment and purchase decision prediction. The temporal nature of
user-generated data on social media has largely been overlooked in much of the
existing user embedding literature. In this survey, we focus on research that
bridges the gap by incorporating temporal/sequential information in user
representation learning. We categorize relevant papers along several key
dimensions, identify limitations in the current work and suggest future
research directions.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07997</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07997</id><submitter>Irene Parada</submitter><version version="v1"><date>Mon, 17 May 2021 16:28:25 GMT</date><size>885kb</size><source_type>D</source_type></version><title>Compacting Squares</title><authors>Irina Kostitsyna, Irene Parada, Willem Sonke, Bettina Speckmann, Jules
  Wulms</authors><categories>cs.CG cs.RO</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Edge-connected configurations of squares are a common model for modular
robots in two dimensions. A well-established way to reconfigure such modular
robots are so-called sliding moves. Dumitrescu and Pach [Graphs and
Combinatorics, 2006] proved that it is always possible to reconfigure one
edge-connected configuration of $n$ squares into any other using at most
$O(n^2)$ sliding moves, while keeping the configuration connected at all times.
  For certain configurations $\Omega(n^2)$ sliding moves are necessary.
However, significantly fewer moves may be sufficient. In this paper we present
a novel input-sensitive in-place algorithm which requires only $O(\bar{P} n)$
sliding moves to transform one configuration into the other, where $\bar{P}$ is
the maximum perimeter of the respective bounding boxes. Our Gather&amp;Compact
algorithm is built on the basic principle that well-connected components of
modular robots can be transformed efficiently. Hence we iteratively increase
the connectivity within a configuration, to finally arrive at a single solid
$xy$-monotone component.
  We implemented Gather&amp;Compact and compared it experimentally to the in-place
modification by Moreno and Sacrist\'an [EuroCG 2020] of the Dumitrescu and Pach
algorithm (MSDP). Our experiments show that Gather&amp;Compact consistently
outperforms MSDP by a significant margin, on all types of square
configurations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.07998</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.07998</id><submitter>Swagat Kumar</submitter><version version="v1"><date>Mon, 17 May 2021 16:30:54 GMT</date><size>317kb</size><source_type>D</source_type></version><title>Controlling an Inverted Pendulum with Policy Gradient Methods-A Tutorial</title><authors>Swagat Kumar</authors><categories>cs.LG cs.AI cs.RO</categories><comments>8 pages, 3 figures, 2 tables etc</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides the details of implementing two important policy gradient
methods to solve the inverted pendulum problem. These are namely the Deep
Deterministic Policy Gradient (DDPG) and the Proximal Policy Optimization (PPO)
algorithm. The problem is solved by using an actor-critic model where an
actor-network is used to learn the policy function and a critic network is to
evaluate the actor-network by learning to estimate the Q function. Apart from
briefly explaining the mathematics behind these two algorithms, the details of
python implementation are provided which helps in demystifying the underlying
complexity of the algorithm. In the process, the readers will be introduced to
OpenAI/Gym, Tensorflow 2.x and Keras utilities used for implementing the above
concepts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08005</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08005</id><submitter>Ainesh Bakshi</submitter><version version="v1"><date>Mon, 17 May 2021 16:40:48 GMT</date><size>48kb</size></version><title>Learning a Latent Simplex in Input-Sparsity Time</title><authors>Ainesh Bakshi, Chiranjib Bhattacharyya, Ravi Kannan, David P. Woodruff
  and Samson Zhou</authors><categories>cs.LG cs.DS stat.ML</categories><comments>ICLR 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of learning a latent $k$-vertex simplex
$K\subset\mathbb{R}^d$, given access to $A\in\mathbb{R}^{d\times n}$, which can
be viewed as a data matrix with $n$ points that are obtained by randomly
perturbing latent points in the simplex $K$ (potentially beyond $K$). A large
class of latent variable models, such as adversarial clustering, mixed
membership stochastic block models, and topic models can be cast as learning a
latent simplex. Bhattacharyya and Kannan (SODA, 2020) give an algorithm for
learning such a latent simplex in time roughly $O(k\cdot\textrm{nnz}(A))$,
where $\textrm{nnz}(A)$ is the number of non-zeros in $A$. We show that the
dependence on $k$ in the running time is unnecessary given a natural assumption
about the mass of the top $k$ singular values of $A$, which holds in many of
these applications. Further, we show this assumption is necessary, as otherwise
an algorithm for learning a latent simplex would imply an algorithmic
breakthrough for spectral low rank approximation.
  At a high level, Bhattacharyya and Kannan provide an adaptive algorithm that
makes $k$ matrix-vector product queries to $A$ and each query is a function of
all queries preceding it. Since each matrix-vector product requires
$\textrm{nnz}(A)$ time, their overall running time appears unavoidable.
Instead, we obtain a low-rank approximation to $A$ in input-sparsity time and
show that the column space thus obtained has small $\sin\Theta$ (angular)
distance to the right top-$k$ singular space of $A$. Our algorithm then selects
$k$ points in the low-rank subspace with the largest inner product with $k$
carefully chosen random vectors. By working in the low-rank subspace, we avoid
reading the entire matrix in each iteration and thus circumvent the
$\Theta(k\cdot\textrm{nnz}(A))$ running time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08007</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08007</id><submitter>Lun Du</submitter><version version="v1"><date>Mon, 17 May 2021 16:41:53 GMT</date><size>298kb</size></version><title>Understanding and Improvement of Adversarial Training for Network
  Embedding from an Optimization Perspective</title><authors>Lun Du, Xu Chen, Fei Gao, Kunqing Xie, Shi Han and Dongmei Zhang</authors><categories>cs.LG</categories><comments>9 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Embedding aims to learn a function mapping the nodes to Euclidean
space contribute to multiple learning analysis tasks on networks. However, the
noisy information behind the real-world networks and the overfitting problem
both negatively impact the quality of embedding vectors. To tackle these
problems, researchers utilize Adversarial Training for Network Embedding
(AdvTNE) and achieve state-of-the-art performance. Unlike the mainstream
methods introducing perturbations on the network structure or the data feature,
AdvTNE directly perturbs the model parameters, which provides a new chance to
understand the mechanism behind. In this paper, we explain AdvTNE theoretically
from an optimization perspective. Considering the Power-law property of
networks and the optimization objective, we analyze the reason for its
excellent results. According to the above analysis, we propose a new activation
to enhance the performance of AdvTNE. We conduct extensive experiments on four
real networks to validate the effectiveness of our method in node
classification and link prediction. The results demonstrate that our method is
superior to the state-of-the-art baseline methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08008</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08008</id><submitter>Marco Valentino</submitter><version version="v1"><date>Mon, 17 May 2021 16:43:43 GMT</date><size>93kb</size><source_type>D</source_type></version><title>Supporting Context Monotonicity Abstractions in Neural NLI Models</title><authors>Julia Rozanova, Deborah Ferreira, Mokanarangan Thayaparan, Marco
  Valentino, Andr\'e Freitas</authors><categories>cs.CL cs.LG</categories><comments>NALOMA'21 (NAtural LOgic Meets MAchine Learning) @IWCS 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language contexts display logical regularities with respect to
substitutions of related concepts: these are captured in a functional
order-theoretic property called monotonicity. For a certain class of NLI
problems where the resulting entailment label depends only on the context
monotonicity and the relation between the substituted concepts, we build on
previous techniques that aim to improve the performance of NLI models for these
problems, as consistent performance across both upward and downward monotone
contexts still seems difficult to attain even for state-of-the-art models. To
this end, we reframe the problem of context monotonicity classification to make
it compatible with transformer-based pre-trained NLI models and add this task
to the training pipeline. Furthermore, we introduce a sound and complete
simplified monotonicity logic formalism which describes our treatment of
contexts as abstract units. Using the notions in our formalism, we adapt
targeted challenge sets to investigate whether an intermediate context
monotonicity classification task can aid NLI models' performance on examples
exhibiting monotonicity reasoning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08013</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08013</id><submitter>Benjamin Seiler</submitter><version version="v1"><date>Mon, 17 May 2021 16:53:16 GMT</date><size>647kb</size><source_type>D</source_type></version><title>What makes you unique?</title><authors>Benjamin B. Seiler, Masayoshi Mase, Art B. Owen</authors><categories>stat.AP cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a uniqueness Shapley measure to compare the extent to
which different variables are able to identify a subject. Revealing the value
of a variable on subject $t$ shrinks the set of possible subjects that $t$
could be. The extent of the shrinkage depends on which other variables have
also been revealed. We use Shapley value to combine all of the reductions in
log cardinality due to revealing a variable after some subset of the other
variables has been revealed. This uniqueness Shapley measure can be aggregated
over subjects where it becomes a weighted sum of conditional entropies.
Aggregation over subsets of subjects can address questions like how identifying
is age for people of a given zip code. Such aggregates have a corresponding
expression in terms of cross entropies. We use uniqueness Shapley to
investigate the differential effects of revealing variables from the North
Carolina voter registration rolls and in identifying anomalous solar flares. An
enormous speedup (approaching 2000 fold in one example) is obtained by using
the all dimension trees of Moore and Lee (1998) to store the cardinalities we
need.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08016</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08016</id><submitter>Ge Zhang</submitter><version version="v1"><date>Mon, 17 May 2021 17:05:42 GMT</date><size>8376kb</size><source_type>D</source_type></version><title>StrobeNet: Category-Level Multiview Reconstruction of Articulated
  Objects</title><authors>Ge Zhang, Or Litany, Srinath Sridhar, Leonidas Guibas</authors><categories>cs.CV cs.CG</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present StrobeNet, a method for category-level 3D reconstruction of
articulating objects from one or more unposed RGB images. Reconstructing
general articulating object categories % has important applications, but is
challenging since objects can have wide variation in shape, articulation,
appearance and topology. We address this by building on the idea of
category-level articulation canonicalization -- mapping observations to a
canonical articulation which enables correspondence-free multiview aggregation.
Our end-to-end trainable neural network estimates feature-enriched canonical 3D
point clouds, articulation joints, and part segmentation from one or more
unposed images of an object. These intermediate estimates are used to generate
a final implicit 3D reconstruction.Our approach reconstructs objects even when
they are observed in different articulations in images with large baselines,
and animation of reconstructed shapes. Quantitative and qualitative evaluations
on different object categories show that our method is able to achieve high
reconstruction accuracy, especially as more views are added.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08018</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08018</id><submitter>Patr\'icia Matsubara</submitter><version version="v1"><date>Mon, 17 May 2021 17:07:31 GMT</date><size>491kb</size></version><title>Buying time in software development: how estimates become commitments?</title><authors>Patricia Matsubara, Igor Steinmacher, Bruno Gadelha, Tayana Conte</authors><categories>cs.SE</categories><comments>10 pages, 3 figures, accepted for publication at CHASE 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite years of research for improving accuracy, software practitioners
still face software estimation difficulties. Expert judgment has been the
prevalent method used in industry, and researchers' focus on raising realism in
estimates when using it seems not to be enough for the much-expected
improvements. Instead of focusing on the estimation process's technicalities,
we investigated the interaction of the establishment of commitments with
customers and software estimation. By observing estimation sessions and
interviewing software professionals from companies in varying contexts, we
found that defensible estimates and padding of software estimates are crucial
in converting estimates into commitments. Our findings show that software
professionals use padding for three different reasons: contingency buffer,
completing other tasks, or improving the overall quality of the product. The
reasons to pad have a common theme: buying time to balance short- and long-term
software development commitments, including the repayment of technical debt.
Such a theme emerged from the human aspects of the interaction of estimation
and the establishment of commitments: pressures and customers' conflicting
short and long-term needs play silent and unrevealed roles in-between the
technical activities. Therefore, our study contributes to untangling the
underlying phenomena, showing how the practices used by software practitioners
help to deal with the human and social context in which estimation is embedded.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08021</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08021</id><submitter>Qingyun Wang</submitter><version version="v1"><date>Mon, 17 May 2021 17:15:29 GMT</date><size>133kb</size><source_type>D</source_type></version><title>Stage-wise Fine-tuning for Graph-to-Text Generation</title><authors>Qingyun Wang, Semih Yavuz, Victoria Lin, Heng Ji, Nazneen Rajani</authors><categories>cs.CL cs.AI</categories><comments>9 pages, Accepted by Proceedings of ACL-IJCNLP 2021 Student Research
  Workshop, Code and Resources at this
  https://github.com/EagleW/Stage-wise-Fine-tuning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-to-text generation has benefited from pre-trained language models
(PLMs) in achieving better performance than structured graph encoders. However,
they fail to fully utilize the structure information of the input graph. In
this paper, we aim to further improve the performance of the pre-trained
language model by proposing a structured graph-to-text model with a two-step
fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting
to the graph-to-text generation. In addition to using the traditional token and
position embeddings to encode the knowledge graph (KG), we propose a novel
tree-level embedding method to capture the inter-dependency structures of the
input graph. This new approach has significantly improved the performance of
all text generation metrics for the English WebNLG 2017 dataset.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08023</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08023</id><submitter>Kun Yuan</submitter><version version="v1"><date>Mon, 17 May 2021 17:16:52 GMT</date><size>430kb</size></version><title>Removing Data Heterogeneity Influence Enhances Network Topology
  Dependence of Decentralized SGD</title><authors>Kun Yuan and Sulaiman A. Alghunaim</authors><categories>math.OC cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider decentralized stochastic optimization problems where a network of
agents each owns a local cost function cooperate to find a minimizer of the
global-averaged cost. A widely studied decentralized algorithm for this problem
is D-SGD in which each node applies a stochastic gradient descent step, then
averages its estimate with its neighbors. D-SGD is attractive due to its
efficient single-iteration communication and can achieve linear speedup in
convergence (in terms of the network size). However, D-SGD is very sensitive to
the network topology. For smooth objective functions, the transient stage
(which measures how fast the algorithm can reach the linear speedup stage) of
D-SGD is on the order of $O(n/(1-\beta)^2)$ and $O(n^3/(1-\beta)^4)$ for
strongly convex and generally convex cost functions, respectively, where
$1-\beta \in (0,1)$ is a topology-dependent quantity that approaches $0$ for a
large and sparse network. Hence, D-SGD suffers from slow convergence for large
and sparse networks.
  In this work, we study the non-asymptotic convergence property of the
D$^2$/Exact-diffusion algorithm. By eliminating the influence of data
heterogeneity between nodes, D$^2$/Exact-diffusion is shown to have an enhanced
transient stage that are on the order of $O(n/(1-\beta))$ and
$O(n^3/(1-\beta)^2)$ for strongly convex and generally convex cost functions,
respectively. Moreover, we provide a lower bound of the transient stage of
D-SGD under homogeneous data distributions, which coincides with the transient
stage of D$^2$/Exact-diffusion in the strongly-convex setting. These results
show that removing the influence of data heterogeneity can ameliorate the
network topology dependence of D-SGD. Compared with existing decentralized
algorithms bounds, D$^2$/Exact-diffusion is least sensitive to network
topology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08024</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08024</id><submitter>Yuting Wei</submitter><version version="v1"><date>Mon, 17 May 2021 17:22:07 GMT</date><size>415kb</size><source_type>D</source_type></version><title>Sample-Efficient Reinforcement Learning Is Feasible for Linearly
  Realizable MDPs with Limited Revisiting</title><authors>Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, Yuting Wei</authors><categories>cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Low-complexity models such as linear function representation play a pivotal
role in enabling sample-efficient reinforcement learning (RL). The current
paper pertains to a scenario with value-based linear representation, which
postulates the linear realizability of the optimal Q-function (also called the
&quot;linear $Q^{\star}$ problem&quot;). While linear realizability alone does not allow
for sample-efficient solutions in general, the presence of a large
sub-optimality gap is a potential game changer, depending on the sampling
mechanism in use. Informally, sample efficiency is achievable with a large
sub-optimality gap when a generative model is available but is unfortunately
infeasible when we turn to standard online RL settings.
  In this paper, we make progress towards understanding this linear $Q^{\star}$
problem by investigating a new sampling protocol, which draws samples in an
online/exploratory fashion but allows one to backtrack and revisit previous
states in a controlled and infrequent manner. This protocol is more flexible
than the standard online RL setting, while being practically relevant and far
more restrictive than the generative model. We develop an algorithm tailored to
this setting, achieving a sample complexity that scales polynomially with the
feature dimension, the horizon, and the inverse sub-optimality gap, but not the
size of the state/action space. Our findings underscore the fundamental
interplay between sampling protocols and low-complexity structural
representation in RL.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08031</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08031</id><submitter>MeiXing Dong</submitter><version version="v1"><date>Mon, 17 May 2021 17:30:30 GMT</date><size>75kb</size><source_type>D</source_type></version><title>Room to Grow: Understanding Personal Characteristics Behind Self
  Improvement Using Social Media</title><authors>MeiXing Dong, Xueming Xu, Yiwei Zhang, Ian Stewart, Rada Mihalcea</authors><categories>cs.CL</categories><comments>10 pages, Accepted to be published at SocialNLP at NAACL'21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many people aim for change, but not everyone succeeds. While there are a
number of social psychology theories that propose motivation-related
characteristics of those who persist with change, few computational studies
have explored the motivational stage of personal change. In this paper, we
investigate a new dataset consisting of the writings of people who manifest
intention to change, some of whom persist while others do not. Using a variety
of linguistic analysis techniques, we first examine the writing patterns that
distinguish the two groups of people. Persistent people tend to reference more
topics related to long-term self-improvement and use a more complicated writing
style. Drawing on these consistent differences, we build a classifier that can
reliably identify the people more likely to persist, based on their language.
Our experiments provide new insights into the motivation-related behavior of
people who persist with their intention to change.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08034</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08034</id><submitter>Hunter Johnston</submitter><version version="v1"><date>Mon, 17 May 2021 17:35:41 GMT</date><size>17886kb</size><source_type>D</source_type></version><title>The Theory of Functional Connections: A journey from theory to
  application</title><authors>Hunter Johnston</authors><categories>math.OC cs.SY eess.SY</categories><comments>283 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Theory of Functional Connections (TFC) is a general methodology for
functional interpolation that can embed a set of user-specified linear
constraints. The functionals derived from this method, called \emph{constrained
expressions}, analytically satisfy the imposed constraints and can be leveraged
to transform constrained optimization problems to unconstrained ones. By
simplifying the optimization problem, this technique has been shown to produce
a numerical scheme that is faster, more accurate, and robust to poor
initialization. The content of this dissertation details the complete
development of the Theory of Functional Connections. First, the seminal paper
on the Theory of Functional Connections is discussed and motivates the
discovery of a more general formulation of the constrained expressions.
Leveraging this formulation, a rigorous structure of the constrained expression
is produced with associated mathematical definitions, claims, and proofs.
Furthermore, the second part of this dissertation explains how this technique
can be used to solve ordinary differential equations providing a wide variety
of examples compared to the state-of-the-art. The final part of this work
focuses on unitizing the techniques and algorithms produced in the prior
sections to explore the feasibility of using the Theory of Functional
Connections to solve real-time optimal control problems, namely optimal landing
problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08036</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08036</id><submitter>Carl Folkestad</submitter><version version="v1"><date>Mon, 17 May 2021 17:38:52 GMT</date><size>2623kb</size><source_type>D</source_type></version><title>Koopman NMPC: Koopman-based Learning and Nonlinear Model Predictive
  Control of Control-affine Systems</title><authors>Carl Folkestad, Joel W. Burdick</authors><categories>cs.RO cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Koopman-based learning methods can potentially be practical and powerful
tools for dynamical robotic systems. However, common methods to construct
Koopman representations seek to learn lifted linear models that cannot capture
nonlinear actuation effects inherent in many robotic systems. This paper
presents a learning and control methodology that is a first step towards
overcoming this limitation. Using the Koopman canonical transform,
control-affine dynamics can be expressed by a lifted bilinear model. The
learned model is used for nonlinear model predictive control (NMPC) design
where the bilinear structure can be exploited to improve computational
efficiency. The benefits for control-affine dynamics compared to existing
Koopman-based methods are highlighted through an example of a simulated planar
quadrotor. Prediction error is greatly reduced and closed loop performance
similar to NMPC with full model knowledge is achieved.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08037</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08037</id><submitter>Haotian Gu</submitter><version version="v1"><date>Mon, 17 May 2021 17:41:56 GMT</date><size>109kb</size><source_type>D</source_type></version><title>An SDE Framework for Adversarial Training, with Convergence and
  Robustness Analysis</title><authors>Haotian Gu, Xin Guo</authors><categories>cs.LG math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial training has gained great popularity as one of the most effective
defenses for deep neural networks against adversarial perturbations on data
points. Consequently, research interests have grown in understanding the
convergence and robustness of adversarial training. This paper considers the
min-max game of adversarial training by alternating stochastic gradient
descent. It approximates the training process with a continuous-time
stochastic-differential-equation (SDE). In particular, the error bound and
convergence analysis is established.
  This SDE framework allows direct comparison between adversarial training and
stochastic gradient descent; and confirms analytically the robustness of
adversarial training from a (new) gradient-flow viewpoint. This analysis is
then corroborated via numerical studies.
  To demonstrate the versatility of this SDE framework for algorithm design and
parameter tuning, a stochastic control problem is formulated for learning rate
adjustment, where the advantage of adaptive learning rate over fixed learning
rate in terms of training loss is demonstrated through numerical experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08039</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08039</id><submitter>Nadir Durrani Dr</submitter><version version="v1"><date>Mon, 17 May 2021 17:43:36 GMT</date><size>30kb</size><source_type>D</source_type></version><title>Fine-grained Interpretation and Causation Analysis in Deep NLP Models</title><authors>Hassan Sajjad, Narine Kokhlikyan, Fahim Dalvi, Nadir Durrani</authors><categories>cs.CL</categories><comments>Accepted at NAACL Tutorial</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a write-up for the tutorial on &quot;Fine-grained Interpretation and
Causation Analysis in Deep NLP Models&quot; that we are presenting at NAACL 2021. We
present and discuss the research work on interpreting fine-grained components
of a model from two perspectives, i) fine-grained interpretation, ii) causation
analysis. The former introduces methods to analyze individual neurons and a
group of neurons with respect to a language property or a task. The latter
studies the role of neurons and input features in explaining decisions made by
the model. We also discuss application of neuron analysis such as network
manipulation and domain adaptation. Moreover, we present two toolkits namely
NeuroX and Captum, that support functionalities discussed in this tutorial.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08040</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08040</id><submitter>Burhaneddin Yaman</submitter><version version="v1"><date>Mon, 17 May 2021 17:43:46 GMT</date><size>9403kb</size><source_type>D</source_type></version><title>Unsupervised Deep Learning Methods for Biological Image Reconstruction</title><authors>Mehmet Ak\c{c}akaya, Burhaneddin Yaman, Hyungjin Chung, Jong Chul Ye</authors><categories>eess.IV cs.CV cs.LG eess.SP physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, deep learning approaches have become the main research frontier for
biological image reconstruction problems thanks to their high performance,
along with their ultra-fast reconstruction times. However, due to the
difficulty of obtaining matched reference data for supervised learning, there
has been increasing interest in unsupervised learning approaches that do not
need paired reference data. In particular, self-supervised learning and
generative models have been successfully used for various biological imaging
applications. In this paper, we overview these approaches from a coherent
perspective in the context of classical inverse problems, and discuss their
applications to biological imaging.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08043</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08043</id><submitter>Markus Brill</submitter><version version="v1"><date>Mon, 17 May 2021 17:46:01 GMT</date><size>48kb</size></version><title>Dynamic Proportional Rankings</title><authors>Jonas Israel and Markus Brill</authors><categories>cs.GT</categories><journal-ref>Proceedings of the the 30th International Joint Conference on
  Artificial Intelligence (IJCAI 2021)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proportional ranking rules aggregate approval-style preferences of agents
into a collective ranking such that groups of agents with similar preferences
are adequately represented. Motivated by the application of live Q&amp;A platforms,
where submitted questions need to be ranked based on the interests of the
audience, we study a dynamic extension of the proportional rankings setting. In
our setting, the goal is to maintain the proportionality of a ranking when
alternatives (i.e., questions) -- not necessarily from the top of the ranking
-- get selected sequentially. We propose generalizations of well-known
aggregation rules to this setting and study their monotonicity and
proportionality properties. We also evaluate the performance of these rules
experimentally, using realistic probabilistic assumptions on the selection
procedure.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08049</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08049</id><submitter>Yang Zhang</submitter><version version="v1"><date>Mon, 17 May 2021 17:54:32 GMT</date><size>89kb</size><source_type>D</source_type></version><title>SGD-QA: Fast Schema-Guided Dialogue State Tracking for Unseen Services</title><authors>Yang Zhang, Vahid Noroozi, Evelina Bakhturina, Boris Ginsburg</authors><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dialogue state tracking is an essential part of goal-oriented dialogue
systems, while most of these state tracking models often fail to handle unseen
services. In this paper, we propose SGD-QA, a simple and extensible model for
schema-guided dialogue state tracking based on a question answering approach.
The proposed multi-pass model shares a single encoder between the domain
information and dialogue utterance. The domain's description represents the
query and the dialogue utterance serves as the context. The model improves
performance on unseen services by at least 1.6x compared to single-pass
baseline models on the SGD dataset. SGD-QA shows competitive performance
compared to state-of-the-art multi-pass models while being significantly more
efficient in terms of memory consumption and training performance. We provide a
thorough discussion on the model with ablation study and error analysis.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08050</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08050</id><submitter>Hanxiao Liu</submitter><version version="v1"><date>Mon, 17 May 2021 17:55:04 GMT</date><size>642kb</size><source_type>D</source_type></version><title>Pay Attention to MLPs</title><authors>Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le</authors><categories>cs.LG cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformers have become one of the most important architectural innovations
in deep learning and have enabled many breakthroughs over the past few years.
Here we propose a simple attention-free network architecture, gMLP, based
solely on MLPs with gating, and show that it can perform as well as
Transformers in key language and vision applications. Our comparisons show that
self-attention is not critical for Vision Transformers, as gMLP can achieve the
same accuracy. For BERT, our model achieves parity with Transformers on
pretraining perplexity and is better on some downstream tasks. On finetuning
tasks where gMLP performs worse, making the gMLP model substantially larger can
close the gap with Transformers. In general, our experiments show that gMLP can
scale as well as Transformers over increased data and compute.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08051</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08051</id><submitter>Soumyadip Sengupta</submitter><version version="v1"><date>Mon, 17 May 2021 17:56:24 GMT</date><size>41127kb</size><source_type>D</source_type></version><title>A Light Stage on Every Desk</title><authors>Soumyadip Sengupta, Brian Curless, Ira Kemelmacher-Shlizerman, Steve
  Seitz</authors><categories>cs.CV cs.GR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Every time you sit in front of a TV or monitor, your face is actively
illuminated by time-varying patterns of light. This paper proposes to use this
time-varying illumination for synthetic relighting of your face with any new
illumination condition. In doing so, we take inspiration from the light stage
work of Debevec et al., who first demonstrated the ability to relight people
captured in a controlled lighting environment. Whereas existing light stages
require expensive, room-scale spherical capture gantries and exist in only a
few labs in the world, we demonstrate how to acquire useful data from a normal
TV or desktop monitor. Instead of subjecting the user to uncomfortable rapidly
flashing light patterns, we operate on images of the user watching a YouTube
video or other standard content. We train a deep network on images plus monitor
patterns of a given user and learn to predict images of that user under any
target illumination (monitor pattern). Experimental evaluation shows that our
method produces realistic relighting results. Video results are available at
http://grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08052</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08052</id><submitter>Boyuan Chen</submitter><version version="v1"><date>Mon, 17 May 2021 17:58:41 GMT</date><size>19996kb</size><source_type>D</source_type></version><title>The Boombox: Visual Reconstruction from Acoustic Vibrations</title><authors>Boyuan Chen, Mia Chiquier, Hod Lipson, Carl Vondrick</authors><categories>cs.CV cs.MM cs.RO cs.SD eess.AS</categories><comments>Website: boombox.cs.columbia.edu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce The Boombox, a container that uses acoustic vibrations to
reconstruct an image of its inside contents. When an object interacts with the
container, they produce small acoustic vibrations. The exact vibration
characteristics depend on the physical properties of the box and the object. We
demonstrate how to use this incidental signal in order to predict visual
structure. After learning, our approach remains effective even when a camera
cannot view inside the box. Although we use low-cost and low-power contact
microphones to detect the vibrations, our results show that learning from
multi-modal data enables us to transform cheap acoustic sensors into rich
visual sensors. Due to the ubiquity of containers, we believe integrating
perception capabilities into them will enable new applications in
human-computer interaction and robotics. Our project website is at:
boombox.cs.columbia.edu
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08053</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08053</id><submitter>Charles Ellis</submitter><version version="v1"><date>Mon, 17 May 2021 17:58:55 GMT</date><size>882kb</size></version><title>Algorithm-Agnostic Explainability for Unsupervised Clustering</title><authors>Charles A. Ellis, Mohammad S.E. Sendi, Sergey M. Plis, Robyn L.
  Miller, and Vince D. Calhoun</authors><categories>cs.LG</categories><comments>11 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Supervised machine learning explainability has greatly expanded in recent
years. However, the field of unsupervised clustering explainability has lagged
behind. Here, we, to the best of our knowledge, demonstrate for the first time
how model-agnostic methods for supervised machine learning explainability can
be adapted to provide algorithm-agnostic unsupervised clustering
explainability. We present two novel algorithm-agnostic explainability methods,
global permutation percent change (G2PC) feature importance and local
perturbation percent change (L2PC) feature importance, that can provide insight
into many clustering methods on a global level by identifying the relative
importance of features to a clustering algorithm and on a local level by
identifying the relative importance of features to the clustering of individual
samples. We demonstrate the utility of the methods for explaining five popular
clustering algorithms on low-dimensional, ground-truth synthetic datasets and
on high-dimensional functional network connectivity (FNC) data extracted from a
resting state functional magnetic resonance imaging (rs-fMRI) dataset of 151
subjects with schizophrenia (SZ) and 160 healthy controls (HC). Our proposed
explainability methods robustly identify the relative importance of features
across multiple clustering methods and could facilitate new insights into many
applications. We hope that this study will greatly accelerate the development
of the field of clustering explainability.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08054</identifier>
 <datestamp>2021-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08054</id><submitter>Yonglong Tian</submitter><version version="v1"><date>Mon, 17 May 2021 17:59:03 GMT</date><size>3551kb</size><source_type>D</source_type></version><title>Divide and Contrast: Self-supervised Learning from Uncurated Data</title><authors>Yonglong Tian, Olivier J. Henaff, Aaron van den Oord</authors><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Self-supervised learning holds promise in leveraging large amounts of
unlabeled data, however much of its progress has thus far been limited to
highly curated pre-training data such as ImageNet. We explore the effects of
contrastive learning from larger, less-curated image datasets such as YFCC, and
find there is indeed a large difference in the resulting representation
quality. We hypothesize that this curation gap is due to a shift in the
distribution of image classes -- which is more diverse and heavy-tailed --
resulting in less relevant negative samples to learn from. We test this
hypothesis with a new approach, Divide and Contrast (DnC), which alternates
between contrastive learning and clustering-based hard negative mining. When
pretrained on less curated datasets, DnC greatly improves the performance of
self-supervised learning on downstream tasks, while remaining competitive with
the current state-of-the-art on curated datasets.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08057</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08057</id><submitter>Bryar Hassan Dr.</submitter><version version="v1"><date>Mon, 17 May 2021 19:47:34 GMT</date><size>134kb</size></version><title>Analysis for the Overwhelming Success of the Web Compared to Microcosm
  and Hyper-G Systems</title><authors>Bryar A. Hassan</authors><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Microcosm, Hyper-G, and the Web were developed and released after 1989. There
were strengths and weaknesses associate with each of these hypertext systems.
The architectures of these systems were relatively different from one another.
Standing above its competitors, the Web became the largest and most popular
information system. This paper analyses the reasons for which the Web became
the first successful hypermedia system by looking and evaluating the
architecture of the Web, Hyper-G, and Microcosm systems. Three reasons will be
given beyond this success with some lessons to learn. Currently, Semantic Web
is a recent development of the Web to provide conceptual hypermedia. More
importantly, study of the Web with its impact on technical, socio-cultural, and
economical agendas is introduced as web science.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08058</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08058</id><submitter>Francesco Guzzi</submitter><version version="v1"><date>Tue, 18 May 2021 10:15:17 GMT</date><size>1899kb</size></version><title>A parameter refinement method for Ptychography based on Deep Learning
  concepts</title><authors>Francesco Guzzi, George Kourousias, Fulvio Bill\`e, Roberto Pugliese,
  Alessandra Gianoncelli and Sergio Carrato</authors><categories>eess.IV cs.CV cs.NA math.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  X-ray Ptychography is an advanced computational microscopy technique which is
delivering exceptionally detailed quantitative imaging of biological and
nanotechnology specimens. However coarse parametrisation in propagation
distance, position errors and partial coherence frequently menaces the
experiment viability. In this work we formally introduced these actors, solving
the whole reconstruction as an optimisation problem. A modern Deep Learning
framework is used to correct autonomously the setup incoherences, thus
improving the quality of a ptychography reconstruction. Automatic procedures
are indeed crucial to reduce the time for a reliable analysis, which has a
significant impact on all the fields that use this kind of microscopy. We
implemented our algorithm in our software framework, SciComPty, releasing it as
open-source. We tested our system on both synthetic datasets and also on real
data acquired at the TwinMic beamline of the Elettra synchrotron facility.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08059</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08059</id><submitter>Salman Ul Hassan Dar</submitter><version version="v1"><date>Sat, 15 May 2021 02:01:21 GMT</date><size>32602kb</size><source_type>D</source_type></version><title>Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial
  Transformers</title><authors>Yilmaz Korkmaz, Salman UH Dar, Mahmut Yurt, Muzaffer \&quot;Ozbey, Tolga
  \c{C}ukur</authors><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised deep learning has swiftly become a workhorse for accelerated MRI
in recent years, offering state-of-the-art performance in image reconstruction
from undersampled acquisitions. Training deep supervised models requires large
datasets of undersampled and fully-sampled acquisitions typically from a
matching set of subjects. Given scarce access to large medical datasets, this
limitation has sparked interest in unsupervised methods that reduce reliance on
fully-sampled ground-truth data. A common framework is based on the deep image
prior, where network-driven regularization is enforced directly during
inference on undersampled acquisitions. Yet, canonical convolutional
architectures are suboptimal in capturing long-range relationships, and
randomly initialized networks may hamper convergence. To address these
limitations, here we introduce a novel unsupervised MRI reconstruction method
based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a
deep adversarial network with cross-attention transformer blocks to map noise
and latent variables onto MR images. This unconditional network learns a
high-quality MRI prior in a self-supervised encoding task. A zero-shot
reconstruction is performed on undersampled test data, where inference is
performed by optimizing network parameters, latent and noise variables to
ensure maximal consistency to multi-coil MRI data. Comprehensive experiments on
brain MRI datasets clearly demonstrate the superior performance of SLATER
against several state-of-the-art unsupervised methods.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08086</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08086</id><submitter>Elizabeth Bennewitz</submitter><version version="v1"><date>Mon, 17 May 2021 18:00:57 GMT</date><size>3559kb</size><source_type>D</source_type></version><title>Neural Error Mitigation of Near-Term Quantum Simulations</title><authors>Elizabeth R. Bennewitz, Florian Hopfmueller, Bohdan Kulchytskyy, Juan
  Carrasquilla and Pooya Ronagh</authors><categories>quant-ph cs.AI cs.LG</categories><comments>18 pages, 4 main figures, 6 supplementary figures, 1 supplementary
  table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the promising applications of early quantum computers is the
simulation of quantum systems. Variational methods for near-term quantum
computers, such as the variational quantum eigensolver (VQE), are a promising
approach to finding ground states of quantum systems relevant in physics,
chemistry, and materials science. These approaches, however, are constrained by
the effects of noise as well as the limited quantum resources of near-term
quantum hardware, motivating the need for quantum error mitigation techniques
to reduce the effects of noise. Here we introduce $\textit{neural error
mitigation}$, a novel method that uses neural networks to improve estimates of
ground states and ground-state observables obtained using VQE on near-term
quantum computers. To demonstrate our method's versatility, we apply neural
error mitigation to finding the ground states of H$_2$ and LiH molecular
Hamiltonians, as well as the lattice Schwinger model. Our results show that
neural error mitigation improves the numerical and experimental VQE computation
to yield low-energy errors, low infidelities, and accurate estimations of
more-complex observables like order parameters and entanglement entropy,
without requiring additional quantum resources. Additionally, neural error
mitigation is agnostic to both the quantum hardware and the particular noise
channel, making it a versatile tool for quantum simulation. Applying quantum
many-body machine learning techniques to error mitigation, our method is a
promising strategy for extending the reach of near-term quantum computers to
solve complex quantum simulation problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08089</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08089</id><submitter>David Hafner</submitter><version version="v1"><date>Mon, 17 May 2021 18:01:47 GMT</date><size>190kb</size><source_type>D</source_type></version><title>A Measure of Research Taste</title><authors>Vladlen Koltun and David Hafner</authors><categories>cs.DL cs.AI</categories><comments>Results can be explored at https://cap-measure.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers are often evaluated by citation-based metrics. Such metrics can
inform hiring, promotion, and funding decisions. Concerns have been expressed
that popular citation-based metrics incentivize researchers to maximize the
production of publications. Such incentives may not be optimal for scientific
progress. Here we present a citation-based measure that rewards both
productivity and taste: the researcher's ability to focus on impactful
contributions. The presented measure, CAP, balances the impact of publications
and their quantity, thus incentivizing researchers to consider whether a
publication is a useful addition to the literature. CAP is simple,
interpretable, and parameter-free. We analyze the characteristics of CAP for
highly-cited researchers in biology, computer science, economics, and physics,
using a corpus of millions of publications and hundreds of millions of
citations with yearly temporal granularity. CAP produces qualitatively
plausible outcomes and has a number of advantages over prior metrics. Results
can be explored at https://cap-measure.org/
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08093</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08093</id><submitter>Gaurav Batra</submitter><version version="v1"><date>Mon, 17 May 2021 18:05:34 GMT</date><size>5561kb</size><source_type>D</source_type></version><title>Multiclass Classification using dilute bandit feedback</title><authors>Gaurav Batra, Naresh Manwani</authors><categories>cs.LG</categories><comments>14 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a new online learning framework for multiclass
classification called learning with diluted bandit feedback. At every time
step, the algorithm predicts a candidate label set instead of a single label
for the observed example. It then receives feedback from the environment
whether the actual label lies in this candidate label set or not. This feedback
is called &quot;diluted bandit feedback&quot;. Learning in this setting is even more
challenging than the bandit feedback setting, as there is more uncertainty in
the supervision. We propose an algorithm for multiclass classification using
dilute bandit feedback (MC-DBF), which uses the exploration-exploitation
strategy to predict the candidate set in each trial. We show that the proposed
algorithm achieves O(T^{1-\frac{1}{m+2}}) mistake bound if candidate label set
size (in each step) is m. We demonstrate the effectiveness of the proposed
approach with extensive simulations.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08095</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08095</id><submitter>Amin Nikanjam</submitter><version version="v1"><date>Mon, 17 May 2021 18:06:11 GMT</date><size>941kb</size><source_type>D</source_type></version><title>Automatic Fault Detection for Deep Learning Programs Using Graph
  Transformations</title><authors>Amin Nikanjam, Houssem Ben Braiek, Mohammad Mehdi Morovati, Foutse
  Khomh</authors><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, we are witnessing an increasing demand in both corporates and
academia for exploiting Deep Learning (DL) to solve complex real-world
problems. A DL program encodes the network structure of a desirable DL model
and the process by which the model learns from the training dataset. Like any
software, a DL program can be faulty, which implies substantial challenges of
software quality assurance, especially in safety-critical domains. It is
therefore crucial to equip DL development teams with efficient fault detection
techniques and tools. In this paper, we propose NeuraLint, a model-based fault
detection approach for DL programs, using meta-modelling and graph
transformations. First, we design a meta-model for DL programs that includes
their base skeleton and fundamental properties. Then, we construct a
graph-based verification process that covers 23 rules defined on top of the
meta-model and implemented as graph transformations to detect faults and design
inefficiencies in the generated models (i.e., instances of the meta-model).
First, the proposed approach is evaluated by finding faults and design
inefficiencies in 28 synthesized examples built from common problems reported
in the literature. Then NeuraLint successfully finds 64 faults and design
inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts
and GitHub repositories. The results show that NeuraLint effectively detects
faults and design issues in both synthesized and real-world examples with a
recall of 70.5 % and a precision of 100 %. Although the proposed meta-model is
designed for feedforward neural networks, it can be extended to support other
neural network architectures such as recurrent neural networks. Researchers can
also expand our set of verification rules to cover more types of issues in DL
programs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08096</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08096</id><submitter>Luca Arnaboldi `</submitter><version version="v1"><date>Mon, 17 May 2021 18:06:51 GMT</date><size>168kb</size><source_type>D</source_type></version><title>A Review of Intrusion Detection Systems and Their Evaluation in the IoT</title><authors>Luca Arnaboldi and Charles Morisset</authors><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Intrusion Detection Systems (IDS) are key components for securing critical
infrastructures, capable of detecting malicious activities on networks or
hosts. The procedure of implementing a IDS for Internet of Things (IoT)
networks is not without challenges due to the variability of these systems and
specifically the difficulty in accessing data. The specifics of these very
constrained devices render the design of an IDS capable of dealing with the
varied attacks a very challenging problem and a very active research subject.
In the current state of literature, a number of approaches have been proposed
to improve the efficiency of intrusion detection, catering to some of these
limitations, such as resource constraints and mobility. In this article, we
review works on IDS specifically for these kinds of devices from 2008 to 2018,
collecting a total of 51 different IDS papers. We summarise the current themes
of the field, summarise the techniques employed to train and deploy the IDSs
and provide a qualitative evaluations of these approaches. While these works
provide valuable insights and solutions for sub-parts of these constraints, we
discuss the limitations of these solutions as a whole, in particular what kinds
of attacks these approaches struggle to detect and the setup limitations that
are unique to this kind of system. We find that although several paper claim
novelty of their approach little inter paper comparisons have been made, that
there is a dire need for sharing of datasets and almost no shared code
repositories, consequently raising the need for a thorough comparative
evaluation.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08098</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08098</id><submitter>Nikita Koval</submitter><version version="v1"><date>Mon, 17 May 2021 18:11:49 GMT</date><size>1333kb</size><source_type>D</source_type></version><title>A Scalable Concurrent Algorithm for Dynamic Connectivity</title><authors>Alexander Fedorov, Nikita Koval, Dan Alistarh</authors><categories>cs.DS cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dynamic Connectivity is a fundamental algorithmic graph problem, motivated by
a wide range of applications to social and communication networks and used as a
building block in various other algorithms, such as the bi-connectivity and the
dynamic minimal spanning tree problems. In brief, we wish to maintain the
connected components of the graph under dynamic edge insertions and deletions.
In the sequential case, the problem has been well-studied from both theoretical
and practical perspectives. However, much less is known about efficient
concurrent solutions to this problem. This is the gap we address in this paper.
  We start from one of the classic data structures used to solve this problem,
the Euler Tour Tree. Our first contribution is a non-blocking single-writer
implementation of it. We leverage this data structure to obtain the first truly
concurrent generalization of dynamic connectivity, which preserves the time
complexity of its sequential counterpart, but is also scalable in practice. To
achieve this, we rely on three main techniques. The first is to ensure that
connectivity queries, which usually dominate real-world workloads, are
non-blocking. The second non-trivial technique expands the above idea by making
all queries that do not change the connectivity structure non-blocking. The
third ingredient is applying fine-grained locking for updating the connected
components, which allows operations on disjoint components to occur in
parallel.
  We evaluate the resulting algorithm on various workloads, executing on both
real and synthetic graphs. The results show the efficiency of each of the
proposed optimizations; the most efficient variant improves the performance of
a coarse-grained based implementation on realistic scenarios up to 6x on
average and up to 30x when connectivity queries dominate.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08100</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08100</id><submitter>Nishanth Anandanadarajah</submitter><version version="v1"><date>Mon, 17 May 2021 18:18:57 GMT</date><size>2783kb</size></version><title>An Integrated Deep Learning and Dynamic Programming Method for
  Predicting Tumor Suppressor Genes, Oncogenes, and Fusion from PDB Structures</title><authors>Nishanth. Anandanadarajah, C.H. Chu, R. Loganantharaj</authors><categories>cs.LG eess.IV q-bio.QM</categories><journal-ref>Computers in Biology and Medicine, Volume 133, 2021, 104323, ISSN
  0010-4825</journal-ref><doi>10.1016/j.compbiomed.2021.104323</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Mutations in proto-oncogenes (ONGO) and the loss of regulatory function of
tumor suppression genes (TSG) are the common underlying mechanism for
uncontrolled tumor growth. While cancer is a heterogeneous complex of distinct
diseases, finding the potentiality of the genes related functionality to ONGO
or TSG through computational studies can help develop drugs that target the
disease. This paper proposes a classification method that starts with a
preprocessing stage to extract the feature map sets from the input 3D protein
structural information. The next stage is a deep convolutional neural network
stage (DCNN) that outputs the probability of functional classification of
genes. We explored and tested two approaches: in Approach 1, all filtered and
cleaned 3D-protein-structures (PDB) are pooled together, whereas in Approach 2,
the primary structures and their corresponding PDBs are separated according to
the genes' primary structural information. Following the DCNN stage, a dynamic
programming-based method is used to determine the final prediction of the
primary structures' functionality. We validated our proposed method using the
COSMIC online database. For the ONGO vs TSG classification problem, the AUROC
of the DCNN stage for Approach 1 and Approach 2 DCNN are 0.978 and 0.765,
respectively. The AUROCs of the final genes' primary structure functionality
classification for Approach 1 and Approach 2 are 0.989, and 0.879,
respectively. For comparison, the current state-of-the-art reported AUROC is
0.924.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08105</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08105</id><submitter>Jaroslaw Szlichta</submitter><version version="v1"><date>Mon, 17 May 2021 18:33:48 GMT</date><size>4870kb</size><source_type>D</source_type></version><title>Discovery and Contextual Data Cleaning with Ontology Functional
  Dependencies</title><authors>Zheng Zheng, Longtao Zheng, Fei Chiang, Lukasz Golab, Jaroslaw
  Szlichta</authors><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional Dependencies (FDs) define attribute relationships based on
syntactic equality, and, when usedin data cleaning, they erroneously label
syntactically different but semantically equivalent values as errors. We
explore dependency-based data cleaning with Ontology Functional
Dependencies(OFDs), which express semantic attribute relationships such as
synonyms and is-a hierarchies defined by an ontology. We study the theoretical
foundations for OFDs, including sound and complete axioms and a linear-time
inference procedure. We then propose an algorithm for discovering OFDs (exact
ones and ones that hold with some exceptions) from data that uses the axioms to
prune the search space. Towards enabling OFDs as data quality rules in
practice, we study the problem of finding minimal repairs to a relation and
ontology with respect to a set of OFDs. We demonstrate the effectiveness of our
techniques on real datasets, and show that OFDs can significantly reduce the
number of false positive errors in data cleaning techniques that rely on
traditional FDs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08106</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08106</id><submitter>Nikita Bhalla</submitter><version version="v1"><date>Mon, 17 May 2021 18:35:24 GMT</date><size>4056kb</size><source_type>D</source_type></version><title>Multi-Modal Image Captioning for the Visually Impaired</title><authors>Hiba Ahsan, Nikita Bhalla, Daivat Bhatt, Kaivankumar Shah</authors><categories>cs.CL</categories><comments>8 pages, 2 figures, 2 tables, accepted to NAACL-HLT SRW 2021</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the ways blind people understand their surroundings is by clicking
images and relying on descriptions generated by image captioning systems.
Current work on captioning images for the visually impaired do not use the
textual data present in the image when generating captions. This problem is
critical as many visual scenes contain text. Moreover, up to 21% of the
questions asked by blind people about the images they click pertain to the text
present in them. In this work, we propose altering AoANet, a state-of-the-art
image captioning model, to leverage the text detected in the image as an input
feature. In addition, we use a pointer-generator mechanism to copy the detected
text to the caption when tokens need to be reproduced accurately. Our model
outperforms AoANet on the benchmark dataset VizWiz, giving a 35% and 16.2%
performance improvement on CIDEr and SPICE scores, respectively.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08109</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08109</id><submitter>Yangming Zhao</submitter><version version="v1"><date>Mon, 17 May 2021 18:36:06 GMT</date><size>573kb</size></version><title>Quantum Transport Protocols for Distributed Quantum Computing</title><authors>Yangming Zhao and Chunming Qiao</authors><categories>cs.NI cs.ET</categories><comments>16 pages, 27 figures, will be submitted to an ACM conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computing holds a great promise and this work proposes to use new
quantum data networks (QDNs) to connect multiple small quantum computers to
form a cluster. Such a QDN differs from existing QKD networks in that the
former must deliver data qubits reliably within itself. Two types of QDNs are
studied, one using teleportation and the other using tell-and-go (TAG) to
exchange quantum data. Two corresponding quantum transport protocols (QTPs),
named Tele-QTP and TAG-QTP, are proposed to address many unique design
challenges involved in reliable delivery of data qubits, and constraints
imposed by quantum physics laws such as the no-cloning theorem, and limited
availability of quantum memory.
  The proposed Tele-QTP and TAG-QTP are the first transport layer protocols for
QDNs, complementing other works on the network protocol stack. Tele-QTP and
TAG-QTP have novel mechanisms to support congestion-free and reliable delivery
of streams of data qubits by managing the limited quantum memory at end hosts
as well as intermediate nodes. Both analysis and extensive simulations show
that the proposed QTPs can achieve a high throughput and fairness. This study
also offers new insights into potential tradeoffs involved in using the two
methods, teleportation and TAG, in two types of QDNs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08110</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08110</id><submitter>Guangzhao Cheng</submitter><version version="v1"><date>Mon, 17 May 2021 18:40:08 GMT</date><size>399kb</size><source_type>D</source_type></version><title>To be a fast adaptive learner: using game history to defeat opponents</title><authors>Guangzhao Cheng and Siliang Tang</authors><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world games, such as traders repeatedly bargaining with
customers, it is very hard for a single AI trader to make good deals with
various customers in a few turns, since customers may adopt different
strategies even the strategies they choose are quite simple. In this paper, we
model this problem as fast adaptive learning in the finitely repeated games. We
believe that past game history plays a vital role in such a learning procedure,
and therefore we propose a novel framework (named, F3) to fuse the past and
current game history with an Opponent Action Estimator (OAE) module that uses
past game history to estimate the opponent's future behaviors. The experiments
show that the agent trained by F3 can quickly defeat opponents who adopt
unknown new strategies. The F3 trained agent obtains more rewards in a fixed
number of turns than the agents that are trained by deep reinforcement
learning. Further studies show that the OAE module in F3 contains
meta-knowledge that can even be transferred across different games.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08111</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08111</id><submitter>Thomas Schumacher</submitter><version version="v1"><date>Mon, 17 May 2021 18:45:22 GMT</date><size>101kb</size><source_type>D</source_type></version><title>Livewired Neural Networks: Making Neurons That Fire Together Wire
  Together</title><authors>Thomas Schumacher</authors><categories>cs.NE cs.LG q-bio.NC</categories><comments>34 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Until recently, artificial neural networks were typically designed with a
fixed network structure. Here, I argue that network structure is highly
relevant to function, and therefore neural networks should be livewired
(Eagleman 2020): dynamically rewired to reflect relationships between higher
order representations of the external environment identified by coincident
activations in individual neurons. I discuss how this approach may enable such
networks to build compositional world models that operate on symbols and that
achieve few-shot learning, capabilities thought by many to be critical to
human-level cognition. Here, I also 1) discuss how such livewired neural
networks maximize the information the environment provides to a model, 2)
explore evidence indicating that livewiring is implemented in the brain, guided
by glial cells, 3) discuss how livewiring may give rise to the associative
emergent behaviors of brains, and 4) suggest paths for future research using
livewired networks to understand and create human-like reasoning.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08113</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08113</id><submitter>Omer Bobrowski</submitter><version version="v1"><date>Mon, 17 May 2021 18:50:03 GMT</date><size>2093kb</size><source_type>D</source_type></version><title>A Coupled Alpha Complex</title><authors>Yohai Reani, Omer Bobrowski</authors><categories>cs.CG math.AT</categories><msc-class>55N31, 62R40, 68T09</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alpha complex is a subset of the Delaunay triangulation and is often used
in computational geometry and topology. One of the main drawbacks of using the
alpha complex is that it is non-monotone, in the sense that if ${\cal
X}\subset{\cal X}'$ it is not necessarily (and generically not) the case that
the corresponding alpha complexes satisfy ${\cal A}_r({\cal X})\subset{\cal
A}_r({\cal X}')$. The lack of monotonicity may introduce significant
computational costs when using the alpha complex, and in some cases even render
it unusable. In this work we present a new construction based on the alpha
complex, that is homotopy equivalent to the alpha complex while maintaining
monotonicity. We provide the formal definitions and algorithms required to
construct this complex, and to compute its homology. In addition, we analyze
the size of this complex in order to argue that it is not significantly more
costly to use than the standard alpha complex.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08114</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08114</id><submitter>Jun-Woo Tak</submitter><version version="v1"><date>Mon, 17 May 2021 18:51:37 GMT</date><size>547kb</size></version><title>Weakly Private Information Retrieval Under R\'enyi Divergence</title><authors>Jun-Woo Tak, Sang-Hyo Kim, Yongjune Kim, Jong-Seon No</authors><categories>cs.IT eess.SP math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Private information retrieval (PIR) is a protocol that guarantees the privacy
of a user who is in communication with databases. The user wants to download
one of the messages stored in the databases while hiding the identity of the
desired message. Recently, the benefits that can be obtained by weakening the
privacy requirement have been studied, but the definition of weak privacy needs
to be elaborated upon. In this paper, we attempt to quantify the weak privacy
(i.e., information leakage) in PIR problems by using the R\'enyi divergence
that generalizes the Kullback-Leibler divergence. By introducing R\'enyi
divergence into the existing PIR problem, the tradeoff relationship between
privacy (information leakage) and PIR performance (download cost) is
characterized via convex optimization. Furthermore, we propose an alternative
PIR scheme with smaller message sizes than the Tian-Sun-Chen (TSC) scheme. The
proposed scheme cannot achieve the PIR capacity of perfect privacy since the
message size of the TSC scheme is the minimum to achieve the PIR capacity.
However, we show that the proposed scheme can be better than the TSC scheme in
the weakly PIR setting, especially under a low download cost regime.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08115</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08115</id><submitter>Eduardo Fernandez</submitter><version version="v1"><date>Mon, 17 May 2021 18:55:15 GMT</date><size>13572kb</size><source_type>D</source_type></version><title>Topology Optimization for Large-Scale Additive Manufacturing: Generating
  designs tailored to the deposition nozzle size</title><authors>Eduardo Fern\'andez, Can Ayas, Matthijs Langelaar, Pierre Duysinx</authors><categories>cs.CG math.OC</categories><journal-ref>Virtual and Physical Prototyping (2021): 1-25</journal-ref><doi>10.1080/17452759.2021.1914893</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Additive Manufacturing (AM) processes intended for large scale components
deposit large volumes of material to shorten process duration. This reduces the
resolution of the AM process, which is typically defined by the size of the
deposition nozzle. If the resolution limitation is not considered when
designing for Large-Scale Additive Manufacturing (LSAM), difficulties can arise
in the manufacturing process, which may require the adaptation of the
deposition parameters. This work incorporates the nozzle size constraint into
Topology Optimization (TO) in order to generate optimized designs suitable to
the process resolution. This article proposes and compares two methods, which
are based on existing TO techniques that enable control of minimum and maximum
member size, and of minimum cavity size. The first method requires the minimum
and maximum member size to be equal to the deposition nozzle size, thus design
features of uniform width are obtained in the optimized design. The second
method defines the size of the solid members sufficiently small for the
resulting structure to resemble a structural skeleton, which can be interpreted
as the deposition path. Through filtering and projection techniques, the thin
structures are thickened according to the chosen nozzle size. Thus, a topology
tailored to the size of the deposition nozzle is obtained along with a
deposition proposal. The methods are demonstrated and assessed using 2D and 3D
benchmark problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08116</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08116</id><submitter>Xie Xie</submitter><version version="v1"><date>Mon, 17 May 2021 19:00:42 GMT</date><size>4kb</size></version><title>A Neat Linked Queue with the Rear Sentinel</title><authors>Xie Xie</authors><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a very simple queue implementation with the singly linked list.
With the help of the rear sentinel instead of the traditional header node, we
avoid additional check steps in the pop operation. The essence of this
representation is the half-opened pointer interval, which can guarantee the
uniform treatment even in the empty queue case. We also present the variants of
linked queue, circularly linked queue and lazy circularly linked queue, which
can also be used to implement stack and improve the time performance in some
special cases.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08118</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08118</id><submitter>Manolis Antonoyiannakis</submitter><version version="v1"><date>Mon, 17 May 2021 19:06:56 GMT</date><size>2156kb</size><source_type>D</source_type></version><title>Does publicity in the science press drive citations? A vindication of
  peer review</title><authors>Manolis Antonoyiannakis</authors><categories>cs.DL cs.SI physics.soc-ph</categories><comments>Preprint of a chapter in the book &quot;Predicting the Dynamics of
  Research Impact&quot;, edited by T. Vergoulis and Y. Manolopoulos (Springer, to
  appear); 18 pages, 6 Figures, 4 Tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study how publicity, in the form of highlighting in the science press,
affects the citations of research papers. After a brief review of prior work,
we analyze papers published in Physical Review Letters (PRL) that are
highlighted across eight different platforms. Using multiple linear regression
we identify how each platform contributes to citations. We also analyze how
frequently the highlighted papers end up in the top 1% cited papers in their
field. We find that the strongest predictors of medium-term citation impact --
up to 7 years post-publication -- are Viewpoints in Physics, followed by
Research Highlights in Nature, Editors' Suggestions in PRL, and Research
Highlights in Nature Physics. Our key conclusions are that (a) highlighting for
importance identifies a citation advantage, which (b) is stratified according
to the degree of vetting during peer review (internal and external to the
journal). This implies that we can view highlighting platforms as predictors of
citation accrual, with varying degrees of strength that mirror each platform's
vetting level.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08120</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08120</id><submitter>Sergey Afanasiev</submitter><version version="v1"><date>Mon, 17 May 2021 19:16:32 GMT</date><size>561kb</size></version><title>Itsy Bitsy SpiderNet: Fully Connected Residual Network for Fraud
  Detection</title><authors>Sergey Afanasiev, Anastasiya Smirnova and Diana Kotereva</authors><categories>cs.LG cs.CR</categories><comments>11 pages, 7 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the development of high technology, the scope of fraud is increasing,
resulting in annual losses of billions of dollars worldwide. The preventive
protection measures become obsolete and vulnerable over time, so effective
detective tools are needed. In this paper, we propose a convolutional neural
network architecture SpiderNet designed to solve fraud detection problems. We
noticed that the principles of pooling and convolutional layers in neural
networks are very similar to the way antifraud analysts work when conducting
investigations. Moreover, the skip-connections used in neural networks make the
usage of features of various power in antifraud models possible. Our
experiments have shown that SpiderNet provides better quality compared to
Random Forest and adapted for antifraud modeling problems 1D-CNN, 1D-DenseNet,
F-DenseNet neural networks. We also propose new approaches for fraud feature
engineering called B-tests and W-tests, which generalize the concepts of
Benford's Law for fraud anomalies detection. Our results showed that B-tests
and W-tests give a significant increase to the quality of our antifraud models.
The SpiderNet code is available at https://github.com/aasmirnova24/SpiderNet
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08122</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08122</id><submitter>Omid Ardakanian</submitter><version version="v1"><date>Mon, 17 May 2021 19:24:36 GMT</date><size>4903kb</size><source_type>D</source_type></version><title>A Data-Efficient Approach to Behind-the-Meter Solar Generation
  Disaggregation</title><authors>Xinlei Chen and Moosa Moghimi Haji and Omid Ardakanian</authors><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of cost effective battery storage and the decline in the
solar photovoltaic (PV) levelized cost of energy (LCOE), the number of
behind-the-meter solar PV systems is expected to increase steadily. The ability
to estimate solar generation from these latent systems is crucial for a range
of applications, including distribution system planning and operation, demand
response, and non-intrusive load monitoring (NILM). This paper investigates the
problem of disaggregating solar generation from smart meter data when
historical disaggregated data from the target home is unavailable, and
deployment characteristics of the PV system are unknown. The proposed approach
entails inferring the physical characteristics from smart meter data and
disaggregating solar generation using an iterative algorithm. This algorithm
takes advantage of solar generation data (aka proxy measurements) from a few
sites that are located in the same area as the target home, and solar
generation data synthesized using a physical PV model. We evaluate our methods
with 4 different proxy settings on around 160 homes in the United States and
Australia, and show that the solar disaggregation accuracy is improved by
32.31% and 15.66% over two state-of-the-art methods using only one real proxy
along with three synthetic proxies. Furthermore, we demonstrate that using the
disaggregated home load rather than the net load data could improve the overall
accuracy of three popular NILM methods by at least 22%.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08123</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08123</id><submitter>Ataberk Olgun</submitter><version version="v1"><date>Mon, 17 May 2021 19:27:48 GMT</date><size>689kb</size><source_type>D</source_type></version><title>MetaSys: A Practical Open-Source Metadata Management System to Implement
  and Evaluate Cross-Layer Optimizations</title><authors>Nandita Vijaykumar (1), Ataberk Olgun (3), Konstantinos Kanellopoulos
  (5), Nisa Bostanci (3), Hasan Hassan (5), Mehrshad Lotfi (4), Phillip B.
  Gibbons (2), Onur Mutlu (5) ((1) University of Toronto, (2) Carnegie Mellon
  University, (3) TOBB University of Economics and Technology, (4) Max Plank
  Institute, (5) ETH Z\&quot;urich)</authors><categories>cs.AR</categories><comments>14 pages, 16 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces the first open-source FPGA-based infrastructure,
MetaSys, with a prototype in a RISC-V core, to enable the rapid implementation
and evaluation of a wide range of cross-layer techniques in real hardware.
Hardware-software cooperative techniques are powerful approaches to improve the
performance, quality of service, and security of general-purpose processors.
They are however typically challenging to rapidly implement and evaluate in
real hardware as they require full-stack changes to the hardware, OS, system
software, and instruction-set architecture (ISA).
  MetaSys implements a rich hardware-software interface and lightweight
metadata support that can be used as a common basis to rapidly implement and
evaluate new cross-layer techniques. We demonstrate MetaSys's versatility and
ease-of-use by implementing and evaluating three cross-layer techniques for:
(i) prefetching for graph analytics; (ii) bounds checking in memory unsafe
languages, and (iii) return address protection in stack frames; each technique
only requiring ~100 lines of Chisel code over MetaSys.
  Using MetaSys, we perform the first detailed experimental study to quantify
the performance overheads of using a single metadata management system to
enable multiple cross-layer optimizations in CPUs. We identify the key sources
of bottlenecks and system inefficiency of a general metadata management system.
We design MetaSys to minimize these inefficiencies and provide increased
versatility compared to previously-proposed metadata systems. Using three use
cases and a detailed characterization, we demonstrate that a common metadata
management system can be used to efficiently support diverse cross-layer
techniques in CPUs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08124</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08124</id><submitter>Giordano Da Lozzo</submitter><version version="v1"><date>Mon, 17 May 2021 19:28:44 GMT</date><size>332kb</size><source_type>D</source_type></version><title>Planar Drawings with Few Slopes of Halin Graphs and Nested Pseudotrees</title><authors>Steven Chaplick and Giordano Da Lozzo and Emilio Di Giacomo and
  Giuseppe Liotta and Fabrizio Montecchiani</authors><categories>cs.CG</categories><comments>Extended version of &quot;Planar Drawings with Few Slopes of Halin Graphs
  and Nested Pseudotrees&quot; to appear in the Proceedings of the 17th Algorithms
  and Data Structures Symposium (WADS 2021)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The $\textit{planar slope number}$ $psn(G)$ of a planar graph $G$ is the
minimum number of edge slopes in a planar straight-line drawing of $G$. It is
known that $psn(G) \in O(c^\Delta)$ for every planar graph $G$ of degree
$\Delta$. This upper bound has been improved to $O(\Delta^5)$ if $G$ has
treewidth three, and to $O(\Delta)$ if $G$ has treewidth two. In this paper we
prove $psn(G) \in \Theta(\Delta)$ when $G$ is a Halin graph, and thus has
treewidth three. Furthermore, we present the first polynomial upper bound on
the planar slope number for a family of graphs having treewidth four. Namely we
show that $O(\Delta^2)$ slopes suffice for nested pseudotrees.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08127</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08127</id><submitter>Luke Melas-Kyriazi</submitter><version version="v1"><date>Mon, 17 May 2021 19:34:24 GMT</date><size>4794kb</size><source_type>D</source_type></version><title>Finding an Unsupervised Image Segmenter in Each of Your Deep Generative
  Models</title><authors>Luke Melas-Kyriazi and Christian Rupprecht and Iro Laina and Andrea
  Vedaldi</authors><categories>cs.CV cs.AI</categories><comments>Project page and GitHub link:
  https://lukemelas.github.io/unsupervised-image-segmentation &amp;
  https://github.com/lukemelas/unsupervised-image-segmentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has shown that numerous human-interpretable directions exist
in the latent space of GANs. In this paper, we develop an automatic procedure
for finding directions that lead to foreground-background image separation, and
we use these directions to train an image segmentation model without human
supervision. Our method is generator-agnostic, producing strong segmentation
results with a wide range of different GAN architectures. Furthermore, by
leveraging GANs pretrained on large datasets such as ImageNet, we are able to
segment images from a range of domains without further training or finetuning.
Evaluating our method on image segmentation benchmarks, we compare favorably to
prior work while using neither human supervision nor access to the training
data. Broadly, our results demonstrate that automatically extracting
foreground-background structure from pretrained deep generative models can
serve as a remarkably effective substitute for human supervision.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08128</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08128</id><submitter>Luke Melas-Kyriazi</submitter><version version="v1"><date>Mon, 17 May 2021 19:36:28 GMT</date><size>20131kb</size><source_type>D</source_type></version><title>PixMatch: Unsupervised Domain Adaptation via Pixelwise Consistency
  Training</title><authors>Luke Melas-Kyriazi and Arjun K. Manrai</authors><categories>cs.CV cs.AI</categories><comments>CVPR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised domain adaptation is a promising technique for semantic
segmentation and other computer vision tasks for which large-scale data
annotation is costly and time-consuming. In semantic segmentation, it is
attractive to train models on annotated images from a simulated (source) domain
and deploy them on real (target) domains. In this work, we present a novel
framework for unsupervised domain adaptation based on the notion of
target-domain consistency training. Intuitively, our work is based on the idea
that in order to perform well on the target domain, a model's output should be
consistent with respect to small perturbations of inputs in the target domain.
Specifically, we introduce a new loss term to enforce pixelwise consistency
between the model's predictions on a target image and a perturbed version of
the same image. In comparison to popular adversarial adaptation methods, our
approach is simpler, easier to implement, and more memory-efficient during
training. Experiments and extensive ablation studies demonstrate that our
simple approach achieves remarkably strong results on two challenging
synthetic-to-real benchmarks, GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes.
  Code is available at: https://github.com/lukemelas/pixmatch
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08131</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08131</id><submitter>Bryar Hassan Dr.</submitter><version version="v1"><date>Mon, 17 May 2021 19:49:45 GMT</date><size>654kb</size></version><title>A New Framework to Adopt Multidimensional Databases for Organizational
  Information System Strategies</title><authors>Bryar A. Hassan, Shko M. Qader</authors><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  As information becomes increasingly sizable for organizations to maintain the
challenge of organizing data still remains. More importantly, the on-going
process of analysing incoming data occurs on a continual basis and
organizations should employ existing procedures that may not be adequate or
efficient when attempting to access specific information to analyse. In these
latter days of technological advancement, organizations can offer their
customers extensive data resources to utilize and thus accomplish individual
objectives and maintain competitiveness; however, it remains a challenge in
providing data in a format that serves each clients suited needs. For some, the
complexity of a data model can be overwhelming to utilize. Furthermore,
companies should secure an understanding of the purchasing power used by
specific consumer groups to remain competitive and ease the operation of data
analysis. This research paper is to examine the use of multi-dimensional models
within a business environment and how it may provide customers and managers
with generating queries that will provide accurate and relevant data for
effective analysis. It also provides a new framework that can aid various types
of organisations using sizable database systems to create their own
multidimensional model from relational databases and present the data in
multidimensional views. It also defines the requirements. Despite the
availability of set tools, the complexity of utilizing the conceptions
discourages customers as they may become apprehensive about exploring these
options for analytical purposes. This could be done by conducting a query.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08132</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08132</id><submitter>Mahvish Samar</submitter><version version="v1"><date>Mon, 17 May 2021 19:53:13 GMT</date><size>148kb</size></version><title>Structured condition numbers for the total least squares problem with
  linear equality constraint and their statistical estimation</title><authors>Mahvish Samar</authors><categories>math.NA cs.NA</categories><msc-class>65F20, 65F35, 65F30, 15A12, 15A60</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive the mixed and componentwise condition numbers for a
linear function of the solution to the total least squares with linear equality
constraint (TLSE) problem. The explicit expressions of the mixed and
componentwise condition numbers by dual techniques under both unstructured and
structured componentwise perturbations is considered. With the intermediate
result, i.e. we can recover the both unstructured and structured condition
number for the TLS problem. We choose the small-sample statistical condition
estimation method to estimate both unstructured and structured condition
numbers with high reliability. Numerical experiments are provided to illustrate
the obtained results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08140</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08140</id><submitter>Yue Wu</submitter><version version="v1"><date>Mon, 17 May 2021 20:16:46 GMT</date><size>37325kb</size><source_type>D</source_type></version><title>Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning</title><authors>Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian
  Zhang, Ruslan Salakhutdinov, Hanlin Goh</authors><categories>cs.LG</categories><comments>To appear in ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offline Reinforcement Learning promises to learn effective policies from
previously-collected, static datasets without the need for exploration.
However, existing Q-learning and actor-critic based off-policy RL algorithms
fail when bootstrapping from out-of-distribution (OOD) actions or states. We
hypothesize that a key missing ingredient from the existing methods is a proper
treatment of uncertainty in the offline setting. We propose Uncertainty
Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs
and down-weights their contribution in the training objectives accordingly.
Implementation-wise, we adopt a practical and effective dropout-based
uncertainty estimation method that introduces very little overhead over
existing RL algorithms. Empirically, we observe that UWAC substantially
improves model stability during training. In addition, UWAC out-performs
existing offline RL methods on a variety of competitive tasks, and achieves
significant performance gains over the state-of-the-art baseline on datasets
with sparse demonstrations collected from human experts.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08141</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08141</id><submitter>Srijan Das</submitter><version version="v1"><date>Mon, 17 May 2021 20:19:47 GMT</date><size>4163kb</size><source_type>D</source_type></version><title>VPN++: Rethinking Video-Pose embeddings for understanding Activities of
  Daily Living</title><authors>Srijan Das, Rui Dai, Di Yang, Francois Bremond</authors><categories>cs.CV cs.AI</categories><comments>submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many attempts have been made towards combining RGB and 3D poses for the
recognition of Activities of Daily Living (ADL). ADL may look very similar and
often necessitate to model fine-grained details to distinguish them. Because
the recent 3D ConvNets are too rigid to capture the subtle visual patterns
across an action, this research direction is dominated by methods combining RGB
and 3D Poses. But the cost of computing 3D poses from RGB stream is high in the
absence of appropriate sensors. This limits the usage of aforementioned
approaches in real-world applications requiring low latency. Then, how to best
take advantage of 3D Poses for recognizing ADL? To this end, we propose an
extension of a pose driven attention mechanism: Video-Pose Network (VPN),
exploring two distinct directions. One is to transfer the Pose knowledge into
RGB through a feature-level distillation and the other towards mimicking pose
driven attention through an attention-level distillation. Finally, these two
approaches are integrated into a single model, we call VPN++. We show that
VPN++ is not only effective but also provides a high speed up and high
resilience to noisy Poses. VPN++, with or without 3D Poses, outperforms the
representative baselines on 4 public datasets. Code is available at
https://github.com/srijandas07/vpnplusplus.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08143</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08143</id><submitter>Steve Heim</submitter><version version="v1"><date>Mon, 17 May 2021 20:22:32 GMT</date><size>2916kb</size><source_type>D</source_type></version><title>On exploration requirements for learning safety constraints</title><authors>Pierre-Fran\c{c}ois Massiani, Steve Heim, Sebastian Trimpe</authors><categories>eess.SY cs.SY</categories><comments>L4DC 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enforcing safety for dynamical systems is challenging, since it requires
constraint satisfaction along trajectory predictions. Equivalent control
constraints can be computed in the form of sets that enforce positive
invariance, and can thus guarantee safety in feedback controllers without
predictions. However, these constraints are cumbersome to compute from models,
and it is not yet well established how to infer constraints from data. In this
paper, we shed light on the key objects involved in learning control
constraints from data in a model-free setting. In particular, we discuss the
family of constraints that enforce safety in the context of a nominal control
policy, and expose that these constraints do not need to be accurate
everywhere. They only need to correctly exclude a subset of the state-actions
that would cause failure, which we call the critical set.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08145</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08145</id><submitter>Neset Unver Akmandor</submitter><version version="v1"><date>Mon, 17 May 2021 20:23:57 GMT</date><size>7017kb</size><source_type>D</source_type></version><title>Reactive Navigation Framework for Mobile Robots by Heuristically
  Evaluated Pre-sampled Trajectories</title><authors>Ne\c{s}et \&quot;Unver Akmandor and Ta\c{s}k{\i}n Pad{\i}r</authors><categories>cs.RO</categories><comments>This paper is accepted for publication in International Journal of
  Robotic Computing (IJRC). arXiv admin note: substantial text overlap with
  arXiv:2001.09199</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes and analyzes a reactive navigation framework for mobile
robots in unknown environments. The approach does not rely on a global map and
only considers the local occupancy in its robot-centered 3D grid structure. The
proposed algorithm enables fast navigation by heuristic evaluations of
pre-sampled trajectories on-the-fly. At each cycle, these paths are evaluated
by a weighted cost function, based on heuristic features such as closeness to
the goal, previously selected trajectories, and nearby obstacles. This paper
introduces a systematic method to calculate a feasible pose on the selected
trajectory, before sending it to the controller for the motion execution.
Defining the structures in the framework and providing the implementation
details, the paper also explains how to adjust its offline and online
parameters. To demonstrate the versatility and adaptability of the algorithm in
unknown environments, physics-based simulations on various maps are presented.
Benchmark tests show the superior performance of the proposed algorithm over
its previous iteration and another state-of-art method. The open-source
implementation of the algorithm and the benchmark data can be found at
\url{https://github.com/RIVeR-Lab/tentabot}.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08146</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08146</id><submitter>Yiqun Yao</submitter><version version="v1"><date>Mon, 17 May 2021 20:24:46 GMT</date><size>190kb</size><source_type>D</source_type></version><title>MUSER: MUltimodal Stress Detection using Emotion Recognition as an
  Auxiliary Task</title><authors>Yiqun Yao, Michalis Papakostas, Mihai Burzo, Mohamed Abouelenien, Rada
  Mihalcea</authors><categories>cs.CL cs.SD eess.AS</categories><comments>NAACL 2021 accepted</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The capability to automatically detect human stress can benefit artificial
intelligent agents involved in affective computing and human-computer
interaction. Stress and emotion are both human affective states, and stress has
proven to have important implications on the regulation and expression of
emotion. Although a series of methods have been established for multimodal
stress detection, limited steps have been taken to explore the underlying
inter-dependence between stress and emotion. In this work, we investigate the
value of emotion recognition as an auxiliary task to improve stress detection.
We propose MUSER -- a transformer-based model architecture and a novel
multi-task learning algorithm with speed-based dynamic sampling strategy.
Evaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our
model is effective for stress detection with both internal and external
auxiliary tasks, and achieves state-of-the-art results.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08147</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08147</id><submitter>Vignav Ramesh</submitter><version version="v1"><date>Mon, 17 May 2021 20:27:32 GMT</date><size>9131kb</size><source_type>D</source_type></version><title>COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN
  on Chest X-rays Automatically Computed from Volumetric CTs</title><authors>Vignav Ramesh, Blaine Rister, Daniel L. Rubin</authors><categories>eess.IV cs.CV cs.LG</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently
obtained to determine the extent of lung disease and are a valuable source of
data for creating artificial intelligence models. Most work to date assessing
disease severity on chest imaging has focused on segmenting computed tomography
(CT) images; however, given that CTs are performed much less frequently than
chest X-rays for COVID-19 patients, automated lung lesion segmentation on chest
X-rays could be clinically valuable. There currently exists a universal
shortage of chest X-rays with ground truth COVID-19 lung lesion annotations,
and manually contouring lung opacities is a tedious, labor-intensive task. To
accelerate severity detection and augment the amount of publicly available
chest X-ray training data for supervised deep learning (DL) models, we leverage
existing annotated CT images to generate frontal projection &quot;chest X-ray&quot;
images for training COVID-19 chest X-ray models. In this paper, we propose an
automated pipeline for segmentation of COVID-19 lung lesions on chest X-rays
comprised of a Mask R-CNN trained on a mixed dataset of open-source chest
X-rays and coronal X-ray projections computed from annotated volumetric CTs. On
a test set containing 40 chest X-rays of COVID-19 positive patients, our model
achieved IoU scores of 0.81 $\pm$ 0.03 and 0.79 $\pm$ 0.03 when trained on a
dataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50
projections from CTs, respectively. Our model far outperforms current baselines
with limited supervised training and may assist in automated COVID-19 severity
quantification on chest X-rays.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08148</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08148</id><submitter>Ryleigh Moore</submitter><version version="v1"><date>Mon, 17 May 2021 20:27:40 GMT</date><size>898kb</size><source_type>D</source_type></version><title>Adaptive Density Tracking by Quadrature for Stochastic Differential
  Equations</title><authors>Ryleigh A. Moore and Akil Narayan</authors><categories>math.NA cs.NA</categories><comments>20 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Density tracking by quadrature (DTQ) is a numerical procedure for computing
solutions to Fokker-Planck equations that describe probability densities for
stochastic differential equations (SDEs). In this paper, we extend upon
existing tensorized DTQ procedures by utilizing a flexible quadrature rule that
allows for unstructured, adaptive meshes. We propose and describe the procedure
for $N$-dimensions, and demonstrate that the resulting adaptive procedure is
significantly more efficient than a tensorized approach. Although we consider
two-dimensional examples, all our computational procedures are extendable to
higher dimensional problems.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08149</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08149</id><submitter>Xiaochen Yang</submitter><version version="v1"><date>Mon, 17 May 2021 20:27:59 GMT</date><size>213kb</size><source_type>D</source_type></version><title>Deep Metric Learning for Few-Shot Image Classification: A Selective
  Review</title><authors>Xiaoxu Li, Xiaochen Yang, Zhanyu Ma, Jing-Hao Xue</authors><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Few-shot image classification is a challenging problem which aims to achieve
the human level of recognition based only on a small number of images. Deep
learning algorithms such as meta-learning, transfer learning, and metric
learning have been employed recently and achieved the state-of-the-art
performance. In this survey, we review representative deep metric learning
methods for few-shot classification, and categorize them into three groups
according to the major problems and novelties they focus on. We conclude this
review with a discussion on current challenges and future trends in few-shot
image classification.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08150</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08150</id><submitter>Luke Eglington</submitter><version version="v1"><date>Mon, 17 May 2021 20:30:36 GMT</date><size>121kb</size></version><title>Modeling the EdNet Dataset with Logistic Regression</title><authors>Philip I. Pavlik Jr, Luke G. Eglington</authors><categories>cs.CY cs.AI cs.LG</categories><comments>5 pages, AAAI Workshop on AI in Education (Imagining Post-COVID
  Education with AI)</comments><journal-ref>AAAI 2021 Workshop on AI in Education</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many of these challenges are won by neural network models created by
full-time artificial intelligence scientists. Due to this origin, they have a
black-box character that makes their use and application less clear to learning
scientists. We describe our experience with competition from the perspective of
educational data mining, a field founded in the learning sciences and connected
with roots in psychology and statistics. We describe our efforts from the
perspectives of learning scientists and the challenges to our methods, some
real and some imagined. We also discuss some basic results in the Kaggle system
and our thoughts on how those results may have been improved. Finally, we
describe how learner model predictions are used to make pedagogical decisions
for students. Their practical use entails a) model predictions and b) a
decision rule (based on the predictions). We point out how increased model
accuracy can be of limited practical utility, especially when paired with
simple decision rules and argue instead for the need to further investigate
optimal decision rules.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08151</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08151</id><submitter>J\'eferson Campos Nobre</submitter><version version="v1"><date>Mon, 17 May 2021 20:39:55 GMT</date><size>62kb</size><source_type>D</source_type></version><title>On Using P2P Technology for Decentralized Detection of Service Level
  Agreement Violations</title><authors>J\'eferson C. Nobre, Lisandro Z. Granville, Alberto G. Prieto,
  Alexander Clemm</authors><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Critical networked services enable significant revenue for network operators
and, in turn, are regulated by Service Level Agreements (SLAs). In order to
ensure SLAs are being met, service levels need to be monitored. One technique
for this involves active measurement mechanisms which employ measurement probes
along the network to inject synthetic traffic and compute the network
performance. However, these mechanisms are expensive in terms of resources
consumption. Thus, these mechanisms usually can cover only a fraction of what
could be measured, which can lead to SLA violations being missed. Besides that,
the definition of this fraction is a practice done by human administrators,
which does not scale well and does not adapt to highly dynamic networking
patterns. In this article, we examine the potential benefits of using P2P
technology to improve the detection of SLA Violations. We first describe the
principles of a P2P-based steering of active measurement mechanisms. These
principles are characterized by a high degree of decentralized decision making
across a network using a self-organizing overlay. In a second step, we present
measurement session activation strategies based on these principles. These
strategies do not require human intervention, are adaptive to changes in
network conditions, and independent of the underlying active measurement
technology.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08154</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08154</id><submitter>Carolyn Matl</submitter><version version="v1"><date>Mon, 17 May 2021 20:49:07 GMT</date><size>5373kb</size><source_type>D</source_type></version><title>StRETcH: a Soft to Resistive Elastic Tactile Hand</title><authors>Carolyn Matl, Josephine Koe, Ruzena Bajcsy</authors><categories>cs.RO</categories><comments>8 pages, 8 figures, To be published in Proc. 2020 IEEE International
  Conference on Robotics and Automation (ICRA), expanded figures, added
  clarification for stiffness estimation (Sec. VI-B, paragraph 4)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Soft optical tactile sensors enable robots to manipulate deformable objects
by capturing important features such as high-resolution contact geometry and
estimations of object compliance. This work presents a variable stiffness soft
tactile end-effector called StRETcH, a Soft to Resistive Elastic Tactile Hand,
that is easily manufactured and integrated with a robotic arm. An elastic
membrane is suspended between two robotic fingers, and a depth sensor capturing
the deformations of the elastic membrane enables sub-millimeter accurate
estimates of contact geometries. The parallel-jaw gripper varies the stiffness
of the membrane by uni-axially stretching it, which controllably modulates
StRETcH's effective modulus from approximately 4kPa to 9kPa. This work uses
StRETcH to reconstruct the contact geometry of rigid and deformable objects,
estimate the stiffness of four balloons filled with different substances, and
manipulate dough into a desired shape.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08155</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08155</id><submitter>Daniel Jeffries</submitter><version version="v1"><date>Mon, 17 May 2021 20:49:56 GMT</date><size>46kb</size></version><title>(Deep) Induction Rules for GADTs</title><authors>Patricia Johann, Enrico Ghiorzi, Daniel Jeffries</authors><categories>cs.LO</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep data types are those that are defined in terms of other such data types,
including, possibly, themselves. In that case, they are said to be truly
nested. Deep induction is an extension of structural induction that traverses
all of the structure in a deep data type, propagating predicates on its
primitive data throughout the entire structure. Deep induction can be used to
prove properties of nested types, including truly nested types, that cannot be
proved via structural induction. In this paper we show how to extend deep
induction to deep GADTs that are not truly nested. We also show that deep
induction cannot be extended to truly nested GADTs.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08157</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08157</id><submitter>Eric Chen</submitter><version version="v1"><date>Mon, 17 May 2021 20:53:23 GMT</date><size>3761kb</size><source_type>D</source_type></version><title>Cardiac Functional Analysis with Cine MRI via Deep Learning
  Reconstruction</title><authors>Eric Z. Chen, Xiao Chen, Jingyuan Lyu, Qi Liu, Zhongqi Zhang, Yu Ding,
  Shuheng Zhang, Terrence Chen, Jian Xu, and Shanhui Sun</authors><categories>eess.IV cs.CV cs.LG</categories><comments>Presented at ISMRM 2021 as the digital poster</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrospectively gated cine (retro-cine) MRI is the clinical standard for
cardiac functional analysis. Deep learning (DL) based methods have been
proposed for the reconstruction of highly undersampled MRI data and show
superior image quality and magnitude faster reconstruction time than CS-based
methods. Nevertheless, it remains unclear whether DL reconstruction is suitable
for cardiac function analysis. To address this question, in this study we
evaluate and compare the cardiac functional values (EDV, ESV and EF for LV and
RV, respectively) obtained from highly accelerated MRI acquisition using DL
based reconstruction algorithm (DL-cine) with values from CS-cine and
conventional retro-cine. To the best of our knowledge, this is the first work
to evaluate the cine MRI with deep learning reconstruction for cardiac function
analysis and compare it with other conventional methods. The cardiac functional
values obtained from cine MRI with deep learning reconstruction are consistent
with values from clinical standard retro-cine MRI.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08158</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08158</id><submitter>Tao Li</submitter><version version="v1"><date>Mon, 17 May 2021 20:54:07 GMT</date><size>4050kb</size><source_type>D</source_type></version><title>The Confluence of Networks, Games and Learning</title><authors>Tao Li, Guanze Peng, Quanyan Zhu and Tamer Basar</authors><categories>cs.MA cs.AI cs.LG cs.SY eess.SY</categories><comments>The manuscript has been submitted to IEEE control system magazine
  under review, as part of the special issue &quot;Distributed Nash Equilibrium
  Seeking over Networks&quot;</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent years have witnessed significant advances in technologies and services
in modern network applications, including smart grid management, wireless
communication, cybersecurity as well as multi-agent autonomous systems.
Considering the heterogeneous nature of networked entities, emerging network
applications call for game-theoretic models and learning-based approaches in
order to create distributed network intelligence that responds to uncertainties
and disruptions in a dynamic or an adversarial environment. This paper
articulates the confluence of networks, games and learning, which establishes a
theoretical underpinning for understanding multi-agent decision-making over
networks. We provide an selective overview of game-theoretic learning
algorithms within the framework of stochastic approximation theory, and
associated applications in some representative contexts of modern network
systems, such as the next generation wireless communication networks, the smart
grid and distributed machine learning. In addition to existing research works
on game-theoretic learning over networks, we highlight several new angles and
research endeavors on learning in games that are related to recent developments
in artificial intelligence. Some of the new angles extrapolate from our own
research interests. The overall objective of the paper is to provide the reader
a clear picture of the strengths and challenges of adopting game-theoretic
learning methods within the context of network systems, and further to identify
fruitful future research directions on both theoretical and applied studies.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08159</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08159</id><submitter>Russell Park</submitter><version version="v1"><date>Mon, 17 May 2021 20:55:10 GMT</date><size>3094kb</size><source_type>D</source_type></version><title>A comparison of six numerical methods for integrating a compartmental
  Hodgkin-Huxley type model</title><authors>R. Park</authors><categories>math.NA cs.NA</categories><comments>86 pages, 25 figures. To be published in Applied Numerical
  Mathematics</comments><msc-class>65M02, 65P02, 65Y02</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We compare six numerical integrators' performance when simulating a regular
spiking cortical neuron model whose 74-compartments are equipped with eleven
membrane ion channels and Calcium dynamics. Four methods are explicit and two
are implicit; three are finite difference PDE methods, two are Runge-Kutta
methods, and one an exponential time differencing method. Three methods are
first-, two commonly considered second-, and one commonly considered
fourth-order. Derivations show, and simulation data confirms, that
Hodgkin-Huxley type cable equations render multiple order explicit RK methods
as first-order methods. Illustrations compare accuracy, stability, variations
of action potential phase and waveform statistics. Explicit methods were found
unsuited for our model given their inability to control spiking waveform
consistency up to 10 microseconds less than the step size for onset of
instability. While the backward-time central space method performed
satisfactorily as a first order method for step sizes up to 80 microseconds,
performance of the Hines-Crank-Nicolson method, our only true second order
method, was unmatched for step sizes of 1-100 microseconds.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08163</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08163</id><submitter>Eric Chen</submitter><version version="v1"><date>Mon, 17 May 2021 21:06:14 GMT</date><size>33322kb</size><source_type>D</source_type></version><title>Accelerating 3D MULTIPLEX MRI Reconstruction with Deep Learning</title><authors>Eric Z. Chen, Yongquan Ye, Xiao Chen, Jingyuan Lyu, Zhongqi Zhang,
  Yichen Hu, Terrence Chen, Jian Xu, and Shanhui Sun</authors><categories>eess.IV cs.LG</categories><comments>Presented at ISMRM 2021 as the digital poster</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-contrast MRI images provide complementary contrast information about
the characteristics of anatomical structures and are commonly used in clinical
practice. Recently, a multi-flip-angle (FA) and multi-echo GRE method
(MULTIPLEX MRI) has been developed to simultaneously acquire multiple
parametric images with just one single scan. However, it poses two challenges
for MULTIPLEX to be used in the 3D high-resolution setting: a relatively long
scan time and the huge amount of 3D multi-contrast data for reconstruction.
Currently, no DL based method has been proposed for 3D MULTIPLEX data
reconstruction. We propose a deep learning framework for undersampled 3D MRI
data reconstruction and apply it to MULTIPLEX MRI. The proposed deep learning
method shows good performance in image quality and reconstruction time.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08164</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08164</id><submitter>John Thickstun</submitter><version version="v1"><date>Mon, 17 May 2021 21:07:02 GMT</date><size>2512kb</size><source_type>D</source_type></version><title>Parallel and Flexible Sampling from Autoregressive Models via Langevin
  Dynamics</title><authors>Vivek Jayaram, John Thickstun</authors><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>16 pages, 8 figures, to appear in ICML 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an alternative approach to sampling from autoregressive
models. Autoregressive models are typically sampled sequentially, according to
the transition dynamics defined by the model. Instead, we propose a sampling
procedure that initializes a sequence with white noise and follows a Markov
chain defined by Langevin dynamics on the global log-likelihood of the
sequence. This approach parallelizes the sampling process and generalizes to
conditional sampling. Using an autoregressive model as a Bayesian prior, we can
steer the output of a generative model using a conditional likelihood or
constraints. We apply these techniques to autoregressive models in the visual
and audio domains, with competitive results for audio source separation,
super-resolution, and inpainting.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08165</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08165</id><submitter>Liming Luke Chen</submitter><version version="v1"><date>Mon, 17 May 2021 21:08:03 GMT</date><size>782kb</size></version><title>Social Behavior and Mental Health: A Snapshot Survey under COVID-19
  Pandemic</title><authors>Sahraoui Dhelim, Liming Luke Chen, Huansheng Ning, Sajal K Das, Chris
  Nugent, Devin Burns, Gerard Leavey, Dirk Pesch and Eleanor Bantry-White</authors><categories>cs.SI cs.CL cs.CY cs.LG</categories><comments>Submitted to ACM Computing Surveys</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social media provides a channel for monitoring people's social
behaviors and their mental distress. Due to the restrictions imposed by
COVID-19 people are increasingly using online social networks to express their
feelings. Consequently, there is a significant amount of diverse user-generated
social media content. However, COVID-19 pandemic has changed the way we live,
study, socialize and recreate and this has affected our well-being and mental
health problems. There are growing researches that leverage online social media
analysis to detect and assess user's mental status. In this paper, we survey
the literature of social media analysis for mental disorders detection, with a
special focus on the studies conducted in the context of COVID-19 during
2020-2021. Firstly, we classify the surveyed studies in terms of feature
extraction types, varying from language usage patterns to aesthetic preferences
and online behaviors. Secondly, we explore detection methods used for mental
disorders detection including machine learning and deep learning detection
methods. Finally, we discuss the challenges of mental disorder detection using
social media data, including the privacy and ethical concerns, as well as the
technical challenges of scaling and deploying such systems at large scales, and
discuss the learnt lessons over the last few years.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08169</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08169</id><submitter>Zixu Zhang</submitter><version version="v1"><date>Mon, 17 May 2021 21:19:30 GMT</date><size>10777kb</size><source_type>D</source_type></version><title>Safe Occlusion-aware Autonomous Driving via Game-Theoretic Active
  Perception</title><authors>Zixu Zhang, Jaime F. Fisac</authors><categories>cs.RO cs.SY eess.SY</categories><comments>To be appeared in Robotics: Science and Systems (RSS), 2021</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Autonomous vehicles interacting with other traffic participants heavily rely
on the perception and prediction of other agents' behaviors to plan safe
trajectories. However, as occlusions limit the vehicle's perception ability,
reasoning about potential hazards beyond the field-of-view is one of the most
challenging issues in developing autonomous driving systems. This paper
introduces a novel analytical approach that poses the problem of safe
trajectory planning under occlusions as a hybrid zero-sum dynamic game between
the autonomous vehicle (evader), and an initially hidden traffic participant
(pursuer). Due to occlusions, the pursuer's state is initially unknown to the
evader and may later be discovered by the vehicle's sensors. The analysis
yields optimal strategies for both players as well as the set of initial
conditions from which the autonomous vehicle is guaranteed to avoid collisions.
We leverage this theoretical result to develop a novel trajectory planning
framework for autonomous driving that provides worst-case safety guarantees
while minimizing conservativeness by accounting for the vehicle's ability to
actively avoid other road users as soon as they are detected in future
observations. Our framework is agnostic to the driving environment and suitable
for various motion planners. We demonstrate our algorithm on challenging urban
and highway driving scenarios using the open-source CARLA simulator. The
experimental results can be found in https://youtu.be/Cdm1T6Iv7GI.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08170</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08170</id><submitter>Yukai Gong</submitter><version version="v1"><date>Mon, 17 May 2021 21:23:02 GMT</date><size>3780kb</size><source_type>D</source_type></version><title>Zero Dynamics, Pendulum Models, and Angular Momentum in Feedback Control
  of Bipedal Locomotion</title><authors>Yukai Gong and Jessy Grizzle</authors><categories>cs.RO cs.SY eess.SY</categories><comments>20 pages, 14 figures. arXiv admin note: text overlap with
  arXiv:2008.10763</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-dimensional models are ubiquitous in the bipedal robotics literature. On
the one hand, are the simplified pendulum models selected to capture the center
of mass dynamics. On the other hand, is the passive low-dimensional model
induced by virtual constraints. In the first case, the low-dimensional model is
valued for its physical insight and analytical tractability. In the second
case, the low-dimensional model is integral to a rigorous analysis of the
stability of walking gaits in the full-dimensional model of the robot. This
paper brings these two approaches together, clarifying their commonalities and
differences. In the process of doing so, we argue that angular momentum about
the contact point is a better indicator of robot state than linear velocity.
Concretely, we show that an approximate (pendulum and zero dynamics) model
parameterized by angular momentum is more accurate on a physical robot (e.g.,
legs with mass) than is a related approximate model parameterized in terms of
linear velocity. We implement an associated angular-momentum-based controller
on Cassie, a 3D robot, and demonstrate high agility and robustness in
experiments.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08175</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08175</id><submitter>Guang Yang A</submitter><version version="v1"><date>Mon, 17 May 2021 21:28:00 GMT</date><size>1673kb</size></version><title>Transfer Learning Enhanced Generative Adversarial Networks for
  Multi-Channel MRI Reconstruction</title><authors>Jun Lv, Guangyuan Li, Xiangrong Tong, Weibo Chen, Jiahao Huang,
  Chengyan Wang, Guang Yang</authors><categories>eess.IV cs.CV cs.LG</categories><comments>29 pages, 11 figures, accepted by CBM journal</comments><msc-class>68T01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning based generative adversarial networks (GAN) can effectively
perform image reconstruction with under-sampled MR data. In general, a large
number of training samples are required to improve the reconstruction
performance of a certain model. However, in real clinical applications, it is
difficult to obtain tens of thousands of raw patient data to train the model
since saving k-space data is not in the routine clinical flow. Therefore,
enhancing the generalizability of a network based on small samples is urgently
needed. In this study, three novel applications were explored based on parallel
imaging combined with the GAN model (PI-GAN) and transfer learning. The model
was pre-trained with public Calgary brain images and then fine-tuned for use in
(1) patients with tumors in our center; (2) different anatomies, including knee
and liver; (3) different k-space sampling masks with acceleration factors (AFs)
of 2 and 6. As for the brain tumor dataset, the transfer learning results could
remove the artifacts found in PI-GAN and yield smoother brain edges. The
transfer learning results for the knee and liver were superior to those of the
PI-GAN model trained with its own dataset using a smaller number of training
cases. However, the learning procedure converged more slowly in the knee
datasets compared to the learning in the brain tumor datasets. The
reconstruction performance was improved by transfer learning both in the models
with AFs of 2 and 6. Of these two models, the one with AF=2 showed better
results. The results also showed that transfer learning with the pre-trained
model could solve the problem of inconsistency between the training and test
datasets and facilitate generalization to unseen data.
</abstract></arXivRaw>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:2105.08176</identifier>
 <datestamp>2021-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXivRaw xmlns="http://arxiv.org/OAI/arXivRaw/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXivRaw/ http://arxiv.org/OAI/arXivRaw.xsd">
 <id>2105.08176</id><submitter>Marshall Buck</submitter><version version="v1"><date>Mon, 17 May 2021 21:36:04 GMT</date><size>11kb</size></version><title>A Simple Search Problem</title><authors>Marshall Buck (1) and Doug Wiedemann (1) ((1) Center for
  Communications Research)</authors><categories>math.OC cs.IT math.IT</categories><comments>15 pages. This article is a newly typeset version of an internal
  publication written in 1984. The second author passed away on November 12,
  2020, and his estate has approved the submission of this paper</comments><msc-class>90B40 (Primary), 94D99 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A simple problem is studied in which there are N boxes and a prize known to
be in one of the boxes. Furthermore, the probability that the prize is in any
box is given. It is desired to find the prize with minimal expected work, where
it takes one unit of work to open a box and look inside. The paper establishes
bounds on the minimal work in terms of the $p=1/2$ H\&quot;older norm of the
probability density and in terms of the entropy of the probability density. We
also introduce the notion of &quot;Cartesian product&quot; of problems, and determine the
asymptotic behavior of the minimal work for the $n$th power of a problem.
  (This article is a newly typeset version of an internal publication written
in 1984. The second author passed away on November 12, 2020, and his estate has
approved the submission of this paper.)
</abstract></arXivRaw>
</metadata>
</record>
<resumptionToken cursor="0" completeListSize="1229">5370528|1001</resumptionToken>
</ListRecords>
</OAI-PMH>
